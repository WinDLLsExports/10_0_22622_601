!This program cannot be run in DOS mode.
\RichF
.text
`.rdata
@.data
.didat
.rsrc
@.reloc
u$WPQ
E$VWj
PVWWRh
D$4,TC
D$P(kE
D$\LkE
Qhp)Q
Y_^Y]
YY_^]
PQQSVW
QSVW3
}.VPW
7@PVW
YY9~,t
~,9~$t
98uHj
K +K$
;~ ~[3
9F$tD
;~ ~[3
QQSVW
F<$"<
QQSVW
QQSVW
;0v:f
F<$"<
6SVQRQ
$_^[]
D$$j@P
D$$j@P
D$ j@P
D$ j@P
_^[Y]
4;;0}
)8_^[]
WSj?Z
WSj?Z
PPPPPWS
QQSVW
<xt><Xu=
<xt <Xt
<:t%<,t!</u%
<:t%<,t!</u%
<:u#W
<:u WSj<j
M$@Pj
89}4w
9u4wn
u$VRQ
<:u#W
<:u WSj<j
D$$j@P
D$$j@P
D$ j@P
D$ j@P
@t"VS
<:u#W
<:u WSj<j
D$$j@P
D$$j@P
D$ j@P
D$ j@P
@t"VS
G_^[]
Af98u
M$@Pj
89}4w
9u4wt
u$VRQ
M$@Pj
89}4w
9u4wt
u$VRQ
<xt"<Xu!
9t$(u
+L$,xd
;T$ t
%WSVR
VVSWj
t"VVV
F4;F8t4
PSWh8
PSWh8
PSWh8
3WVVQ
;u t5
;u te
;u tzk
G(;G,u
G,+G(j
G8_^[]
t,jHh
t,jMh|,D
u:;T$
3WVVQ
PQSVW
t jh[Sj
QQSVW
QQSVW
j Z_^
H|+Hx
jyhP D
j(Y!>
Ph`#D
Ph $D
Vh@$D
Phd%D
YY_^]
A|+Ax
A|+Ax
J|hp#Q
Jxhp#Q
Vh@*D
RQVSQ
t4j$h
N8;N<t
7;3tQ
Ph<.D
Vh`.D
PWj%S
PQQSVW
Phd0D
PQSVW
F f99_
X9F(s
[h@>D
`h\?D
dh\?D
MhX@D
1hD@D
u%hp@D
4hDAD
GP+GL
u.h CD
vYh@GD
|8h(GD
chxED
CP+CL
`hXLD
fhXLD
Yh0MD
[h`MD
9h|ND
PhLQD
VhxQD
9wLtQ
QQSVW
QQSVW
QQSVW
QQSVW
jahP3D
VhTSD
u6j Y
t@j Y
u7j8Y
tAj8Y
PhxTD
VQh0[D
WVQh0[D
\hl]D
ehl]D
[hxaD
hhxaD
u6h$bD
L$=QP
L$<_^3
truej
nullj
FD`gD
FD4gD
D$(PQ
FD(hD
FDliD
FD`hD
FD iD
FD@jD
FD0kD
FDplD
FD lD
FDXmD
FD@nD
FDxoD
FD(oD
FDhpD
FDXqD
FD8rD
 !!"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%&&&&&&&&&&&&'&&()))*
j Y+M
j Y+M
X[_^]
wLh4tD
Vh uD
~h_^]
PhpuD
uOPPV
t$pVQ
t$<WP
QQhluD
PPhluD
9_(s3Sj
YPh|vD
YPh|vD
PhTvD
t,j.h
G|+Gx
N|+Nx
tBVW+
WQPQQV
YPh|vD
PQSVW
PQQSVW
D$\VW3
SVWj$
t6j(X
t j%X
t#j.X
t$j#Y
t,j-Y
t#j+X
D$DPQ
G|+Gx
G|F+Gx
F|+Fx
A|+Ax
t6VW+
_^[Y]
D$ SVW
L$,_^[3
9zLvr
9HLvB
+N +N
J(+J$
O(9O$t'+O$
G$;G(u
SRPSQ
_D$ YY
L$$_^[3
t_=UUU
+D$ YY
QQSVW
A,+A(j
A8+A4
A,+A(
A8+A4
A8+A4
A8+A4
A8+A4P
_^[Y]
PQQSVW
PQQSVW
PQQSVW
PQQSVW
Cp_^[
PQSVW
O0QPQ
PQQSVW
PQSVW
GL+GH
+O(+W
P|+Px
~X_[^
PQQSVW
G,+G(
F,+F(
Q;CPt
KX;K\t
F,+F(
H|+Hx
H|+Hx
H|+Hx
PQQSVW
A|+AxVW
PQSVW
PQQSVW
PQQSVW
A|+Ax
PQQSVW
PQQSVW
PQQSVW
PQQSVW
PQQSVW
PQQSVW
PQQSVW
PQQSVW
PQQSVW
PQQSVW
A|+Ax
PQSVW
p|+px
PQSVW
t^=333
~B;~,r
~B;~,r
~B;~,r
QQSVW
SVWj0
wVSWP
9_,vE
PPPPPPPPPPP
t6VW+
u?j Y
tIj Y
D$ SVW
PQQSVW
PQQSVW
tchh~J
QVVPP
+0Sh\
QVVPP
Wh|vD
p|+px
CdPRj
H|+Hx
H|+Hx
H|+Hx
H|+Hx
T$@RQ
L$|_^[3
H|+Hx
H|+Hx
PQSVW
D$$H#
L$4_^
8S t 
PQQSVW
t$$jpY
T$@R3
G88WP
D$8;Wxu
DD$<PRQ
D$4PRPj
L$<_^3
PQQSVW
t$(jpY
L$<_^3
T$@R3
G88WP
D$8;Wtu
DD$<PRQ
PQQSVW
WVWVP
A88QP
PQQSVW
PQPQW
B(9J,u
B09J4u
B(9J,u
PQQSVW
QSVWj
;F t9jp_9
;F t+
t=j%XP
PQQSVW
QQSVW
A88QP
PQQSVW
PQQSVW
PRPVW
A88QP
PQQSVW
PQQSVW
B(9J,u
t%;A0t 
YPh|vD
p|+pxXf
H|+Hx
F(+F$j
F(+F$
u"RWQ
p;S u
C0PRj
C\+CX
A88QP
9pHt*
A\+AX
Ah+Ad
?w;WV
VWjp+
D$0SV
T$,A;
9D$,u_
Vd^[3
YPh|vD
Wj8Y+
@ 90u
Hl;Hp
Al+Ah
Hx+Ht
EX`;Jx
Php#Q
;u$s8
;u,s/
PQQSVW
Ap+Al
A|+Ax
EF`;xD
EF`;X4
EF`;x$}i
EF`;X
;_$}2
Php#Q
Php#Q
PQSVW
QVhh~J
VWhh~J
PSWh(
RRQPRh
VVSQVh
PQQSVW
PQQSVW
PQQSVW
PQQSVW
_^[Y]
VhHWD
uJh$VD
?wBVS
u-h\-D
r,;G(t'
r,;G0t'
S;B(t@
A0;A4uM
PQSVW
PWj3S
C|+Cx
G|+Gx
u#WSV
u#WSV
u#WSV
u#WSV
u#WSV
u#WSV
u#WSV
u WSV
V;8u"
@|+Cx
[hD$E
A|+Ax
[hD$E
[hD$E
[hD$E
XhD$E
XhD)E
XhD)E
^hD)E
Sh\(E
Sh\(E
Vh\(E
`h$2E
]h$2E
[h$2E
[h$2E
]h$2E
F|+Fx
[h4<E
]h4<E
F0;F4tE
ZH9^,
=;F(r
9^,|,
9^(v%
PhT>E
PhT>E
j"hx>E
PhT>E
j%hx>E
PhT>E
j)hx>E
PhT>E
jJhx>E
A|+Ax
A|+Ax
H|+Hx
PhT>E
F|+Fx
j7h0@E
fhXCE
jXhXCE
qh`DE
eh`DE
 hpEE
&h FE
F|+Fx
CxhXHE
Ph<GE
jSh`GE
Ph<GE
jMh`GE
j?hxLE
Uh`LE
j8hxLE
j{hxLE
[hPNE
t[hDQE
[hDQE
t}h RE
F|+Fx
Qj+h(RE
98uOj
A|+Ax
Ph@SE
rD;rDt&
PRhp#Q
C|+Cx
C|+Cx
F|+Fx
PhHVE
jNhXWE
VhlVE
jzhXWE
Vh8VE
j?hPXE
jMh\YE
9Y u:
9X u+
9_ u%
Vh`[E
tbh\-D
9_$voh\-D
;G u7hL
SVW;B
GL;CLud
G@;C@u\
GH;CHuT
GD;CDuL
s4+s0;
VhH]E
L$$_^[3
L$,_^[3
B|+Bx
B|+Bx
jAh<_E
j!hP`E
;ALtK
F|+Fx
Ph\bE
Ph\bE
Ph\bE
tCj*h
tHj2h
GYYf;
PWj>S
PWj=S
t.j*h
tCj2h
QQSVW
Vh@fE
G@PRC
Vh|hE
dh8iE
tKhpiE
t,j9h
t/jZh\jE
t,jfh
t2jrh
t#jxh
F|+Fx
D$ SVW3
L$4_^[3
L$$_^3
Ph|lE
Ph4nE
Ph4nE
PhDpE
Vh,pE
wx;w|
PhxoE
t'jph
(htqE
PQQSV
VhHtE
s4WQS
VhHvE
Vh,xE
D$<PQ
?w6VS
Ph$|E
H|+Hx
tVh$|E
tVh$|E
Qh$|E
Ph$|E
t/jwh(zE
t j}h(zE
Ph(zE
t0j~h
9u,|^
9u(vW
9u,|c
9u(v\
H|+Hx
Ph@~E
Vhl~E
H|+Hx
t+j]h
J|+Jx
@$QPW
t.j'hx
t.j'h
t7jThD
t+juh
D$$Pj
D$0PR
D$,_^[
t/jEh
t4jYh
t/jEh\
t4jYh\
}N+L$
}L+L$
D$(9F
L$,^3
L$4^3
D$(9F
L$,^3
L$4^3
D$(9F
L$<^3
D$ 9F
L$$^3
D$$9F
L$,^3
D$ 9F
L$$^3
L$<_^3
L$<_^3
L$<_^3
L$<_^3
D$(SV
L$4_^[3
D$(S3
L$4_^[3
D$8Php
L$D_^[3
<SVWj
<SVWj
PWjTS
L$D_^3
PWjTS
L$D_^3
PWjTS
PWjTS
L$D_^3
PWjTS
L$D_^3
PWjTS
L$D_^3
PWjTS
L$D_^3
PWjTS
PWjTS
L$D_^3
PWjTS
PWjTS
L$D_^3
PWjTS
PWjTS
L$D_^3
PWjTS
L$D_^3
PWjTS
PWjTS
PWjTS
PWjTS
PWjTS
PWjTS
L$D_^3
PWjTS
PWjTS
PWjTS
L$D_^3
PWjTS
L$D_^3
PWjTS
D$XSV
L$d_^[3
D$XSV
L$d_^[3
B(90t:
D$4PQ
D$$PQ
D$$PQ
QQSVW
D$ PD
D$$pD
D$ P8
D$$03
D$ p0
D$$p.
L$$^3
CF;t$
CF;t$
QQSVW
L$D_^3
L$D_^3
L$D_^3
QRjpZjoY
jpZjoY
0;0u=
0;0td
t$PPRPjp
|$PA;H
C8PQW
8F$tA
8G$t5
D$(SVW3
L$4_^[3
;E,vG
$VRWjpZ
L$HxK
T$$;\$H|
L$,xO
t$@;\$
;\$,|
T$0;t$
L$PxK
T$4;\$P|
D$HxV
D$$;D$
D$$;D$H|
T$8;t$
PWjTS
PWjTS
L$|_^3
PWjTS
L$\_^[3
L$l_^[3
L$\_^[3
PWjTS
PWjTS
PWjTS
t,j3hD
t'j3hD
t6jHh
t6jIh
t6jJh
t6jKh
t6jQh
t6jRh
t6jTh
t3jUh
L$|_^[3
t,j"h
t'j"h
QQV;U
A ;A$u
QQSVW
9u,|-
9u(v&
9} |E
9]$|]
9] vV
9u,|.
9u(v'
9} |I
9]$|]
9] vV
9u,|-
9u(v&
9} |E
9]$|W
9] vP
9} |K
9]$|`
9] vY
u$8\$
QSVPW
t:j?h|
t7jvh|
H|+Hx
9yHwG
A;M t<
p|+px
tBVW+
L$@9L$$
D$0;F
L$(|3
D$0;F
;FL|\
;^HrU
D$$;D$@
t$LQP
L$P9L$0
D$D;F
D$D;F
;^L|\
;~HrU
D$0;D$P
|$TPW
L$P9L$4
D$<;F
L$L|I
t$ ;s
D$<;F
;FL|\
;^HrU
D$4;D$P
L$@9L$$
D$0;F
L$(|3
D$0;F
;FL|\
;^HrU
D$$;D$@
L$(9L$,
D$8;F
D$8;F
;VL|]
;FHrV
t$XQP
L$P9L$$
L$4;N
D$D|W
\$(;_
L$4;N
;FL|\
;~HrU
D$$;D$P
L$@9L$$
D$0;F
L$(|;
D$0;F
;FL|\
;^HrU
D$$;D$@
|$LPW
L$P9L$$
D$4;F
L$(|Q
D$4;F
;FL|\
;^HrU
D$$;D$P
L$@9L$$
D$0;F
L$(|3
D$0;F
;FL|\
;^HrU
D$$;D$@
|$LPW
L$H9L$$
D$4;F
L$D|C
T$(;S
D$4;F
;FL|\
;^HrU
D$$;D$H
L$`9L$H
t$$;w
;_L|Y
;OHrR
|$`PW
\$8VQ
L$h9L$H
\$L;X
|$(;{L|n
;CHrg
D$H;D$h
L$`9L$@
;_L|Y
;OHrR
L$H9L$,
L$<;N
T$ 9\$ |
D$<;F
;FL|`
;^HrY
D$,;D$H
t$PQP
L$H9L$$
L$@|N
\$(;X
D$4;F
;FL|\
;~HrU
D$$;D$H
t$hPW
L$`9L$<
L$0;F
L$h9|
D$ 9\$ |
D$L;F
;FL|`
;^HrY
D$<;D$`
L$H9L$,
L$<;N
T$ 9\$ |
D$<;F
;FL|`
;^HrY
D$,;D$H
t$XQP
L$P9L$(
|$0;{
D$<;F
;FL|\
;~HrU
D$(;D$P
t$XQP
L$P9L$(
|$0;{
D$<;F
;FL|\
;~HrU
D$(;D$P
L$P9L$ 
D$0;F
D$0;F
;VL|\
;FHrU
D$ ;D$P
L$X9L$(
L$D;N
D$,|I
L$D;N
;FL|`
;^HrY
D$(;D$X
L$P9L$(
D$8;F
D$8;F
;FL|`
;^HrY
D$(;D$P
L$H9L$,
L$<;N
T$ 9\$ |
D$<;F
;FL|`
;^HrY
D$,;D$H
t$PQP
L$H9L$$
L$@|N
\$(;X
D$4;F
;FL|\
;~HrU
D$$;D$H
t$hPW
L$`9L$<
L$0;F
L$h9|
D$ 9\$ |
D$L;F
;FL|`
;^HrY
D$<;D$`
L$H9L$,
L$<;N
T$ 9\$ |
D$<;F
;FL|`
;^HrY
D$,;D$H
t$XQP
L$P9L$(
|$0;{
D$<;F
;FL|\
;~HrU
D$(;D$P
t$XQP
L$P9L$(
|$0;{
D$<;F
;FL|\
;~HrU
D$(;D$P
L$@9L$$
D$0;F
L$(|3
D$0;F
;FL|\
;^HrU
D$$;D$@
t$PQP
L$H9L$,
L$<|N
D$0;F
;^L|\
;~HrU
D$,;D$H
|$HQP
L$P9L$,
L$$;w
D$8|b
L$0RV
D$8;Y
t$@;w
;WL|\
;GHrU
D$,;D$P
L$@9L$$
D$0;F
L$(|;
D$0;F
;FL|\
;^HrU
D$$;D$@
t$XQP
L$P9L$,
D$8;F
L$0|Q
D$8;F
;^L|\
;~HrU
D$,;D$P
L$@9L$$
D$0;F
L$(|;
D$0;F
;FL|\
;^HrU
D$$;D$@
|$<QP
L$X9L$$
t$8;w
;WL|\
;GHrU
D$$;D$X
t$xPW
L$p9L$H
|$49\$4|
D$T;F
;FL|`
;^HrY
D$H;D$p
|$<QP
L$X9L$$
D$p;w
t$8;w
;WL|\
;GHrU
D$$;D$X
|$`PW
L$h9L$D
|$ ;~
D$L;F
;FL|\
;^HrU
D$D;D$h
|$<QP
L$X9L$$
t$8;w
;WL|\
;GHrU
D$$;D$X
t$xPW
L$p9L$H
|$49\$4|
D$T;F
;FL|`
;^HrY
D$H;D$p
|$<QP
L$X9L$$
t$8;w
;WL|\
;GHrU
D$$;D$X
t$xPW
L$p9L$H
|$49\$4|
D$T;F
;FL|`
;^HrY
D$H;D$p
|$<QP
L$X9L$$
D$p;w
t$8;w
;WL|\
;GHrU
D$$;D$X
|$`PW
L$h9L$D
|$ ;~
D$L;F
;FL|\
;^HrU
D$D;D$h
|$<QP
L$X9L$$
t$8;w
;WL|\
;GHrU
D$$;D$X
t$xPW
L$p9L$H
|$49\$4|
D$T;F
;FL|`
;^HrY
D$H;D$p
L$,_^[3
L$,_^[3
L$,_^[3
\$ ;^
\$ ;^
t$$G9^
D$$;L$
\$ ;^
t$$G9^
D$ |V
\$ ;^
\$ ;^
t$$G9^
D$$;L$
\$ ;^
t$$G9^
D$$VW
L$,_^3
D$$VW
L$,_^3
SVW9A
SVW9A
CY;\$
CY;\$
CY;\$
CY;\$
CY;\$
CY;\$
CY;\$
CY;\$
CY;\$
CY;\$
CY;\$
CY;\$
D$0SVW
;|$,|
L$<_^[3
D$0SVW
;|$,|
L$<_^[3
D$0SVW
;|$,|
L$<_^[3
D$0SVW
;|$,|
L$<_^[3
QQSVW
D$,PQ
D$,PQ
D$$PQ
D$$PQ
D$ |U
PRWQf
PRWQf
u';D$
,SVWj
t2jih
@H;AHt.h
=Genuu
ineIu
nteltF=Authu
entiu
cAMDt
=AMDiu!
sbetu
ter!u
t/jlh
D$(xs
NH;OHt*h
PQQQV
PQQQR
PQQQV
PQQQR
D$$xP
;L$$|
D$<_^[
H|+Hx
H|+Hx
L$$_^[3
H|+Hx
G;~$r
9~$v}
G;~$r
9~$v\
G;~$r
9~$v9
G;~$r
G;~$r
t.j}h
D$<PQ
;^,tA
;^,r;w
;F(v4
@H;AH
qH9wH
L$$_^[3
YY90t
L$$^3
t/jEh
9t$(v^
;t$(r
D$4PQ
jp_h$
E RjpZ
t/jdh
t/jdh
t/jdh
L$$_^[3
_^[YY]
WVSQQ
D$<PQ
_^[Y]
_^[Y]
_^[Y]
_^[Y]
_^[Y]
_^[Y]
_^[Y]
_^[Y]
_^[Y]
_^[Y]
_^[Y]
_^[Y]
_^[Y]
_^[Y]
_^[Y]
_^[Y]
_^[Y]
_^[Y]
_^[Y]
_^[Y]
_^[Y]
_^[Y]
_^[Y]
_^[Y]
_^[Y]
_^[Y]
_^[Y]
_^[Y]
_^[Y]
_^[Y]
_^[Y]
_^[Y]
_^[Y]
_^[Y]
_^[Y]
_^[Y]
D$,;D$0|
;L$4r
D$,;D$0|
;L$4r
D$,;D$0|
;L$4r
\$ ;}
D$,;D$0|
;L$4r
p|+px
t.j7h@
G4+G0
G@+G<
C4+C0
D$ |<
;t$0r
L$|_^3
D$LVP
L$l_^[3
D$$SP
L$D_^[3
D$$SP
L$D_^[3
D$$SP
L$D_^[3
D$$SP
L$D_^[3
D$$SP
L$D_^[3
T$ +\$
G;|$4
T$<9s 
;L$`|
;t$dr
D$H+D$
t$0+|$8
+|$ Q
t$4+|$<P
t$DVW
t$0+|$8
+|$ Q
t$4+|$<P
t$DVW
L$0;K
t$@;t$\
L$t_^[3
T$ +\$
G;|$4
T$<9s 
;L$`|
;t$dr
D$H+D$
t$0+|$8
+|$ Q
t$4+|$<P
t$DVW
t$0+|$8
+|$ Q
t$4+|$<P
t$DVW
L$0;K
t$@;t$\
L$t_^[3
T$ +\$
G;|$4
D$09K
T$<9s 
;L$`|
;t$dr
D$P+D$
9T$@u
t$(+|$8
t$,+|$<Q
t$DVW
9T$@u
t$(+|$8
t$,+|$<Q
t$DVW
L$(;K
t$,;t$\
L$t_^[3
T$ +\$
G;|$4
D$09K
T$<9s 
;L$`|
;t$dr
D$P+D$
9T$@u
t$(+|$8
t$,+|$<Q
t$DVW
9T$@u
t$(+|$8
t$,+|$<Q
t$DVW
L$(;K
t$,;t$\
L$t_^[3
L$091~
;D$@r
T$$+\$
G;|$L
T$<9s 
;L$`|
;t$dr
D$H+D$
t$,+|$8
+|$ Q
t$0+|$<P
t$DVW
t$,+|$8
+|$ Q
t$0+|$<P
t$DVW
L$,;K
t$@;t$\
L$t_^[3
L$091}
;D$@r
T$$+\$
G;|$L
T$<9s 
;L$`|
;t$dr
D$H+D$
t$,+|$8
+|$ Q
t$0+|$<P
t$DVW
t$,+|$8
+|$ Q
t$0+|$<P
t$DVW
L$,;K
t$@;t$\
L$t_^[3
L$09y
;D$@r
T$ +\$$
L$ ;N
G;|$L
D$(9K
T$89s 
\$H;L$X
;L$X|
;t$dr
9D$<u
D$$+|$4
t$<+|$8Q
D$@PW
L$L+L$
D$P+D$
D$LYY
9D$<u
D$$+|$4
t$<+|$8Q
D$@PW
L$$;K
t$@;t$`
L$t_^[3
L$09y
;D$@r
T$ +\$$
L$ ;N
G;|$L
D$(9K
T$89s 
\$H;L$X
;L$X|
;t$dr
9D$<u
D$$+|$4
t$<+|$8Q
D$@PW
L$L+L$
D$P+D$
D$LYY
9D$<u
D$$+|$4
t$<+|$8Q
D$@PW
L$$;K
t$@;t$`
L$t_^[3
D$$YY9D$
L$$_^[3
D$$YY9D$
L$$_^[3
D$$YY9D$
L$$_^[3
D$$YY9D$
L$$_^[3
D$$YY9D$
L$$_^[3
D$$YY9D$
L$$_^[3
D$$YY9D$
L$$_^[3
D$$YY9D$
L$$_^[3
Dz-hX%F
Dz-hX%F
u6hX%F
PhT)F
PhT)F
PhT)F
PhT)F
PhT)F
PhT)F
PhT)F
PhT)F
PhT)F
PhT)F
PhL*F
Ph,,F
j'hX,F
t6jXh
xp+xl
Ph`.F
VhL.F
P|+Px3
us9M$un
p|+px
PjoZRY
H|+Hx
;{@})
jVh85F
Vh46F
Ph<9F
Q|+Qx
Ph<9F
G8+G4
Ph<9F
Ph<9F
PhT8F
PhT8F
Ph0<F
Vhx9F
NPu53
PPPPPR
PPPPPR
Vh([D
Vh`<F
;D$ |
;t$$r
jMhX?F
M0+M(
M$PRV
APQVW
APQRV
M$PRV
APQVW
APQRV
M$PRV
APQVW
APQRV
M$PRV
APQVW
APQRV
Ph8AF
Ph8AF
YYj,hd@F
VjdRP
t$d;u
D$ QQ
D$(YY
D$(QVPW
t$,RP
t$$RQW
09T$$
|$D;x
t$d;u
VjdRP
t$l;u
D$(QQ
$RVQPRVQP
t$XRP
t$PRQW
D$@9L$
D$(9L$
09T$P
|$h;x
t$l;u
!H !H$!H(!H,
jAh0DF
jCh0DF
jMh0DF
VhlEF
j?h0DF
jYh0DF
j[h0DF
jWh0DF
C\QWR
D$@;A
H|+Hx
PhXGF
t/j6hXGF
t/j9hXGF
t.jJhXGF
tEjdhXGF
PhXGF
Ph$HF
t/jGh$HF
t jKh$HF
Ph$HF
t==fff
j&hPKF
j(hPKF
j/hPKF
j1hPKF
j3hPKF
t/j9h
t j=h
t/jYh
j$hPKF
VhLRF
H|+Hx
Vh8TF
H|+Hx
Vh8TF
PhhUF
H|+Hx
PhhUF
Vh8TF
PhhUF
PhhUF
PhhUF
PhhUF
U,PQQQ
D$,9V
D$ 9W
L$0;O
L$<;N
$RVWS
T$ ;V
D$4;G
L$8;J
U,PQQQ
D$,9V
D$ 9W
L$0;O
L$<;N
$RVWS
L$0;O
D$4;B
U,PQQQ
T$$;V
$RVWS
T$0;Q
D$P|[
T$ ;U
T$,;U
D$(SVW
L$4_^[3
D$X|]
T$4;U
T$@;U
D$`|]
T$T;U
 _^[]
 _^[]
L$T_^[3
L$T_^[3
PRkND
PRkNp
L$l_^[3
D$(SV
L$4_^[3
L$t_^[3
D$(SV
L$4_^[3
D$0SV
L$<_^[3
Y_^[]
PhdYF
PhdYF
PhdYF
PhdYF
YYVQP
PhdYF
PhdYF
PhdYF
F$+F 
YYVQP
PhdYF
.h8^F
YYVQP
PhdYF
3h4ZF
A,+Q0+A(
PhdYF
PhdYF
:hH[F
PhdYF
A$+A 
A$G+A 
PhdYF
PhdYF
Mhd_F
PhdYF
PhdYF
PhdYF
PhdYF
C4+C0
C4+C0
L$@;L$(
D$ 9D$@|7
D$ 9D$
9D$@r+
9T$ u
T$ 9T$(|;
9\$,r3
L$|_^[3
9x@t{
9x`ti
Ph$`F
VhP`F
Ph$`F
VhP`F
Phl`F
Phl`F
t-hHaF
t.j-h
j8hxdF
j'hxdF
j&hxdF
PhheF
jshxdF
PhheF
juhxdF
PWj{S
PWj|S
PWj}S
PWj~S
PhheF
PhheF
jrhxdF
D$4PQ
D$4PQ
PhHiF
PhHiF
Ph4gF
Vh(gF
r3;O4
;G0s';Gxu
PhhgF
Ph@kF
j*hhkF
Ph@kF
Ph@kF
j+hhkF
Ph\lF
Ph\lF
Ph\lF
Ph\lF
Ph\lF
Ph\lF
Ph<mF
Ph<mF
Ph<mF
PhtmF
PhtmF
PhtmF
T$ u8
T$ u8
L$L_^3
j hXrF
j&hXrF
j#hXrF
j=hXrF
jLhXrF
j<hXrF
Ph|pF
j`hXrF
Ph|pF
jlhXrF
Ph|pF
jrhXrF
Ph|pF
johXrF
Ph|pF
j_hXrF
PhHqF
PhHqF
PhHqF
;D$ u
p,;L$
D$(SV
D$(SV
PQQWQ
D$(SV
PhHzF
PhHzF
PhHzF
D$(SV
Ph |F
PQQWQ
Ph |F
Ph |F
D$(SV
L$$;C
Ph`|F
Ph`|F
Ph`|F
D$(SV
L$$;G
D$(SV
L$$;G
!>j$Y
j+h }F
VhT~F
Pj2jP
Pj2jP
;C sV
C8+C4
;C sV
C8+C4
C8+C4
C8F+C4
F8+F4
F8G+F4
C8+C4
C8F+C4
;t$$|
F8+F4
F8G+F4
C8+C4
C8F+C4
D$$;D$
F;t$0
F8+F4
F8G+F4
C8+C4
C8F+C4
D$$;D$
F;t$0
F8+F4
F8G+F4
C8+C4
C8F+C4
F8+F4
F8G+F4
C8+C4
C8F+C4
F8+F4
F8G+F4
C8+C4
C8F+C4
D$$;D$
F;t$0
F8+F4
F8G+F4
C8+C4
C8F+C4
D$$;D$
F;t$0
F8+F4
F8G+F4
YY_^]
~,j4j
+GduJ
W8+W4PQ
W8+W4PQ
+GduJ
W8+W4PQ
W8+W4PQ
+GduJ
W8+W4PQ
W8+W4PQ
+GduJ
W8+W4PQ
W8+W4PQ
;C sV
C8+C4
;C sV
C8+C4
C8+C4
C8F+C4
G8+G4
G8F+G4
C8+C4
C8F+C4
G8+G4
G8F+G4
C8+C4
C8F+C4
G8+G4
G8F+G4
C8+C4
C8F+C4
G8+G4
G8F+G4
9K,|,
9K(v%
s(+CH
C@+C<
tMj=h
Vh(2Q
jpZjoY
9u,|>
9u(v7+M
jpZjoY
Od+O`
Gp+Gl
G@+G<
G|+GxR
Gp+O`+Gl
t/jsh
Fd+F`
SjpZjoY
E VVQ
SjpZjoY
G8;G<
G0+G,
SjpZjoY
8M<t1
G<+G8
GH+GD
Vh,xE
;D$ u
t$,Vj
G;|$$
D$ ;M
;\$ r
;D$ u
p,;L$
?"_^t
L$$_^3
L$$_^3
D$XSV
7GY;}
;E vG
$VWjoY
$VWQjpZ
;E vJ
t/j,h
t5jIh
B QPR
B QPR
t/j,h
t5jIh
B QPR
B QPR
;D$ u
G;|$$
P|+Px
BhPVWQQ
P|+Px
BhPVWQQ
P|+Px
BhPVWQQ
P|+Px
BhPVWQQ
F4+F0j
tT9sXvHV
Ch+C`
Ct+Cl
Cd;Cht
Q;Ctt
>PWPW
PSVRWQ
D$8|T
|$$;}
D$8|X
|$$;}
M(PSW
BH;AH
PQVQQ
M(PSW
BH;AH
M(PSW
BH;AH
M(PSW
BH;AH
B|+Bx
F;u$|
RRRRP9Wdu7
SSSSP9_du7
At;Al|
Ap;Ahst
Np;Vl
;NhsU
r3;O4
;G0s';Gxu
YY_^[
QBC;]
;]@}Y
E<9ED
]4;]@}<
yGF;u
;{<}~
K89K@
;{<}+
9s$~&Vj
$QQVV
t.jJh@
t.jJh
t.jJh
G _^[
t.jJh
G _^[
PWjTS
PWjTS
t/jQh
t.j h\
t.jNh\
t.j h(
t.jNh(
YY_^[
YY_^[
T$4VW
L$\_^3
Ph0[D
Ch4_E
u'RWSV
u$RWSV
sK;9w
sC;:w
u<+}8
]<+u8
?QPSjpZ
U<+u8
C8;C<t
;A(r3
G4;G8
u%;E$u 
E(QQPQPQ
t5jnh
J@QPR
J@QPR
t/j,hH
9L$,r
9~,|C
D$$;T$
9~,|F
9~(v?
;D$4r
9A t&
jpZjoY
PWVRjoZRY
t/j#h
3jpZjoY
D$`9s
D$H9r
D$$RP
D$0YY
t$DVS
D$p;B
D$|;C
t.j-h
{Xpu!
{\pu"
8+D$ f
9~$|=
9~ v6
9~$|{
9~ vt
8+D$ f
9~$|=
9~ v6
9~$|{
9~ vt
t5jJh(
D$PSVW
D$09p
t/j`h
QQQQQh
PQQQQQh 
G8PRQS
QQQQQh
PQQQQQh
t/jFh
K<WPQ
D$#VQ
K8WPQ
D$#VQ
K<WPQ
D$#VQ
K8WPQ
D$#VQ
K<WPQ
D$#VQ
K8WPQ
D$#VQ
K<WPQ
D$#VQ
K8WPQ
D$#VQ
B(90t:
YY_^[]
L$,RP
L$,RP
H|+Hx
YY_^[
u VWS
M$SV3
;E@}P
E<9ED
;U,|@
t$LQQ
CD;0|<
L$ ;N
t$LQQ
CD;0|<
G _[]
M0;M,|:
v PjoZ
D$`SV
D$(Af
D$PPVRj
joZRY
L$l_^[3
L$\_^3
L$\_^3
Ph0 G
Ph0 G
Ph0 G
PVRQjoY
L$\_^3
PhH G
E,SVW
L$\QP
D$4QRP
D$<;L$0
|$89},
t$4;U,|
L$(;U$
L$$;L$0
T$X;\$<sG
D$8;D$p
t$DPQ
D$`9}0
uR9|$(uL
|$,+D$
|$,+L$
|$HPQ
L$8;M0
L$|;M(
;]lu";Epu
YY_^[
YY_^[
YY_^[
0_^[]
0_^[]
u VWS
;uD}P
;MD}C
Gh8!G
j hh"G
PQSVW
GD+G@
PhH#G
PQSVW
PhD%G
Vht%G
PhD%G
PhD%G
PhD%G
Vht%G
PhD%G
Vht%G
Phh$G
Vh`%G
Phh$G
j/h8&G
Ph\(G
Phh+G
Phh+G
Phx-G
Phx-G
Phx,G
Phx,G
PhL*G
Yhp!B
YhP"B
Xh &B
D$<PQ
D$<QQ
D$<QQ
D$<QQ
D$<QQ
D$<QQ
PhP2G
PQQSVW
Gl_^[
PhD1G
PhD1G
Phx0G
9XDt[hT6G
9CDui
Vhp7G
YVWRP
Vh|5G
9pDt~h
Ph\4G
Ph\4G
PhX:G
PhX:G
98uch
99tgh
9yDugh
9Q@tgh
vI9~`
YY9t$
Phh;G
jCh ;G
jMh ;G
jUh ;G
PQQVW
v)hL<G
Vh4>G
Vh4>G
9_,~6
C;_,|
9~,~@
j h@BG
Q|+Qx
jNh@BG
PhxAG
jUh@BG
P|+Px
Ph AG
H|+Hx
Ph4CG
H|+Hx
8^hYYj`XjPY
D$0SV
xV;\$
L$<_^[3
Fl;Fpu
Fl;Vpt
Fx;N|
D$8;A
L$8;H
L$48X
D$(SVW
L$4_^[3
D$ Pj
jtYjt
Fp+Fl
F|+Fx
Yj)htEG
NLjt[
Ap+Al
A|+AxjtY
Ph GG
j%hxDG
VhlGG
SLjt_
B|+Bx
Ph GG
j%hxDG
VhlGG
C<+C8
C<+C8
PSjZW
9Y u%9_ u
u}h|IG
uZh|IG
PSWh`
PSWh@
P;O0t
Ph|RG
O ;O t
P;KPt
FHjhj
tDj_h
t6hdXG
PhhVG
PhhVG
D$@SVW
;D$ t/
L$L_^[3
D$HSVW
D$(;L$
T$DRP
F;t$4
L$T_^[3
D$HSVW
D$(;L$
;D$ t0
T$DRP
L$T_^[3
D$@SVW
D$@;L$
L$L_^[3
D$@SVW
T$8;A
T$8RP
L$L_^[3
T$<RP
L$D_^[3
D$ W;
T$<RP
L$D_^[3
D$ W;
T$<RP
L$D_^[3
D$(W;
T$DRP
L$L_^[3
D$ W;
T$DRP
L$L_^[3
t/j~hdYG
QQVW3
urh,rE
ubh`rE
u2hxrE
'hd]G
jhXPj
Ph \G
VhL\G
Ph ]G
Vh\ZG
?wpVW
C(;C(u@
j h _G
PQQSVW
jCh _G
jqh _G
PhhdG
Vh@dG
A(;A(
Php`G
VhL`G
VhPiG
Ph jG
PhLjG
PhHgG
QQRQP
PhxgG
PhxgG
QQSVW
u1hLlG
tSh mG
PhpmG
VhXmG
PhHnG
H|+Hx
Ph@oG
Ph@oG
Ph,rG
jThHrG
Vh\pG
Vh|sG
Ph8tG
Ph8tG
fhtxG
j>hXwG
Ph4wG
jEhXwG
Ph4wG
jDhXwG
Ph4wG
jChXwG
Ph4wG
jGhXwG
PhhvG
j[hXwG
VhTvG
Ph0zG
VhTzG
Ph8yG
VhTzG
Phh|G
Vh8|G
PhL}G
8_^[]
gh`~G
gh`~G
t,j)hx}G
;P0td
DO`;O`t
t.j!h$
t4j%h$
K,;K0t
t.jQhp
t>jbhp
H|+Hx
H|+Hx
H|+Hx
H|+Hx
F$W;F(t
O$;C,tB
X|+Xx
H|+Hx
H|+Hx
H|+Hx
H|+Hx
Jx;J|t8
F|+Fx
F|+Fx
C,+C(
^|+^x
t=8B t8
D$4PV
t1jPh@
t9jfh@
Php#Q
Php#Q
C`Php#Q
C|+Cx
Bp+Bl
Ap+Al
QQSVW
E PQ3
E$RRQQ
A|+Ax
CD;CHt
G|+Gx
Yj!h@
;D$$u
L$4_^[3
QQSVW
JDhp#Q
JLhp#Q
D$ SVW
|$ YY
L$$_^[3
D$$PW
+D$ YY
t+j+h
t.j?h
C,;C,
F$;F$tR
QQSVW
G<+G8
G<+G8j
GP;FPt
QQRPj
QQRPj
8F,t,j
RAQQRVQh0[D
QQRVQh0[D
QQFVQh
Qh([D
'7QVh
fff?QVh
VQh0[D
QQFVQh
Qh([D
QQRVQh
QQFVQh
QQRVQh
QQFVQh
QQRVQh
QQFVQWQh
Qh([D
Qh([D
Qh([D
Qh([D
Ah\QH
AQQQWQh
QWQh([D
Ah\QH
AQQQhX
QWQh8
RAQQRWQh0[D
AQQQWQh
Ah`QH
QWQh([D
BRRQVh|
QQQVh
BRRQVh
QVh([D
QVVQh
QVVQh
QVh([D
QVh$!H
QVh,/H
QVh0)H
0ht+H
Vh@(H
Vhd(H
QVh([D
QVh8%H
Whp:H
QVh46H
Vh(8H
0h`5H
AQQQVh@
AQQQhL
0hXGH
QVhHDH
QVh`DH
AQQQh
QVh`DH
0h ?H
AQQQVh
0h,QH
jx^Pj
r|+rx
Vh0XH
PhHVH
PhtVH
F|G+Fx
A|+Ax
Ph8TH
VhHUH
Wj(Y+
tGj.h
t/jFhL]H
t'jKhL]H
t/j|h
Php#Q
uXhXZH
uXhXZH
uXhXZH
uYhXZH
uXhXZH
uXhXZH
Ph<_H
Phx_H
;F,^u
Al+Ah
Ax+Atj
ta;GLt\j
9J$~^
WQVQRQPQ
PhtcH
VhXdH
9^$~C
7C;^$|
F<+F8j
FH+FDj
P9QTt
@`WVQ
F|+Fx
J|+Jx
DhX`H
VCSSVh
PQSPVQh
Qh0iH
Qhh:H
RhLgH
Y954<Q
Qhh:H
QhxgH
Rh\fH
Y95$<Q
RAQQRhX
RAQQRh
AQQQh
Y95,<Q
RAQQRhX
Rh4lH
RAQQRhX
VQhljH
RhtjH
Rh4lH
Rh kH
RAQQRh
Qh@jH
Qh\jH
QhLjH
BQRh,sH
Qh0[D
QhDsH
QhLsH
QhTqH
Rh qH
RAQQRh
Qh0[D
QhDsH
QhLsH
QhTqH
Rh qH
Qh0[D
QhDnH
QhTnH
Qh`nH
AQQQQVQh<uH
VQhTqH
Qh8tH
Rh0iH
Rh@tH
QQRVQh
WQhljH
Rh\sH
RhtuH
Rh@uH
F9q<u
RRQVQhh:H
QhxgH
RRQVQhh:H
QVQh4
QVQh0[D
QhxgH
Rh8vH
QWWQVQh0[D
WQhxgH
Rh qH
Rh<}H
QWQh0[D
VQhDnH
WQhTnH
WQh`nH
FVVVWQh
QVVWQh
QVVWQh<uH
QVVWQh
L$$_^[3
L$D9U
|$T9}
L$lWRPu
t$PQWR
t$PQWRP
t$P;u
|$T;}
\$D9]
T$09U
t$,SQW
t$$WSQWR
T$0;U
\$D;]
(d$ f
L$4 D$
|$  w
t$8VQP
D$ 9D$$
D$ ;D$$
t$XQP
t$lSP
D$8;D$<
t$(Vj
t$DSQP
t$XQP
t$tQP
L$L;T$
+T$ ;
t$LRP
\$T;\$X
D$\;D$`
L$`WR
D$<;D$(
D$(+D$
t$$PQ
t$XjoZRY
\$,;|$$
L$ ;|$$
D$ PW
D$xjo
8J(Wj
T$@9|$\
B|$\H
D$P9T$D
;D$dQ
GD$hWP
YY_^]
L$|_^3
)D$p;|$
)D$`;T$
(D$pf
)D$`v
v$;|$
v ;D$
(D$pf
L$LRPV
t5VPWf
L$|_^3
D$H9E
L$d9U
t$pVR
T$x;U
D$H;E
T$XY3
D$P9E
|$Du/3
L$`9M
t$tWR
L$`;M
D$P;E
L$09M
PRQVQ
T$t9D$luL
D$X+D$8
D$P)T$(
L$0;M
Vh`~H
Vh`~H
SSQSPS
jAZf;Q
jaZf;Q
V$_^[
YVWRP
ja^f;1w
jA^f;1
QQSVW
uVWjH
V<;V@t
A8;A<tB
A8;A<tT
A8;A<
F09F,t8
|pj\O
PQSVW
F4@9E0
A4@9E4
QQSVW
9_4vd
D$@SVW
QQQQSPQQ
QSVPP
QQSPQh
PQVVhB
u Vjd
QSVWQ
8_,t*j
t+jAhl
t,jJh
t,jVh0
t0jjh<
t$jtX
PShp#Q
C0Php#Q
KHQhp#Q
^xjtX
$_^[]
tRjtSV
G$9G$t
Qh0[D
Qh0[D
QVQh0[D
Qh0[D
QWQh0[D
Qh0[D
QWQh0[D
Qh0[D
VhLwF
Qh0[D
VSj(P
Qh0[D
Qh0[D
Qh0[D
Qh0[D
Qh0[D
Qh0[D
Qh0[D
Qh0[D
Qh0[D
Qh0[D
&hxiH
Qh([D
WWWQhX
Qh([D
VQh8tH
AQQQh
VQh([D
VQh([D
WVQh([D
WVQh([D
VQh([D
VQh([D
AQQQh
VQh([D
WVQh([D
VQh([D
VQh0[D
VQh0[D
AQQQhT
SQh([D
VQh([D
WQh([D
9q u#
9q u(
Qh0[D
WWQhX
Qh0[D
AQQQj
WQh0[D
WQh([D
QVQh0[D
QhDnH
VQh0[D
VQh([D
WVQh([D
AQQQh
VQh([D
AQQQhh
QVQh0[D
VQh,4H
Rh`5H
QWQhD
Qh([D
Qh([D
Qh([D
SSSPVQh
WQh([D
WWWQh
Qh0[D
QVQh0[D
WWWQh
Qh0[D
Qh0[D
Qh0[D
WWWQh
Qh0[D
WWWQh
Qh0[D
Wh,sH
Qh0[D
Qh0[D
WQh0[D
Qh0[D
Qh0[D
WWWQh
WQh([D
WWWQh
WQh([D
WWWQh
WQh([D
WWWQh
Qh0[D
WWWQh
Qh0[D
WWWQh
Qh0[D
WQh0[D
WWWQh
Qh0[D
AQQQVQhT
VQh([D
Qh([D
Qh0[D
AQQQWh
AQQQQh
WWWQh
QVQh0[D
WWWQh
WQh([D
WWWQh
WQh([D
WWWQh
WQh([D
WWWQh
WQh([D
WWWQh
WQh([D
WWWQh
WQh([D
VQh([D
WWWQh
WQh([D
WWWQh
WQh([D
WWWQh
WQh([D
WWWQh
WQh([D
WWWQh
WQh([D
VQh([D
VQh([D
WWQhX
WQhljH
RhtjH
Rh4lH
VQhh:H
Qh0[D
WWWQh
Qh0[D
8,t'V
t,h<[F
Qh<[F
VQh([D
AQQQQVQh
QVQh([D
t69H,}
Qh([D
Qh([D
WQh([D
WQh([D
Qh([D
Qh([D
QWWQhX
WQh([D
WQh([D
QWWQhX
QQWQhT
Qh([D
t2hD'I
SQh0[D
Qh0[D
Qh([D
YPQhd*I
WQh0[D
WQh0[D
SSSPWQh0[D
SSSSWQhh
WhXxH
Qhh:H
VQhxgH
QVQh4
Rhp1I
Qhh:H
RhX0I
VhP?I
Vhh>I
SSSPWQh0[D
SSSSWQhh
Wh :I
WhXxH
WWWPSQh0[D
PQhd*I
GWWWS
SQh0[D
fff?Qj
Qh0[D
AQQQV
VQh`3I
VQhT3I
QRhp3I
WQh([D
WQh([D
_WhHDI
AQQQhX
Qh([D
QRh,QH
t'htCI
VQh([D
VQh([D
@?QWh
WQh0[D
Qh0[D
WVQh0[D
9^ ua
QVQh0[D
QWWQhX
QQWQhT
Qh([D
VQh8tH
QVQh0
QVQh([D
VQh([D
QVQh([D
VQh([D
QVQh([D
QVQh([D
QVQh([D
QVQh([D
WVQh([D
QVQh([D
VQh0[D
VQh([D
QVQh([D
Qh0[D
QVQh0[D
QVQh([D
WVQh([D
Qh([D
QWWQhX
QQWQhT
Qh([D
WQh8tH
QWQh0
VQh8tH
QVQh0
AQQQWQh
QVQh([D
Qh([D
Rhx`I
QVQh0[D
Rh8_I
VQh0[D
VQh0[D
VQh0[D
QVQh([D
QVQh([D
QVQh([D
QVQh([D
QVQh([D
QWQh([D
9q u#
9q u(
QVQh([D
^VhXdI
QVQh([D
QVQh([D
SQh([D
AQQQj
WQh0[D
WQh([D
WGWWj
WWWhX
WWWhT
QQWQh
Qh([D
QWWhT
QRh,QH
QVQh([D
WQh0[D
VQh0[D
t2hD'I
PWQh0[D
SQh0[D
QWWQhX
Qh0[D
Qh([D
QVQh0[D
YPQhd*I
PWQh0[D
SSPWQh0[D
SSSWQhh
PQSPWQh
WhXxH
SSPWQh0[D
SSSWQhh
PQSPWQh
Wh(iI
WhXxH
WQh0[D
'7QVh
fff?QVh0hI
VQh0[D
fff?Qj
Qh0[D
AQQQVh
fff?Qj
VQh0[D
AQQQj
VQh`3I
VQhT3I
QWQh([D
QWQh([D
QWQh([D
Qh([D
Qh([D
'7QVh
fff?QVh0hI
VQh0[D
QWQh([D
QVQh([D
'7QVh
fff?QVh
VQh0[D
ShpnH
Qh0[D
SQhDnH
GWWWh
QhTnH
WQWWh
WQWWh
Qh<uH
AQQQj
AQQQQVQhh
AQQQSWQh\
CSSSSWQhh
AQQQWSQh\
AQQQQSQhh
SQh`nH
AQQQQSQh$
VQhDnH
AQQQhh
AQQQhh
AQQQhh
WWQhX
Qhh:H
Qhh:H
WQhxgH
QWQh4
WWQhX
Qhh:H
QQWQhT
YY98uS
SSSPVQh
SSPVQh
PQSPVQh
PQWPSQh([D
Qh0[D
QVQh0[D
Qh0[D
Qh0[D
Qh0[D
Qh0[D
Qh0[D
WWWQh
Qh0[D
Qh([D
Qh([D
Qh([D
WWWQh
Qh0[D
Qh0[D
QQWQh
Qh0[D
PSQVP
AQQQVQhT
QVQh([D
QVQh([D
QVQh([D
QVQh([D
GWWWj
Qhh:H
Rhd>H
Qh<[F
Qh([D
PQWPSQh([D
PQWPSQh
PQSPVQh
Qh0[D
Qh0[D
Qh0[D
Qh0[D
Qh0[D
Qh0[D
Qh0[D
Qh0[D
Qh0[D
WQh0[D
WQh0[D
WQh0[D
Qh([D
Qh([D
Qh([D
Qh0[D
Qh0[D
Qh0[D
Qh0[D
WQh0[D
QWQh([D
Qh0[D
Qh0[D
QWQh([D
WQh([D
PSQVP
SSPWQh
Qh0[D
Qh0[D
QVQh([D
WQh([D
QVQh([D
QVQh([D
GYY;~
PSSPVQh
PQSPh
PSSPVQh
PQSPVQh
YY98u
PSSPVQh
PQSPh
PSSPVQh
PQSPh
QVQh([D
VQh8tH
AQQQht
AQQQht
SSPWQh
PQSPhX
PQSPhX
SSPWQh
PQSPhX
QVQh0[D
VQh,4H
Rh`5H
QWQhD
WWQhX
Qhh:H
Qhh:H
WQhxgH
QWQh4
ShpnH
WSSWh
Qh0[D
WQhDnH
QhTnH
Qh<uH
SCSSSWQhh
CShpnH
WSSWh
Qh0[D
WQhDnH
QhTnH
Qh<uH
WVQh\
SSSWQhh
SSSWQhh
SSSWQh`nH
SSSWQh$
QSSWQh
CShpnH
WSSWh
Qh0[D
WQhDnH
QhTnH
Qh<uH
WWWVQhh
SSSVQhh
VVVSQhh
VVVSQh`nH
VVVSQh$
PVhp#Q
VQRQPQ
VQRQPQ
VQRQPQ
VQRQPQ
9K,~2
9K<~2
9KL~2
9Kl~2
9K\~2
8O$u,
u,9{$u'
QRQPQQ
?"_^t
Php#Q
Php#Q
Php#Q
Php#Q
LPhp#Q
Php#Q
Php#Q
QSQQVQQP
QQVQQP
QQVQQP
QQVQQP
QQVQQP
QQVQQP
QQVQQP
QQVQQP
QQVQQP
QQVQQP
SQQVQQP
QQVQQP
;Fp^u
AT+AP
A`+A\j
H C;]
j$Z_[
t>=333
t.h`IP
Cxp#Q
Rhp#Q
C|p#Q
Rhp#Q
Rhp#Q
Rhp#Q
L$HWQ
|$L;8
;8sLf
L$\_^[3
t*;7r
t*;7r
t%;7r
 t";7r
~8;7r
~6;7r
t4;7r
CDWVj
t2;7r
@t";7r
t2;7r
t$;7r
y";7r
t2;7r
PRhp#Q
PRhp#Q
PRhp#Q
PRhp#Q
t";7r
PRhp#Q
PRhp#Q
G@p#Q
Php#Q
GDp#Q
Php#Q
GHp#Q
Php#Q
GLp#Q
Rhp#Q
D$ PQ
t_;7s4
tH;>r
PRhp#Q
PRhp#Q
PRhp#Q
PRhp#Q
S4WVj
t2;7r
t2;7r
CPp#Q
Rhp#Q
CTp#Q
Rhp#Q
CXp#Q
Rhp#Q
C\p#Q
Rhp#Q
D$8PQ
D$<PS
 t$;;r
@t$;;r
V`SWj
tH;;r
t4;;r
F,SWj
tH;;r
tH;;r
PRhp#Q
PRhp#Q
PRhp#Q
PRhp#Q
Php#Q
Php#Q
PRhp#Q
PRhp#Q
A p#Q
t2;3r
PRhp#Q
Php#Q
Rhp#Q
t_;7s4
tF;;r
t2;;r
t4;;r
F<SWj
t4;;r
FLSWj
t4;;r
F\SWj
tF;;r
t2;;r
PRhp#Q
PRhp#Q
t!;3r
9F(t&
9FXt&
Rhp#Q
Rhp#Q
Rhp#Q
D$ PW
L$lSP
~4;;r
t$;;r
t6;;r
~0;;r
~_;;r
~a;;r
~0;;r
~a;;r
t5;;r
 t!;;r
PRhp#Q
PRhp#Q
PRhp#Q
t";7r
S WVj
~4;7r
Php#Q
Php#Q
[9_ t
PRhp#Q
O _^[]
t`;>s7
tF;;r
Php#Q
D$ PW
D$$PW
D$,PW
D$0PW
D$4PW
PRhp#Q
t!;3r
PRhp#Q
G`p#Q
Php#Q
Gdp#Q
Php#Q
Ghp#Q
Php#Q
tH;>r
PRhp#Q
PRhp#Q
PRhp#Q
L$$_^3
~4;=8+Q
;=4+Q
;=0+Q
9N8tE8N
vAj0h|
UjAX;
D$$+D$ @
QSVWj
_^[Y]
CL;CL_^
~h_^Y]
u)8G,t~
SS8_,
8O,tU3
QSVWj
uPf9_
!D$ SV3
;T$,u@3
D$@Y;
QQSVW
8{un@
:,u0B
QQSVW3
8\u"j
>Anu 
8-uJ@
8^u(@
<_u2A;
<Bu#j
<Pujj
L$<_^[3
O8_^[
t$,PV
N8;N0
N8;N0
x';N(}"
[tX9]
HC;^$|
<zw:jaY;
_^[Y]
QSVWj
uLjih
Vh ]E
.WhxlJ
PWj&S
PWj(S
PWj%S
t hH7B
u"hT"Q
Y__^[
5ineI
5Genu
QQSVWd
jjjjj
@Vj(j
uf_^[
@_^[]
9p u"
ft&9q
@_^[]
URPQQh
Rhy=<
SVWUj
;t$,v-
UQPXY]Y[
Y__^[
w_^[]
DtEVW
;|84u
PQSVW
QRPh(
QRPhH
>@s5f
?@s-f
api-ms-win-core-synch-l1-2-0.dll
kernel32.dll
SleepConditionVariableCS
WakeAllConditionVariable
bad exception
d3d12.dll
dxgi.dll
KERNEL32.DLL
AcquireSRWLockExclusive
ReleaseSRWLockExclusive
DirectML.dll
OLEAUT32.dll
api-ms-win-core-com-l1-1-0.dll
Unknown exception
bad array new length
string too long
D:\a\_work\1\s\engine\lotus\cmake\external\protobuf\src\google/protobuf/parse_context.h
(cannot determine missing fields for lite message)
D:\a\_work\1\s\engine\lotus\cmake\external\protobuf\src\google\protobuf\message_lite.cc
Can't 
 message of type "
" because it is missing required fields: 
parse
 exceeded maximum protobuf size of 2GB: 
vector too long
D:\a\_work\1\s\engine\lotus\cmake\external\protobuf\src\google\protobuf\io\zero_copy_stream.cc
This ZeroCopyOutputStream doesn't support aliasing. Reaching here usually means a ZeroCopyOutputStream implementation bug.
iostream
bad cast
bad locale name
ios_base::badbit set
ios_base::failbit set
ios_base::eofbit set
D:\a\_work\1\s\engine\lotus\cmake\external\protobuf\src\google\protobuf\io\zero_copy_stream_impl.cc
close() failed: 
CHECK failed: !is_closed_: 
iostream stream error
D:\a\_work\1\s\engine\lotus\cmake\external\protobuf\src\google\protobuf\io\zero_copy_stream_impl_lite.cc
CHECK failed: (count) >= (0): 
CHECK failed: backup_bytes_ == 0 && buffer_.get() != NULL: 
 BackUp() can only be called after Next().
CHECK failed: (count) <= (buffer_used_): 
 Can't back up over more bytes than were returned by the last call to Next().
 Parameter to BackUp() can't be negative.
CHECK failed: (backup_bytes_) == (0): 
CHECK failed: (buffer_used_) == (buffer_size_): 
WARNING
ERROR
FATAL
[libprotobuf %s %s:%d] %s
Can't happen
D:\a\_work\1\s\engine\lotus\cmake\external\protobuf\src\google\protobuf\arena.cc
CHECK failed: (min_bytes) <= (std::numeric_limits<size_t>::max() - SerialArena::kBlockHeaderSize): 
false
\x%02x
invalid string position
INVALID_ARGUMENT
D:\a\_work\1\s\engine\lotus\cmake\external\protobuf\src\google/protobuf/repeated_field.h
D:\a\_work\1\s\engine\lotus\cmake\external\protobuf\src\google\protobuf\repeated_field.cc
CHECK failed: (static_cast<int64>(new_size)) <= (static_cast<int64>((std::numeric_limits<size_t>::max() - kRepHeaderSize) / sizeof(old_rep->elements[0]))): 
Requested size is too large to fit into size_t.
CHECK failed: (index) >= (0): 
CHECK failed: (index) < (current_size_): 
map/set too long
unordered_map/set too long
invalid hash bucket count
generic
device or resource busy
invalid argument
no such process
not enough memory
operation not permitted
resource deadlock would occur
resource unavailable try again
bad allocation
bad function call
address family not supported
address in use
address not available
already connected
argument list too long
argument out of domain
bad address
bad file descriptor
bad message
broken pipe
connection aborted
connection already in progress
connection refused
connection reset
cross device link
destination address required
directory not empty
executable format error
file exists
file too large
filename too long
function not supported
host unreachable
identifier removed
illegal byte sequence
inappropriate io control operation
interrupted
invalid seek
io error
is a directory
message size
network down
network reset
network unreachable
no buffer space
no child process
no link
no lock available
no message available
no message
no protocol option
no space on device
no stream resources
no such device or address
no such device
no such file or directory
not a directory
not a socket
not a stream
not connected
not supported
operation canceled
operation in progress
operation not supported
operation would block
owner dead
permission denied
protocol error
protocol not supported
read only file system
result out of range
state not recoverable
stream timeout
text file busy
timed out
too many files open in system
too many files open
too many links
too many symbolic link levels
value too large
wrong protocol type
unknown error
0123456789ABCDEFabcdef-+Xx
0123456789ABCDEFabcdef-+XxPp
0123456789-+Ee
0123456789abcdefghijklmnopqrstuvwxyz
:Sun:Sunday:Mon:Monday:Tue:Tuesday:Wed:Wednesday:Thu:Thursday:Fri:Friday:Sat:Saturday
:Jan:January:Feb:February:Mar:March:Apr:April:May:May:Jun:June:Jul:July:Aug:August:Sep:September:Oct:October:Nov:November:Dec:December
:Sun:Sunday:Mon:Monday:Tue:Tuesday:Wed:Wednesday:Thu:Thursday:Fri:Friday:Sat:Saturday
:Jan:January:Feb:February:Mar:March:Apr:April:May:May:Jun:June:Jul:July:Aug:August:Sep:September:Oct:October:Nov:November:Dec:December
%b %d %H : %M : %S %Y
%m / %d / %y
:AM:am:PM:pm
%I : %M : %S %p
%H : %M
%H : %M : %S
%d / %m / %y
%.0Lf
+v$x+v$xv$+xv+$xv$+x+$vx+$vx$v+x+$vx$+vx+v $+v $v $+v +$v $++$ v+$ v$ v++$ v$+ v+xv$+ v$v$ +v+ $v$ ++x$v+ $v$v ++ $v$ +v
:AM:am:PM:pm
0123456789-
0123456789abcdefghijklmnopqrstuvwxyz
 was false.
Stacktrace:
ai.onnx.ml
ai.onnx.training
ai.onnx.preview.training
VIWEF
D:\a\_work\1\s\engine\lotus\include\onnxruntime\core/framework/data_types_internal.h
D:\a\_work\1\s\engine\lotus\include\onnxruntime\core/framework/tensor.h
CPUExecutionProvider
Tensor sequence must contain only primitive types
elem_type_ != nullptr
onnxruntime::TensorSeq::SetType
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/TensorSeq.h
i < tensors_.size()
onnxruntime::TensorSeq::Get
Trying to get a Tensor, but got: 
IsTensor()
OrtValue::Get
D:\a\_work\1\s\engine\lotus\include\onnxruntime\core/framework/ort_value.h
OrtValue::GetMutable
Trying to get a TensorSeq, but got: 
IsTensorSequence()
Trying to get a SparseTensor, but got: 
IsSparseTensor()
ORT_LOAD_CONFIG_FROM_MODEL
Integer overflow
SafeIntExceptionHandler<class onnxruntime::OnnxRuntimeException>::SafeIntOnOverflow
D:\a\_work\1\s\engine\lotus\onnxruntime\core/common/safeint.h
tried creating tensor with negative value in shape
size overflow
, got 
not enough space: expected 
Not able to find appropriate IDataTransfer to copy sparse data
`anonymous-namespace'::GetDataTransfer
D:\a\_work\1\s\engine\lotus\onnxruntime\core\session\onnxruntime_c_api.cc
Strings can only reside in CPU memory
`anonymous-namespace'::ValidateFillInputArgs
tried Filling sparse tensor with negative value in values shape
OrtApis::FillSparseTensorCoo
OrtApis::FillSparseTensorCsr
tried Filling sparse tensor with negative value in block sparse indices shape
OrtApis::FillSparseTensorBlockSparse
Can not use strings in pre-allocated memory. Use CreateSparseTensorAsOrtValue() to allocate memory inside and copy
OrtApis::UseCooIndices
OrtApis::UseCsrIndices
OrtApis::UseBlockSparseIndices
the ort_value must contain a constructed tensor
Use GetStringTensor*() API to retrieve strings
RegisterCustomOpsLibrary: Failed to load library
RegisterCustomOps
RegisterCustomOpsLibrary: Entry point RegisterCustomOps not found in library
EnableOrtCustomOps: Custom operators in onnxruntime-extensions are not enabled
onnxruntime_profile_
input name cannot be empty
output name cannot be empty
lengths allocation failed
string buffer allocation failed
Output buffer allocation failed
input array doesn't equal tensor size
element index is out of bounds
OrtValue should contain a Tensor or a Sparse Tensor
Sparse Tensor does not contain sparse data
This API supports Tensors or SparseTensors
shape is invalid
index is out of bounds
offsets buffer is not equal to tensor size
output buffer is too small. Use GetStringTensorDataLength.
buffer size is too small for string element
out of index
internal error
index out of range
Input is not of one of the supported sequence types.
Input is not of type sequence or map.
input array is too short
Input is not of one of the supported map types.
Expecting all elements to be tensors. Got: 
in[idx]->IsTensor()
OrtCreateValueImplSeqHelper
Sequences must have tensors of the same data type. There was at least one tensor in the input that was different.
Each element of the sequence should be either tensor or map.
At least one element in the sequence is of a type different from others.
Unsupported input type
For map type num_values MUST be 2
Either the key tensor or the value tensor has NumDimensions > 1
Key and value tensors have unequal number of elements.
Key type is not supported yet.
Number of values should be at least 1.
opaque(
Specified domain and type names combination does not refer to a registered opaque type
ml_type != nullptr
OrtApis::CreateOpaqueValue
Opaque type is not a non_tensor type!!!
non_tensor_base != nullptr
OrtApis::GetOpaqueValue
Specified provider is not supported.
this API does not support strings
location dimensions do not match shape size
invalid location range
max_mem
arena_extend_strategy
initial_chunk_size_bytes
max_dead_bytes_per_chunk
initial_growth_chunk_size_bytes
Invalid key found: 
The given version [%u] is not supported, only version 1 to %u is supported in this build.
1.10.0
Tensor type mismatch. 
utils::IsPrimitiveDataType<T>(dtype_)
onnxruntime::Tensor::MutableData
onnxruntime::Tensor::DataAsSpan
static_cast<uint64_t>(num_kv_pairs) < std::numeric_limits<size_t>::max()
OrtGetValueImplMapHelper
Invalid index requested for map type.
Tensor must always contain primitive types. Found: 
value_type != nullptr
OrtCreateValueImplMapHelper
Value type is not supported yet: 
Map is missing type entry for its value
++index < c.size()
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<__int64,double,struct std::less<__int64>,class std::allocator<struct std::pair<__int64 const ,double> > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<__int64,float,struct std::less<__int64>,class std::allocator<struct std::pair<__int64 const ,float> > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<__int64,__int64,struct std::less<__int64>,class std::allocator<struct std::pair<__int64 const ,__int64> > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<__int64,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,struct std::less<__int64>,class std::allocator<struct std::pair<__int64 const ,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,double,struct std::less<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,double> > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,float,struct std::less<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,float> > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,__int64,struct std::less<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,__int64> > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,struct std::less<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > > > >::check
Sequence is missing type entry for its element
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::vector<class std::map<__int64,float,struct std::less<__int64>,class std::allocator<struct std::pair<__int64 const ,float> > >,class std::allocator<class std::map<__int64,float,struct std::less<__int64>,class std::allocator<struct std::pair<__int64 const ,float> > > > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::vector<class std::map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,float,struct std::less<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,float> > >,class std::allocator<class std::map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,float,struct std::less<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,float> > > > > >::check
invalid vector subscript
onnxruntime::DataTypeImpl::GetType<T>() == type_
len >= 0 && static_cast<uint64_t>(len) < std::numeric_limits<size_t>::max()
OrtCreateMapMLValue
onnxruntime::Tensor::Data
not implemented
execution_mode is not valid
graph_optimization_level is not valid
 is not implemented
ai.onnx
onnxruntime::OpKernel::ComputeAsync
Result buffer is not large enough
Unsupported version '
' in custom op '
D:\a\_work\1\s\engine\lotus\onnxruntime\core\session\custom_ops.cc
onnxruntime::CustomOpKernel::CustomOpKernel
custom op registered at runtime
all types
Input
type_id_counter == 1
There must be one (and only one) dynamic typed input to the custom op. Its type info at runtime will be used to infer the type info of this dynamic typed output which is required for the success of the model loading step. More than one dynamic typed inputs are currently not supported as differing types at runtime means the output type cannot be inferred without which model loading cannot proceed.
onnxruntime::CreateCustomRegistry
Output
Attempt to use DefaultLogger but none has been registered.
D:\a\_work\1\s\engine\lotus\include\onnxruntime\core/common/logging/logging.h
onnxruntime::logging::LoggingManager::DefaultLogger
type == dtype_
Tensor type mismatch.
onnxruntime::Tensor::DataRaw
onnxruntime::Tensor::MutableDataRaw
Tensor size (
) != new size (
onnxruntime::Tensor::Reshape
shape_.Size() == new_shape.Size()
TensorSeq: tensor to be added has a different data type.
onnxruntime::TensorSeq::Add
IsSameDataType(tensor)
 is not present.
output_ptr
Required output at index 
D:\a\_work\1\s\engine\lotus\include\onnxruntime\core/framework/op_kernel_context.h
onnxruntime::OpKernelContext::RequiredOutput
p_ml_value
Please fetch output tensor with specified shape.
onnxruntime::OpKernelContext::Output
onnxruntime::ProviderSharedLibrary::Ensure
onnxruntime_providers_shared.dll
Provider_SetHost
D:\a\_work\1\s\engine\lotus\onnxruntime\core\session\provider_bridge_ort.cc
onnxruntime::ProviderLibrary::Get
onnxruntime::ProviderSharedLibrary::Unload
onnxruntime::ProviderLibrary::Unload
GetProvider
onnxruntime_providers_rocm.dll
onnxruntime_providers_cuda.dll
onnxruntime_providers_openvino.dll
onnxruntime_providers_dnnl.dll
onnxruntime_providers_tensorrt.dll
SessionOptionsAppendExecutionProvider_Tensorrt: Failed to load shared library
CUDA and/or ROCM execution provider is either not enabled or not available.
SessionOptionsAppendExecutionProvider_OpenVINO: Failed to load shared library
OrtSessionOptionsAppendExecutionProvider_Rocm: Failed to load shared library
OrtSessionOptionsAppendExecutionProvider_Cuda: Failed to load shared library
TensorRT execution provider is not enabled in this build.
invalid unordered_map<K, T> key
Missing Input: 
Required input at index 
onnxruntime::OpKernelContext::Input
onnxruntime::OpKernelContext::RequiredInput
input_ptr
Env is null
No requested allocator available
Provided allocator is null
OrtMemoryInfo is null
Provided OrtMemoryInfo is null
Please register the allocator as OrtDeviceAllocator even if the provided allocator has arena logic built-in. OrtArenaAllocator is reserved for internal arena logic based allocators only.
env_ptr == p_instance_
D:\a\_work\1\s\engine\lotus\onnxruntime\core\session\ort_env.cc
OrtEnv::Release
onnxruntime::IOBinding::BindInput
onnxruntime::SyncProviders
D:\a\_work\1\s\engine\lotus\onnxruntime\core\session\IOBinding.cc
onnxruntime::IOBinding::SynchronizeOutputs
onnxruntime::IOBinding::SynchronizeInputs
p_provider
 DeviceId:
DeviceType:
 MemoryType:
 OrtAllocatorType:
Device:[
 OrtMemType:
OrtMemoryInfo:[
name:
CUDAExecutionProvider
TensorrtExecutionProvider
DmlExecutionProvider
D:\a\_work\1\s\engine\lotus\include\onnxruntime\core/graph/graph.h
Validating no unexpected access using an invalid node_index. Got:
 Max:
onnxruntime::Graph::NodeAtIndexImpl
node_index < nodes_.size()
 has already been registered.
onnxruntime::ExecutionProviders::Add
Provider 
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/execution_providers.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/feeds_fetches_manager.h
onnxruntime::FeedsFetchesInfo::FeedsFetchesInfo
Create_State_
Compute_
Release_State_
IsOptionalTensor(type)
Provided type is not an optional tensor
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/mldata_type_utils.h
onnxruntime::utils::GetElementTypeFromOptionalTensor
IsOptionalSeqTensor(type)
Provided type is not an optional sequence tensor
onnxruntime::utils::GetElementTypeFromOptionalSeqTensor
MemcpyTransformer
[json.exception.
other_error
session.use_env_allocators
session.save_model_format
session.load_model_format
session.set_denormal_as_zero
session.intra_op.allow_spinning
session.inter_op.allow_spinning
optimization.save_runtime_optimizations
session.use_ort_model_bytes_directly
memory.enable_memory_arena_shrinkage
%Y-%m-%d_%H-%M-%S
) node with name '
Could not find an implementation for 
onnxruntime::`anonymous-namespace'::VerifyEachNodeIsAssignedToAnEp
Node placements
D:\a\_work\1\s\engine\lotus\onnxruntime\core\session\inference_session.cc
All nodes have been placed on [
The environment variable contained the value: 
 Provider: [
The only supported values for the environment variable 
 are '0' and '1'. 
onnxruntime::FinalizeSessionOptions
Reading the provided model for the ORT config
is_model_proto_parsed
ModelProto needs to be parsed to check for ORT config within it
onnxruntime::InferenceSession::ConstructorCommon::<lambda_3216ea50ba086ad9b600d540bb145c88>::operator ()
Flush-to-zero and denormal-as-zero are 
status.IsOK()
Could not finalize session options while constructing the inference session. Error Message: 
onnxruntime::InferenceSession::ConstructorCommon
-intra-op
Creating and using per session threadpools since use_per_session_threads_ is true
custom join thread function not set for intra op thread pool
session-
-inter-op
to.custom_join_thread_fn
Failed to create the inter-op thread pool for the parallel executor, setting ExecutionMode to SEQUENTIAL
custom join thread function not set for inter op thread pool
When the session is not configured to use per session threadpools, the env must be created with the the CreateEnvWithGlobalThreadPools API.
Using global/env threadpools since use_per_session_threads_ is false
CastFloat16Transformer
session_env.EnvCreatedWithGlobalThreadPools()
onnxruntime::InferenceSession::InferenceSession
Given model could not be parsed while creating inference session. Error message: 
Could not parse model successfully while constructing the inference session
Error during EndProfiling(): 
result
Unknown error during EndProfiling()
onnxruntime::InferenceSession::{dtor}::<lambda_80a20a44de674d401d97ead9ef557a73>::operator ()
Received nullptr for exec provider
onnxruntime::InferenceSession::~InferenceSession
onnxruntime::InferenceSession::RegisterExecutionProvider
Execution providers must be registered before the session is initialized. 
So disabling it for this session since it uses the DML Execution Provider.
Execution providers must be registered before the session is initialized.
So making the execution mode sequential for this session since it uses the DML Execution Provider.
Having memory pattern enabled is not supported while using the DML Execution Provider. 
So making the execution mode sequential for this session since it uses the CUDA Execution Provider.
Parallel execution mode does not support the DML Execution Provider. 
onnxruntime::InferenceSession::AddCustomOpDomains
Parallel execution mode does not support the CUDA Execution Provider. 
Received nullptr for graph transformer
Received nullptr for custom registry
onnxruntime::InferenceSession::RegisterGraphTransformer
Graph transformers must be registered before the session is initialized.
onnxruntime::InferenceSession::SaveToOrtFormat
This session already contains a loaded model.
Exception during loading: 
Unknown exception in Load()
onnxruntime::InferenceSession::Load
Encountered unknown exception in Load()
Failed to load model because protobuf parsing failed.
ModelProto corresponding to the model to be loaded has already been parsed. Invoke Load().
model_loading_proto
model_loading_array
ModelProto corresponding to the model to be loaded has not been parsed yet. This API should be called in conjunction with a ctor that takes a model abstraction.
onnxruntime::InferenceSession::TransformGraph
model_loading_from_saved_proto
onnxruntime::InferenceSession::LoadOrtModel::<lambda_31da21da588bb3698a015fc212bb17cf>::operator ()
This session has already been initialized.
onnxruntime::InferenceSession::LoadOrtModel
InferenceSession is null. Invalid ORT format model.
ORT model verification failed.
] is not supported this build 
Serialized version info is null. Invalid ORT format model.
Missing Model. Invalid ORT format model.
The ORT format model version [
The provided PrePackedWeightsContainer instance to be added to the session is null
SessionState is null. Invalid ORT format model.
onnxruntime::`anonymous-namespace'::PartitionOrtFormatModel
The session already has a PrePackedWeightsContainer instance
onnxruntime::`anonymous-namespace'::AssignNodesToEpsFromHashesImpl
Failed to find kernel def hash (
) in kernel registries for 
Exception during initialization: 
onnxruntime::`anonymous-namespace'::AssignNodesToEpsFromHashes
onnxruntime::InferenceSession::Initialize::<lambda_4bbd332ce2e58a1b6a340868690db847>::operator ()
onnxruntime::InferenceSession::Initialize::<lambda_ab5819bf676de610a6a0f676072ed7ea>::operator ()
onnxruntime::InferenceSession::Initialize
Initializing session.
Model was not loaded.
Model was not loaded
Adding default CPU execution provider.
Session has already been initialized.
loading_ort_format && serialized_session_state != nullptr
This session will use the allocator registered with the environment.
NchwcTransformer
Unable to serialize model as it contains compiled nodes. Please disable any execution providers which generate compiled nodes.
Session successfully initialized.
Serializing optimized model with Graph Optimization level greater than ORT_ENABLE_EXTENDED and the NchwcTransformer enabled. The generated model may contain hardware specific optimizations, and should only be used in the same environment the model was optimized in.
session_initialization
Encountered unknown exception in Initialize()
 Expected: 
 Please fix either the inputs or the model.
Invalid rank for input: 
 Got: 
Got invalid dimensions for input: 
 for the following indices
Unexpected input data type. Actual: (
 index: 
)) , expected: (
elements, but feeds has 
 elements.
Invalid Feed Input Name:
Size mismatch: feed_names has 
Input with name: 
 is not expected to be of type tensor.
onnxruntime::InferenceSession::ValidateInputs
tensor
sparse_tensor
 is not expected to be of type sparse tensor.
 is not expected to be of type tensor sequence.
At least one output should be requested.
Output vector pointer is NULL
Output vector incorrectly sized: output_names.size(): 
p_fetches->size(): 
Session was not initialized
Invalid Output Name:
Session not initialized.
onnxruntime::InferenceSession::Run
Encountered unknown exception in Run()
Running with tag: 
onnxruntime::InferenceSession::GetModelMetadata
model_run
onnxruntime::InferenceSession::GetOverridableInitializers
onnxruntime::InferenceSession::GetModelInputs
onnxruntime::InferenceSession::NewIOBinding
onnxruntime::InferenceSession::GetModelOutputs
onnxruntime::InferenceSession::EndProfiling
Profiler is disabled.
Could not write a profile because no model was loaded.
Unsupported device specified in the memory arena shrink list: 
 combination in the memory arena shrink list: 
Unsupported device id in the memory arena shrink list: 
 combination is not an arena based allocator: 
Did not find an arena based allocator registered for device-id 
 error message: 
The registered allocator for device-id 
onnxruntime::InferenceSession::ShrinkMemoryArenas
Unable to shrink arena: 
run_options.run_log_severity_level >= 0 && run_options.run_log_severity_level <= static_cast<int>(logging::Severity::kFATAL)
Invalid run log severity level. Not a valid onnxruntime::logging::Severity value: 
Invalid session log severity level. Not a valid onnxruntime::logging::Severity value: 
onnxruntime::InferenceSession::CreateLoggerForRun
onnxruntime::InferenceSession::InitLogger
session_options_.session_log_severity_level >= 0 && session_options_.session_log_severity_level <= static_cast<int>(logging::Severity::kFATAL)
onnxruntime::InferenceSession::AddPredefinedTransformers
961c151d2e87f2686a955a9be24d316f1362bf21 3.9.1
.json
model_loading_uri
Load model from 
 failed:
 bytes were able to be read.
onnxruntime::LoadOrtModelBytes
 failed. Only 
list too long
onnxruntime::`anonymous-namespace'::GetCurrentTimeString
localtime_s(&local_tm, &in_time_t) == 0
com.microsoft.experimental
com.microsoft
com.microsoft.dml
com.microsoft.nchwc
Input type was null
 expected to have tensor or sparse tensor type. Got: 
[TypeInferenceError] 
 unknown
Input 
Output 
Element type of input 
Element type of sequence input 
 expected to have sequence type
Element type of optional input 
 expected to have optional type
 expected to have type but instead is null
Mismatch between source and target type. Source=
 Target=
Unsupported Source/Target type=
[ShapeInferenceError] 
tensor(uint32)
unknown
tensor(int32)
tensor(uint64)
tensor(float16)
tensor(int64)
tensor(double)
tensor(float)
tensor(uint8)
tensor(bfloat16)
tensor(int8)
tensor(uint16)
seq(tensor(uint8))
tensor(int16)
seq(tensor(uint32))
seq(tensor(uint16))
seq(tensor(int8))
seq(tensor(uint64))
seq(tensor(int32))
seq(tensor(int16))
seq(tensor(float16))
seq(tensor(int64))
seq(tensor(double))
seq(tensor(float))
tensor(bool)
tensor(string)
tensor(complex128)
tensor(complex64)
seq(tensor(bool))
seq(tensor(string))
seq(tensor(complex128))
seq(tensor(complex64))
 line 
Schema error: 
) from file 
, but it is already registered from file 
 (domain: 
 version: 
 known by the checker.
Trying to register schema with name 
in onnx/defs/schema.h).
, but its domain is not
bumped the operator version but 
forgot to update the version range in DomainToVersionRange 
in the inclusive range [
] (usually, this means you 
, but its version is not 
An allocator for this device has already been registered for sharing.
Only CPU allocators can be shared between multiple sessions for now.
Only CPU devices are supported for now.
No allocator for this device has been registered for sharing.
seq(tensor(bfloat16))
string
intra-op
Exception caught: 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\session\environment.cc
inter-op
input
MemcpyFromHost
output
Constrain to all fixed size tensor and sequence types. If the dtype attribute is not provided this must be a valid output type.
MemcpyToHost
value
parse error
parse_error
 at line 
, column 
type_error
invalid_iterator
ort_config
out_of_range
Unsupported value for intra_op_num_threads: 
session_options
D:\a\_work\1\s\engine\lotus\onnxruntime\core\session\inference_session_utils.cc
onnxruntime::SetIntraOpNumThreads
Unsupported value for inter_op_num_threads: 
Setting intra_op_num_threads to 
Setting inter_op_num_threads to 
onnxruntime::SetInterOpNumThreads
onnxruntime::SetExecutionMode
Unsupported execution_mode value in ORT config: 
Parallel mode
Sequential mode
Setting graph_optimization_level to ORT_DISABLE_ALL
Setting execution_mode to 
Setting graph_optimization_level to ORT_ENABLE_BASIC
onnxruntime::SetGraphOptimizationLevel
Setting graph_optimization_level to ORT_ENABLE_ALL
Setting graph_optimization_level to ORT_ENABLE_EXTENDED
Unsupported value for enable_profiling option: 
Unsupported graph_optimization_level value in ORT config: 
Setting enable_profiling to 
onnxruntime::SetEnableProfiling
onnxruntime::inference_session_utils::JsonConfigParser::ParseOrtConfigJsonInModelProto::<lambda_8bac32b7d0f8f8e4174abd4e312abcfd>::operator ()
Json stored in the `ort_config` key cannot be parsed. Error message: 
Found session/run/environment configuration in the model file to be used while running the model
The Model Proto has already been checked for the ORT config json.
ORT config json from the model: 
onnxruntime::inference_session_utils::JsonConfigParser::ParseOrtConfigJsonInModelProto
Did not find session options in the model file to be used while running the model
The Model Proto hasn't been checked for the ORT config json.
intra_op_num_threads
onnxruntime::inference_session_utils::JsonConfigParser::ParseSessionOptionsFromModelProto
inter_op_num_threads
intra_op_num_threads option in the model file must be an integer
execution_mode
inter_op_num_threads option in the model file must be an integer
graph_optimization_level
execution_mode option in the model file must be an integer
enable_profiling
graph_optimization_level option in the model file must be an integer
Ignoring unsupported session option in ORT config: 
enable_profiling option in the model file must be an integer
' not found
cannot use at() with 
key '
cannot use key() for non-object iterators
invalid map<K, T> key
array
object
binary
boolean
number
discarded
cannot compare iterators of different containers
cannot get value
syntax error 
while parsing 
unexpected 
; last read: '
<U+%.4X>
; expected 
invalid literal
invalid BOM; must be 0xEF 0xBB 0xBF if given
true literal
<uninitialized>
null literal
false literal
number literal
string literal
end of input
<parse error>
unknown token
'[', '{', or a literal
invalid number; expected digit after '.'
invalid number; expected digit after '-'
invalid number; expected digit after exponent sign
invalid number; expected '+', '-', or digit after exponent
invalid comment; expecting '/' or '*' after '/'
invalid comment; missing closing '*/'
invalid string: '\u' must be followed by 4 hex digits
invalid string: missing closing quote
invalid string: surrogate U+DC00..U+DFFF must follow U+D800..U+DBFF
invalid string: surrogate U+D800..U+DBFF must be followed by U+DC00..U+DFFF
invalid string: control character U+0000 (NUL) must be escaped to \u0000
invalid string: forbidden character after backslash
invalid string: control character U+0002 (STX) must be escaped to \u0002
invalid string: control character U+0001 (SOH) must be escaped to \u0001
invalid string: control character U+0004 (EOT) must be escaped to \u0004
invalid string: control character U+0003 (ETX) must be escaped to \u0003
invalid string: control character U+0006 (ACK) must be escaped to \u0006
invalid string: control character U+0005 (ENQ) must be escaped to \u0005
invalid string: control character U+0008 (BS) must be escaped to \u0008 or \b
invalid string: control character U+0007 (BEL) must be escaped to \u0007
invalid string: control character U+000A (LF) must be escaped to \u000A or \n
invalid string: control character U+0009 (HT) must be escaped to \u0009 or \t
invalid string: control character U+000C (FF) must be escaped to \u000C or \f
invalid string: control character U+000B (VT) must be escaped to \u000B
invalid string: control character U+000E (SO) must be escaped to \u000E
invalid string: control character U+000D (CR) must be escaped to \u000D or \r
invalid string: control character U+0010 (DLE) must be escaped to \u0010
invalid string: control character U+000F (SI) must be escaped to \u000F
invalid string: control character U+0012 (DC2) must be escaped to \u0012
invalid string: control character U+0011 (DC1) must be escaped to \u0011
invalid string: control character U+0014 (DC4) must be escaped to \u0014
invalid string: control character U+0013 (DC3) must be escaped to \u0013
invalid string: control character U+0016 (SYN) must be escaped to \u0016
invalid string: control character U+0015 (NAK) must be escaped to \u0015
invalid string: control character U+0018 (CAN) must be escaped to \u0018
invalid string: control character U+0017 (ETB) must be escaped to \u0017
invalid string: control character U+001A (SUB) must be escaped to \u001A
invalid string: control character U+0019 (EM) must be escaped to \u0019
invalid string: control character U+001C (FS) must be escaped to \u001C
invalid string: control character U+001B (ESC) must be escaped to \u001B
invalid string: control character U+001E (RS) must be escaped to \u001E
invalid string: control character U+001D (GS) must be escaped to \u001D
invalid string: ill-formed UTF-8 byte
invalid string: control character U+001F (US) must be escaped to \u001F
vector<bool> too long
object separator
object key
number overflow parsing '
iterator out of range
iterator does not fit current value
cannot use erase() with 
type must be number, but is 
LogHr
FailFast
Exception
ReturnHr
(caller: %p) 
%hs(%d) tid(%x) %08X %ws
%hs(%u)\%hs!%p: 
%hs!%p: 
CallContext:[%hs] 
[%hs(%hs)]
Msg:[%ws] 
kernelbase.dll
RaiseFailFastException
[%hs]
lstd::exception: %hs
D:\a\_work\1\s\engine\lotus\cmake\external\wil\include\wil\resource.h
WilError_03
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\dml_provider_factory.cc
Local\SM0:%d:%d:%hs
^|*W?
DML_OPERATOR_ELEMENT_WISE_IDENTITY
DML_OPERATOR_ELEMENT_WISE_ABS
DML_OPERATOR_ELEMENT_WISE_ASIN
DML_OPERATOR_ELEMENT_WISE_ATAN
DML_OPERATOR_ELEMENT_WISE_ACOS
DML_OPERATOR_ELEMENT_WISE_ADD
DML_OPERATOR_ELEMENT_WISE_CLIP1
DML_OPERATOR_ELEMENT_WISE_CLIP_GRAD
DML_OPERATOR_ELEMENT_WISE_CEIL
DML_OPERATOR_ELEMENT_WISE_CLIP
DML_OPERATOR_ELEMENT_WISE_DIVIDE
DML_OPERATOR_ELEMENT_WISE_EXP
DML_OPERATOR_ELEMENT_WISE_CLIP_GRAD1
DML_OPERATOR_ELEMENT_WISE_COS
DML_OPERATOR_ELEMENT_WISE_LOGICAL_AND
DML_OPERATOR_ELEMENT_WISE_LOGICAL_EQUALS
DML_OPERATOR_ELEMENT_WISE_FLOOR
DML_OPERATOR_ELEMENT_WISE_LOG
DML_OPERATOR_ELEMENT_WISE_LOGICAL_GREATER_THAN_OR_EQUAL
DML_OPERATOR_ELEMENT_WISE_LOGICAL_LESS_THAN_OR_EQUAL
DML_OPERATOR_ELEMENT_WISE_LOGICAL_GREATER_THAN
DML_OPERATOR_ELEMENT_WISE_LOGICAL_LESS_THAN
DML_OPERATOR_ELEMENT_WISE_LOGICAL_XOR
DML_OPERATOR_ELEMENT_WISE_MAX
DML_OPERATOR_ELEMENT_WISE_LOGICAL_NOT
DML_OPERATOR_ELEMENT_WISE_LOGICAL_OR
DML_OPERATOR_ELEMENT_WISE_MULTIPLY
DML_OPERATOR_ELEMENT_WISE_POW
DML_OPERATOR_ELEMENT_WISE_MEAN
DML_OPERATOR_ELEMENT_WISE_MIN
DML_OPERATOR_ELEMENT_WISE_SIN
DML_OPERATOR_ELEMENT_WISE_SQRT
DML_OPERATOR_ELEMENT_WISE_CONSTANT_POW
DML_OPERATOR_ELEMENT_WISE_RECIP
DML_OPERATOR_ELEMENT_WISE_SUBTRACT
DML_OPERATOR_ELEMENT_WISE_TAN
DML_OPERATOR_ELEMENT_WISE_DIFFERENCE_SQUARE
DML_OPERATOR_ELEMENT_WISE_ATAN_YX
DML_OPERATOR_ELEMENT_WISE_DEQUANTIZE_LINEAR
DML_OPERATOR_CONVOLUTION
DML_OPERATOR_ELEMENT_WISE_THRESHOLD
DML_OPERATOR_ELEMENT_WISE_QUANTIZE_LINEAR
DML_OPERATOR_ARGMIN
DML_OPERATOR_ARGMAX
DML_OPERATOR_GEMM
DML_OPERATOR_REDUCE
DML_OPERATOR_MAX_POOLING
DML_OPERATOR_MAX_POOLING1
DML_OPERATOR_AVERAGE_POOLING
DML_OPERATOR_LP_POOLING
DML_OPERATOR_CAST
DML_OPERATOR_SPLIT
DML_OPERATOR_ROI_POOLING
DML_OPERATOR_SLICE
DML_OPERATOR_PADDING1
DML_OPERATOR_VALUE_SCALE_2D
DML_OPERATOR_JOIN
DML_OPERATOR_PADDING
DML_OPERATOR_SPACE_TO_DEPTH
DML_OPERATOR_DEPTH_TO_SPACE
DML_OPERATOR_UPSAMPLE_2D
DML_OPERATOR_GATHER
DML_OPERATOR_BATCH_NORMALIZATION
DML_OPERATOR_BATCH_NORMALIZATION_GRAD
DML_OPERATOR_TILE
DML_OPERATOR_TOP_K
DML_OPERATOR_LOCAL_RESPONSE_NORMALIZATION
DML_OPERATOR_LOCAL_RESPONSE_NORMALIZATION_GRAD
DML_OPERATOR_BATCH_NORMALIZATION_TRAINING_GRAD
DML_OPERATOR_MEAN_VARIANCE_NORMALIZATION
DML_OPERATOR_LSTM
DML_OPERATOR_GRU
DML_OPERATOR_LP_NORMALIZATION
DML_OPERATOR_RNN
DML_OPERATOR_ELEMENT_WISE_NEGATE
DML_OPERATOR_ELEMENT_WISE_ERF
DML_OPERATOR_ELEMENT_WISE_SIGN
DML_OPERATOR_ELEMENT_WISE_IS_NAN
DML_OPERATOR_ELEMENT_WISE_TANH
DML_OPERATOR_ELEMENT_WISE_ASINH
DML_OPERATOR_ELEMENT_WISE_SINH
DML_OPERATOR_ELEMENT_WISE_COSH
DML_OPERATOR_ELEMENT_WISE_IF
DML_OPERATOR_ELEMENT_WISE_ADD1
DML_OPERATOR_ELEMENT_WISE_ACOSH
DML_OPERATOR_ELEMENT_WISE_ATANH
DML_OPERATOR_SCATTER
DML_OPERATOR_ONE_HOT
DML_OPERATOR_MAX_UNPOOLING
DML_OPERATOR_DIAGONAL_MATRIX
DML_OPERATOR_ELEMENT_WISE_BIT_SHIFT_RIGHT
DML_OPERATOR_ELEMENT_WISE_ROUND
DML_OPERATOR_RESAMPLE
DML_OPERATOR_ELEMENT_WISE_BIT_SHIFT_LEFT
DML_OPERATOR_ELEMENT_WISE_MODULUS_FLOOR
DML_OPERATOR_FILL_VALUE_CONSTANT
DML_OPERATOR_ELEMENT_WISE_IS_INFINITY
DML_OPERATOR_ELEMENT_WISE_MODULUS_TRUNCATE
DML_OPERATOR_CUMULATIVE_PRODUCT
DML_OPERATOR_REVERSE_SUBSEQUENCES
DML_OPERATOR_FILL_VALUE_SEQUENCE
DML_OPERATOR_CUMULATIVE_SUMMATION
DML_OPERATOR_SCATTER_ND
DML_OPERATOR_MAX_POOLING2
DML_OPERATOR_GATHER_ELEMENTS
DML_OPERATOR_GATHER_ND
DML_OPERATOR_DEPTH_TO_SPACE1
DML_OPERATOR_SPACE_TO_DEPTH1
DML_OPERATOR_SLICE1
DML_OPERATOR_TOP_K1
DML_OPERATOR_MATRIX_MULTIPLY_INTEGER
DML_OPERATOR_QUANTIZED_LINEAR_MATRIX_MULTIPLY
DML_OPERATOR_MEAN_VARIANCE_NORMALIZATION1
DML_OPERATOR_RESAMPLE1
DML_OPERATOR_ELEMENT_WISE_BIT_AND
DML_OPERATOR_ELEMENT_WISE_BIT_OR
DML_OPERATOR_CONVOLUTION_INTEGER
DML_OPERATOR_QUANTIZED_LINEAR_CONVOLUTION
DML_OPERATOR_ELEMENT_WISE_BIT_COUNT
DML_OPERATOR_ACTIVATION_RELU_GRAD
DML_OPERATOR_ELEMENT_WISE_BIT_XOR
DML_OPERATOR_ELEMENT_WISE_BIT_NOT
DML_OPERATOR_RANDOM_GENERATOR
DML_OPERATOR_NONZERO_COORDINATES
DML_OPERATOR_AVERAGE_POOLING_GRAD
DML_OPERATOR_MAX_POOLING_GRAD
DML_OPERATOR_ADAM_OPTIMIZER
DML_OPERATOR_ROI_ALIGN
DML_OPERATOR_RESAMPLE_GRAD
DML_OPERATOR_SLICE_GRAD
DML_OPERATOR_DYNAMIC_QUANTIZE_LINEAR
DML_OPERATOR_ELEMENT_WISE_QUANTIZED_LINEAR_ADD
DML_OPERATOR_ROI_ALIGN1
DML_OPERATOR_GATHER_ND1
DML_OPERATOR_ROI_ALIGN_GRAD
DML_OPERATOR_BATCH_NORMALIZATION_TRAINING
ScaleBias
ATensor
InputTensor
OutputTensor
MinMaxDataType
BTensor
ExponentTensor
Exponent
InputGradientTensor
OutputGradientTensor
FilterTensor
BiasTensor
ScaleTensor
ZeroPointTensor
DimensionCount
Strides
Direction
EndPadding
OutputPadding
Dilations
StartPadding
CTensor
TransA
GroupCount
FusedActivation
Function
TransB
Alpha
AxisDirection
WindowSize
AxisCount
OutputIndicesTensor
ROITensor
IncludePadding
Offsets
Sizes
SpatialScale
PooledSize
InputCount
OutputCount
OutputTensors
PaddingValue
PaddingValueDataType
InputTensors
PaddingMode
ScaleSize
Scale
ChannelCount
IndexDimensions
BlockSize
InterpolationMode
IndicesTensor
OutputValueTensor
OutputIndexTensor
RepeatsCount
Repeats
VarianceTensor
Spatial
MeanTensor
OutputBiasGradientTensor
CrossChannel
Epsilon
OutputScaleGradientTensor
WeightTensor
RecurrenceTensor
NormalizeVariance
LocalSize
OutputSequenceTensor
OutputSingleTensor
HiddenInitTensor
SequenceLengthsTensor
CellMemInitTensor
PeepholeTensor
ActivationDescCount
ActivationDescs
UseClipThreshold
CoupleInputForget
OutputCellSingleTensor
ClipThreshold
Offset
Value
LinearBeforeReset
ConditionTensor
ScaleCount
Scales
UpdatesTensor
ValuesTensor
ValueDataType
ValueStart
RoundingMode
InfinityMode
HasExclusiveProduct
InputDimensionCount
ValueDelta
HasExclusiveSum
InputWindowSizes
InputWindowStrides
IndicesDimensionCount
InputWindowOffsets
OutputPixelOffsets
AZeroPointTensor
Order
InputPixelOffsets
BScaleTensor
OutputScaleTensor
BZeroPointTensor
AScaleTensor
FilterZeroPointTensor
InputScaleTensor
OutputZeroPointTensor
InputZeroPointTensor
OutputStateTensor
FilterScaleTensor
InputStateTensor
InputParametersTensor
InputFirstMomentTensor
OutputCountTensor
OutputCoordinatesTensor
TrainingStepTensor
OutputParametersTensor
InputSecondMomentTensor
GradientTensor
LearningRate
Beta1
OutputFirstMomentTensor
OutputSecondMomentTensor
ReductionFunction
SpatialScaleX
Beta2
BatchIndicesTensor
MinimumSamplesPerOutput
MaximumSamplesPerOutput
SpatialScaleY
OutOfBoundsInputValue
AlignRegionsToCorners
BatchDimensionCount
InputPixelOffset
OutputPixelOffset
OutputMeanTensor
OutputVarianceTensor
OutputROIGradientTensor
FusedAddTensor
DML_OPERATOR_ACTIVATION_HARDMAX
DML_OPERATOR_ACTIVATION_HARD_SIGMOID
DML_OPERATOR_ACTIVATION_ELU
DML_OPERATOR_ACTIVATION_CELU
DML_OPERATOR_ACTIVATION_LINEAR
DML_OPERATOR_ACTIVATION_LOG_SOFTMAX
DML_OPERATOR_ACTIVATION_IDENTITY
DML_OPERATOR_ACTIVATION_LEAKY_RELU
DML_OPERATOR_ACTIVATION_PARAMETRIC_SOFTPLUS
DML_OPERATOR_ACTIVATION_RELU
SlopeTensor
DML_OPERATOR_ACTIVATION_PARAMETERIZED_RELU
DML_OPERATOR_ACTIVATION_SCALED_TANH
DML_OPERATOR_ACTIVATION_SIGMOID
Gamma
DML_OPERATOR_ACTIVATION_SCALED_ELU
DML_OPERATOR_ACTIVATION_SOFTPLUS
DML_OPERATOR_ACTIVATION_SOFTSIGN
DML_OPERATOR_ACTIVATION_SOFTMAX
Steepness
Threshold
DML_OPERATOR_ACTIVATION_SHRINK
DML_OPERATOR_ACTIVATION_TANH
DML_OPERATOR_ACTIVATION_THRESHOLDED_RELU
equation
fused_activation
fused_activation_domain
fused_activation_since_version
Sigmoid
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\GraphTransformer.cpp
Dml::GraphTransformer::ApplyImpl
QLinearSigmoid
decomposed_QLinearSigmoid_input_
) + (
fused op (
DequantizeLinear
decomposed_QLinearSigmoid_Sigmoid_
decomposed_QLinearSigmoid_output_
decomposed_QLinearSigmoid_DequantizeLinear_
decomposed_QLinearSigmoid_QuantizeLinear_
QuantizeLinear
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\GpuEvent.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/dml/OperatorAuthorHelper/MLOperatorAuthorHelper.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\ExecutionProvider.cpp
Dml::ExecutionProviderImpl::CopyTensors
D:\a\_work\1\s\engine\lotus\cmake\external\wil\include\wil/wrl.h
lambd
fused_alpha
fused_beta
LeakyRelu
fused_gamma
ScaledTanh
HardSigmoid
ThresholdedRelu
Softplus
BILINEAR
Softsign
BatchNormalization
InstanceNormalization
ConvTranspose
MatMul
MeanVarianceNormalization
ParametricSoftplus
PRelu
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\OperatorUtility.cpp
Linear
Dropout
Fused
nearest
NEAREST
Shrink
fused_
bilinear
linear
DmlFusedNode_
DmlFusedNodeDomain
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\GraphPartitioner.cpp
Dml::ComputationCapacityFromPartition
auto_pad
ceil_mode
border
dilations
kernel_shape
output_padding
output_shape
sample_size
scale
split
strides
NOTSET
VALID
SAME_UPPER
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\OperatorAuthorHelper\OperatorHelper.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\OperatorAuthorHelper\OperatorHelper.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\OperatorAuthorHelper\MLOperatorAuthorHelper.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\External/DirectMLHelpers/GeneratedSchemaHelpers.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\MLOperatorAuthorImpl.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\MLOperatorAuthorImpl.cpp
tensor(complext64)
tensor(complext128)
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\AbiCustomRegistry.cpp
DML CPU
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\BucketizedBufferAllocator.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\PooledUploadHeap.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\DmlCommon.cpp
alpha
broadcast
blocksize
batch_dims
hidden_size
group
select_last_index
keepdims
output_width
output_height
scales
pooled_shape
starts
transB
transA
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/dml/OperatorAuthorHelper/OperatorHelper.h
AveragePool
LpPool
GlobalMaxPool
MaxPool
GlobalAveragePool
RoiAlign
MaxRoiPool
GlobalLpPool
LpNormalization
Concat
Transpose
Split
ConvTransposeWithDynamicPads
DepthToSpace
SpaceToDepth
Slice
Gather
ConstantOfShape
Expand
ScatterElements
Scatter
GatherND
GatherElements
Flatten
Identity
EyeLike
ScatterND
Reshape
Unsqueeze
Squeeze
Reciprocal
Floor
Affine
Asinh
IsNaN
Where
Atanh
Acosh
ReduceProd
ReduceMean
Einsum
ReduceSum
ReduceL1
ReduceSumSquare
ReduceLogSumExp
ReduceLogSum
ArgMax
ReduceMin
ReduceMax
ReduceL2
Greater
ArgMin
Equal
LessOrEqual
GreaterOrEqual
Resize
Upsample
ImageScaler
Hardmax
LogSoftmax
Softmax
FusedConvTranspose
FusedConv
OneHot
FusedGemm
FusedMeanVarianceNormalization
FusedBatchNormalization
FusedInstanceNormalization
IsInf
FusedSum
FusedAdd
FusedMatMul
ReverseSequence
Round
BitShift
QLinearAdd
MaxUnpool
Range
CumSum
ConvInteger
MatMulInteger
QLinearMatMul
QLinearConv
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\OperatorRegistration.cpp
DynamicQuantizeLinear
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\ReadbackHeap.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\ExecutionContext.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\FusedGraphKernel.cpp
bad variant access
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\External/DirectMLHelpers/ApiHelpers.h
FusedNode's nodeArgList does not contain one of the nodeArg
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\GraphDescBuilder.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\External/DirectMLHelpers/ApiTraits.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\External/DirectMLHelpers/SchemaHelpers.h
deque<T> too long
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorGather.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorMaxUnpool.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorGemm.cpp
detect_positive
detect_negative
direction
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorElementWise.cpp
RIGHT
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorExpand.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorConvolution.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorReduce.cpp
count_include_pad
storage_order
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorPooling.cpp
gamma
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorActivation.cpp
epsilon
spatial
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorBatchNormalization.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorScatter.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorQLinearConv.cpp
exclude_outside
coordinate_transformation_mode
extrapolation_value
nearest_mode
tf_crop_and_resize
tf_half_pixel_for_nn
round_prefer_ceil
round_prefer_floor
pytorch_half_pixel
half_pixel
asymmetric
align_corners
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorResize.cpp
floor
cubic
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorSplit.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorConstantOfShape.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorQLinearAdd.cpp
reflect
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorPadding.cpp
constant
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorMemcpy.cpp
spatial_scale
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorRoiPooling.cpp
activation_alpha
activations
activation_beta
linear_before_reset
input_forget
reverse
bidirectional
forward
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorRecurrentNeuralNetwork.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorSlice.cpp
sampling_ratio
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorRoiAlign.cpp
batch_axis
time_axis
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorReverseSequence.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorLpNormalization.cpp
largest
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorTopk.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorCopy.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorEyeLike.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorInstanceNormalization.cpp
across_channels
normalize_variance
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorMeanVarianceNormalization.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorMatMul.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorEinSum.cpp
exclusive
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorCumSum.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorAffine.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorConvInteger.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorDepthToSpace.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorNeg.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorOneHot.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorCast.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorMatMulInteger.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorSpaceToDepth.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorCrop.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorDynamicQuantizeLinear.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorTile.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorLocalResponseNormalization.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorConcat.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorQLinearMatMul.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorRange.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorTranspose.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorValueScale2D.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\CommandAllocatorRing.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\DmlCommandRecorder.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\CommandQueue.cpp
ZvD:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\GraphKernelHelper.cpp
_DmlExecutionProvider_
_DmlExecutionProvider
_token_
Bad optional access
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperator.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\TensorDesc.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\DescriptorPool.cpp
C++/WinRT version:2.0.201201.7
Fop_name
provider
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/dml/OperatorAuthorHelper/SchemaInferenceOverrider.h
Session
Kernel
Failed to serialize model!
File not found!
The model contains a 16-bit input (
), but the current device does not support 16-bit float.
Expected a single int64 value!
Expected a single string value!
Expected a single float value!
Failed to parse model file!
Model file not found!
Engine failed to create a model!
Failed to parse model stream!
unused
Undefined tensor type!
IdentityTo
DATA_BATCH
D:\a\_work\1\s\engine\lotus\winml\adapter\winml_adapter_dml.cpp
DMLCreateDevice1
DirectML.dll
Windows.Foundation.Collections.IKeyValuePair`2<String, UInt32>
Windows.Foundation.Collections.IIterator`1<Windows.Foundation.Collections.IKeyValuePair`2<String, UInt32>>
Windows.Foundation.Collections.IMap`2<String, UInt32>
Esession_state_ != nullptr
Session must be initialized to create session state.
D:\a\_work\1\s\engine\lotus\onnxruntime\core/session/inference_session.h
onnxruntime::InferenceSession::GetSessionState
length
RoOriginateLanguageException
Sequential execution should be enabled when using DML execution provider.
Mem pattern should be disabled when using DML execution provider.
D:\a\_work\1\s\engine\lotus\winml\adapter\winml_adapter_session.cpp
Windows::AI::MachineLearning::Adapter::SessionRegisterCustomRegistry
GetSessionGetInputDevice
combase.dll
Out of memory
RemoveDuplicateCastTransformer
cast node to cast from float16 to float32 on cpu
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\insert_cast_transformer.cc
onnxruntime::RemoveDuplicateCastTransformer::ApplyImpl
dtype
onnxruntime::InsertCastTransformer::ApplyImpl
dtype_attribute->second.has_i()
InsertCastTransformer works on the assumption that `dtype` attribute holds an integer.
onnxruntime::GraphTransformer::Recurse
InsertedCast_
D:\a\_work\1\s\engine\lotus\include\onnxruntime\core/optimizer/graph_transformer.h
ArmNNExecutionProvider
ACLExecutionProvider
ROCMExecutionProvider
BiasDropoutFusion
AttentionFusion
BiasSoftmaxFusion
BiasGeluFusion
CommonSubexpressionElimination
CastElimination
EliminateDropout
DivMulFusion
EmbedLayerNormFusion
DynamicQuantizeMatMulFusion
FastGeluFusion
ExpandElimination
GeluFusion
GeluApproximation
ConvAddFusion
ConvActivationFusion
ConvMulFusion
ConvBNFusion
MatmulTransposeFusion
MatMulScaleFusion
NoopElimination
NhwcTransformer
QDQPropagationTransformer
NotWhereFusion
ReluQuantRewrite
QDQS8ToU8Transformer
GemmSumFusion
GemmActivationFusion
EliminateIdentity
GemmTransposeFusion
SimplifiedLayerNormFusion
LayerNormFusion
MatMulIntegerToFloatFusion
MatMulAddFusion
EliminateSlice
SkipLayerNormFusion
UnsqueezeElimination
TransposeOptimizer
session.disable_quant_qdq
optimization.enable_gelu_approximation
ReshapeFusion
FuseReluClip
_RuleBasedTransformer
Unsupported level
Level
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\graph_transformer_utils.cc
onnxruntime::optimizer_utils::GenerateRewriteRules
onnxruntime::optimizer_utils::GenerateRuleBasedGraphTransformer
onnxruntime::optimizer_utils::GenerateTransformers
Unsupported optimization level: 
onnxruntime::optimizer_utils::GenerateTransformersForRuntimeOptimizations
onnxruntime::Node::ForEachWithIndex
VitisAIExecutionProvider
onnxruntime::TransformerMemcpyImpl::ProcessInitializers::<lambda_339aa899e5780231e41cee84a9ac8a33>::operator ()
dup_replacements.find(&arg) == dup_replacements.end()
onnxruntime::TransformerMemcpyImpl::ProcessInitializers
onnxruntime::MemcpyTransformer::ApplyImpl
onnxruntime::TransformerMemcpyImpl::ProcessDefs
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\transformer_memcpy.cc
Execution type '
' doesn't support memcpy 
Copy from/to host memory
Memcpy
onnxruntime::GraphTransformer::Apply
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\graph_transformer.cc
This transformer is already registered 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\graph_transformer_mgr.cc
onnxruntime::GraphTransformerManager::ApplyTransformers
onnxruntime::Initializer::Initializer
size is different
D:\a\_work\1\s\engine\lotus\onnxruntime\core/optimizer/initializer.h
unsupported data type: 
size_ == size
ReadExternalRawData() failed: 
start
ConstantFolding
onnxruntime::ConstantFolding::ApplyImpl
Shape
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\constant_folding.cc
Could not find OrtValue with name '
can't constant fold 
 node '
fetches.size() == node->OutputDefs().size()
Could not find a CPU kernel and hence 
Unsupported output type of 
. Can't constant fold 
Unsupported data type: 
Constant initializer NodeArg shape should not be null. NodeArg: 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\matmul_scale_fusion.cc
div_inputs.size() == 2
shape
onnxruntime::`anonymous-namespace'::GetScalarConstantInitializer
onnxruntime::MatMulScaleFusion::ApplyImpl
input_node.InputDefs().size() == 2 && scale_and_index->second < 2
onnxruntime::`anonymous-namespace'::GetInputNodeMerges
onnxruntime::`anonymous-namespace'::GetScaleFromNode
mul_inputs.size() == 2
Fused MatMul and Scale
_FusedMatMulAndScale
output_node.OutputDefs().size() == 1
onnxruntime::`anonymous-namespace'::GetOutputNodeMerges
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<struct onnxruntime::BFloat16>::operator ()
onnxruntime::utils::mltype_dispatcher_internal::UnsupportedTypeDefaultPolicy<class onnxruntime::common::Status>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<struct onnxruntime::MLFloat16>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<__int64>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<double>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<float>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<unsigned int>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<int>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<unsigned __int64>::operator ()
Slice does not have enough number of inputs
Slice ends is less than INT_MAX
onnxruntime::AttentionFusionHelper::CheckSliceParameters
D:\a\_work\1\s\engine\lotus\onnxruntime\core/optimizer/attention_fusion_helper.h
Start MatchGemmSubgraph
onnxruntime::AttentionFusionHelper::MatchGemmSubgraph
Expected value:
Slice parameter is not expected. Input index:
input_indices.size() == expected_values.size() && input_indices.size() > 0
concat_after_gather input 2 does not have expected value
Faild to match gemm gather path
Faild to match concat node for Gather paths
concat_after_gather does not have expected number of inputs or output edges
gather axis value not expected
gather input 1 value is not expected
Output edge count not expected for nodes in gemm gather path
unsqueeze_after_gather axes value not expected
Output edge count not expected for nodes in gemm path
Gemm does not have 3 inputs
Faild to match gemm path
Input of reshape_before_gemm is not the input of subgraph
CheckSliceParameters return false
concat first input value is not -1
Gemm bias is not constant
Gemm bias shape not expected
unidir mask shape not expected
This optimizer does not support external data for unidirectional mask right now
unidir mask is not constant
onnxruntime::AttentionFusionHelper::ValidateUnidirMask
Start MatchUnidirMaskSubgraph
onnxruntime::AttentionFusionHelper::MatchUnidirMaskSubgraph
Mask is neither unidirectional nor all ones
Expect mask data type is uint8 or float
onnxruntime::AttentionFusionHelper::ValidateGemmInitializer
Gemm bias is not constant initializer
Pass MatchGemmSubgraph
Start ValidateGemmInitializer
Gemm weight shape is not expected
Pass ValidateGemmInitializer
Gemm bias shape is not expected
Gemm weight is not constant initializer
Faild to match path 3 for unidirectional mask
Output edge count not expected for unsqueeze3 of unidirectional mask
Faild to match path 2 for unidirectional mask
Output edge count not expected for unsqueeze2 of unidirectional mask
Output edge count not expected for squeeze_2/slices2/shape2 of unidirectional mask
CheckSliceParameters return false for slice2
Faild to match path 4 for unidirectional mask
Div and Shape does not have edge
Output edge count not expected for nodes in path 1 of unidirectional mask
Div and Shape1 does not have edge
Faild to match the path (Div-->Where-->Add) for unidirectional mask
Faild to match path 1 for unidirectional mask
ValidateUnidirMask returns false for mask_slice
CheckSliceParameters returns false for slice1
CheckSliceParameters returns false for last_slice
CheckSliceParameters returns false for mask_slice
mask_sub const input not matched
mask_mul const input not matched
mask_unsqueeze_1 axes not matched. Expect: 1
mask_unsqueeze_2 axes not matched. Expect: 2
Failed to find mask path
where const not matched.
Pass MatchInputMaskSubgraph
Start MatchInputMaskSubgraphDistilBert
onnxruntime::AttentionFusionHelper::MatchInputMaskSubgraph
Failed to find Softmax node
Pass MatchUnidirMaskSubgraph
Start MatchInputMaskSubgraph
Output edge count not expected for mask nodes
Softmax attribute axis is expected to be 3
Output edge count not expected for Softmax
Failed to find path for mask
Failed to find path for present_v and past_v
Failed to match v_concat
Failed to find path for past_k
Failed to find path for present_k
present_k_unsqueeze axes value not expected
present_v_unsqueeze axes value not expected
past_k_transpose perm attribute not matched
present_k_transpose perm attribute not matched
Failed to find reshape shape path 1
Failed to find reshape shape path 2
Failed to find shape path
equal const not matched.
Start MatchPastSubgraph
onnxruntime::AttentionFusionHelper::MatchPastSubgraph
gather indices not matched.
Pass MatchInputMaskSubgraphDistilBert
Failed in match v_transpose attribute perm. Expected: 0, 2, 1, 3
hidden_size != num_heads * head_size
Output edge count not expected for nodes in path v
Failed in match Transpose attribute perm. Expected: 0, 2, 1, 3
reshape initializer value is not expected
Start CheckNodesInPathQ
v_reshape initializer value is not expected
Pass CheckNodesInPathV
past_v_gather and past_k_gather does not have same past input
Output edge count not expected for nodes in past subgraph
past_v_gather indices != 1
past_k_gather indices != 0
Start CheckNodesInPathV
onnxruntime::AttentionFusionHelper::CheckNodesInPathV
Pass MatchPastSubgraph
onnxruntime::AttentionFusionHelper::CheckDistilBertReshapeShape
Pass CheckNodesInPathK
Mask_Int32
k_transpose perm attribute not matched
k_reshape const not matched
Start FuseGptAttention
onnxruntime::AttentionFusionHelper::FuseGptAttention
Cast mask from int64 to int32
MaskCast
qk_div const not matched.
q_transpose perm attribute not matched
onnxruntime::AttentionFusionHelper::CheckNodesInPathQ
q_reshape const not matched
onnxruntime::AttentionFusionHelper::CheckNodesInPathK
k_transpose has not perm attribute
Pass CheckNodesInPathQ
Start CheckNodesInPathK
Failed to find path for k
k and v are not from same Split node
Using transpose optimized pattern
opt_k_transpose perm attribute not matched
Fused Attention subgraphs 
Attention
CheckNodesInPathK returns false
MatchPastSubgraph returns false
CheckNodesInPathV return false
MatchInputMaskSubgraph returns false
Faild to find path to qkv_matmul
Faild to find path v to Split
q and v are not from same Split node
CheckNodesInPathQ returns false
MatchUnidirMaskSubgraph returns NULL
Failed to find path for q
LayerNormalization
shape of layer norm bias tensor not expected
Mask data type is not int32 or int64 or float32
onnxruntime::AttentionFusion::ApplyImpl
q root should be layer normalization
q_matmul and q_add shape not matched
Total fused Attention node count: 
onnxruntime::FuseSubGraphQKImpl
Fused an attention node for GPT.
qkv_weights
num_heads
unidirectional
onnxruntime::ConvertMaskToInt32
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\attention_fusion.cc
qkv_bias
Mask shape is unknown or not 2D, or data type unknown
Output edge count not expected for Add or MatMul in path v
Failed in match v_matmul and v_add input shape
Faild to find path v
onnxruntime::AttentionFusion::FuseSubGraph
Failed in match input mask subgraph
Failed to load Q, K and V weights, or data type is not float or float16.
Failed to load Q, K and V bias tensors, or data type is not float or float16.
k root is not layer norm
k_matmul and k_add shape not matched
Fused an attention node.
onnxruntime::FuseSubGraphQKDistilBert
Failed to convert mask to int32
onnxruntime::FuseSubGraphQK
bn_scale_tensor_proto
onnxruntime::Initializer::ToProto
bn_B_tensor_proto
bn_mean_tensor_proto
onnxruntime::ConvBNFusion::Apply
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\conv_bn_fusion.cc
data type is not supported
conv_B_tensor_proto
ConvBnFusion_W_
bn_var_tensor_proto
conv_W_tensor_proto
ConvBnFusion_BN_B_
qdq_s8_to_u8_quant
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\qdq_transformer\qdq_s8_to_u8.cc
qdq_s8_to_u8_zp_conversion
onnxruntime::QDQS8ToU8Transformer::ApplyImpl
QDQSelectorActionTransformer
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\conv_activation_fusion.cc
onnxruntime::ConvActivationFusion::ApplyImpl
Unexpected data type for Clip input of 
onnxruntime::`anonymous-namespace'::GetClipConstantMinMax::<lambda_706aa48b16f5813a9c1cc576607ea9a2>::operator ()
fused Conv 
fused 
activation
with activation 
activation_params
UnsqueezeElimination_
UnsqueezeElimination cannot remove node 
onnxruntime::UnsqueezeElimination::Apply
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\unsqueeze_elimination.cc
is not supported.
Unexpected data type for Clip 'min' input of 
data type 
_min_zero_constant
FuseReluClip_
onnxruntime::FuseReluClip::Apply
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\relu_clip_fusion.cc
onnxruntime::GeluApproximation::ApplyImpl
BiasGelu
FastGelu
Total Gelu Approximation (FastGelu) node count: 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\gelu_approximation.cc
Gelu approximation
onnxruntime::ReshapeFusion::Fuse_Subgraph
onnxruntime::ReshapeFusion::ApplyImpl
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\reshape_fusion.cc
Total fused reshape node count: 
Cannot replace concat node with initializer:
allowzero
Fused reshape node: 
sum_output_edge.src_arg_index == 0
!sum_input_moved
_sum_transformed
gemm_input_edge.src_arg_index < 2
graph.RemoveNode(sum_node.Index())
other_sum_input != nullptr
graph.RemoveNode(gemm_node.Index())
sum_node.GetOutputEdgesCount() == 0
new_gemm_input_defs.size() == 3
new_gemm_output_defs.size() == 1
Fused Gemm with Sum
onnxruntime::GemmSumFusion::Apply
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\gemm_sum_fusion.cc
onnxruntime::GemmSumFusion::SatisfyCondition
onnxruntime::SwapAdjacentNodes
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\qdq_transformer\qdq_propagation.cc
up_node should have only one Edge that points to down_node and its output is not graph output
optimizer_utils::CheckOutputEdges(graph, up_node, 1)
SwapAdjacentNodes
onnxruntime::QDQPropagationTransformer::ApplyImpl
up_node should be parent of down_node and NodeArg slots of the edge between up_node and down_node should be (0, 0).
edge_it->GetDstArgIndex() == 0 && edge_it->GetSrcArgIndex() == 0 && edge_it->GetNode().Index() == down_node.Index()
Second input of Gather in path 1 of position shape should be a constant with value 0.
Failed to find path 2 of position shape.
onnxruntime::MatchInputToConcatSubgraph
Output edge count not expected for nodes in path 1 of position shape.
Second input of Gather in path 2 of position shape should be a constant with value 1.
The parent of two shape nodes are expected to be input_ids.
Output edge count not expected for nodes in path 2 of position shape.
Gather node in path 2 is not linked to another subgraph.
_Cast
Input shape is unknown or not 2D, or data type unknown
_Int32
Cast Input from int64 to int32
Input data type is not int32 or int64
Failed to find path 1 of position shape.
onnxruntime::CheckInput
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\embed_layer_norm_fusion.cc
Optional position subgraph nodes Where node is expected to be the parent of Reshape.
Failed to match position subgraph.
The parent of shape nodes are expected to be input_ids.
Optional position subgraph nodes number of outputs unexpected.
fused EmbedLayerNorm subgraphs 
EmbedLayerNormalization
position_embeddings
mask_index
Output edge count not expected for nodes in path1.
onnxruntime::MatchPositionEmbeddingSubgraphsFromGather
two paths share the same shape
NonZero
Second input of Gather should be a constant with value 1. 
Failed to match Shape node. 
The first input of Range should be a constant with value 0.
The third input of Range should be a constant with value 1.
Segment id is not valid. 
Input_ids and segment id should have the same shape. 
Position embedding shape is not expected.
Input id is not valid. 
onnxruntime::FuseSubGraphDistilBert
onnxruntime::EmbedLayerNormFusion::ApplyImpl
Gamma should be of shape (hidden_size). 
Beta should be of shape (hidden_size). 
Input is expected to have dim value in all dimensions.
Failed to get initializer tensor.
Word embedding shape not expected.
onnxruntime::FuseSubGraph
Failed to match position embedding subgraph.
Failed to get position embedding weights.
Position embedding shape not matched.
Position embedding data type shall be float or float16.
+YieldOp
onnxruntime::DynamicQuantizeMatMulFusion::ApplyImpl
DynamicQuantizeMatMul
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\dynamic_quantize_matmul_fusion.cc
MatMulIntegerToFloat
The model has input '
onnxruntime::FreeDimensionOverrideTransformer::ApplyImpl
onnxruntime::FreeDimensionOverrideTransformer::FreeDimensionOverrideTransformer
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\free_dim_override_transformer.cc
FreeDimensionOverrideTransformer
Invalid free dimension override.
with a fixed dimension size 
Conflicting free dimension overrides.
which does not equal the specified override of 
fused Matmul and Add 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\matmul_add_fusion.cc
onnxruntime::MatMulAddFusion::ApplyImpl
activation_
fused Gemm 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\gemm_activation_fusion.cc
onnxruntime::GemmActivationFusion::ApplyImpl
_transformed
Fused Gemm with Transpose
onnxruntime::ReorderCastAndTranspose
cast != nullptr
onnxruntime::UpdateConsumerCount
!node_consumers.empty()
fused MatMul and Transpose 
onnxruntime::MatmulTransposeFusion::ApplyImpl
Created a new Transpose node to interchange Cast and Transpose nodes
Created a new Cast node to interchange Cast and Transpose nodes
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\matmul_transpose_fusion.cc
onnxruntime::GetTransposePerms
transpose_node.InputDefs().size() == 1
MatMul_With_Transpose
ConvMulFusion_Mul_B_
ConvMulFusion_W_
mul_B_tensor_proto
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\conv_mul_fusion.cc
onnxruntime::ConvMulFusion::Apply
onnxruntime::SimplifiedLayerNormFusion::ApplyImpl
fused LayerNorm subgraphs 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\layer_norm_fusion.cc
onnxruntime::LayerNormFusion::ApplyImpl
layer_norm_out
SimplifiedLayerNormalization
cast scale of layer norm
Cast_Scale
cast output of layer norm
'7SkipLayerNormalization
fused SkipLayerNorm subgraphs 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\skip_layer_norm_fusion.cc
onnxruntime::SkipLayerNormFusion::ApplyImpl
+SAME_LOWER
channels_last
 and 
 into softmax(input + bias)
onnxruntime::BiasSoftmaxFusion::ApplyImpl
broadcast_axis
softmax_axis
BiasSoftmax
axis >= -tensor_rank && axis <= tensor_rank - 1
axis 
 is not in valid range [-
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/common.h
onnxruntime::HandleNegativeAxis
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\bias_softmax_fusion.cc
Not eliminating output 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\common_subexpression_elimination.cc
onnxruntime::CommonSubexpressionElimination::ApplyImpl
 of node 
] because it's the graph's output.
representative.output_index != kInvalidOutputIndex
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\transpose_optimizer\ort_transpose_optimizer.cc
onnxruntime::TransposeOptimizer::ApplyImpl
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\nhwc_transformer.cc
onnxruntime::NhwcTransformer::ApplyImpl
fused Add-Dropout-(Add) for 
BiasDropout
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\bias_dropout_fusion.cc
onnxruntime::BiasDropoutFusion::ApplyImpl
onnxruntime::MatMulIntegerToFloatFusion::ApplyImpl
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\matmul_integer_to_float.cc
onnxruntime::RuleBasedGraphTransformer::ApplyImpl
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\rule_based_graph_transformer.cc
onnxruntime::RuleBasedGraphTransformer::ApplyRulesOnNode
fused Gelu subgraphs 
onnxruntime::GeluFusion::ApplyImpl
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\gelu_fusion.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\bias_gelu_fusion.cc
fused Add and Gelu
onnxruntime::BiasGeluFusion::ApplyImpl
fast_gelu_output
fused GPT2Gelu subgraphs 
GPT2Gelu
onnxruntime::FastGeluFusion::ApplyImpl
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\fast_gelu_fusion.cc
onnxruntime::ConvAddFusion::Apply
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\conv_add_fusion.cc
add_B_tensor_proto
ConvAddFusion_B_
ConvAddFusion_Add_B_
, external_data.length: 
Computed size: 
TensorProto external data size mismatch. 
External data type must not be UNDEFINED or STRING.
onnxruntime::Initializer::ReadExternalRawData
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\initializer.cc
model_path must not be empty. Ensure that a path is provided when the model is created or loaded.
Multinomial
RandomUniform
RandomNormal
RandomUniformLike
RandomNormalLike
onnxruntime::OptimizerExecutionFrame::Info::{ctor}::<lambda_35452b6fa29b4a84e3a3d83c879b06cd>::operator ()
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\optimizer_execution_frame.cc
Failed to get allocator for optimizer
allocator_ptr_
onnxruntime::OptimizerExecutionFrame::Info::Info
Tried to allocate without valid type information, ort_value index=
static_cast<size_t>(index) < nodes_.size() && ((node = nodes_[index]) != nullptr || !required)
onnxruntime::NodesToOptimize::GetNode
D:\a\_work\1\s\engine\lotus\onnxruntime\core/optimizer/selectors_actions/helpers.h
b33fd0fa-cd7b-4b10-ae5a-df64cabfe1f8
b33f88f7-c464-43e3-8692-97ac832bb14a
generated at runtime
QLinear
Existing registration with name 
selectors_and_actions_map_.find(name) == selectors_and_actions_map_.cend()
onnxruntime::SelectorsAndActions::RegisterSelectorAndAction
Matched 
Multiple entries for operator is not supported. OpType=
inserted
onnxruntime::SelectorActionTransformer::SelectorActionTransformer
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\selectors_actions\selector_action_transformer.cc
onnxruntime::SelectorActionTransformer::MatchAndProcess
Saving runtime optimizations is not enabled in this build.
onnxruntime::SelectorActionTransformer::ApplyImpl
onnxruntime::MergeIntoTarget::Run
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\selectors_actions\actions.cc
onnxruntime::CreateReplacementNode
onnxruntime::ReplaceWithNew::Run
onnxruntime::ReplaceWithNew::RunForSave
Failed to set node op schema.
Failed to remove node.
noop_with_empty_axes
NhwcMaxPool
com.microsoft.QLinearAveragePool
com.microsoft.QLinearGlobalAveragePool
com.microsoft.
com.microsoft.QLinearLeakyRelu
com.microsoft.QLinearConcat
com.microsoft.QLinearAdd
com.microsoft.QLinearMul
HardSwish
CastLike
com.microsoft.QLinearReduceMean
com.microsoft.QLinearSigmoid
TypeProto must have shape for this to run
onnxruntime::utils::GetShape
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/tensorprotoutils.h
size >= 0
onnxruntime::ApiTensor::NumElements
onnxruntime::ApiTensor::Data
No NodeArg found for name 
 out of bounds for shape 
Permutation entry 
0 <= p && p_int < shape_proto->dim_size()
Failed to get size of TensorProto
Permutation length 
perm.size() == gsl::narrow_cast<size_t>(shape_proto->dim_size())
onnxruntime::ApiValueInfo::PermuteDims
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\transpose_optimizer\api_impl.cc
 does not match rank 
const_transpose_optimizer
onnxruntime::ApiGraph::CopyValueInfo
Cannot reshape initializer 
new_num_elts == old_num_elts
Added in transpose optimizer
onnxruntime::ApiGraph::TransposeInitializer
Failed to find initializer to reshape with name 
onnxruntime::ApiGraph::ReshapeInitializer
 to have different number of elements
node_arg_ != nullptr
onnxruntime::ApiGraph::GetValueInfo
Failed to find initializer for name: 
success
A target node must be set.
target_node != NodesToOptimizeIndices::kEmptyNodeIndex
onnxruntime::NodesToOptimizeIndicesBuilder::Build
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\selectors_actions\helpers.cc
Index out of range
onnxruntime::`anonymous-namespace'::MoveInputOutputImpl
onnxruntime::MoveInputOutput
DnnlExecutionProvider
MIGraphXExecutionProvider
CoreMLExecutionProvider
OpenVINOExecutionProvider
NupharExecutionProvider
NnapiExecutionProvider
RknpuExecutionProvider
onnxruntime::CPUExecutionProvider::GetKernelRegistry
onnxruntime::RegisterCPUKernels
onnxruntime::RegisterOnnxOperatorKernels
onnxruntime::ml::RegisterOnnxMLOperatorKernels
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\cpu_execution_provider.cc
onnxruntime::TransposeBase::TransposeBase
v >= 0 && static_cast<uint64_t>(v) <= std::numeric_limits<size_t>::max()
 does not align with rank of input data: 
perm: 
 is outside range.
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/transpose.h
 is repeated.
Attribute perm of Transpose has an invalid value. Value 
(local_source >= source) && (local_source < source + num_blocks * num_elts_in_block)
num_axes > 0
onnxruntime::DoTransposeEltWise
Transpose of element size not supported in this build. Size=
(local_source >= source) && (local_source < source + num_blocks * blocksize)
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\transpose.cc
Transpose not implemented for empty tensors.
onnxruntime::DoTransposeImpl
Method IncrementIndexAndComputeOffset assumes this value is strictly positive.
onnxruntime::IncrementIndexAndComputeOffsetSetup
naxes > 0
onnxruntime::Transpose::Compute
(local_source >= source) && (local_source < source + num_blocks)
input_tensor_ptr != nullptr
Mismatched data types between input and output Tensors. 
(local_source >= source) && (local_source < source + sizeof(T) * num_blocks)
onnxruntime::TypedDoTransposeEltWise
0 == center_point_box_ || 1 == center_point_box_
center_point_box only support 0 or 1
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\object_detection\non_max_suppression.h
onnxruntime::NonMaxSuppressionBase::NonMaxSuppressionBase
center_point_box
output != nullptr
onnxruntime::NonMaxSuppression::Compute
The most inner dimension in boxes must have 4 data.
boxes and scores should have same spatial_dimension.
onnxruntime::NonMaxSuppressionBase::GetThresholdsFromInputs
iou_threshold must be in range [0, 1].
boxes must be a 3D tensor.
scores_tensor
boxes and scores should have same num_batches.
scores must be a 3D tensor.
boxes_tensor
NonMaxSuppression
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\object_detection\non_max_suppression.cc
onnxruntime::NonMaxSuppressionBase::PrepareCompute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\gather.cc
onnxruntime::Gather::Compute
Gather Tind type not supported in this build.
info.GetAttr<int64_t>("axis", &axis_).IsOK()
Missing/Invalid 'axis' attribute value
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\gatherbase.h
onnxruntime::GatherBase::GatherBase
indices element out of data bounds, idx=
 must be within the inclusive range [
Missing/Invalid 'axes' attribute value
onnxruntime::UnsqueezeBase::UnsqueezeBase
info.GetAttrs("axes", axes_).IsOK()
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/unsqueeze.h
nullptr != p.output_tensor
'axes' has a duplicate axis
onnxruntime::Unsqueeze::Compute
An axes tensor must be a scalar or a 1-D tensor.
axes_tensor != nullptr
'axes' has an out of range axis
axes_tensor->Shape().NumDimensions() == 0 || axes_tensor->Shape().NumDimensions() == 1
onnxruntime::UnsqueezeBase::PrepareCompute
X != nullptr
Axes input is null
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\unsqueeze.cc
 Got:
num_subgraph_outputs - 1 == num_outputs
Inconsistent shape in loop output for output. 
 Expected:
 but has 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\controlflow\loop.cc
'Loop' node has 
 outputs so the subgraph requires 
Graph in 'body' attribute of Loop should have 
 inputs. Found:
onnxruntime::Loop::Info::Info
static_cast<size_t>(num_subgraph_inputs) == subgraph_inputs.size()
Loop subgraph input 0 has unknown shape: 
'Loop' input 'cond' should be a scalar tensor. Got shape of 
Loop subgraph input 1 has unknown shape: 
onnxruntime::LoopImpl::Initialize
CreateFeedsFetchesManager must be called prior to execution of graph.
onnxruntime::Loop::Compute
'Loop' input 'M' should be a scalar tensor. Got shape of 
feeds_fetches_manager_
onnxruntime::Loop::SetupSubgraphExecutionInfo
info_ == nullptr
session_state
Subgraph SessionState was not found for 'body' attribute.
info.GetAttr<ONNX_NAMESPACE::GraphProto>("body", &proto).IsOK()
SetupSubgraphExecutionInfo should only be called once for each subgraph.
onnxruntime::Loop::Init
onnxruntime::LoopImpl::Execute
onnxruntime::LoopImpl::Execute::<lambda_db6f760d5bdf70bdc754d8ffbb13a528>::operator ()
Loop had zero iterations and the shape of subgraph output 
 was not found. Defaulting to a rank 1 shape of {0}.
last_outputs[j + 1].IsTensor()
All scan outputs MUST be tensors
onnxruntime::LoopImpl::ConcatenateLoopOutput
onnxruntime::LoopImpl::SaveOutputsAndUpdateFeeds
info.GetAttr<ONNX_NAMESPACE::GraphProto>("then_branch", &proto).IsOK()
then_branch
else_branch
onnxruntime::If::Init
num_subgraph_outputs == static_cast<size_t>(num_outputs)
'If' node has 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\controlflow\if.cc
onnxruntime::If::Info::Info
 outputs which doesn't match the subgraph's 
 outputs.
Only tensors, tensor sequence, optional tensor, and optional tensor sequence types are supported
Failed to create output tensor for 
onnxruntime::IfImpl::Execute
Failed to create output tensor for If output 
' attribute.
onnxruntime::If::Compute
onnxruntime::IfImpl::Initialize
Subgraph SessionState was not found for '
info == nullptr
info.GetAttr<ONNX_NAMESPACE::GraphProto>("else_branch", &proto).IsOK()
then_feeds_fetches_manager_ && else_feeds_fetches_manager_
onnxruntime::If::SetupSubgraphExecutionInfo
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/identity_op.h
onnxruntime::IdentityOp<0>::Compute
onnxruntime::IdentityOp<1>::Compute
Unable to get an allocator
attr->has_tp()
Optional op must have a TypeProto in the 'type' attribute if the attribute is present
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\optional\optional_ops.h
onnxruntime::Optional::Optional
OptionalGetElement
OptionalHasElement
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\optional\optional_ops.cc
onnxruntime::PropagateInputOrtValueToFirstOutput
Optional
onnxruntime::Optional::Compute
Only Optional type OrtValues containing Tensors and Sequence Tensors are acceptable
onnxruntime::OptionalGetElement::Compute
Trying to use OptionalGetElement on an optional type OrtValue which contains no data
Invalid input var: 0th dimension != 
Invalid input var: NumDimensions() != 
Invalid input var: 
Invalid input mean: NumDimensions() != 
Invalid input B: 
Invalid input mean: 
Invalid input mean: 0th dimension != 
Invalid input scale: 
 dimension != 
Invalid input B: 0th dimension != 
Invalid input B: NumDimensions() != 
Invalid input scale: 0th dimension != 
Invalid input scale: NumDimensions() != 
is_spatial_
Training mode only supports spatial BN
onnxruntime::BatchNorm<float>::Compute
onnxruntime::BatchNorm<double>::BatchNorm
!is_train_ || ((!saved_mean && !saved_inv_std) || (saved_mean && saved_inv_std))
Invalid number of outputs for BN training
momentum
training_mode
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/batch_norm.h
onnxruntime::BatchNorm<double>::Compute
onnxruntime::BatchNorm<float>::BatchNorm
onnxruntime::utils::mltype_dispatcher_internal::CallableDispatchableHelper::CheckCalledOnce
called_ == 1
Invalid start/ending offset [
) for tensor of length:
onnxruntime::OutputBroadcaster::OutputBroadcaster
start_offset >= 0 && real_end >= 0 && start_offset <= real_end && real_end <= len
InputBroadcaster can only start at span boundary!
onnxruntime::Broadcaster::Broadcaster
onnxruntime::InputBroadcaster::AdvanceBy
offset % span_size_ == 0
 is invalid.
onnxruntime::BroadcastIterator::Append
largest <= 1
Can broadcast 0 by 0 or 1. 
axis == 1 || axis == largest
Attempting to broadcast an axis by a dimension other than 1. 
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/math/element_wise_ops.h
onnxruntime::BroadcastIterator::Init
Broadcast Output range [
) are not at boundary of span with size:
onnxruntime::TensorAllocator::TensorAllocator
start_offset % span_size == 0 && real_end % span_size == 0
!helper.HaveTwoTensorInputs()
ExpandBroadcastLooper should only have a shape for the second input.
Tensor with shape information must be 1 dimensional.
onnxruntime::ExpandBroadcastLooper
onnxruntime::Max_6<float>::Compute
data_n.Shape() == shape
data_1.Shape() == shape
onnxruntime::Mean_6<float>::Compute
onnxruntime::Min_6<float>::Compute
inputCount >= 1
All inputs must have the same shape
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\element_wise_ops.cc
Must have 1 or more inputs
Unsupported X type: 
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Exp<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Log<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Exp<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Exp<double> >::ElementWiseKernel
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/element_wise_ranged_transform.h
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Log<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Log<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Log<double> >::ElementWiseKernel
fmod attribute must be true for floating point types
onnxruntime::Mod::Mod
input_size < std::numeric_limits<std::ptrdiff_t>::max()
onnxruntime::mod_internal::CallModImpl<struct onnxruntime::MLFloat16,void>::operator ()
onnxruntime::UntypedExpand
shape_data_tensor.Shape().GetDims().size() == 1
(fmod == 0) || (fmod == 1)
fmod must have value either 0 or 1
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<__int64> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Floor<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<int> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<__int64> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Ceil<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Reciprocal<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Floor<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Ceil<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Reciprocal<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Sqrt<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Reciprocal<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Reciprocal<double> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Sqrt<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Exp<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Sqrt<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Sqrt<double> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned char> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned short> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<__int64> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned char> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned int> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned __int64> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned short> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned int> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<double> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned __int64> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<signed char> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<int> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<signed char> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<double> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<signed char> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<short> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<signed char> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<int> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<__int64> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<short> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<int> >::ElementWiseKernel
onnxruntime::Sum_6<float>::Compute
input_count >= 1
onnxruntime::BitShift<unsigned char>::BitShift
onnxruntime::Sum_6<double>::Compute
helper.HaveTwoTensorInputs()
BroadcastLooper requires two tensors as input.
onnxruntime::BroadcastLooper
Unsupported Y type: 
onnxruntime::BitShift<unsigned __int64>::Compute::<lambda_91eefeb04a6b3331ce7d69cfb07d2f86>::operator ()
onnxruntime::Expand_8<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::Compute::<lambda_f16ca9954ddc2a61d6a46f1a5f825891>::operator ()
Invalid usage. Input 1 is a shape with no data.
onnxruntime::BitShift<unsigned int>::BitShift
cur_out == end_out
onnxruntime::BitShift<unsigned __int64>::BitShift
onnxruntime::BitShift<unsigned int>::Compute::<lambda_5511229e878cc46c7126f212269f5ffa>::operator ()
Invalid direction value of '
'. Valid values are 'LEFT' or 'RIGHT'.
onnxruntime::BitShift<unsigned char>::Compute::<lambda_8ec84423321713518948f166ef49c9bf>::operator ()
cur1 == end1
onnxruntime::mod_internal::CallModImpl<float,void>::operator ()
onnxruntime::mod_internal::CallModImpl<double,void>::operator ()
p7M}6p7M
info.GetAttr("direction", &direction_).IsOK()
onnxruntime::RNN<float>::RNN
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\rnn\rnn.cc
layout
onnxruntime::RNN<float>::Compute
layout_ == 0
allowed_activations.find(activations_[direction]) != allowed_activations.end()
Batchwise recurrent operations (layout == 1) are not supported. If you need support create a github issue with justification.
activations_.size() == static_cast<size_t>(num_directions)
RNN op: Invalid activation attribute - 
info.GetAttrs("activations", activations_).IsOK()
info.GetAttr("hidden_size", &hidden_size_).IsOK()
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/rnn/rnn.h
allowed_directions.find(direction_) != allowed_directions.end()
info.GetAttr("direction", &direction).IsOK()
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/rnn/rnn_helpers.h
Invalid 'direction' argument of '
onnxruntime::rnn::detail::MakeDirection
'. Must be one of 'forward', 'reverse', or 'bidirectional'.
Invalid data type for LSTM operator of 
onnxruntime::DeepCpuLstmOp::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\rnn\deep_cpu_lstm.cc
LSTM operator does not support double yet
activation_func_names.size() == static_cast<size_t>(num_directions_) * 3
onnxruntime::DeepCpuLstmOp::PrePack
sigmoid
info.GetAttr("hidden_size", &int64_value).IsOK() && int64_value > 0
clip_ > 0.f
onnxruntime::LSTMBase::LSTMBase
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\rnn\lstm_base.h
activation_func_names.size() == static_cast<size_t>(num_directions_) * 2
GRU operator does not support double yet
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/rnn/deep_cpu_gru.h
info.GetAttr("linear_before_reset", &int64_value).IsOK()
onnxruntime::DeepCpuGruOp::DeepCpuGruOp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\rnn\deep_cpu_gru.cc
Invalid data type for GRU operator of 
onnxruntime::DeepCpuGruOp::Compute
onnxruntime::DeepCpuGruOp::ComputeImpl
onnxruntime::Tensor::MutableDataAsSpan
C + (M * ldc - (ldc - N)) <= C_end
cur + size <= end
A + (M * lda - (lda - K)) <= A_end
B + (N * ldb - (ldb - K)) <= B_end
lda >= K && ldb >= K && ldc >= N
onnxruntime::rnn::detail::ComputeGemm
onnxruntime::rnn::detail::SafeRawPointer
onnxruntime::rnn::detail::SafeRawConstPointer
offset + size <= size_t(span.size())
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\reshape_helper.h
, requested shape:
requested_shape[i] >= -1
onnxruntime::ReshapeHelper::ReshapeHelper
A dimension cannot be less than -1, got 
size != 0 && (input_shape.Size() % size) == 0
gsl::narrow_cast<int64_t>(input_shape.Size()) == size
The dimension with value zero exceeds the dimension size of the input tensor.
i < input_shape.NumDimensions()
At most one dimension can be -1.
unknown_dim == -1
The input tensor cannot be reshaped to the requested shape. Input shape:
!allow_zero
Attribute shape is not set.
onnxruntime::Reshape_1::Reshape_1
onnxruntime::Reshape::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/reshape.h
A shape tensor must be a vector tensor.
shapeTensor->Shape().NumDimensions() == 1
Unsupported input data type of 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\trilu.cc
Input tensor should have a rank of at least 2
IsScalarOr1ElementVector(k)
onnxruntime::Trilu::Compute
Trilu
k should be a 1-D or 0-D tensor.
onnxruntime::Trilu::Trilu
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\trilu.h
upper
info.GetAttr<int64_t>("upper", &temp).IsOK()
Attribute name and type don't match for '
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::ThresholdedRelu<float> >::Compute
'is defined.
No attribute with name:'
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Selu<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Sigmoid<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Sigmoid<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Softplus<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Softsign<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Tanh<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Celu<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Tanh<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::ThresholdedRelu<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Celu<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Elu<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::HardSigmoid<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Elu<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::LeakyRelu<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::HardSigmoid<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::LeakyRelu<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Relu<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Relu<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Relu<signed char> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Selu<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Relu<int> >::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/activation/activations.h
onnxruntime::functors::HardSigmoid<float>::Init
onnxruntime::functors::Selu<float>::Init
onnxruntime::functors::ElementWiseRangedTransform<float>::Create
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\activation\activations.cc
unknown kernel type
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops/cpu/activations.h
onnxruntime::functors::ParametricSoftplus<float>::Init
onnxruntime::functors::ScaledTanh<float>::Init
onnxruntime::SliceIteratorBase::Init
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/utils.h
dims.size() == starts.size() && dims.size() == extents_.size() && dims.size() >= steps.size()
dims.size() == extents.size() && dims.size() >= steps.size()
onnxruntime::SliceSkips::SliceSkips
Axis tensor should be 0D or 1D
Axis tensor should be of type `int32_t` or `int64_t`
Axis tensor must be provided to the CumSum op
onnxruntime::CumSum<int>::Compute
onnxruntime::CumSum<double>::Compute
Cannot apply CumSum operator on a scalar
onnxruntime::CumSum<float>::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\cumsum.cc
onnxruntime::CumSum<__int64>::Compute
dims.size() == steps.size()
onnxruntime::WritableSliceIterator<int>::Init
dims.size() == extents_.size()
steps.size()=
onnxruntime::WritableSliceIterator<__int64>::Init
extents.size()=
dims.size()=
dims.size() == starts.size()
starts.size()=
onnxruntime::WritableSliceIterator<double>::Init
onnxruntime::WritableSliceIterator<float>::Init
onnxruntime::Softmax<double>::ComputeImplOpset13
onnxruntime::Softmax<float>::ComputeImplOpset13
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\softmax.cc
Hardmax inputs N, D and N * D must be < 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\hardmax.cc
onnxruntime::Hardmax<float>::Compute
] is not supportted!
coordinate_transform_mode:[
mode attribute is 
onnxruntime::UpsampleBase::StringToUpsampleMode
(default) or 
. It can only be 
exclude_outside can be set to 1 only when mode is CUBIC. Current mode is set to 
`tf_half_pixel_for_nn` is deprecated since opset 13, 
cubic_coeff_a
 model uses the deprecated attribute
yet this opset 
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/upsample.h
info.GetAttrs<float>("scales", scales_).IsOK()
info.GetAttr<std::string>("mode", &mode).IsOK()
onnxruntime::UpsampleBase::UpsampleBase
'Cubic' mode only support 2-D inputs ('Bicubic') or 4-D inputs with the corresponding outermost 2 scale values being 1 in the 
scales.size() == 2 || (scales.size() == 4 && scales[0] == 1 && scales[1] == 1)
'Linear' mode only support:
  * 2-D inputs or
  * 3-D inputs ('Bilinear', 'Trilinear') or
  * 4-D inputs with the corresponding outermost 2 scale values being 1 or the corresponding outermost and innermost scale values being 1 or
  * 5-D inputs with the corresponding outermost 2 scale values being 1in the 
scales.size() == 2 || (scales.size() == 4 && scales[0] == 1 && scales[1] == 1) || (scales.size() == 4 && scales[0] == 1 && scales[3] == 1) || scales.size() == 3 || (scales.size() == 5 && scales[0] == 1 && scales[1] == 1)
Resize operator
Upsample operator
Scale value should be greater than 0.
scale > 0
scale >= 1
onnxruntime::UpsampleBase::ScalesValidation
onnxruntime::UpsampleBase::StringToNearestMode
Scale value should be greater than or equal to 1.
] is not supported!
nearest_mode:[
onnxruntime::UpsampleBase::StringToCoordinateTransformationMode
onnxruntime::UpsampleBase::ParseScalesData
scales size should be greater than 0.
scales_size > 0
Only works on matrices with two dimensions.
fast_shape.size() == 2
must be overloaded.
onnxruntime::ValidateMustBeOverloaded
last_loop_size > 0
projected_index.size() > 0
onnxruntime::ResultsNoTransposePrepareForReduce::ValidateNotEmpty
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\reduction\reduction_ops.cc
last_loop_red_size > 0
Can't reduce on dim with value of 0 if 'keepdims' is false. Invalid output shape would be produced. input_shape:
onnxruntime::ValidateKeepDims
An axes tensor must be a vector tensor.
axes_tensor->Shape().NumDimensions() == 1
onnxruntime::ValidateNoTransposeReduce
onnxruntime::ValidateCommonFastReduce
Reduction on all axes, output size should be 1.
count == 1
onnxruntime::ValidateFastReduceKRK
fast_shape[0] * fast_shape[2] == output.Shape().Size()
fast_shape[1] == output.Shape().Size()
fast_shape.size() == 3
fast_shape[0] == output.Shape().Size()
onnxruntime::ValidateFastReduceRK
onnxruntime::ValidateFastReduceKR
Output size mismatch.
onnxruntime::ReduceKernelBase<1>::ReduceKernelBase
onnxruntime::ReduceKernelBase<0>::ReduceKernelBase
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/reduction/reduction_ops.h
info.GetAttr("keepdims", &keepdims).IsOK()
Invalid 'pads' attribute value
onnxruntime::PadBase::PadBase
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\padbase.h
Invalid 'mode' attribute value
Pads tensor should be a 1D tensor of shape [2 * input_rank] or a 2D tensor of shape [1, 2 * input_rank]
pads_tensor_dims.size() == 1 || (pads_tensor_dims.size() == 2 && pads_tensor_dims[0] == 1)
pads_tensor.IsDataType<int64_t>()
onnxruntime::Pad::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\pad.cc
Pads tensor should be an INT64 tensor
Unexpected mode of 
onnxruntime::PadValueFromFloat
Cannot use 'edge' mode to pad dimension with a value of 0. Input shape:
Cannot use 'reflect' mode to pad dimension with a value of 0. Input shape:
Value tensor should be a 1D tensor of size 1 with the same type as that of the input tensor
value_tensor->DataType() == data_type && value_tensor->Shape().Size() == 1
Pads tensor size should be equal to twice the input dimension count 
pads_size == 2 * data_rank
data_rank * 2 == pads.size()
onnxruntime::PadImpl
'pads' has wrong number of values
Input tensor has no dimensions
data_rank > 0
onnxruntime::PadInputWithDimValueOfZero
last dimension of indices must not be larger than rank of input tensor
indices tensor data type not supported
onnxruntime::GatherND::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\gather_nd.cc
GatherNDBase PrepareForCompute: Input count mismatch
input_tensor != nullptr && indices_tensor != nullptr
invalid index found, index = 
indices tensor must has rank larger than 0
onnxruntime::NonZero<int>::Compute
onnxruntime::NonZero<__int64>::Compute
failed to get first output!
onnxruntime::NonZero<unsigned char>::Compute
onnxruntime::NonZero<bool>::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\nonzero_op.cc
X input is required!
onnxruntime::NonZero<float>::Compute
Null input ptr
updates tensor should have shape equal to indices.shape[:-1] + data.shape[indices.shape[-1]:]. 
onnxruntime::ScatterNDBase::PrepareForCompute
, data shape: 
updates shape: 
input shape: 
input tensor and indices tensor must has rank larger than 0. 
, indices shape: 
onnxruntime::ScatterND::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\scatter_nd.cc
invalid indice found, indice = 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\scatter.cc
onnxruntime::Scatter<struct onnxruntime::TypeList<float,double,__int64,unsigned __int64,int,unsigned int,short,unsigned short,signed char,unsigned char,struct onnxruntime::MLFloat16,struct onnxruntime::BFloat16,bool,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > >::Scatter
Indices type is not supported.
Indices dim=
 at pos=
 is greater than input dim=
Indices must have the same rank as Input. Indices rank=
. Input rank=
Indices vs updates dimensions differs at position=
Indices and updates must have the same rank
data type is different from updates type
DepthToSpace op: only 'DCR' and 'CRD' modes are supported
DepthToSpace requires input depth to be a multiple of (block_size * blok_size)
SpaceToDepth requires input width to be a multiple of block_size
SpaceToDepth requires input height to be a multiple of block_size
SpaceDepth ops require a 4-D input. Provided rank: 
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/space_depth_ops.h
onnxruntime::SpaceDepthBase::SpaceDepthBase
info.GetAttr("blocksize", &blocksize_).IsOK()
Attribute blocksize is not set.
input count mismatch
onnxruntime::DepthToSpace::DepthToSpace
Unsupported input type in DepthToSpace op: 
onnxruntime::DepthToSpace::Compute
Unsupported input type in SpaceToDepth op: 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\space_depth_ops.cc
onnxruntime::SpaceToDepth::Compute
GatherElements op: Data type of input 'data' should match the data type of the output
GatherElements op: 'indices' shape should have values within bounds of 'data' shape. Invalid value in indices shape is: 
GatherElements op: Rank of input 'data' needs to be equal to rank of input 'indices'
GatherElements op: Cannot operate on scalar input
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\gather_elements.h
onnxruntime::GatherElements::GatherElements
onnxruntime::core_impl::<lambda_380759b69dafbe1155f8c031dc4b985c>::operator ()
GatherElements op: Value in indices must be within bounds [
]. Actual value is 
onnxruntime::core_impl::<lambda_9931dee8adac3330994d4bf96a89cbe1>::operator ()
onnxruntime::core_impl::<lambda_4038c6c41ccc2af39d2bcb85c83daf02>::operator ()
onnxruntime::core_impl::<lambda_68f04e9ae619f08036d765527117f897>::operator ()
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\gather_elements.cc
Tile doesn't support string type yet
'repeat' input tensor must have the same length as the 'input' tensor
'repeat' input tensor must be 1 dimensional
the tensor to be tiled using Tile OP must be atleast 1 dimensional
Input count of Tile OP mismatch, the second one is empty
Input count of Tile OP mismatch, the first one is empty
Tile doesn't have an implementation yet for the type: 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\tile.cc
onnxruntime::Tile::Compute
!input_tensor.IsDataType<std::string>()
onnxruntime::Squeeze::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/squeeze.h
onnxruntime::SqueezeBase::ComputeOutputShape
input_shape[i] == 1
Dimension of input 
 must be 1 instead of 
. shape=
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/split.h
onnxruntime::SplitBase::SplitBase
std::all_of(split_sizes_.cbegin(), split_sizes_.cend(), [](int64_t value) { return value >= 0; })
Invalid value in 'split' attribute. All values must be > 0
 Input shape=
 Num entries in 'split' (must equal number of outputs) was 
 Sum of sizes in 'split' (must equal size of selected axis) was 
Input cannot be split evenly on selected axis. Input shape=
 Axis=
 NumOutputs=
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\split.cc
onnxruntime::Split::Compute
Split operator does not support 
Cannot split using values in 'split' attribute. Axis=
onnxruntime::Split::ComputeImpl
split_tensor->Shape().NumDimensions() == 1
An split tensor must be a vector tensor.
has_starts && has_ends && attr_starts_.size() == attr_ends_.size()
Missing or invalid starts and ends attribute
'step' value cannot be 0
'axes' has duplicates
'axes' has an axis outside of the tensor dimension count
!has_axes || attr_axes_.size() == attr_starts_.size()
Invalid axes attribute, axes attribute (if present) should have the same size as starts/ends attributes
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/slice.h
onnxruntime::SliceBase::SliceBase
onnxruntime::SliceBase::Compute
Cannot slice scalars
Data type for starts and ends inputs' is not supported in this build. Got 
Starts and steps shape mismatch
Starts and axes shape mismatch
Starts and ends shape mismatch
Ends must be a 1-D array
onnxruntime::SliceBase::FillVectorsFromInput
Starts must be a 1-D array
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\slice.cc
onnxruntime::SliceBase::PrepareForCompute
onnxruntime::SliceIteratorBase::CopyInnermostAxisNonSolitaryInnerStep
Unexpected element size of 
onnxruntime::SliceImpl::<lambda_d22eb87071c64b2326104c92538d0533>::operator ()
output == output_end
onnxruntime::SliceImpl::<lambda_3062cbfc3de47173984232698e8a1305>::operator ()
onnxruntime::SliceImpl::<lambda_1ca539266a3f7de690711da52dd46463>::operator ()
onnxruntime::SliceImpl::<lambda_e09d0c7995e76488be3df01db27d6997>::operator ()
onnxruntime::SliceImpl::<lambda_63257b9ef94d1e010b58918661d013c1>::operator ()
onnxruntime::ConcatBase::PrepareForCompute
new_axis
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\concatbase.h
onnxruntime::ConcatBase::ConcatBase
Must have valid 'axis' attribute
onnxruntime::ConcatBase::ComputeImpl
Data type mismatch
Non concat axis dimensions must match: Axis 
 has mismatched dimensions of 
input_rank == reference_rank
Ranks of input data are different, cannot concatenate them. expected rank: 
 got: 
Cannot concatenate scalars
input != nullptr
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\concat.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/copy.h
onnxruntime::DispatchStridedCopy
dst.DataType() == src.DataType()
src and dst types must match
onnxruntime::StridedCopy
dst_strides.size() == src_strides.size() && src_strides.size() == copy_shape.size() && !copy_shape.empty()
src and dst must have same shape and not be rank 0.
onnxruntime::StridedCopy::<lambda_6b8e74af1b2b937274f6d872f7931a4b>::operator ()
counter.current_offset == last
onnxruntime::StridedCopy::<lambda_34078c7540b6602389f40bae3a86d150>::operator ()
last >= first
onnxruntime::StridedCopy::<lambda_e8aeb42bf0aa89a79a408edee79cf184>::operator ()
onnxruntime::StridedCopy::<lambda_dada8d57643005b7e2297cabdd6baa97>::operator ()
onnxruntime::StridedCopy::<lambda_22dae262e3744994c648a6a28f79603c>::operator ()
onnxruntime::StridedCopy::<lambda_8687679c83f6820abda420b1312e974a>::operator ()
onnxruntime::StridedCopy::<lambda_f43464c7a9a747b50c25ae66c092bf51>::operator ()
onnxruntime::StridedCopy::<lambda_a99d8d5e4f7ccd722937dd9eb05ffd73>::operator ()
onnxruntime::StridedCopy::<lambda_31433bd7aba335c4dcfc2d21c7b0556b>::operator ()
onnxruntime::StridedCopy::<lambda_3ec6b12ad4cd55e04d715e409cf94caa>::operator ()
onnxruntime::Dropout<double,float>::Compute
onnxruntime::Dropout<float,double>::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/dropout_op.h
onnxruntime::Dropout<float,float>::Compute
!mask || mask->Shape() == X_shape
X and mask should have the same shape
onnxruntime::Dropout<double,double>::Compute
0.0f <= ratio_value && ratio_value < 1.0f
ratio must be in the range [0, 1)
onnxruntime::`anonymous-namespace'::GetRatioOrDefault
ratio_tensor->Shape().Size() == 1
ratio input should have a single value.
Input is expected to have four dimensions corresponding to [N,C,H,W]
info.GetAttr<int64_t>("normalize_variance", &normalize_variance_).IsOK()
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/mean_variance_normalization.h
onnxruntime::MeanVarianceNormalization_0<float>::MeanVarianceNormalization_0
info.GetAttr<int64_t>("across_channels", &across_channels_).IsOK()
beta_ > 0.0f
info.GetAttr<float>("beta", &beta_).IsOK()
alpha_ > 0.0f
info.GetAttr<float>("alpha", &alpha_).IsOK()
size_ % 2 == 1
size_ > 0
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/lrn.h
onnxruntime::LRN<float>::LRN
info.GetAttr<int64_t>("size", &size).IsOK()
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\nn\lrn.cc
onnxruntime::LRN<float>::Compute
X->Shape().NumDimensions() == 4
onnxruntime::Flatten::Compute
gsl::narrow_cast<int64_t>(X_shape.NumDimensions()) >= axis
The rank of input tensor must be >= axis
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/flatten.h
onnxruntime::Flatten::Flatten
zero_point_ptr == nullptr || (zero_point_ptr->Shape().NumDimensions() == 1 && zero_point_ptr->Shape()[0] == broadcast_dim)
x_zero_point must be null or 1D tensor with size 
scale.Shape().NumDimensions() == 1 && scale.Shape()[0] == broadcast_dim
scale must be 1D tensor with size 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\quantize_linear.cc
onnxruntime::PrepareForQDQ
zero_point_ptr == nullptr || IsScalarOr1ElementVector(zero_point_ptr)
x_zero_point must be null or a scalar or 1D tensor or size 1.
onnxruntime::DequantizeLinear<int>::Compute
zero_point == nullptr || std::all_of(zero_point, zero_point + x_zero_point->Shape().Size(), [](int32_t zp) { return zp == 0; })
DequantizeLinear with type int32 should have no zero point or all zero points should be 0
info.GetAttr<int64_t>("transB", &temp).IsOK()
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\gemm_base.h
onnxruntime::GemmBase::GemmBase
info.GetAttr<int64_t>("transA", &temp).IsOK()
M_ >= 0 && K_ > 0 && N_ >= 0
Gemm: Invalid bias shape for broadcast
GEMM: Dimension mismatch, W: 
right.NumDimensions() == 2
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\gemm_helper.h
onnxruntime::GemmHelper::GemmHelper
left.NumDimensions() == 2 || left.NumDimensions() == 1
onnxruntime::GemmBroadcastBias
c_shape != nullptr
c_shape is required if c_data is provided
left_num_dims and right_num_dims must be >= 1
onnxruntime::MatMulComputeHelper::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/math/matmul_helper.h
MatMul dimension mismatch
left operand cannot broadcast on dim 
right operand cannot broadcast on dim 
num_dims_with_pad != num_output_dims
M_ == 1 && N_ == 1 was false
num_dims_with_pad - 1 != num_output_dims
onnxruntime::MatMul<float>::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\matmul.cc
m_dims_with_pad - 2 != num_output_dims
onnxruntime::MatMul<int>::Compute
onnxruntime::MatMul<__int64>::Compute
onnxruntime::MatMul<double>::Compute
min_ <= max_
onnxruntime::clip_internal::Clip_6Base<float>::Clip_6Base
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/math/clip.h
onnxruntime::Clip::ComputeImpl<float>::operator ()
min should be a scalar.
min->Shape().IsScalar()
onnxruntime::Clip::ComputeImpl<unsigned __int64>::operator ()
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\clip.cc
max should be a scalar.
max->Shape().IsScalar()
onnxruntime::Clip::ComputeImpl<__int64>::operator ()
onnxruntime::Clip::ComputeImpl<unsigned char>::operator ()
onnxruntime::Clip::ComputeImpl<signed char>::operator ()
onnxruntime::Clip::ComputeImpl<double>::operator ()
stoull argument out of range
invalid stod argument
stod argument out of range
invalid stoll argument
stoll argument out of range
invalid stoull argument
Attribute to is not set.
onnxruntime::`anonymous-namespace'::Cast::Cast
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\cast_op.cc
snprintf() failed with return value: 
snprintf_result > 0
onnxruntime::`anonymous-namespace'::CastToString
Failed to write value with snprintf().
snprintf_result > 0 && gsl::narrow_cast<size_t>(snprintf_result) == buffer_span.size() - 1
invalid expand shape
Missing 'equation' attribute
info.GetAttr<std::string>("equation", &equation_).IsOK()
onnxruntime::Einsum::Einsum
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\einsum.h
Einsum op: There must be atleast one input
There was a problem acquiring temporary memory allocator in Einsum op
onnxruntime::Einsum::DeviceCompute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\einsum.cc
 is not supported yet
Einsum op: An implementation for the input type 
Unknown AutoPadType String
onnxruntime::StringToAutoPadType
No kernel shape is set.
info.GetAttrs<int64_t>("kernel_shape", kernel_shape).IsOK()
onnxruntime::PoolAttributes::PoolAttributes
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/pool_attributes.h
info.GetAttr<std::string>("auto_pad", &auto_padding).IsOK()
info.GetAttr<int64_t>("count_include_pad", &temp).IsOK()
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/pool_base.h
Input dimension cannot be less than 3.
onnxruntime::PoolBase::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\nn\pool.cc
Unsupported pooling size.
kernel_shape num_dims is not compatible with X num_dims.
info.GetAttr("storage_order", &storage_order).IsOK()
kernel_shape[dim] > 0
Pad should be smaller than kernel.
pads[dim] < kernel_shape[dim] && pads[dim + kernel_shape.size()] < kernel_shape[dim]
strides.size() == kernel_shape.size()
Dilations dimensions should match kernel shape
dilations.size() == kernel_shape.size()
Invalid input shape. Only N can be zero. Got:
input_shape.Size() > 0 || input_shape[0] == 0
onnxruntime::PoolAttributes::SetOutputSize
input_dims.size() >= 2
onnxruntime::PoolAttributes::InferOutputSize
Unsupported AutoPad Type.
onnxruntime::PoolAttributes::ComputeSizePadDilations
info.GetAttr<int64_t>("p", &p_).IsOK()
onnxruntime::PoolProcessContext::init
onnxruntime::Pool<float,class onnxruntime::LpPool>::Compute
Unsupported pooling size : 
onnxruntime::MaxPoolV8::ComputeImpl
onnxruntime::OneHotOp<__int64,__int64,__int64>::Compute
Invalid argument for depth; it's not a scalar.
Invalid argument for values; either it's rank is more than 1 or it has more than 2 elements
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\onehot.cc
Depth is negative.
onnxruntime::OneHotOp<float,__int64,__int64>::Compute
onnxruntime::OneHotOp<__int64,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,__int64>::Compute
onnxruntime::OneHotOp<float,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,__int64>::Compute
onnxruntime::OneHotOp<__int64,float,__int64>::Compute
onnxruntime::OneHotOp<int,float,int>::Compute
onnxruntime::OneHotOp<int,float,float>::Compute
onnxruntime::OneHotOp<float,float,float>::Compute
onnxruntime::OneHotOp<__int64,int,float>::Compute
onnxruntime::OneHotOp<__int64,float,float>::Compute
onnxruntime::OneHotOp<__int64,float,int>::Compute
op_kernel_info.GetAttr<int64_t>("k", &k_temp).IsOK()
onnxruntime::TopkOpset9ConstructorCommon
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\top_k.cc
k_temp > 0
op_kernel_info.GetAttr<int64_t>("axis", &axis_temp).IsOK()
onnxruntime::TopkOpset10ConstructorCommon
onnxruntime::TopkOpset11ConstructorCommon
op_kernel_info.GetAttr<int64_t>("largest", &largest_temp).IsOK()
sorted
op_kernel_info.GetAttr<int64_t>("sorted", &sorted_temp).IsOK()
] should not be greater than specified axis dim value [
k argument [
input count mismatch, expected 1 input - the tensor to be processed
input count mismatch, expected 2 inputs - the tensor to be processed and a tensor containing k value
k tensor should be a 1D tensor of size 1
value of k must not be negative
output count mismatch, expected 2 outputs to be present for TopK operator
Unique
Unsupported tensor type of 
start in Range operator should be scalar like tensor, yet got shape:
limit in Range operator should be scalar like tensor, yet got shape:
delta in Range operator should be scalar like tensor, yet got shape:
delta in Range operator can not be zero!
SequenceLength
SequenceAt
onnxruntime::GetSeqIdx
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\sequence\sequence_ops.cc
) specified for sequence of size (
Invalid sequence index (
SequenceEmpty
Unsupported 'dtype' value: 
onnxruntime::SequenceEmpty::Compute
SequenceInsert
onnxruntime::CreateCopyAndAppendCpuTensor
), input tensor data type (
Data type of the input tensor MUST be same as that of the input sequence. Sequence data type (
onnxruntime::SequenceInsert::Compute
SequenceErase
onnxruntime::SequenceErase::Compute
SequenceConstruct
num_inputs >= 1
onnxruntime::SequenceConstruct::Compute
Violation of the requirment that all input tensors must have the same data type.
SplitToSequence
SplitToSequence operator does not support 
) != split_dim_size (
split_size_sum (
Invalid data type for split tensor 
onnxruntime::GetScalarSplitInput
onnxruntime::GetSplitSizesInput
Split should be > 0
split_scalar > 0
onnxruntime::SplitToSequence::ComputeImpl
Invalid value in 'split' input. All values must be >= 0
std::all_of(split_sizes.cbegin(), split_sizes.cend(), [](int64_t value) { return value >= 0; })
ConcatFromSequence
Got nullptr for sequence input.
onnxruntime::ConcatFromSequence::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\sequence\concat_from_sequence.cc
Invalid input shape: 
group count is <= 0
 num_input_channels: 
 filter_number: 
filter number not equal to input channel number.
Input channels is not divisible by group.
onnxruntime::ConvTransposeAttributes::PrepareForCompute
A Conv/ConvTranspose node has both 'auto_pad' and 'pads' attributes
auto_pad == AutoPadType::NOTSET
onnxruntime::ConvAttributes::ConvAttributes
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/conv_attributes.h
 kernel_shape: 
kernel_shape num_dims is not compatible with W num_dims.
kernel_shape is not compatible with W shape.
X num_dims does not match W num_dims.
 group: 
onnxruntime::ConvTranspose<float>::DoConvTranspose
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\nn\conv_transpose.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/conv_transpose_attributes.h
dim_size > 0
onnxruntime::ConvTransposeAttributes::ComputePadsAndOutputShape
*out_size >= 0
onnxruntime::ConvTransposeAttributes::ComputeTransposePadAndOutputShape
Dilation not supported for AutoPadType::SAME_UPPER or AutoPadType::SAME_LOWER.
ComputePad: pad type not supported.
onnxruntime::ComputePadAndOutputShape
 kernel channels: 
Input channels C is not equal to kernel channels * group.
Output channels M is not divisible by group.
Not enough elements in strides. Expected: 
onnxruntime::Conv<float>::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\nn\conv.cc
Not enough elements in kernel shape. Expected: 
Not enough elements in dilations. Expected: 
Not enough elements in pads. Expected: 
onnxruntime::ConvAttributes::InferOutputShape
info.GetAttrs<int64_t>("kernel_shape", kernel_shape_).IsOK()
onnxruntime::MaxUnpool::MaxUnpool
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/unpool.h
kernel_shape_[dim] > 0
pads_[dim] < kernel_shape_[dim] && pads_[dim + kernel_shape_.size()] < kernel_shape_[dim]
strides_.size() == kernel_shape_.size()
onnxruntime::MaxUnpool::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\nn\Unpool.cc
Index tensor shape should be same as that of the input data tensor to unpool.
Shape must be 1 dimensional as it's tensor data of a shape
 inferred output shape:
output_shape is smaller than minimum required. output_shape:
onnxruntime::Det<float>::Compute
Matrix dimensions are not equal. Square matrix is expected
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\det.cc
axes_right_stride >= 0 && static_cast<uint64_t>(axes_right_stride) < std::numeric_limits<size_t>::max()
Compress
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\compress.cc
onnxruntime::Compress::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/controlflow/scan_utils.h
onnxruntime::scan::detail::OutputIterator::GetOutput
final_output_mlvalue_
Attempt to retrieve final output before it was set.
info.GetAttr<int64_t>("num_scan_inputs", &num_scan_inputs_).IsOK()
num_scan_inputs
scan_output_directions
scan_input_directions
 but expected 
scan_input_axes
gsl::narrow_cast<int64_t>(input_axes_.size()) == num_scan_inputs_
Number of entries in 'scan_input_axes' was 
Number of entries in 'scan_output_axes' was 
scan_output_axes
onnxruntime::Scan<9>::SetupSubgraphExecutionInfo
gsl::narrow_cast<int64_t>(output_axes_.size()) == num_scan_outputs
onnxruntime::Scan<9>::Compute
feeds_fetches_manager_ && info_
 dimensions or more but input had shape of 
onnxruntime::ScanImpl::Initialize
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\controlflow\scan_9.cc
onnxruntime::Scan<9>::Init
onnxruntime::ScanImpl::Execute
output_mlvalue
Invalid value in scan_output_axes for output 
. Output tensor rank was 
onnxruntime::ScanImpl::TransposeOutput
Outputs from Scan are not optional and should never be null.
Invalid scan input:
 Expected 
' dimension 
 has length of 
Scan inputs have inconsistent sequence lengths. Previous value was 
 but input '
. Input tensor rank was 
onnxruntime::ScanImpl::ValidateInput
Invalid value in scan_input_axes for input 
 outputs but Scan expects 
onnxruntime::ScanImpl::SetupInputs
onnxruntime::ScanImpl::AllocateOutputTensors
Subgraph in 'body' produces 
Output OrtValue has not been created for loop state variable output 
onnxruntime::ScanImpl::CreateLoopStateVariables
x_ptr != nullptr
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\dynamicquantizelinear.cc
onnxruntime::DynamicQuantizeLinear<unsigned char>::Compute
onnxruntime::ReverseSequenceOp::ReverseSequenceOp
info.GetAttr<int64_t>("batch_axis", &batch_axis).IsOK()
sequence_lens shape must be {batch_size}. Got:
. batch_size=
info.GetAttr<int64_t>("time_axis", &time_axis).IsOK()
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/reverse_sequence.h
Invalid batch_axis of 
. Must be 0 or 1
Invalid time_axis of 
batch_axis < 2
time_axis and batch_axis must have different values but both are 
time_axis < 2
batch_axis != time_axis
onnxruntime::ReverseSequenceOp::Compute
Unknown tensor type of 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\reverse_sequence.cc
. Value must be in range [0,
Invalid sequence length: 
Null rois_ptr
Null input X ptr
Number of dimensions for batch indices should be exactly 1
Null batch_indices_ptr
Second dimension for rois should be exactly 
Number of dimensions for rois should be exactly 
First dimension (num_rois) of batch_indices and rois don't match
 specified. It should be either avg or max
onnxruntime::RoiAlignBase::RoiAlignBase
Invalid mode of value 
Sampling ratio should be >=0, but it was 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\object_detection\roialign.h
sampling_ratio_ >= 0
onnxruntime::IsInf::IsInf
Failed to obtain detect_positive
Failed to obtain detect_negative
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\isinf.cc
QLinearConv : filter scale shape invalid
IsScalarOr1ElementVector(Y_scale)
prepacked_buffers[0].get() == nullptr
IsValidQuantParam(W_scale, M)
onnxruntime::QLinearConv::Compute
onnxruntime::QLinearConv::UseSharedPrePackedBuffers
IsScalarOr1ElementVector(X_zero_point)
QLinearConv : input zero point must be a scalar or 1D tensor of size 1
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\nn\qlinearconv.cc
onnxruntime::QLinearConv::ComputeOffset
IsScalarOr1ElementVector(Y_zero_point)
QLinearConv : result zero point must be a scalar or 1D tensor of size 1
IsValidQuantParam(W_zero_point, M)
QLinearConv : filter zero point shape invalid
W_zero_point_data[i] == W_zero_point_value
QLinearConv : zero point of per-channel filter must be same
IsScalarOr1ElementVector(X_scale)
QLinearConv : input scale must be a scalar or 1D tensor of size 1
QLinearConv : result scale must be a scalar or 1D tensor of size 1
onnxruntime::QLinearConv::ComputeOutputScale
IsScalarOr1ElementVector(X_Zero_Point)
Must be a scalar or 1D tensor or size 1.
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\nn\conv_integer.cc
onnxruntime::ConvInteger::Compute
IsScalarOr1ElementVector(W_Zero_Point)
Non per-tensor quantization is not supported now.
MatmulInteger : input1 zero point must be a scalar or 1D tensor of size 1
onnxruntime::MatMulInteger::Compute
IsScalarOr1ElementVector(a_zero_point)
MatmulInteger : B zero point is not valid
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\matmul_integer.cc
IsBQuantParamSupported(b_zero_point->Shape(), b ? b->Shape() : b_shape_)
onnxruntime::MatMulComputeHelper::Compute::<lambda_74826040142e475fff8a25f7fa284d9f>::operator ()
Per-column quantization parameter of batched matrix should have same dimension as the matrix,and its size by K should be equal to the matrix's size.
QLinearMatmul : input zero point must be a scalar or 1D tensor of size 1
onnxruntime::QLinearMatMul::Compute
IsScalarOr1ElementVector(a_offset)
QLinearMatmul : weight zero point must be a scalar, 1D tensor of size 1, or last to second dimension is 1
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\quantize_linear_matmul.cc
QLinearMatmul : result zero point must be a scalar or 1D tensor of size 1
IsBQuantParamSupported(b_offset->Shape(), b ? b->Shape() : b_shape_)
QLinearMatmul : input scale must be a scalar or 1D tensor of size 1
IsScalarOr1ElementVector(y_offset)
QLinearMatmul : weight scale must be a scalar, 1D tensor of size 1, or last to second dimension is 1
IsScalarOr1ElementVector(a_scale)
QLinearMatmul : result scale must be a scalar or 1D tensor of size 1
IsBQuantParamSupported(b_scale->Shape(), b ? b->Shape() : b_shape_)
IsScalarOr1ElementVector(y_scale)
Conversion Error
StringNormalizer
:Please, install necessary language-pack-XX and configure locales
Conversion Error
onnxruntime::string_normalizer::Locale::Locale
Failed to construct locale with name:
en-US
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\nn\string_normalizer.cc
attribute is_case_sensitive is not set
is_case_sensitive
case_change_action
onnxruntime::StringNormalizer::StringNormalizer
LOWER
attribute case_change_action is not set
bad conversion
UPPER
locale
attribute case_change_action has invalid value
Empty stopwords not allowed
stopwords
Duplicate stopwords not allowed
!sw.empty()
Stopword contains invalid utf8 chars
p.second
Single dimension value must be greater than 0
wstr != wconv_error
Input contains invalid utf8 chars
Input dimensions are either[C > 0] or [1][C > 0] allowed
 Output dim value: 
Dimension: 
 Input dim value: 
Input dim is zero but required output dim is non-zero. 
Cannot scale 0 by any factor to generate a non-zero value. 
onnxruntime::UpsampleBase::ParseScalesDataFromOutputSize
output_dims[i] == 0
Upsample: input/output value is nullptr
Resize: input/output value is nullptr
Upsample: input/output value's dimension mismatch
Resize: input/output value's dimension mismatch
Upsample: input shape needs to be at least a single dimension.
Resize: input shape needs to be at least a single dimension
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\upsample.cc
onnxruntime::Upsample<float>::Compute
roi_input_idx_ > 0
Invalid roi input index.
sizes == nullptr
Only one of scales or sizes must be provided as input.
sizes != nullptr && sizes->Shape().Size() != 0
Either scales or sizes MUST be provided as input.
X->Shape().GetDims().size() == output_dims.size()
Resize: input tensor's rank does not match the output tensor's rank.
onnxruntime::Upsample<unsigned char>::Compute
onnxruntime::Upsample<int>::Compute
onnxruntime::Upsample<unsigned char>::BaseCompute
onnxruntime::UpsampleNearest
onnxruntime::Upsample<float>::BaseCompute
output_dims.size() == dims.size()
Rank of input and output tensor should be same.
Upsample: input tensor's dimension does not match the scales.
Resize: input tensor's dimension does not match the scales.
: 'Linear' mode only support 2-D inputs or 3-D inputs ('Bilinear', 'Trilinear') or 4-D inputs or 5-D inputs with the corresponding outermost 2 scale values being 1.
Resize: size of roi array should be 2 * N where N is the rank of input tensor X.
Resize: unexpected mode
: 'Cubic' mode only support 2-D inputs ('Bicubic') or 4-D inputs with the corresponding outermost 2 scale values being 1.
onnxruntime::Upsample<int>::BaseCompute
Upsample: unexpected mode
mode is required
TfIdfVectorizer
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\nn\tfidfvectorizer.cc
onnxruntime::TfIdfVectorizer::TfIdfVectorizer
 is unrecognized, acceptable values are TF,IDF,TFIDF
TFIDF
impl_->weighting_criteria_ != kNone
mode: 
min_gram_length is required
min_gram_length
size_t(impl_->max_gram_length_) <= impl_->ngram_counts_.size()
max_gram_length must be inbounds of ngram_counts: 
Non-empty ngram_indexes is required
ngram_indexes
Negative ngram_indexes values are not allowed
status.IsOK() && !impl_->ngram_indexes_.empty()
weights
std::all_of(impl_->ngram_indexes_.cbegin(), impl_->ngram_indexes_.cend(), [](int64_t i) { return i >= 0; })
 but ngram_indexes size: 
 must be of equal size
impl_->weights_.size() == impl_->ngram_indexes_.size()
Got weights of size: 
pool_strings must not be empty if specified
pool_strings
pool_int64s
!pool_strings.empty()
impl_->min_gram_length_ > 0
Required min_gram_length must be positive: 
max_gram_length
impl_->max_gram_length_ >= impl_->min_gram_length_
min_gram_length >= max_gram_length required: 
max_skip_count is required
max_skip_count
impl_->max_skip_count_ >= 0
max_skip_count must be non-negative: 
Non-empty ngram_counts is required
ngram_counts
status.IsOK() && !impl_->ngram_counts_.empty()
size_t(impl_->min_gram_length_) <= impl_->ngram_counts_.size()
min_gram_length must be inbounds of ngram_counts: 
status.IsOK() && !pool_int64s.empty()
non-empty pool_int64s is required if pool_strings not provided
n-gram counts out of bounds for 
-grams
Number of items must compose whole 
end_idx >= start_idx && end_idx <= total_items
Input shape must have either [C] or [B,C] dimensions with B > 0.
(items % ngram_size == 0)
Duplicate ngram detected, size: 
 id: 
onnxruntime::ngram_details::PopulateGrams
p.first->second->id_ == 0
onnxruntime::Shrink::Shrink
op_kernel_info.GetAttr<float>("bias", &bias_temp).IsOK()
op_kernel_info.GetAttr<float>("lambd", &lambd_temp).IsOK()
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/shrink.h
EyeLike : Input tensor dimension is not 2
Must have a single dimension of 1
t_proto_p->dims()[0] == 1
onnxruntime::`anonymous-namespace'::ConstantOfShape::Compute
Must have a valid input shape.
onnxruntime::ConstantOfShapeBase<struct onnxruntime::TypeList<__int64,struct onnxruntime::MLFloat16,float,double,signed char,short,int,unsigned char,unsigned short,unsigned int,unsigned __int64,bool> >::PrepareCompute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\generator\constant_of_shape.cc
Unsupported output datatype with size: 
t_proto_p->dims_size() == 1
onnxruntime::ConstantOfShapeBase<struct onnxruntime::TypeList<__int64,struct onnxruntime::MLFloat16,float,double,signed char,short,int,unsigned char,unsigned short,unsigned int,unsigned __int64,bool> >::ConstantOfShapeBase
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/generator/constant_of_shape_base.h
Must have a single dimension
Tensor proto with external data for value attribute is not supported.
!utils::HasExternalData(t_proto)
onnxruntime::ConstantOfShapeBase<struct onnxruntime::TypeList<__int64,struct onnxruntime::MLFloat16,float,double,signed char,short,int,unsigned char,unsigned short,unsigned int,unsigned __int64,bool> >::SetValueFromTensorProto
ONNX_NAMESPACE::TensorProto::DataType_IsValid(t_proto.data_type())
Unsupported value attribute datatype: 
utils::HasDataType(t_proto)
onnxruntime::Scan8Impl::Execute
position_ >= 0 && position_ < sequence_length_
p_mlvalue
onnxruntime::Scan8Impl::CreateLoopStateVariables
onnxruntime::OrtValueTensorSlicer<struct OrtValue const >::Iterator::operator *
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/ort_value_tensor_slicer.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\controlflow\scan_8.cc
directions
Scan<8> spec does not support transpose of output. This should never be called.
onnxruntime::Scan<8>::Init
onnxruntime::Scan8Impl::Initialize
 has batch size of 
onnxruntime::Scan<8>::SetupSubgraphExecutionInfo
onnxruntime::Scan<8>::Compute
onnxruntime::Scan8Impl::ValidateInput
 did not match batch size of 
 but 
Scan inputs have inconsistent batch size. Previous value was 
onnxruntime::Scan8Impl::AllocateOutputTensors
sequence_lens length of 
Invalid entries in sequence_lens. Max sequence length was 
info.GetAttr<float>("spatial_scale", &spatial_scale_).IsOK()
spatial_scale_ > 0
pooled_height_ > 0
pooled_width_ > 0
R->Shape()[1] == 5
roi_batch_id >= 0
roi_batch_id < batch_size
onnxruntime::RoiPool<float>::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\nn\roi_pool.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/roi_pool.h
pooled_shape.size() == 2
info.GetAttrs<int64_t>("pooled_shape", pooled_shape).IsOK()
onnxruntime::RoiPool<float>::RoiPool
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/lp_norm.h
op_kernel_info.GetAttr<int64_t>("p", &p_).IsOK()
op_kernel_info.GetAttr<int64_t>("axis", &axis_).IsOK()
onnxruntime::LpNorm<double>::LpNorm
p_ == 1 || p_ == 2
onnxruntime::LpNorm<float>::LpNorm
Invalid input data: number of dimensions is less than 3: 
onnxruntime::InstanceNorm<float>::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\nn\instance_norm.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/instance_norm.h
op_kernel_info.GetAttr<float>("epsilon", &epsilon_).IsOK()
onnxruntime::InstanceNorm<float>::InstanceNorm
Mismatch between input data and scale: size of scale != input channel count 
Invalid input B: number of dimensions is not 1: 
Invalid input scale: number of dimensions is not 1: 
 vs. 
Mismatch between input data and B: size of B != input channel count 
info.GetAttr<float>("low", &low_).IsOK()
info.GetAttr<float>("high", &high_).IsOK()
onnxruntime::RandomUniform::RandomUniform
onnxruntime::Multinomial::Multinomial
ONNX_NAMESPACE::TensorProto::DataType_IsValid(output_dtype_) && output_dtype_ != ONNX_NAMESPACE::TensorProto::UNDEFINED
onnxruntime::RandomUniformLike::RandomUniformLike
info.GetAttr<int64_t>("sample_size", &num_samples_).IsOK()
info.GetAttr<float>("mean", &mean_).IsOK()
onnxruntime::RandomNormal::RandomNormal
info.GetAttr<int64_t>("dtype", &dtype).IsOK()
Invalid dtype of 
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/generator/random.h
info.GetAttr<float>("scale", &scale_).IsOK()
onnxruntime::RandomNormalLike::RandomNormalLike
ONNX_NAMESPACE::TensorProto::DataType_IsValid(dtype_) && dtype_ != ONNX_NAMESPACE::TensorProto::UNDEFINED
info.GetAttrs<int64_t>("shape", shape).IsOK()
Invalid data type of 
Output type not supported in this build: 
num_classes is < 1
num_samples is < 1
onnxruntime::RandomNormalLike::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\generator\random.cc
Empty dimensions for input tensor
batch_size is < 1
Could not infer data type from input tensor with data type 
onnxruntime::RandomUniformLike::Compute
onnxruntime::MultinomialCompute
onnxruntime::contrib::RegisterQuantizationKernels
onnxruntime::contrib::RegisterCpuContribKernels
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\cpu_contrib_kernels.cc
default_int64
info.GetAttr<int64_t>("default_int64", &default_int_).IsOK()
default_string
info.GetAttr<std::string>("default_string", &default_string_).IsOK()
Input of tensor(int64) must have output of tensor(string)
keys_floats
LabelEncoder
Input of tensor(string) must have output of tensor(int64)
keys_strings
values_floats
values_strings
_Unused
values_int64s
info.GetAttrs<TKey>(_key_field_name, keys).IsOK()
default_float
keys_int64s
onnxruntime::ml::LabelEncoder::LabelEncoder
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/ml/label_encoder.h
classes_strings
info.GetAttrs<std::string>("classes_strings", string_classes).IsOK()
values is 
 and the number of 
onnxruntime::ml::LabelEncoder_2<__int64,__int64>::LabelEncoder_2
info.GetAttrs<TValue>(_value_field_name, values).IsOK()
(name: 
 attribtues in LabelEncoder 
However, the number of key is 
) must have the same length. 
onnxruntime::ml::LabelEncoder_2<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,__int64>::LabelEncoder_2
onnxruntime::ml::LabelEncoder_2<__int64,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::LabelEncoder_2
num_keys == num_values
onnxruntime::ml::LabelEncoder_2<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,float>::LabelEncoder_2
onnxruntime::ml::LabelEncoder_2<float,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::LabelEncoder_2
onnxruntime::ml::LabelEncoder_2<float,__int64>::LabelEncoder_2
onnxruntime::ml::LabelEncoder_2<__int64,float>::LabelEncoder_2
classlabels_strings
Must provide classlabels_strings or classlabels_int64s but not both.
ZipMap
classlabels_int64s
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\zipmap.cc
Zipmap does not support empty dim count
classlabels_strings_.empty() ^ classlabels_int64s_.empty()
onnxruntime::ml::ZipMapOp::ZipMapOp
Input features_per_batch[
Zipmap only supports 1D or 2D input tensors
] != number of classlabels[
BRANCH_LEQ
TreeEnsembleRegressor
BRANCH_GTE
BRANCH_GT
BRANCH_LT
SOFTMAX
SOFTMAX_ZERO
BRANCH_EQ
LOGISTIC
AVERAGE
target_weights
target_ids
post_transform
target_treeids
target_nodeids
nodes_treeids
nodes_nodeids
nodes_values
nodes_truenodeids
nodes_hitrates
nodes_featureids
nodes_modes
nodes_missing_value_tracks_true
base_values
aggregate_function
nodes_falsenodeids
n_targets
Input shape needs to be at least a single dimension.
 (falsenode).
 (weights).
Unable to find node 
One falsenode is pointing either to itself, either to another tree.
onnxruntime::ml::detail::TreeEnsembleCommon<double,float>::TreeEnsembleCommon
onnxruntime::ml::detail::TreeEnsembleCommon<double,float>::compute
Unknown aggregation function in TreeEnsemble.
onnxruntime::ml::detail::TreeEnsembleCommon<float,float>::compute
onnxruntime::ml::detail::TreeEnsembleCommon<float,float>::TreeEnsembleCommon
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\tree_ensemble_common.h
n_targets_or_classes > 0
nodes_falsenodeids.size() == nodes_nodeids.size()
nodes_falsenodeids.size() == nodes_treeids.size()
nodes_falsenodeids.size() == nodes_featureids.size()
nodes_falsenodeids.size() == nodes_modes.size()
target_class_ids.size() == target_class_nodeids.size()
target_class_ids.size() == target_class_treeids.size()
nodes_falsenodeids.size() == nodes_truenodeids.size()
nodes_falsenodeids.size() == nodes_values.size()
Node 
 (truenode).
 is already there.
 in tree 
onnxruntime::ml::detail::TreeAggregatorMin<double,float>::MergePrediction
this->base_values_.size() == predictions.size()
onnxruntime::ml::detail::TreeAggregatorMax<double,float>::MergePrediction
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\tree_ensemble_aggregator.h
it->i < (int64_t)predictions.size()
onnxruntime::ml::detail::TreeAggregatorSum<double,float>::ProcessTreeNodePrediction
onnxruntime::ml::detail::TreeAggregatorAverage<double,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorSum<double,float>::MergePrediction
onnxruntime::ml::detail::TreeAggregatorMax<float,float>::MergePrediction
onnxruntime::ml::detail::TreeAggregatorMin<float,float>::MergePrediction
predictions.size() == (size_t)n_targets_or_classes_
onnxruntime::ml::detail::TreeAggregator<double,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorSum<float,float>::ProcessTreeNodePrediction
onnxruntime::ml::detail::TreeAggregator<float,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorAverage<float,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorSum<float,float>::MergePrediction
predictions.size() == predictions2.size()
TreeEnsembleClassifier
class_treeids
class_nodeids
class_weights
class_ids
X dims is empty.
onnxruntime::ml::detail::TreeEnsembleCommonClassifier<float,float>::compute
onnxruntime::ml::detail::TreeEnsembleCommonClassifier<double,float>::compute
onnxruntime::ml::detail::TreeEnsembleCommonClassifier<__int64,float>::compute
onnxruntime::ml::detail::TreeEnsembleCommonClassifier<int,float>::compute
onnxruntime::ml::detail::TreeAggregatorClassifier<int,float>::_set_score_binary
onnxruntime::ml::detail::TreeAggregatorClassifier<__int64,float>::_set_score_binary
classes.size() == 2 || classes.size() == 1
onnxruntime::ml::detail::TreeAggregatorClassifier<double,float>::_set_score_binary
onnxruntime::ml::detail::TreeAggregatorClassifier<float,float>::_set_score_binary
onnxruntime::ml::detail::TreeEnsembleCommon<__int64,float>::TreeEnsembleCommon
onnxruntime::ml::detail::TreeEnsembleCommon<int,float>::TreeEnsembleCommon
predictions.size() == 2
onnxruntime::ml::detail::TreeAggregatorClassifier<int,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorClassifier<__int64,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorSum<__int64,float>::MergePrediction
onnxruntime::ml::detail::TreeAggregatorSum<int,float>::MergePrediction
onnxruntime::ml::detail::TreeAggregatorSum<int,float>::ProcessTreeNodePrediction
onnxruntime::ml::detail::TreeAggregatorClassifier<float,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorSum<__int64,float>::ProcessTreeNodePrediction
onnxruntime::ml::detail::TreeAggregatorClassifier<double,float>::FinalizeScores
kernel_params
info.GetAttrs<float>("kernel_params", kernel_params).IsOK()
kernel_type
SVMRegressor
onnxruntime::ml::SVMCommon::SVMCommon
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\svmclassifier.h
LINEAR
n_supports
support_vectors
onnxruntime::ml::SVMRegressor<float>::SVMRegressor
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\svmregressor.cc
info.GetAttrs<float>("rho", rho_).IsOK()
!coefficients_.empty()
one_class
coefficients
info.GetAttrs<float>("coefficients", coefficients_).IsOK()
Unexpected mode:
num_features == feature_count_
onnxruntime::ml::SVMRegressor<float>::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\ml_common.h
Unsupported data type of 
coefficients_.size() > 0
onnxruntime::ml::SVMClassifier::Compute
SVMClassifier
vectors_per_class
onnxruntime::ml::SVMClassifier::SVMClassifier
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\svmclassifier.cc
prob_a
prob_b
info.GetAttrs<std::string>("classlabels_strings", classlabels_strings_).IsOK() || info.GetAttrs<int64_t>("classlabels_ints", classlabels_ints_).IsOK()
classlabels_strings_.size() > 0 || classlabels_ints_.size() > 0
proba_.size() == probb_.size()
classlabels_ints
onnxruntime::ml::batched_update_scores_inplace
Unexpected value for 'add_second_class' of 
scores.size() == static_cast<size_t>(expected_num_scores)
Scaler
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\scaler.cc
) != (
!scale_.empty()
onnxruntime::ml::ScalerOp<float>::ScalerOp
Invalid argument: input has empty dimensions.
) or 1
Scale size: (
scale_.size() == offset_.size()
onnxruntime::ml::ScalerOp<__int64>::ScalerOp
onnxruntime::ml::ScalerOp<int>::ScalerOp
Either both scale and offset can be of feature size (
onnxruntime::ml::ScalerOp<double>::ScalerOp
offset
Empty scale in attributes
OneHotEncoder
Unknown Category and zeros = 0.
cats_strings
One and only one of the 'cats_*' attributes must be defined
zeros
cats_int64s
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\onehotencoder.cc
num_categories_ > 0
tmp_cats_int64s.empty() || tmp_cats_strings.empty()
onnxruntime::ml::OneHotEncoderOp<__int64>::OneHotEncoderOp
onnxruntime::ml::OneHotEncoderOp<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::OneHotEncoderOp
onnxruntime::ml::OneHotEncoderOp<float>::OneHotEncoderOp
onnxruntime::ml::OneHotEncoderOp<double>::OneHotEncoderOp
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/ml/ml_common.h
Invalid normalize value of 
onnxruntime::ml::MakeNormalize
onnxruntime::ml::Normalizer::Normalizer
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/ml/normalizer.h
info.GetAttr<std::string>("norm", &norm).IsOK()
Normalizer
Rank of input to Normalized must be less than 2. Got 
Unexpected NORMALIZE value of 
Input shape had more than 2 dimension. Dims=
intercepts
LinearRegressor
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\linearregressor.cc
onnxruntime::ml::LinearRegressor::LinearRegressor
info.GetAttr<int64_t>("targets", &num_targets_).IsOK()
targets
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\linearclassifier.cc
onnxruntime::ml::LinearClassifier::LinearClassifier
multi_class
LinearClassifier
onnxruntime::ml::LinearClassifier::ComputeImpl
scores_output_data.length() >= scores_output_size
Scores output is incorrect size. Expected:
 Found:
Unsupported input element type of 
onnxruntime::ml::CastInputToFloat
shape_size == out.length()
onnxruntime::ml::ImputerOp::ImputerOp
Expected 'replaced_value_float' attribute since 'imputed_value_floats' is specified
replaced_value_float
imputed_value_int64s
Must provide imputed_values_float_ or imputed_values_int64_ but not both.
Expected 'replace_value_int64' attribute since 'imputed_values_int64' is specified
replaced_value_int64
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\imputer.cc
Invalid type
onnxruntime::ml::ImputerOp::Compute
imputed_values_float_.empty() ^ imputed_values_int64_.empty()
imputed_value_floats
Imputer
Empty input dimensions.
Empty value of imputed values.
FeatureVectorizer
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/ml/feature_vectorizer.h
onnxruntime::ml::FeatureVectorizer::FeatureVectorizer
status.IsOK() && !input_dimensions_.empty()
input_count >= 0 && static_cast<size_t>(input_count) == input_dimensions_.size()
Number of inputs (
) does not match number of inputdimensions values (
Invalid input type:
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\feature_vectorizer.cc
onnxruntime::ml::FeatureVectorizer::Compute
inputdimensions attribute must be provided
inputdimensions
int64_vocabulary
string_vocabulary
DictVectorizer
onnxruntime::ml::DictVectorizerOp<__int64,float>::DictVectorizerOp
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/ml/dictvectorizer.h
onnxruntime::ml::DictVectorizerOp<__int64,double>::DictVectorizerOp
info.GetAttrs(std::is_same<AttrType, std::string>::value ? "string_vocabulary" : "int64_vocabulary", vocabulary_).IsOK()
onnxruntime::ml::DictVectorizerOp<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,__int64>::DictVectorizerOp
onnxruntime::ml::DictVectorizerOp<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,float>::DictVectorizerOp
onnxruntime::ml::DictVectorizerOp<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,double>::DictVectorizerOp
onnxruntime::ml::DictVectorizerOp<__int64,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::DictVectorizerOp
info.GetAttrs<std::string>("cats_strings", string_categories).IsOK()
num_entries == int_categories.size()
info.GetAttrs<int64_t>("cats_int64s", int_categories).IsOK()
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/ml/category_mapper.h
onnxruntime::ml::CategoryMapper::CategoryMapper
Input of int64 must have output of string 
Input of string must have output of int64
CategoryMapper
invalid stof argument
stof argument out of range
TO_FLOAT
map_form_ != PACK_MAP::SPARSE || max_map_ > 0
max_map must be > 0 if map_form is SPARSE
info.GetAttr<int64_t>("max_map", &max_map_).IsOK()
max_map
Invalid input type of value: 
 Expected std::map<int64_t, float> or std::map<int64_t, std::string>
CastMap
Unexpected CAST_TO value of 
Invalid CAST_TO value of 
 Expected TO_FLOAT, TO_STRING or TO_INT64
TO_INT64
TO_STRING
 Expected DENSE or SPARSE
SPARSE
DENSE
onnxruntime::ml::MakeCast
info.GetAttr<std::string>("cast_to", &attr).IsOK()
cast_to
onnxruntime::ml::MakePack
Invalid PACK_MAP value of 
info.GetAttr<std::string>("map_form", &attr).IsOK()
map_form
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/ml/cast_map.h
onnxruntime::ml::CastMap::CastMap
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\cast_map.cc
onnxruntime::ml::CastMap::ComputeImpl
cur_input == end_input || cur_input->first >= 0
Negative index values are not permitted. First entry in map has index value of 
Binarizer
threshold
Input data with index: 
 is NaN
ArrayFeatureExtractor
Invalid Y argument: index is out of range: Y[
Invalid Y argument: num_indices = 0
Invalid argument: X input has empty dimensions.
Unsupported type
Input X must have 3 dimensions only. Actual:
A + (M * K) <= A_end
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\rnn\rnn_helpers.cc
onnxruntime::rnn::detail::NormalizeActivationArgumentAndGetAlphaBetaCount
Expecting activation to be one of Affine, Relu, LeakyRelu, ThresholdedRelu, Tanh, ScaledTanh, Sigmoid, HardSigmoid, Elu, Softsign, Softplus. Got 
Invalid activation function of 
alpha == 1.0f && (beta == 0.0f || beta == 1.0f)
Quantized GEMM only support alpha equal to 1.0f and beta equal to 0.0f or 1.0f
weights.quant_para_
Invalid GRU reset gate activation function: 
onnxruntime::rnn::detail::deepcpu::LstmMergeGatesFuncByName
Invalid LSTM merge activation function of 
onnxruntime::rnn::detail::deepcpu::ActivationFuncByName
onnxruntime::rnn::detail::deepcpu::GruOutputGateFuncByName
Invalid GRU hidden gate activation function: 
onnxruntime::rnn::detail::deepcpu::GruResetGateFuncByName
Input B must have shape {
Input R must have shape {
Input W must have shape {
}. Actual:
affine
Input initial_h must have shape {
Invalid value/s in sequence_lens. All values must be > 0 and < seq_length. seq_length=
Input sequence_lens must have shape {
scaledtanh
thresholdedrelu
leakyrelu
softplus
softsign
hardsigmoid
Input initial_c must have shape {
Input P must have shape {
onnxruntime::LSTMBase::ComputeImpl
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\rnn\lstm_base.cc
SoftmaxCPU inputs N, D and N * D must be < 
Left shape: 
 Left shape override: 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\einsum_utils\einsum_typed_compute_processor.cc
onnxruntime::EinsumTypedComputeProcessor<float>::PairwiseOperandProcess
left.Shape().Size() == left_shape_override.Size()
The override dims are not compatible with given tensor's shape. 
onnxruntime::EinsumTypedComputeProcessor<__int64>::PairwiseOperandProcess
onnxruntime::EinsumTypedComputeProcessor<double>::FinalizeOutput
onnxruntime::EinsumTypedComputeProcessor<double>::PairwiseOperandProcess
onnxruntime::EinsumTypedComputeProcessor<int>::FinalizeOutput
onnxruntime::EinsumTypedComputeProcessor<__int64>::FinalizeOutput
 Right shape: 
right.Shape().Size() == right_shape_override.Size()
Right shape: 
 Right shape override: 
left_dim == right_dim
Einsum op: Input dimensions must be equal along an axis to be reduced across all inputs
left_rank == right_rank
Ranks of pair-wise operands must be equal. 
onnxruntime::EinsumTypedComputeProcessor<float>::FinalizeOutput
candidate_output.Shape().Size() == output_shape.Size()
Einsum op: The candidate output cannot be reshaped into the op's output
Einsum op: Input shapes do not align
onnxruntime::EinsumTypedComputeProcessor<int>::PairwiseOperandProcess
Einsum op: Could not copy the intermediate output's buffer into the op's output buffer. Error: 
candidate_output_dims[iter] == 1
Not all dimensions to be reduced have been reduced in the candidate output. Candidate output dims: 
Einsum subscripts does not contain enough subscript labels and there is no ellipsis for input 
Einsum subscripts string contains too many subscript labels when compared to the rank of the input 
Einsum operands could not be broadcast together. Please check input shapes/equation provided.Input shape of operand 
 is incompatible in the dimension 
dim_iter == rank
The broadcasted dimensions of the inputs are incompatible
onnxruntime::EinsumComputePreprocessor::PostProcessBroadcastedDims
num_broadcasted_indices < num_of_ellipsis_dims_
Found '.' not part of an ellipsis in the output subscript provided
Found a '.' not part of an ellipsis in the output subscript provided
Inputs have ellipses in them but the provided output subscript does not contain an ellipsis
Rank of the input must match number of subscript labels corresponding to the input
Output subscript contains letters not seen in the inputs
Output subscript contains repeated letters
Number of subscripts in the input equation does not match number of input tensors
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\einsum_utils\einsum_compute_preprocessor.cc
onnxruntime::EinsumComputePreprocessor::Run
Found '.' not part of an ellipsis in input: 
Ellipsis must indicate a fixed number of dimensions across all inputs
Found a '.' not part of an ellipsis in input: 
. The shape is: 
Another operand has a dim value of 
 in the same dimension
The only subscript labels allowed are lower-cased letters (a-z) and upper-cased letters (A-Z)
onnxruntime::EinsumOp::Transpose
Length of permutation must match the rank of the input to be permutated
onnxruntime::EinsumOp::IsTransposeRequired
input_rank == permutation.size()
Einsum op: Transpose failed: 
Einsum op: The candidate output does not match the actual output's shape
The innermost dims should have the same dim value to parse the diagonal elements
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\einsum_utils\einsum_auxiliary_ops.cc
onnxruntime::EinsumOp::DeviceHelpers::CpuDeviceHelpers::DataCopy
output.SizeInBytes() == input.SizeInBytes()
 for input shape 
Einsum op: Unsupported data type for Diagonal 
onnxruntime::EinsumOp::DeviceHelpers::CpuDeviceHelpers::DiagonalInnermostDims
input_dims[rank - 2] == input_dims[rank - 1]
The rank of the input must match permutation size for Transpose
onnxruntime::EinsumOp::DeviceHelpers::CpuDeviceHelpers::Diagonal
rank >= 2 && dim_1 != dim_2 && input_dims[dim_1] == input_dims[dim_2]
Cannot parse the diagonal elements along dims 
Data types of the inputs must match for MatMul
input_shape_1_override.size() == 3 && input_shape_2_override.size() == 3
Only 1 batch dimension is allowed for MatMul
onnxruntime::EinsumOp::MatMul
input_1.DataType() == input_2.DataType()
input_shape_1_override[2] == input_shape_2_override[1]
Incompatible matrix dimensions for matMul
input_shape_1_override[0] == input_shape_2_override[0]
Batch dimension should match for MatMul;
Einsum op: Exception during MatMul operation: 
num_variadic_inputs == num_subgraph_inputs
The subgraph in 'body' requires 
 inputs but Scan was only given 
Number of entries in '
' was 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\controlflow\scan_utils.cc
onnxruntime::scan::detail::Info::Info
onnxruntime::scan::detail::OutputIterator::AllocateFinalBuffer
Failed to create output tensor for output #
onnxruntime::scan::detail::OutputIterator::Initialize
Mismatch between expected shape and shape from first output
cur_iteration_ < num_iterations_
onnxruntime::scan::detail::OutputIterator::AllocateFinalOutput
!is_concrete_shape_
If shape was concrete we shouldn't be using a custom allocator
Expected AllocateFinalOutput to have been called to before we increment the iterator
is_concrete_shape_
Expected AllocateFinalOutput to have been called to before we read the OrtValue from the iterator.
onnxruntime::scan::detail::OutputIterator::operator *
onnxruntime::OrtValueTensorSlicer<struct OrtValue>::Iterator::operator *
onnxruntime::scan::detail::OutputIterator::operator ++
Invalid values in '
'. 0 == forward. 1 == reverse.
onnxruntime::scan::detail::ReadDirections
directions.size() == num_entries
onnxruntime::scan::detail::AllocateOutput
Subgraph must have the shape set for all outputs but 
 did not.
valid
onnxruntime::scan::detail::IterateSequence
onnxruntime::scan::detail::IterateSequence::<lambda_59d9acea5faf21b910fd8b35ea3c81b3>::operator ()
onnxruntime::scan::detail::CreateFeedsFetchesManager
 is not compatible with 
onnxruntime::scan::detail::LoopStateVariable::Next
iteration_num_ < sequence_len_
Misuse of LoopStateVariable. Attempt to move beyond end of sequence
Inverse
onnxruntime::contrib::SkipLayerNorm<float>::SkipLayerNorm
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\skip_layer_norm.cc
epsilon_ >= 0
beta is expected to have 1 dimension, got 
Last dimension of beta and input does not match
bias is expected to have 1 dimension, got 
Last dimension of bias and input does not match
input is expected to have 3 dimensions, got 
skip is expected to have same shape as input
gamma is expected to have 1 dimension, got 
Last dimension of gamma and input does not match
onnxruntime::contrib::SkipLayerNorm<double>::SkipLayerNorm
onnxruntime::contrib::LayerNorm<float,0>::LayerNorm
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\layer_norm.cc
op_kernel_info.GetAttr("axis", &axis_).IsOK()
onnxruntime::contrib::LayerNorm<double,0>::Compute
onnxruntime::contrib::LayerNorm<double,1>::LayerNorm
onnxruntime::contrib::LayerNorm<double,1>::Compute
onnxruntime::contrib::LayerNorm<float,0>::Compute
onnxruntime::contrib::LayerNorm<float,1>::LayerNorm
onnxruntime::contrib::LayerNorm<float,1>::Compute
onnxruntime::contrib::LayerNorm<double,0>::LayerNorm
onnxruntime::contrib::Scale<float>::Scale
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\element_wise_ops.h
info.GetAttr("alpha", &alpha_).IsOK()
onnxruntime::contrib::Affine<float>::Affine
info.GetAttr("scale", &scale_).IsOK()
info.GetAttr("beta", &beta_).IsOK()
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::ScaledTanh<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::ScaledTanh<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::ParametricSoftplus<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::ParametricSoftplus<float> >::ElementWiseKernel
) does not match the number of channels (
Bias size (
onnxruntime::contrib::ImageScaler<float>::ImageScaler
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\image_scaler.h
Input is expected to have four dimensions corresponding to [N,C,H,W], got 
info.GetAttrs<float>("bias", bias_).IsOK()
DynamicSlice
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\crop.h
Attribute border needs to be specified with four border elements, got 
 input dimensions instead
) + bottomBorder (
Input's width (
) + scale_[0] (
) + scale_[1] (
onnxruntime::contrib::Crop<float>::Compute
) needs to be greater than or equal to the topBorder (
Input's height (
) + rightBorder (
) needs to be greater than or equal to the leftBorder (
min_ngram_size_ > 0
max_ngram_size
info.GetAttr<int64_t>("max_ngram_size", &max_ngram_size_).IsOK()
max_ngram_size_ > 0
min_ngram_size
info.GetAttr<int64_t>("min_ngram_size", &min_ngram_size_).IsOK()
onnxruntime::contrib::BifurcationDetector::BifurcationDetector
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\bert\bifurcation_detector.h
BifurcationDetector
max_ngram_size_ >= min_ngram_size_
src_tokens_len >= prev_suffix_match_idx_data
onnxruntime::contrib::BifurcationDetector::Compute
pred_tokens_len == (src_tokens_len + 1 - prev_suffix_match_idx_data)
scores_dims.size() == 2
scores_dims[0] == batch_size
NGramRepeatBlock
ngram_size
token_id < vocab_size
onnxruntime::contrib::NGramRepeatBlock::Compute::<lambda_611887ee8e00747c2e4fe06c46473fdf>::operator ()
input_ids_dims.size() == 2
onnxruntime::contrib::NGramRepeatBlock::Compute
info.GetAttr<int64_t>("ngram_size", &ngram_size_).IsOK()
onnxruntime::contrib::NGramRepeatBlock::NGramRepeatBlock
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\bert\ngram_repeat_block.h
ngram_size_ > 0
use_approximation
onnxruntime::contrib::BiasGelu<float,1>::Compute
onnxruntime::contrib::BiasGelu<float,0>::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\bert\bias_gelu.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\cdist.h
sqeuclidean
euclidean
onnxruntime::contrib::CDist<float>::CDist
CDist
metric
info.GetAttr<std::string>("metric", &metric).IsOK()
onnxruntime::contrib::CDist<double>::CDist
The second input of CDist kernel has wrong shape: 
Input shape dimensions mismatch:
The first input of CDist kernel has wrong shape: 
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops/cpu/crop_and_resize.h
CropAndResize
 specified. It should be either bilinear or nearest
onnxruntime::contrib::CropAndResize<float>::CropAndResize
Null crop_size_ptr
Number of dimensions for crop size should be exactly 1
Input tensor to Unique op should be 1D
onnxruntime::contrib::MaxpoolWithMask::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\maxpool_with_mask.h
MaxpoolWithMask
TransposeMatMul
positive
MurmurHash3
input_num_bytes % 4 == 0
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\murmur_hash3.cc
onnxruntime::contrib::MurmurHash3::Compute
sizeof(uint32_t) == output_element_bytes
Invalid assumption of output element size
Can not multiply A and B as inner dimension does not match. inner_A: 
 vs inner_B: 
Expecting 2xValues == indices
Expecting COO 2-D indices shape
Currently supporting only 2-D matrices
SparseToDenseMatMul
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\math\sparse_dense_matmul.cc
onnxruntime::contrib::SparseToDenseMatMul::Compute
WASM and 32-bit builds support only COO format
COO indices must be 2-D, got: 
onnxruntime::contrib::`anonymous-namespace'::SparseToDenseCoo<__int64>::operator ()
COO m index: 
onnxruntime::contrib::`anonymous-namespace'::SparseToDenseCoo<int>::operator ()
onnxruntime::contrib::`anonymous-namespace'::SparseToDenseCoo<unsigned int>::operator ()
 is out of bounds of lhs_right: 
onnxruntime::contrib::`anonymous-namespace'::SparseToDenseCoo<unsigned __int64>::operator ()
 is out of bounds of out_left: 
COO k index: 
onnxruntime::contrib::`anonymous-namespace'::SparseToDenseCoo<float>::operator ()
onnxruntime::contrib::`anonymous-namespace'::SparseToDenseCoo<double>::operator ()
embedding_size
char_embedding_size
conv_window_size
 conv_window_size attribute: 
 conv kernal size 1: 
 Char embedding size: 
Conv kernal size 1 does not match conv_window_size attribute .
 conv filter size: 
Conv filter size does not match embedding_size attribute.
 embedding_size attribute: 
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\word_conv_embedding.cc
onnxruntime::contrib::WordConvEmbedding::Compute
WordConvEmbedding
Char embedding size does not match char_embedding_size attribute.
 char_embedding_size attribute: 
Char embedding size does not match conv kernal size 2.
 Conv kernal size 2 : 
attribute pad_value is not set
pad_value
attribute mincharnum is not set
mincharnum
attribute mark is not set
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\tokenizer.cc
onnxruntime::contrib::Tokenizer::Tokenizer
Expecting a non-empty tokenexp
Either one of the separators OR tokenexp attributes required but none is set
separators must not be empty
!tokenexp.empty()
mincharnum_ > 0
attribute mincharnum must have a positive value
tokenexp
separators
Tokenizer
Input string contains invalid utf8 chars
Can not digest tokenexp: 
Input string contains invalid utf8 chars: 
mincharnum is too big for char level tokenezation
!separators.empty()
Can not digest separators: 
!char_tokenezation_ || mincharnum_ < 2
Match contains invalid utf8 chars: 
Input dimensions are either [C] or [N][C] allowed
tensor(string) expected as input
AttnLSTM
static_cast<int>(activation_func_names.size()) == num_directions_ * 3
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\attnlstm\deep_cpu_attn_lstm.cc
onnxruntime::contrib::DeepCpuAttnLstmOp::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\attnlstm\deep_cpu_attn_lstm.h
onnxruntime::contrib::DeepCpuAttnLstmOp::DeepCpuAttnLstmOp
Attention mechanism memory sequence lengths value must in (0, 
], while 
Attention memory layer weight shape error! Expected:{
, am_attn_size}, Got:
Attention mechanism memory shape error! Expected: {
}, actural: 
 found!
Attention mechanism memory sequence lengths must have shape {
Attention layer weight shape error! Expected: {
, aw_attn_size}. Got:
onnxruntime::contrib::DeepCpuAttnLstmOp::ValidateInputs
Attention query layer weight shape error! Expected:{
}, Got: 
Attention v weight shape error! Expected:{
}. Got: 
onnxruntime::contrib::DeepCpuAttnLstmOp::ComputeImpl
onnxruntime::contrib::FusedGemm<float>::FusedGemm
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\fused_gemm.cc
GetFusedActivationAttr(info, activation_).IsOK()
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\fused_conv.cc
onnxruntime::contrib::FusedConvFloat::FusedConvFloat
Axis must be within range [
 Axis is 
ExpandDims
axis <= X_NumDims && axis >= -X_NumDims
axis_tensor->Shape().IsScalar()
An axis tensor must be a scalar tensor.
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\expand_dims.h
onnxruntime::contrib::ExpandDims::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\bert\embed_layer_norm.cc
onnxruntime::contrib::EmbedLayerNormBase::EmbedLayerNormBase
input index out of range
onnxruntime::contrib::EmbedLayerNorm<float>::Compute
info.GetAttr("num_heads", &num_heads).IsOK() && num_heads > 0
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\bert\attention_base.h
onnxruntime::contrib::AttentionBase::AttentionBase
qkv_hidden_sizes
Input 'bias' dimension 0 should have same length as dimension 1 of input 'weights'
qkv_sizes doesn't match the wights dimension
Inputs 'past' dimension 0 shall have length of 2
Input 'past' is expected to have 5 dimension, got 
qkv_hidden_sizes attribute should have 3 elements
hidden_size should be divisiable by num_heads.
hidden_size should be divisiable by num_heads:
qkv_hidden_sizes first element should be same as the second
Inputs 'mask_index' with 3D data shall have shape batch_size x sequence_length x (past_sequence_length + sequence_length)
Inputs 'mask_index' with 2D data shall have shape batch_size x (past_sequence_length + sequence_length)
Inputs 'mask_index' with 4D data shall have is_unidirectional_ set to false
Inputs 'mask_index' with 4D data shall have shape batch_size x 1 x max_sequence_length x max_sequence_length)
Inputs 'past' dimension 2 shall have length of num_heads
Inputs 'past' dimension 1 shall have same length as dimension 0 of input 0
Inputs 'mask_index' with 1D data shall have length of batch_size or 2 * batch_size
Inputs 'past' dimension 2 shall have length of 
Input 1 dimension 0 should have same length as dimension 2 of input 0
Input 'weights' is expected to have 2 dimensions, got 
Input 1 dimension 1 should be 3 times of hidden dimension
Input 'bias' is expected to have 1 dimension, got 
Input 'input' is expected to have 3 dimensions, got 
Attention cannot have past sequence and extra add qk
Input 'extra_add_qk' dimension 3 should be same as sequence_length, got 
Input 'extra_add_qk' dimension 2 should be same as sequence_length, got 
Expect to have present state output when past state input is given
Input 'extra_add_qk' is expected to have 4 dimensions, got 
Input 'mask_index' is expected to have 1, 2, 3 or 4 dimensions, got 
Input 'extra_add_qk' dimension 1 should be same as number of heads, got 
Input 'extra_add_qk' dimension 0 should be same as batch_size, got 
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\bert\attention.cc
onnxruntime::contrib::AttentionBase::GetPresent
onnxruntime::contrib::AttentionCPUBase::ApplyAttention
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\bert\attention_cpu_base.h
onnxruntime::contrib::Attention<float>::Compute
4D mask in attention cpu kernel is not supported
GridSample
mode_str == "bilinear" || mode_str == "nearest" || mode_str == "bicubic"
mode "
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\grid_sample.cc
onnxruntime::contrib::GridSample<float>::GridSample
padding_mode
" not supported, expect bilinear, nearest or bicubic
bicubic
 does not match input batch size 
Only 4-D tensor is supported
grid_dims[0] == N
Grid batch size 
" not supported, expect zeros, border or reflection
reflection
padding_mode_str == "zeros" || padding_mode_str == "border" || padding_mode_str == "reflection"
padding_mode "
, expect 2
onnxruntime::contrib::GridSample<float>::Compute
grid_dims[3] == 2
Last dimension of grid: 
SampleOp
b_scale_shape.NumDimensions() == 0 || (b_scale_shape.NumDimensions() == 1 && (b_scale_shape[0] == 1 || b_scale_shape[0] == helper.N()))
QGemm : scale of input b must be a scalar or 1D tensor of size 1 or N
b_scale_shape.NumDimensions() == b_zp_shape.NumDimensions() && (b_scale_shape.NumDimensions() == 0 || (b_scale_shape[0] == b_zp_shape[0]))
QGemm : zero point and scale of input b should have same shape size
IsScalarOr1ElementVector(a_zp)
QGemm : zero point of input a must be a scalar or 1D tensor of size 1
b_zp_shape.NumDimensions() == 0 || (b_zp_shape.NumDimensions() == 1 && (b_zp_shape[0] == 1 || b_zp_shape[0] == helper.N()))
QGemm : zero point of input b must be a scalar or 1D tensor of size 1 or N
y_zp == nullptr || IsScalarOr1ElementVector(y_zp)
QGemm : zero point of y must be null or a scalar or 1D tensor of size 1
y_scale == nullptr || IsScalarOr1ElementVector(y_scale)
QGemm : scale of y must be null or a scalar or 1D tensor of size 1
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\quantization\quant_gemm.cc
onnxruntime::contrib::QGemm::Compute
onnxruntime::contrib::QGemm::CheckInputs
QGemm : scale of input a must be a scalar or 1D tensor of size 1
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/math/gemm_helper.h
QGemm
Position embedding scale must be a scalar or 1D tensor of size 1
Word embedding scale must be a scalar or 1D tensor of size 1
Segment embedding zero point must be a scalar or 1D tensor of size 1
Position embedding zero point must be a scalar or 1D tensor of size 1
Beta zero point must be a scalar or 1D tensor of size 1
Gamma zero point must be a scalar or 1D tensor of size 1
Gamma scale must be a scalar or 1D tensor of size 1
Segment embedding scale must be a scalar or 1D tensor of size 1
Word embedding zero point must be a scalar or 1D tensor of size 1
Beta scale must be a scalar or 1D tensor of size 1
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\bert\qembed_layer_norm.cc
onnxruntime::contrib::QEmbedLayerNorm<float>::Compute
QEmbedLayerNormalization
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\quantization\nhwc_max_pool.cc
onnxruntime::contrib::NhwcMaxPool::Compute
input_shape.Size() > 0 || N == 0
} for per-tensor/layer quantization or shape {
W_zero_point
 must have shape {
onnxruntime::contrib::DynamicQuantizeLSTM::PrePack
} for per-channel quantization. Actual:
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\quantization\dynamic_quantize_lstm.cc
Weight point must be constant
DynamicQuantizeLSTM : 
DynamicQuantizeLSTM
Recurrent
W_scale
R_zero_point
Weight zero point must be zero
R_scale
MatMulIntegerToFloat : input a zero point must be a scalar or 1D tensor of size 1. Per-Channel is not supported yet.
onnxruntime::contrib::DynamicQuantizeMatMul::Compute
onnxruntime::contrib::MatMulIntegerToFloat::Compute
IsScalarOr1ElementVector(a_zero_point_tensor)
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\quantization\dynamic_quantize_matmul.cc
onnxruntime::contrib::MatMulIntegerToFloatBase::ComputeCommon
IsBQuantParamSupported(b_zp_tensor->Shape(), b_tensor ? b_tensor->Shape() : b_shape_)
MatmulInteger : b zero point is not valid
QAttention
input zero point must be a scalar or 1D tensor of size 1.
onnxruntime::contrib::QAttention<float>::Compute
input scale must be a scalar or 1D tensor of size 1
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\quantization\attention_quant.cc
QLinearMul
MatmulInteger : input1 A_scale must be a scalar or 1D tensor of size 1
IsScalarOr1ElementVector(tensor_a_scale)
MatmulInteger : input1 B_zero_point must be a scalar or 1D tensor of size 1 if given
tensor_b_zero_point == nullptr || IsScalarOr1ElementVector(tensor_b_zero_point)
MatmulInteger : input1 B_scale must be a scalar or 1D tensor of size 1
IsScalarOr1ElementVector(tensor_b_scale)
MatmulInteger : input1 A_zero_point must be a scalar or 1D tensor of size 1 if given
tensor_a_zero_point == nullptr || IsScalarOr1ElementVector(tensor_a_zero_point)
onnxruntime::contrib::`anonymous-namespace'::QLinearImpl
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\qlinear_binary_op.cc
MatmulInteger : input1 C_zero_point must be a scalar or 1D tensor of size 1 if given
tensor_c_zero_point == nullptr || IsScalarOr1ElementVector(tensor_c_zero_point)
MatmulInteger : input1 C_scale must be a scalar or 1D tensor of size 1
IsScalarOr1ElementVector(tensor_c_scale)
QLinearLeakyRelu
input y_zero_point must be a scalar or 1D tensor of size 1 if given
tensor_y_zero_point == nullptr || IsScalarOr1ElementVector(tensor_y_zero_point)
input y_scale must be a scalar or 1D tensor of size 1
IsScalarOr1ElementVector(tensor_y_scale)
input x_zero_point must be a scalar or 1D tensor of size 1 if given
tensor_x_zero_point == nullptr || IsScalarOr1ElementVector(tensor_x_zero_point)
onnxruntime::contrib::QLinearAveragePool::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\qlinear_pool.cc
QLinearAveragePool
Unsupported 'dtype' in QLinear Pooling:
QLinear Pooling unsupported pooling size!
Input x_scale must be a scalar or 1D tensor of size 1
IsScalarOr1ElementVector(tensor_x_scale)
At least two inputs are needed, and each input must be (tensor, scale, zero_point) tuple!
input_def_count >= 8 && (input_def_count - 2) % 3 == 0
input_count_x3 >= 6 && input_count_x3 % 3 == 0
onnxruntime::contrib::QLinearConcat::Compute
Wrong input type encountered for zero point input def @
tensor_x_zero_point->GetElementType() == tensor_y_zero_point->GetElementType()
Input scale is not float for input def @
tensor_x_scale->IsDataType<float>()
onnxruntime::contrib::QLinearConcat::QLinearConcat
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\qlinear_concat.cc
QLinearConcat
Input scale is not float for quantized input @
Wrong input type encountered for zero point of quantized input @
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\qlinear_global_average_pool.cc
IsScalarOr1ElementVector(tensor_x_zero_point)
onnxruntime::contrib::QLinearGlobalAveragePool::Compute
IsScalarOr1ElementVector(tensor_y_zero_point)
QLinearGlobalAveragePool
A != nullptr && B != nullptr
onnxruntime::contrib::MatMulInteger16<short,short,int>::Compute
MatMulInteger16
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\matmul_integer16.cc
Input 1 is expected to have 1 dimensions, got 
Input 1 dimension 0 should have same length as the last dimension of input 0
Input 0 is expected to have 1 or more dimensions, got 
 rows[
 row[
 [seqno=
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\attnlstm\bahdanau_attention.cc
 is not in (0, 
!normalize_
onnxruntime::contrib::BahdanauAttention<float>::BahdanauAttention
not support normalize yet.
onnxruntime::contrib::BahdanauAttention<float>::PrepareMemory
Real memory steps 
mem_steps <= max_memory_steps_ && mem_steps > 0
activation_params count mismatch
unimplemented activation: 
beta is expected to have size of 
Input 0 and 7 (mask) shall have same shape
input_ids is expected to have 2 dimensions, got 
input_ids and position_ids shall have same shape
Input 0 and 1 shall have same shape
gamma is expected to have size of 
beta is expected to have 1 dimensions, got 
word_embedding and position_embedding shall have same dimension 1
gamma is expected to have 1 dimensions, got 
segment_embedding is expected to have 2 dimensions, got 
word_embedding and segment_embedding shall have same dimension 1
word_embedding is expected to have 2 dimensions, got 
position_embedding is expected to have 2 dimensions, got 
QlinearBuildLookupTable : input X_scale must be a scalar or 1D tensor of size 1
onnxruntime::contrib::QlinearBuildLookupTable
QlinearBuildLookupTable : input Y_scale must be a scalar or 1D tensor of size 1
QlinearBuildLookupTable : input Y_zero_point must be a scalar or 1D tensor of size 1
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\qlinear_lookup_table.cc
QlinearBuildLookupTable : input X_zero_point must be a scalar or 1D tensor of size 1
Received invalid value for allow_spinning. Valid values are 0 or 1
D:\a\_work\1\s\engine\lotus\onnxruntime\core\util\thread_utils.cc
Received null OrtThreadingOptions
custom join thread function not set
onnxruntime::concurrency::CreateThreadPoolHelper
CblasNoTrans Unexpected CBLAS_TRANSPOSE for TransB of 
onnxruntime::math::Gemm
onnxruntime::math::NextPosition
Unexpected CBLAS_TRANSPOSE for TransA of 
dims[d_i] < d_max
D:\a\_work\1\s\engine\lotus\onnxruntime\core\util\math_cpu.cc
CblasTrans Unexpected CBLAS_TRANSPOSE for TransB of 
 is not supported
OrtValue is TensorSequence type but has no element Tensor DataType.
Tensor types should have been handled already
CudaPinned
OpenVINO_GPU
onnxruntime::IAllocator::CalcMemSizeForArrayWithAlignment::<lambda_6c2fbce111bd53fabf7be0ec544a2aae>::operator ()
Specified device is not supported.
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\allocator.cc
`anonymous-namespace'::GetIndicesTensor
Unsupported indices_format passed
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\tensor_type_and_shape.cc
OrtApis::GetTensorTypeAndShape
Argument is not a tensor
the ort_value must contain a constructed tensor or sparse tensor
Not implemented
type_proto is not of type sequence!
type_proto is not of type map!
Tensor is expected to contain one of the primitive data types. Got: 
onnxruntime::Tensor::Init
onnxruntime::Tensor::SizeInBytes
tensor size overflow
tensor failed memory size calculation
shape.Size() must >=0
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\tensor.cc
onnxruntime::Tensor::Tensor
dtype_ != nullptr
p_type != nullptr
onnxruntime::IDataTransfer::CopyTensors
onnxruntime::CPUDataTransfer::CopyTensor
src.SizeInBytes() == dst.SizeInBytes()
onnxruntime::IDataTransfer::CopySparseTensors
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\data_transfer.cc
We do not expect duplicate registration of types for: 
onnxruntime::data_types_internal::DataTypeRegistry::RegisterDataType
proto != nullptr
Only ONNX MLDataType can be registered
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\data_types.cc
onnxruntime::data_types_internal::IsCompatible
onnxruntime::data_types_internal::SequenceTypeHelper::Set
elem_proto != nullptr
D:\a\_work\1\s\engine\lotus\include\onnxruntime\core/framework/data_types.h
onnxruntime::data_types_internal::MapTypeHelper::Set
value_proto != nullptr
expected a registered ONNX type
onnxruntime::SequenceTensorTypeBase::GetElementType
onnxruntime::OptionalTypeBase::GetElementType
onnxruntime::OptionalTypeBase::GetDeleteFunc
onnxruntime::SparseTensorTypeBase::GetElementType
onnxruntime::TensorTypeBase::GetElementType
onnxruntime::data_types_internal::OptionalTypeHelper::Set
double
float
(null)
onnxruntime::NonTensorTypeBase::ToDataContainer
onnxruntime::NonTensorTypeBase::FromDataContainer
uint64
int64
uint32
int32
uint16
int16
uint8
onnxruntime::SequenceTensorTypeBase::IsCompatible
thisProto->value_case() == TypeProto::ValueCase::kSequenceType
utils::HasElemType(thisProto->sparse_tensor_type())
onnxruntime::SparseTensorTypeBase::IsCompatible
thisProto->value_case() == TypeProto::ValueCase::kSparseTensorType
utils::HasElemType(thisProto->tensor_type())
onnxruntime::TensorTypeBase::IsCompatible
thisProto->value_case() == TypeProto::ValueCase::kTensorType
onnxruntime::NonTensorTypeBase::IsSequenceCompatible
utils::HasKeyType(thisProto->map_type())
onnxruntime::NonTensorTypeBase::IsMapCompatible
thisProto->value_case() == TypeProto::ValueCase::kMapType
utils::HasElemType(thisProto->optional_type())
onnxruntime::OptionalTypeBase::IsCompatible
thisProto->value_case() == TypeProto::ValueCase::kOptionalType
utils::HasElemType(thisProto->sequence_type())
MLDataType for: 
 is not currently registered or supported
sparse tensor type 
tensor type 
(unknown type)
bfloat16
float16
onnxruntime::utils::ContainerChecker::ContainerChecker
Invalid DataTypeImpl TypeProto definition
Format() == SparseFormat::kCoo
Must contain Coo format. Got: 
SparseTensor Allocation failed for size: 
onnxruntime::SparseTensor::AllocateBuffer
Values size 
 must be less than total buffer size: 
this tensor already has populated sparse_indices
onnxruntime::SparseTensor::GetSparseTensorFromOrtValue
Sparse format must not be set. Already contains format: 
onnxruntime::SparseTensor::GetCooIndexDims
values_count == index_size
Index size: 
 must be equal to or twice the values size: 
format_data_.size() == 1U
Expecting to contain one index, got: 
onnxruntime::SparseTensor::AsCoo
the ort_value must contain a constructed sparse tensor
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\sparse_tensor.cc
onnxruntime::`anonymous-namespace'::CopyData
Must have the same size. Got src_size: 
 dst_size: 
SafeIntExceptionHandler<class onnxruntime::OnnxRuntimeException>::SafeIntOnDivZero
Divide by zero
Use MakeCsrStrings
onnxruntime::SparseTensor::UseCsrIndices
This method does not expect allocator to be set
Outer index count must be rows + 1 or zero. Got: 
 rows: 
Expecting inner index size: 
 the same as values size: 
Inner and Outer indices must either be both zero or non-zero
onnxruntime::SparseTensor::ValidateBlockSparseShapes
Expecting to have at lest 3-D shape. Got:
Expecting one index. Got: 
onnxruntime::SparseTensor::AsBlockSparse
Format() == SparseFormat::kBlockSparse
Must contain BlockSparse format. Got: 
onnxruntime::SparseTensor::MakeCsrStrings
onnxruntime::SparseTensor::MakeCsrData
This method should follow a call to constructor that supplies the allocator
Format() == SparseFormat::kUndefined
onnxruntime::SparseTensor::MakeCooStrings
Expecting data type to be set as string
onnxruntime::SparseTensor::MakeCooData
Use MakeCooStrings
Not expecting an allocator set
onnxruntime::SparseTensor::UseCooIndices
onnxruntime::SparseTensor::ValidateCsrIndices
dense shape must 2-D. Got: 
format_data_.size() == 2U
Expecting two indices. Got: 
onnxruntime::SparseTensor::AsCsr
Format() == SparseFormat::kCsrc
Must contain Csr format. Contains: 
allocator_ != nullptr
Must have the same shape
Src and Dst must be of the same type
X-device copy of strings not supported
Use MakeBlockSparseStrings
onnxruntime::SparseTensor::UseBlockSparseIndices
Expecting fully sparse tensors to have indices shape {0}
Expecting fully sparse tensors to have value shape {0}
Expecting index blocks: 
 to be equal to values blocks: 
Indices shape must have dim[0] == 2
Expecting indices to have 2-D shape . Got: 
Destination must have a CPU allocator set
Destination should be empty
This instance should not be empty
onnxruntime::SparseTensor::Copy
Unable to find a data transfer for copying from device type: 
 to device type: 
onnxruntime::SparseTensor::MakeBlockSparseStrings
onnxruntime::SparseTensor::MakeBlockSparseData
 dimensions.
dimstart <= dimend && dimend <= values_.size()
Invalid tensor shape slice argument.
onnxruntime::TensorShape::SizeFromDimension
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\tensor_shape.cc
onnxruntime::TensorShape::SizeToDimension
dimension <= num_dims
Invalid dimension of 
 for SizeFromDimension. Tensor has 
onnxruntime::TensorShape::Slice
] already exists with value [
]. It will be overwritten
Config value is longer than maximum length 1024
Config key is empty or longer than maximum length 128
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\config_options.cc
onnxruntime::ConfigOptions::AddConfigEntry
Config with key [
An OrtValue for this name has already been added.
Buffer containing the initializer must be owned by the user.
Received OrtValue is not a tensor. Only tensors are supported.
Received nullptr for OrtValue.
Received nullptr for name.
values.size() == static_cast<size_t>(attr->floats_size())
No attribute with this name is defined.
Attibute name and type don't match
 is defined.
values.size() == static_cast<size_t>(attr->ints_size())
onnxruntime::OpNodeProtoHelper<struct onnx::InferenceContext>::GetAttrs
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\op_node_proto_helper.cc
onnxruntime::OpNodeProtoHelper<class onnxruntime::ProtoHelperNodeContext>::GetAttrs
Attribute: 
 expected to be of type: 
 but is of type: 
No attribute with name: 
Requested attribute: 
 is expected to have type: 
node_offsets_index < node_offsets_size_
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/node_index_info.h
onnxruntime::NodeIndexInfo::GetNodeOffset
onnxruntime::OpKernelContext::GetOrCreateOutputMLValue
TempSpace allocator not found
onnxruntime::OpKernelContext::NumVariadicInputs
arg_num < arg_counts.size()
Invalid arg_num of 
. Num args is 
onnxruntime::OpKernelContext::OutputMLValue
kernel != nullptr
OpKernel was null
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\op_kernel.cc
onnxruntime::OpKernelContext::OpKernelContext
frame != nullptr
Execution frame was null
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\op_kernel_info.cc
onnxruntime::OpKernelInfo::GetMemoryInfo
cannot find allocator
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\kernel_def_builder.cc
onnxruntime::KernelDefBuilder::VariadicAlias
input_offset >= 0 && output_offset >= 0
Kernel not found
 kernel is not supported in 
 Encountered following errors: (
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\kernel_registry.cc
onnxruntime::KernelRegistry::TryCreateKernel
 but the node in the model has the following type (
Found kernel for Op with name (
onnxruntime::KernelRegistry::Register
: Conflict with existing kernel def hash.
Failed to add kernel for 
: Conflicting with a registered kernel with op versions.
kernel def can't be NULL
 kernel start version: 
 kernel_end_version: 
 in the supported version range
 (node_version: 
 However the types are incompatible.
 This op has been implemented only for the following types (
Op with name (
 and type (
 Version mismatch.
 node_version: 
onnxruntime::`anonymous-namespace'::TraverseFormalParametersWithTypeProto
len <= op_schema.inputs().size()
Candidate for fallback CPU execution: 
ORT optimization- Force fallback to CPU execution for node: 
 because the CPU execution path is deemed faster than overhead involved with execution on other EPs 
 capable of executing this node
onnxruntime::GetCpuPreferredNodes
kernel_info != nullptr
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\fallback_cpu_capability.cc
onnxruntime::GetCpuPreferredNodes::<lambda_f72360673da28b091dd40b649eb46e9c>::operator ()
 type: 
`anonymous-namespace'::GetExternalDataInfo
Tensor does not have external data to read from.
UnpackTensor: the pre-allocated size does not match the raw data size, expected 
The preallocated buffer is too small. Requires 
, Got 
string tensor can not use pre-allocated buffer
TensorProtoToMLValue() must take a pre-allocated MemBuffer!
Initialized tensor with unexpected type: 
tensor can't contain negative dims
string tensor can not have raw data
onnxruntime::utils::TensorProtoToTensor
Sparse indices int32 data size does not match expected
Sparse indices int64 data size does not match expected
onnxruntime::utils::CopySparseData
Sparse Indices raw data size does not match expected.
Unsupported attribute value type of 
 in 'Constant' node '
onnxruntime::utils::ConstantNodeProtoToTensorProto
onnxruntime::utils::TensorProtoToMLValue
) in proto
onnxruntime::utils::UnpackTensorWithExternalDataImpl
nullptr == p_data
`anonymous-namespace'::ReadExternalDataForTensor
TensorProto external data size mismatch. Computed size: 
External data type cannot be UNDEFINED or STRING.
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\tensorprotoutils.cc
TensorProto type 
 can not be writen into Tensor type 
TensorProtoToTensor() tensor shape mismatch!
onnxruntime::utils::GetFileContent
data overflow
UnpackTensor: the pre-allocate size does not match the size in proto
corrupted protobuf data: tensor shape size(
) does not match the data size(
 is not supported.
onnxruntime::utils::SparseTensorProtoToDenseTensorProto
Invalid SparseTensor indices. Should be rank 0 or 1. Got:
cur_index == &*indices_data.cend()
indices_shape[1] > 0 && static_cast<size_t>(indices_shape[1]) == dims.size()
Invalid SparseTensor indices. Should one of the following types: int8, int16, int32 or int64
Invalid SparseTensor indices. INT8 indices must be in the raw data of indices tensor
Invalid SparseTensor indices. INT16 indices must be in the raw data of indices tensor
Unsupported type: 
onnxruntime::utils::UnpackInitializerData
 data_type: 
onnxruntime::utils::DenseTensorToSparseTensorProto
HasDataType(dense_proto)
Must have a valid data type
Unsupported sparse tensor data type of 
Element_size of: 
Invalid TensorProto
onnxruntime::AllocatorManager::InsertAllocator
duplicated allocator
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\allocatormgr.cc
onnxruntime::CreateAllocator
Received invalid value of arena_extend_strategy 
IExecutionProvider::Compile with fused Node and dll path is not implemented by 
IExecutionProvider::Compile with fused Node is not implemented by 
onnxruntime::IExecutionProvider::TryInsertAllocator
duplicated allocator: 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\execution_provider.cc
onnxruntime::IExecutionProvider::InsertAllocator
onnxruntime::IExecutionProvider::GenerateMetaDefId
metadef_id_generator_
IExecutionProvider constructor must be called with true for use_metadef_id_creator
IExecutionProvider::Compile with FusedNodeAndGraph is not implemented by 
Currently do not support dims higher than 2 dimensions: 
Invalid index: 
onnxruntime::sparse_utils::DenseTensorToSparseCoo
inner_num == src.Values().Shape().Size()
onnxruntime::sparse_utils::SparseCsrToDenseTensor
Outer indices must be M + 1. Got: 
outer_num == (rows + 1)
Input must be of COO format
Expecting indices to be equal the number of values or be twice as many
onnxruntime::sparse_utils::SparseCooToDenseTensor
 > dense_size: 
Unable to convert strings tensor to a sparse tensor that not on CPU
onnxruntime::sparse_utils::DenseTensorToSparseCsr
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\sparse_utils.cc
Unsupported element size: 
Support 2-D matrices only
Input must be of CSR format
Unable to convert strings tensor to a sparse tensor that is not on CPU
Expecting inner indices to be same as nnz. Got: 
There's no data transfer registered for copying tensors from 
onnxruntime::DataTransferManager::CopyTensors
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\data_transfer_manager.cc
onnxruntime::DataTransferManager::CopySparseTensors
data_transfer registered is nullptr.
Tensor size mismatch
fetch_alloc_info.size() == copy_info.size()
onnxruntime::utils::FinalizeCopyInfoForFetches
copy_info.size() == num_feeds
onnxruntime::utils::CopyInputsAcrossDevices
onnxruntime::utils::CopyOneInputAcrossDevices
onnxruntime::utils::CopyOutputsAcrossDevices
Only one thread was configured for parallel execution. Hence will use sequential execution.
onnxruntime::utils::ExecuteGraphImpl
Unsupported OrtValue type to copy between device.
onnxruntime::utils::FindMemoryInfoForValue
exec_plan_ptr
onnxruntime::utils::CalculateStaticCopyInfoForFeed
onnxruntime::utils::CalculateStaticCopyInfoForFeeds
onnxruntime::utils::InitializeFeedFetchCopyInfo
feed_locations.size() == copy_info.size()
onnxruntime::utils::FinalizeCopyInfoForFeeds
invalid allocator.
Unsupported OrtValue type.
Failed to find allocator for device 
allocator != nullptr
onnxruntime::utils::BatchOrCopyMLValue
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\utils.cc
InternalTestingExecutionProvider
onnxruntime::utils::ExecuteGraph
Could not find OrtValue with idx '
session.disable_prepacking
. Ignoring allocator from 
Allocator already registered for 
onnxruntime::SessionState::SetupAllocators
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\session_state.cc
SaveMLValueNameIndexMapping
Using an input in multiple nodes on different devices is not supported currently. Input:
Failed to find input name in the mapping: 
Only one node should produce an output. Existing entry for 
output_names_to_nodeinfo.empty()
onnxruntime::SessionState::AddOutputNameToNodeInfoMapping
 for attribute 
Entry exists in node 
The op type of a node cannot be empty
!op_type.empty()
 which is of op type: 
 used in the node: 
Using cached version of pre-packed weight for constant initializer: 
Unable to write the provided PrePackedWeights instance into the container
) and node 
 is used by node 
onnxruntime::KernelUseSharedPrePackedBuffers
 doesn't have an implementation that can consume provided pre-packed weights
The kernel corresponding to the node 
allocator_for_caching.get() != nullptr
onnxruntime::SessionState::PrepackConstantInitializedTensors::<lambda_8969ecba8249304e2d1178678ef22189>::operator ()
 doesn't have an implementation that can cache computed pre-packed weights
weights_to_be_filled_in.buffers_.size() > 0
onnxruntime::SessionState::CreateGraphInfo
Done saving OrtValue mappings.
onnxruntime::SessionState::PopulateKernelCreateInfo
entry != kernel_create_info_map_.cend()
onnxruntime::SessionState::GetNodeKernelCreateInfo
. Do you have duplicated calls to SessionState::AddInitializedTensor function?
duplicated ort_value index:
 Index:
' OpType:
Missing session state for subgraph. Node:'
entry != node_to_subgraph_ss.second.cend()
p_op_kernel
onnxruntime::SessionState::FinalizeSessionState
onnxruntime::OuterScopeNodeArgLocationAccumulator::<lambda_3e31c198cbd0b452ff5ea25fd603eaa6>::operator ()
onnxruntime::OuterScopeNodeArgLocationAccumulator::<lambda_ee881615ba5030f55f651fb9e91a2637>::operator ()
onnxruntime::OuterScopeNodeArgLocationAccumulator
subgraphs_kernel_create_info_maps.find(local_subgraph_kernel_create_info_map_key) == subgraphs_kernel_create_info_maps.end()
onnxruntime::AccumulateAllNestedSubgraphsInfo
onnxruntime::SessionState::FinalizeSessionStateImpl
 Attribute:
Main Graph instance should have populated all subgraphs when being resolved.
subgraph
onnxruntime::SessionState::CreateSubgraphSessionState
onnxruntime::SessionState::LoadFromOrtFormat::<lambda_b0c552e77b6d02e6c53c6386edafcda3>::operator ()
onnxruntime::SessionState::LoadFromOrtFormat
. Invalid ORT format model.
Can't find node with index 
Unable to find compiled kernel hash for node '
existing_entries.find(attribute_name) == existing_entries.cend()
onnxruntime::SessionState::AddSubgraphSessionState
SetGraphAndCreateKernels must be called prior to GetExecutionInfo.
node_index_info_
onnxruntime::SessionState::GetNodeIndexInfo
onnxruntime::SessionState::UpdateToBeExecutedNodes
onnxruntime::GetSubGraphSessionStatesOrtFormat
onnxruntime::SessionState::SaveToOrtFormat
Kernel create info node indices are null. Invalid ORT format model.
Kernel create info hashes are null. Invalid ORT format model.
Size mismatch for kernel create info node indexes and hashes. Invalid ORT format model.
 is missing. Invalid ORT format model.
Subgraph SessionState entry for 
 is null. Invalid ORT format model.
Subgraph SessionState for 
SessionState for subgraphs is null. Invalid ORT format model.
Kernel create info is null. Invalid ORT format model.
found duplicated provider 
 (node 
The node is not placed on any Execution Provider. 
Failed to find kernel for 
 in KernelRegistryManager
nullptr != type_proto
onnxruntime::utils::GetMLDataType
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\mldata_type_utils.cc
input_copy_needed != DeviceCopyCheck::Unknown && output_copy_needed != DeviceCopyCheck::Unknown
onnxruntime::FeedsFetchesManager::SetDeviceCopyChecks
onnxruntime::FeedsFetchesInfo::MapNamesToMLValueIdxs
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\feeds_fetches_manager.cc
Error mapping feeds: 
Error mapping output names: 
onnxruntime::FunctionKernel::FunctionKernel
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/func_kernel.h
onnxruntime::GraphPartitioner::PartitionOrtFormatModel
No provider specified.
onnxruntime::GraphPartitioner::Partition
Compiled kernel hashes must be provided
compiled_kernel_hashes != nullptr
onnxruntime::InlineNodes
onnxruntime::GraphPartitioner::PartitionOnnxFormatModel
onnxruntime::PartitionOrtFormatModelImpl
 has Compile error: 
single_node_compute_func should have 1 elements
. Execution Provider must generate unique names across the entire model.
Existing entry in compiled kernel hashes for 
compute_info_->create_state_func(&context, &func_state_) == 0
1 == capability.nodes.size()
onnxruntime::PlaceNode
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\graph_partitioner.cc
onnxruntime::PartitionOnnxFormatModelImpl
Must use Function based fusion when exporting compiled nodes to dll.
fusion_style == IExecutionProvider::FusionStyle::Function
 did not return correct number of compiled functions
0 == memory_size % kMinAllocationSize
onnxruntime::BFCArena::AllocationRegion::AllocationRegion
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/bfc_arena.h
p_int >= base_int
onnxruntime::BFCArena::AllocationRegion::IndexFor
Incorrect arena extend strategy.
onnxruntime::BFCArena::Extend::<lambda_7230621d141a3a3097634b97b00dbfd7>::operator ()
 is smaller than requested bytes of 
Available memory of 
Failed to allocate memory for requested buffer of size 
 bytes.
Extended allocation by 
onnxruntime::BFCArena::Extend
BinForSize(bin_size) == BinFromIndex(b)
BinForSize(bin_size + 255) == BinFromIndex(b)
BinForSize(bin_size * 2 - 1) == BinFromIndex(b)
BinForSize(bin_size * 2) != BinFromIndex(b)
h < chunks_.size()
onnxruntime::BFCArena::ChunkFromHandle
cudaMalloc
hipMalloc
 initial_growth_chunk_size_bytes: 
 max_dead_bytes_per_chunk: 
 with following configs: initial_chunk_size_bytes: 
Creating BFCArena for 
onnxruntime::BFCArena::BFCArena
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\bfc_arena.cc
 bins of max chunk size 
Creating 
p_int < base_int + memory_size_
Could not find Region for: 
entry != regions_.end()
onnxruntime::BFCArena::RegionManager::RemoveAllocationRegion
Could not find Region for 
onnxruntime::BFCArena::RegionManager::RegionFor
 arena_extend_strategy: 
 memory limit: 
 The total allocated bytes is now 
 bytes. 
 BFC Arena shrunk by 
onnxruntime::BFCArena::Shrink
onnxruntime::BFCArena::DeallocateRawInternal
!c1->in_use() && !c2->in_use()
onnxruntime::BFCArena::Merge
c2->prev == h1
Extending BFCArena for 
Failed to find a free memory block despite calling Extend. rounded_bytes=
!chunk->in_use()
onnxruntime::BFCArena::FindChunkPtr
!c->in_use() && (c->bin_num == kInvalidBinNum)
onnxruntime::BFCArena::SplitChunk
h != kInvalidChunkHandle
tried to allocate 0 bytes
onnxruntime::BFCArena::AllocateRawInternal
 (actual) rounded_bytes:
 (requested) num_bytes: 
. bin_num:
Total allocated bytes: 
Allocated memory at 
 size: 
Reserving memory in BFCArena for 
onnxruntime::BFCArena::Reserve
reserved_chunks_.find(ptr) == reserved_chunks_.end()
onnxruntime::BFCArena::InsertFreeChunkIntoBin
!c->in_use() && (c->bin_num != kInvalidBinNum)
onnxruntime::BFCArena::RemoveFreeChunkIterFromBin
onnxruntime::BFCArena::RemoveFreeChunkFromBin
Could not find chunk in bin
BinFromIndex(c->bin_num)->free_chunks.erase(h) > 0
c->in_use() && (c->bin_num == kInvalidBinNum)
onnxruntime::BFCArena::FreeAndMaybeCoalesce
onnxruntime::ExLibLoader::LoadExternalLib
onnxruntime::ExLibLoader::{dtor}::<lambda_5c396c9fa2f7b5412218295bb7bb0fcf>::operator ()
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\ex_lib_loader.cc
Unloading DSO 
onnxruntime::ExLibLoader::~ExLibLoader
Failed to unload DSO: 
Caught exception while loading custom ops with message: 
 has already been loaded.
A dso with name 
Caught exception while destructing CustomOpsLoader with message: 
offset >= 0 && static_cast<size_t>(offset) < node_values_size_
ort_value_index >= 0 && static_cast<size_t>(ort_value_index) < all_values_size_
onnxruntime::IExecutionFrame::GetMLValue
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/execution_frame.h
onnxruntime::NodeIndexInfo::GetMLValueIndex
ort_value.Fence() == nullptr
onnxruntime::ExecutionFrame::AllocateMLValueTensorSelfOwnBufferHelper
, fall back to default allocation behavior
 but the actually size is: 
, block in memory pattern size is: 
For ort_value with index: 
. Validate usage of dim_value (values should be > 0) and dim_param (all values with the same string should equate to the same size) in shapes in the model.
Shape mismatch attempting to re-use buffer. 
Allocation of memory pattern buffer for 
onnxruntime::ExecutionFrame::{ctor}::<lambda_9340f42f5f8bcfd7c2f8707644905958>::operator ()
buffers_.find(location) == buffers_.end()
onnxruntime::ExecutionFrame::ExecutionFrame
 returned nullptr
Trying to allocate memory for unused optional inputs/outputs
Tensor shape cannot contain any negative value
Tensor shape is too large
shape && sp_tensor.DenseShape() == *shape
invalid index 
feeds.size() == feed_mlvalue_idxs.size()
onnxruntime::IExecutionFrame::Init
fetches.empty() || fetches.size() == fetch_mlvalue_idxs_.size()
 entries which doesn't match the number of fetches the frame was initialized with of 
Fetches vector passed to GetOutputs contains 
 failed. Error:
node_index_info and ort_value_idx_map are out of sync and cannot be used
node_index_info_.GetMaxMLValueIdx() == ort_value_idx_map.MaxIdx()
onnxruntime::IExecutionFrame::IExecutionFrame
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\execution_frame.cc
 Requested shape:
OrtValue shape verification failed. Current shape:
shape && tensor.Shape() == *shape
onnxruntime::IExecutionFrame::GetOrCreateNodeOutputMLValue
 failed: 
 size=
TraceAllocation for ort_value_idx=
onnxruntime::ExecutionFrame::TraceAllocate
onnxruntime::ExecutionFrame::TraceFree
TraceFree for ort_value_idx=
Memory pattern planner is not enabled on this execution framework.
Invalid allocation kind: 
 for output 
 does not match actual shape of 
Expected shape from model of 
onnxruntime::ExecutionFrame::VerifyOutputSizes
onnxruntime::ExecutionFrame::ReleaseMLValueImpl
ort_value_idx >= 0 && static_cast<size_t>(ort_value_idx) < alloc_plan.size()
onnxruntime::ExecutionFrame::GetAllocationPlan
onnxruntime::ExecutionFrame::AllocateMLValueTensorPreAllocateBuffer
mlvalue.Fence() == nullptr
onnxruntime::AllocateSparseTensor
onnxruntime::ExecutionFrame::AllocateReusedOrtValueIfNotAllocatedHelper
ort_value_index >= 0 && static_cast<size_t>(ort_value_index) < alloc_plan.size()
onnxruntime::ExecutionFrame::AllocateAsPerAllocationPlan
We don't expect custom allocators for non-tensor types, so a shape is mandatory here.
Allocation of tensor types requires a shape.
model format error!
model format error! Missing 'location'
model format error! Need a key for the external data info
model format error! Need a value for the external data info
location
 failed
parsing 
checksum
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\node_index_info.cc
onnxruntime::NodeIndexInfo::Init::<lambda_dc786c9bfe42f2772e18a10475fce9cf>::operator ()
onnxruntime::NodeIndexInfo::Init::<lambda_68f4cdf20c40d247dba9fe0156c932f5>::operator ()
ort_value.IsAllocated()
. Shape:
Insufficient dimensions to slice on 
gsl::narrow_cast<int64_t>(tensor_shape.NumDimensions()) >= slice_dimension
. Dimension 0 is 
Invalid dim0_offset of 
dim0_offset < dim0_size
onnxruntime::OrtValueTensorSlicer<struct OrtValue>::Iterator::Iterator
Can't slice a non-tensor OrtValue. Type was 
ort_value.IsTensor()
onnxruntime::OrtValueTensorSlicer<struct OrtValue>::Create
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\ort_value_tensor_slicer.cc
OrtValue has not been allocated so can't be sliced.
onnxruntime::OrtValueTensorSlicer<struct OrtValue const >::Create
onnxruntime::OrtValueTensorSlicer<struct OrtValue const >::Iterator::Iterator
 already exist.
func info for node: 
Can't use func with null ptr
 not found.
onnxruntime::FuncManager::GetFuncs
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\fuse_nodes_funcs.cc
onnxruntime::utils::detail::CopyLittleEndian
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\endian_utils.cc
source and destination buffer size mismatch
Got nullptr from GetKernel for node: 
_fence_before
' Status Message: 
 node. Name:'
Non-zero status code returned while running 
thread_scheduling_stats
_kernel_time
_fence_after
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/op_kernel_context_internal.h
Multiple errors were found.
onnxruntime::ParallelExecutor::Execute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\parallel_executor.cc
ParallelExecutor::Execute
Begin execution
onnxruntime::ParallelExecutor::RunNodeAsync
Exiting due to terminate flag being set to true.
 does not.
All implicit inputs should have OrtValue instances by now. 
entry != nullptr
onnxruntime::OpKernelContextInternal::OpKernelContextInternal
Unknown exception was caught by catch-all handler.
Exception running nodes starting at 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\sequential_executor.cc
graph_index
exec_plan_index
activation_size
parameter_size
output_size
SequentialExecutor::Execute
onnxruntime::ReleaseNodeMLValues
onnxruntime::SequentialExecutor::Execute
Previous entry was not terminated.
starts_.size() == ends_.size()
onnxruntime::AllocPlanPerValue::ProgramCounter::AddStart
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\allocation_planner.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/sequential_execution_plan.h
Invalid 'start'. Value is smaller than previous 'end'.
starts_.empty() || start > ends_.back()
No matching 'start' entry.
starts_.size() == ends_.size() + 1
onnxruntime::AllocPlanPerValue::ProgramCounter::AddEnd
Invalid 'end'. Value is larger than 'start'.
end >= starts_.back()
onnxruntime::PlannerImpl::GetLocationForNodeInput
specific_subgraph_kernel_create_info_map != subgraphs_kernel_create_info_maps_.end()
onnxruntime::PlannerImpl::GeneratePlanForWeightsHelper
onnxruntime::PlannerImpl::ComputeReusePlan
Only tensors are supported for external outputs for now.
!IsNonTensor(*node_output)
Invalid program_counter entries at index 
found_in_outer_scope_location_map
onnxruntime::PlannerImpl::ComputeUseCounts::<lambda_4a61ff195999a8a104bf36ba131bf403>::operator ()
Can not find the node 
Should not have entry in kernel create info with nullptr for kernel_def
p_kernel_def
onnxruntime::PlannerImpl::ComputeUseCounts
Can not find the execution provider 
allocator
onnxruntime::PlannerImpl::AllocPlan
id >= 0 && static_cast<size_t>(id) < ort_value_info_.size()
onnxruntime::PlannerImpl::ProcessDef
reused != reused_for
onnxruntime::PlannerImpl::Reuse
nullptr != tensor_type_base
onnxruntime::PlannerImpl::GetElementSize
There is no location for this node arg in the outer scope location map
SessionState should have saved the KernelCreateInfo prior to this running. NodeIndex:
entry != kernel_create_info_map.cend()
onnxruntime::GetKernelCreateInfo
onnxruntime::PlannerImpl::Index
n >= 0 && static_cast<size_t>(n) < ort_value_info_.size()
onnxruntime::PlannerImpl::UseCount
onnxruntime::PlannerImpl::Buffer
n >= 0 && static_cast<size_t>(n) < plan_.allocation_plan.size()
entry.program_counter.HasValidEntries()
onnxruntime::PlannerImpl::VerifyMemoryTimeSchedule
AllocPlan(ml_value_idx).program_counter.Ends().back() == program_counter
onnxruntime::PlannerImpl::GenerateDeallocationPlan
onnxruntime::PlannerImpl::CreatePlan
buffers_.size() == buffer_sizes_.size()
onnxruntime::PrePackedWeights::GetHash
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\prepacked_weights.cc
onnxruntime::session_state_utils::SaveInitializedTensors::<lambda_3778dc86d07cb69d69c8323ef7fb1134>::operator ()
Saving initialized tensors.
onnxruntime::session_state_utils::SaveInitializedTensors
OrtValue indexes should have been populated.
ort_value_name_idx_map.MaxIdx() > -1
entry != initialized_tensors_to_allocate.end() && entry->second->data_type() != ONNX_NAMESPACE::TensorProto_DataType_STRING
 bytes for 
[Memory] SessionStateInitializer statically allocates 
Internal error. The preallocated buffer is too small. Requires 
onnxruntime::session_state_utils::DeserializeTensorProto
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\session_state_utils.cc
string tensor is not supported for copying between allocators
Failed to copy tensor to 
 ) is different from what is supplied (
) because the ORT planned memory location device 
Cannot use user supplied initializer with name: (
session.use_device_allocator_for_initializers
Failed memory size calculation
DeserializeTensorProto() takes either pre-allocated buffer or an allocator!
 input with name 
Subgraph
Graph
Using user supplied initializer with name (
 failed.
Deserialize tensor 
Done saving initialized tensors
onnxruntime::session_state_utils::SaveInputOutputNamesToNodeMapping::<lambda_27534cc625d5ab04dc11bfb2c47d616f>::operator ()
onnxruntime::session_state_utils::SaveInputOutputNamesToNodeMapping::<lambda_217727e3bc662f496961b1bb8f9e821c>::operator ()
onnxruntime::session_state_utils::SaveInputOutputNamesToNodeMapping
 is not used by any node.
Unsupported device allocator in the context of pre-packed weights caching: 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\prepacked_weights_container.cc
onnxruntime::PrepackedWeightsContainer::GetOrCreateAllocator
!using_counters_
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/mem_pattern_planner.h
onnxruntime::MemPatternPlanner::TraceAllocation
current <= buffer_size_
Can't merge shape info. Both source and target dimension have values but they differ. Source=
 Dimension=
Mismatch between number of source and target dimensions. Source=
 are '0' and '1'. The environment variable contained the value: 
ALLOW_RELEASED_ONNX_OPSET_ONLY
D:\a\_work\1\s\engine\lotus\onnxruntime\core/graph/model_load_utils.h
onnxruntime::model_load_utils::IsAllowReleasedONNXOpsetsOnlySet
Constant
D:\a\_work\1\s\engine\lotus\onnxruntime\core\graph\graph.cc
onnxruntime::MergeShapeInfo
 Input=
Tensor element type mismatch. 
onnxruntime::NodeArg::UpdateTypeAndShape
Type mismatch. Current=
Optional Type mismatch. Expected: 
 . Got: 
Source and target must both be tensors
Output:
 , or sparse tensors
 , or optional typed entities
 target:
. Falling back to lenient merge.
Error merging shape info for output. '
' source:
' Model is invalid.
onnxruntime::Graph::Graph
Sparse initializer must have a name. This model is invalid
Duplicate constant node sparse initializer name: '
Duplicate sparse_tensor_initializer: '
utils::HasName(sparse_tensor)
' the model will use the latest encountered initializer
. Please, fix your model.
input index: 
 is not the same as this node's index:
input edges
onnxruntime::Node::LoadEdgesFromOrtFormat
This is an invalid model. The sum of input arg count is not equal to size of input defs in node (
output edges
graph_proto != nullptr
graph_proto cannot be null
LoadNodeArgsFromOrtFormat: Node [
], could not find NodeArg 
fbs_attr cannot be null
onnxruntime::Node::LoadFromOrtFormat
Node::LoadFromOrtFormat, input_arg_counts is missing
Serialization error. Graph attribute was serialized without Graph instance
onnxruntime::Node::LoadEdgesFromOrtFormat::<lambda_8629f0a4d7cae9a2f0cd36f438d51fee>::operator ()
Node::LoadEdgesFromOrtFormat, edge is missing for 
Node [
] op_type [
does not have the graph for key 
Serialization of fused function body is not currently supported, 
fbs_node_arg_names cannot be null
onnxruntime::Node::SaveToOrtFormat
node_arg_name cannot be null
onnxruntime::Node::LoadFromOrtFormat::<lambda_9d9512158c4d36214ecdd758e8025182>::operator ()
Invalid source node arg slot specified when removing edge.
onnxruntime::Graph::RemoveEdge
Argument mismatch when removing edge.
Invalid destination node arg slot specified when removing edge.
 Graph may not conform to the ONNX spec and contain initializers that are not graph inputs.
onnxruntime::Graph::BuildConnections
This is an invalid model. Failed to find NodeArg in all parent graphs. Name=
This is an invalid model. At top level graph without matching NodeArg that subgraph consumes. Name=
onnxruntime::Graph::SetOuterScopeNodeArgs
This is an invalid model. Error: Duplicate definition of name (
onnxruntime::Graph::AddEdge
Invalid node indexes specified when adding edge.
Invalid destination node arg slot specified when adding edge.
Invalid source node arg slot specified when adding edge.
Invalid node indexes specified when removing edge.
Argument type mismatch when adding edge.
graph_inputs_excluding_initializers_.empty() && graph_inputs_including_initializers_.empty() && value_info_.empty() && graph_outputs_.empty()
Graph state to be loaded into must be empty.
Graph ctor should have created NodeArg for initializer. Missing:
onnxruntime::Graph::InitializeStateFromModelFileGraphProto
) does not exist in the graph.
node_arg
This is an invalid model. Error: two nodes with same node name (
This is an invalid model. Graph output (
This is an invalid model. Tensor does not have type information.
Duplicate initializer (dense, sparse or ConstantNode): '
by either re-generating the model with latest exporter/converter 
or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.
This may prevent some of the graph optimizations, like const folding. 
Move it out of graph inputs if there is no need to override it, 
Initializer 
 appears in graph inputs and will not be treated as constant value/weight. 
) is invalid.
This is an invalid model. Node (
) of operator (
) in node (
This is an invalid model. Type Error: Type '
' of input parameter (
) bound to different types (
 in node (
UpdateTypeShapeInference is not intended to be used with control flow nodes containing subgraphs
onnxruntime::Graph::InferAndVerifySubgraphTypes
onnxruntime::Graph::UpdateShapeInference
node.GetAttributeNameToMutableSubgraphMap().empty()
Node (
) Op (
) input arg (
) does not have type information set by parent node.
No Graph instance was found for attribute 
 in node 
 inputs and requires 
 inputs. Either provide all subgraph inputs, or just the required inputs.
Size mismatch validating subgraph inputs. Got 
 inputs but subgraph has 
Subgraph input missing type.
Node:
Invalid model. Node input '
' is not a graph input, initializer, or output of a previous node.
onnxruntime::Graph::KahnsTopologicalSort
Some nodes are not included in the topological sort, graph have a cycle.
Graph attribute inferencing failed: 
This is an invalid model. Error: the graph is not acyclic.
. Error message 
. Execution will fail if ORT does not have a specialized kernel for this op
Function body initialization failed for node '
' optype 
Error: Duplicate definition-site for (
onnxruntime::Graph::InitFunctionBodyForNode
This is an invalid model. In Node, 
, Error 
 is not a registered function/op
onnxruntime::Graph::VerifyNodeAndOpMatch
) is required but not specified.
Fatal error: 
) attribute (
) does not have type information.
Type Error: Type (
 but usage of initializer in graph expects 
This is an invalid model. Model input (
Type Error: Data in initializer '
' has element type 
Type Error: Shape of initializer 
 does not match. 
Type Error: Type parameter (
) of Optype (
) type inference failed
onnxruntime::Graph::InferAndVerifyTypeMatch
) does not match expected type (
) output arg (
) of output arg (
) of node (
 as it still has output edges.
onnxruntime::Graph::SaveToOrtFormat
node->GetOutputEdgesCount() == 0
Can't remove node 
onnxruntime::Graph::RemoveNode
onnxruntime::Graph::ToGraphProto
Failed to convert dense initializer to sparse
onnxruntime::Graph::AddInitializedTensor
existing->second == &tensor
sparse_tensor_names_.count(tensor_name) == 0
sparse_tensor_names_ not in sync with name_to_initial_tensor_
graph_proto_ is not in sync with name_to_initial_tensor_.
onnxruntime::Graph::RemoveInitializedTensor
!found
onnxruntime::Graph::PerformTypeAndShapeInferencing
onnxruntime::Graph::InitInputsInitializersOutputs
onnxruntime::Graph::Resolve
onnxruntime::Graph::ForThisAndAllSubgraphs
outer_scope_node_args_consumed.empty()
Shouldn't be possible to have NodeArgs that haven't been handled already.
AddInitializedTensor already has tensor with name 
 but different TensorProto.
onnxruntime::Graph::SetInputs
input->Exists()
) : (
) -> (
onnxruntime::Graph::CreateFusedSubGraphNode
nullptr != func_meta_def
dst_implicit_input_idx < (int)node->ImplicitInputDefs().size()
onnxruntime::Graph::FinalizeFuseSubGraph
onnxruntime::Graph::InlineFunction
Input to set must exist.
Removing initializer '
'. It is not used by any node and should be removed from the model.
outer_scope_node_arg != nullptr
'. It is no longer used by any node.
 must be either specified in graph inputs or graph initializers.
Removing NodeArg '
onnxruntime::Graph::AllocateNode
nodes_.size() < static_cast<unsigned int>(std::numeric_limits<int>::max())
Outer scope node arg name '
'was added but does not exist. 
Cannot find NodeArgs for [
onnxruntime::Graph::ToGraphProtoInternal
onnxruntime::Graph::CleanUnusedInitializersAndNodeArgs
initializer_node_arg != nullptr
NodeArg is missing. Invalid ORT format model.
Sparse Initializer tensor is missing. Invalid ORT format model.
Node index is out of range
Node is missing. Invalid ORT format model.
NodeEdge is missing. Invalid ORT format model.
onnxruntime::Graph::LoadFromOrtFormat
onnxruntime::Graph::LoadFromOrtFormat::<lambda_002dfff45d93392e8dba7424e351b660>::operator ()
NodeArg Name is missing. Invalid ORT format model.
Duplicate initializer (dense or ConstantNode): '
Initializer tensor is missing. Invalid ORT format model.
onnxruntime::ConstPointerContainer<class std::vector<class onnxruntime::NodeArg *,class std::allocator<class onnxruntime::NodeArg *> > >::at
index < data_.size()
D:\a\_work\1\s\engine\lotus\include\onnxruntime\core/common/const_pointer_container.h
Domain already set in registry
D:\a\_work\1\s\engine\lotus\onnxruntime\core\graph\schema_registry.cc
onnxruntime::OnnxRuntimeOpSchemaRegistry::RegisterOpSet
known by the checker.
onnxruntime::OnnxRuntimeOpSchemaRegistry::RegisterOpSchemaInternal
than the operator set version 
, but it its domain is not
, but it its version is higher
Mismatch between Graph and IndexedSubGraph. Node not found: 
onnxruntime::GraphViewer::GetNodesInTopologicalOrder
Invalid ExecutionOrder
filter_info_ == nullptr
Not supported with filtered graph.
onnxruntime::GraphViewer::GetRootNodes
IndexedSubGraph contains values not present in the Graph
onnxruntime::GraphViewer::GraphViewer
graph_->GetNode(idx) != nullptr
Mismatch between Graph and IndexedSubGraph. Input not found:
D:\a\_work\1\s\engine\lotus\onnxruntime\core\graph\graph_viewer.cc
Mismatch between Graph and IndexedSubGraph. Output not found:
nodearg
onnxruntime::Model::Model
ModelProto does not have a graph.
Missing opset in the model. All ModelProtos MUST have at least one entry that specifies which version of the ONNX OperatorSet is being imported.
D:\a\_work\1\s\engine\lotus\onnxruntime\core\graph\model.cc
, max supported IR version: 
Missing model IR version.
 model may run depending upon legacy support of some older opset version operators.
Unsupported model IR version: 
 is till opset 
ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 
 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain 
 is under development and support for this is limited. The operator schemas and or other functionality could possibly change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain 
onnxruntime::model_load_utils::ValidateOpsetForDomain
Null entry in metadata_props. Invalid ORT format model.
onnxruntime::Model::SaveToOrtFormat
Graph is null. Invalid ORT format model.
onnxruntime::Model::LoadFromOrtFormat
Protobuf parsing failed.
<p_fd> is less than 0.
<p_fd> less than 0.
Protobuf serialization failed.
onnxruntime::Model::Save
ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain 'ai.onnx'. Please upgrade your model to opset 7 or higher. For now, this opset 
No graph was found in the protobuf.
Failed to load model with error: 
onnxruntime::Model::Load
onnxruntime::SaveModel
Load model 
 failed. File doesn't exist
system error number 
 expected to have tensor type
Incompatible dimensions
 expected to have tensor or sparse type
Attribute expected to have tensor or sparse tensor type
data_0
saved_var
saved_mean
D:\a\_work\1\s\engine\lotus\onnxruntime\core\graph\dml_ops\dml_defs.cc
Incompatible dimensions for matrix multiplication
fused_ratio
First input does not have rank 2
Input tensors of wrong rank (0).
Second input does not have rank 2
 is null
 expected to have tensor or sparse tensor type: 
 expected to have rank >
Dimension mismatch in unification between 
Attribute 
Negative values are not allowed in a shape specification
 should specify a shape
 expected to have rank 
 but has rank 
Invalid bias shape
Inputs 0 shall be 3 dimensions
Inputs 4 shall be 5 dimensions
qkv_hidden_sizes should have 3 elements
key and value cache dimensions value shall not be null
key and value cache shall be 4 dimensions
input_ids shall be 2 dimensions
Pads has incorrect number of values
word_embedding should have 2 dimensions and dimension size is known.
segment_ids input shall be 2 dimensions
segment_embedding should have 2 dimensions, dimension size known, and same hidden size as word_embedding.
position_embedding should have 2 dimensions, dimension size known, and same hidden size as word_embedding.
beta should have 1 dimension, dimension size known, and same hidden size as word_embedding.
gamma should have 2 dimension, dimension size known, and same hidden size as word_embedding.
Error unexpected extra input in node:
Error parsing node:
'pads' input must be a 1D (shape: [2 * n_input_dims]) tensor of type int64
 = Constant()
extra_add
present
Constrain mask index to integer types
Constrain input and output types to float tensors.
Hidden layer sizes of Q, K, V paths in Attention
weight
X_bias = Identity (X)
X_bias = Add (X, bias)
                T1 = Mul (X_bias, X_bias)
                T2 = Mul (c, T1)
                T3 = Add (b, T2)
                T4 = Mul (X_bias, T3)
                T5 = Tanh (T4)
                T6 = Add (one, T5)
                T7 = Mul (X_bias, T6)
                Y = Mul (a, T7)
            
D:\a\_work\1\s\engine\lotus\onnxruntime\core\graph\contrib_ops\contrib_defs.cc
Whether every token can only attend to previous tokens. Default value is 0.
Number of attention heads
DecoderAttention
Constrain to integer types
query
q_weight
global_weight
global_bias
global
input_zero_point
Constrain input and output types to int8 tensors.
weight_zero_point
One sided attention windows length W, or half of total window length
LongformerAttention
window
input_scale
weight_scale
segment_ids
word_embedding
position_embedding
segment_embedding
new_key_cache
Constrain input and output types to float and float16 tensors.
new_value_cache
The epsilon value to use to avoid division by zero.
Constrain key_padding_mask to bool tensors.
input_ids
static_kv
use_past
has_layer_state
has_key_padding_mask
kv_weight
key_padding_mask
key_cache
value_cache
word_embedding_zero_point
position_embedding_zero_point
segment_embedding_zero_point
gamma_zero_point
position_embedding_scale
segment_embedding_scale
gamma_scale
beta_scale
word_embedding_quant
Constrain input and output float tensors types.
gamma_quant
position_embedding_quant
beta_quant
word_embedding_scale
position_ids
Constrain input and output integer tensors types
embedding_sum
scores
scores_out
Constrain scores input and output types to float tensors.
Constrain indices to integer types
The maximum NGram size for suffix matching.
The minimum NGram size for suffix matching.
Saved inverse standard variance used during training to speed up gradient computation.
Saved mean used during training to speed up gradient computation
Constrain mean and inv_std_var to float tensors.
inv_std_var
The NGram size.
output tensor
Constrain input and output types to float or half tensors.
beta_zero_point
layernorm_out
mask_index_out
input tensor
Constrain input and output types to float32 tensors.
'Scale' must contain exactly 2 values - (height, width)
'Border' attribute must be present and must contain exactly 4 values - (left_border, top_border, right_border, bottom_border)
) needs to be greater than or equal to the top_border (
) + bottom_border (
) needs to be greater than or equal to the left_border (
) + right_border (
) + scale[1] (
) + scale[0] (
Crop and image to the specified spatial dimensions. If scale is given,
then optionally start the crop offset by the left/top border amounts.
If scale is not provided, crop the borders as provided.
input_as_shape
Produces a slice of the input tensor along multiple axes. Similar to numpy:
https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html
Slices uses `axes`, `starts` and `ends` inputs to specify the start and end
dimension for each axis in the list of axes, it uses this information to
slice the input `data` tensor. If a negative value is passed for any of the
start or end indices, it represent number of elements before the end of that
dimension. If the value passed to start or end is larger than the `n` (the
number of elements in this dimension), it represents `n`. For slicing to the
end of a dimension with unknown size, it is recommended to pass in `INT_MAX`.
If `axes` are omitted, they are set to `[0, ..., ndim-1]`.
Example 1:
  data = [
      [1, 2, 3, 4],
      [5, 6, 7, 8],
  axes = [0, 1]
  starts = [1, 0]
  ends = [2, 3]
  result = [
      [5, 6, 7],
Example 2:
  data = [
      [1, 2, 3, 4],
      [5, 6, 7, 8],
  starts = [0, 1]
  ends = [-1, 1000]
  result = [
      [2, 3, 4],
Scale takes one input data (Tensor<float>) and produces one output data
(Tensor<float>) whose value is the input data tensor scaled element-wise.
extra_shape
Input's shape must be 4-D
GRUUnit computes the activations of a standard GRU,
in a sequence-length aware fashion.
Concretely, given the (fused) inputs X (TxNxD), the previous hidden
state (NxD), and the sequence lengths (N), computes the GRU
activations, avoiding computation if the input is invalid (as in, the
value at X[t][n] >= seqLengths[n].
tokens
suffix_match_idx
Affine takes one input data (Tensor<T>) and produces one output data
(Tensor<T>) where the affine function, y = alpha * x + beta,
is applied to the tensor elementwise.
Constrain to integer types.
Scale and bias the input image. Bias values are stored in
the same ordering as the image pixel format.
ParametricSoftplus takes one input data (Tensor<T>) and produces one output data
(Tensor<T>) where the softplus function, y = alpha * ln(exp(beta * x) + 1), is applied to
the tensor elementwise.
src_tokens
cur_tokens
prev_suffix_match_idx
pred_tokens
Square = Mul (XU, XU)
Mean2D = ReduceMean <axes = [1]> (XU)
SquareOfMean = Mul (Mean2D, Mean2D)
MeanOfSquare = ReduceMean <axes = [1]> (Square)
VarPlusEpsilon = Add (Var, Epsilon)
Var = Sub (MeanOfSquare, SquareOfMean)
Deviation = Sub (XU, Mean2D)
StdDev = Sqrt (VarPlusEpsilon)
PrefixShape = Slice (XShape, Zero1D, Axis1D)
Axis1D = Constant()
NumReducedAxes = Neg (Axis1D)
NumReducedAxes = Sub (Rank, Axis1D)
ReducedShape = Concat <axis = 0> (PrefixShape, SuffixShape)
SuffixShape = ConstantOfShape (NumReducedAxes)
XU = Cast (X2D)
X2D = Flatten (X)
rois input tensor has wrong dimension
first input tensor has wrong dimension
crop_size shape input tensor has wrong dimension
batch_indices shape input tensor has wrong dimension
XShape = Shape (X)
stash_type
Zero1D = Constant()
Rank = Size (XShape)
  Tokenizer divides each string in X into a vector of strings along the last axis. Allowed input shapes are [C] and [N, C].
  If the maximum number of tokens found per input string is D, the output shape would be [N, C, D] when input shape is [N, C].
  Similarly, if input shape is [C] then the output should be [C, D]. Tokenizer has two different operation modes.
  The first mode is selected when "tokenexp" is not set and "separators" is set. If "tokenexp" is set and "separators" is not set,
  the second mode will be used. The first mode breaks each input string into tokens by matching and removing separators.
  "separators" is a list of strings which are regular expressions. "tokenexp" is a single regular expression.
  Let's assume "separators" is [" "] and consider an example.
  If input is
  ["Hello World", "I love computer science !"] whose shape is [2],
  then the output would be
 [["Hello", "World", padvalue, padvalue, padvalue],
 ["I", "love", "computer", "science", "!"]]
 whose shape is [2, 5] because you can find at most 5 tokens per input string.
 Note that the input at most can have two axes, so 3-D and higher dimension are not supported.
 If "separators" contains a single empty string, the Tokenizer will enter into character tokenezation mode. This means all strings
 will be broken part into individual characters.
 For each input string, the second mode searches matches of "tokenexp" and each match will be a token in Y.
 The matching of "tokenexp" is conducted greedily (i.e., a match should be as long as possible).
 This operator searches for the first match starting from the beginning of the considered string,
 and then launches another search starting from the first remained character after the first matched token.
 If no match found, this operator will remove the first character from the remained string and do another search.
 This procedure will be repeated until reaching the end of the considered string.
  Let's consider another example to illustrate the effect of setting "mark" to true.
  If input is ["Hello", "World"],
  then the corresponding output would be [0x02, "Hello", "World", 0x03].
  This implies that if mark is true, [C]/[N, C] - input's output shape becomes [C, D+2]/[N, C, D+2].
If tokenizer removes the entire content of [C]-input, it will produce [[]].
I.e. the output shape should be [C][0] or [N][C][0] if input shape was [N][C].
If the tokenizer receives empty input of [0] then the output is [0] if empty input
of [N, 0] then [N, 0].
Input axis is invalid: 
Duplicate of FusedMatMul. Going forward FusedMatMul should be used. This OP will be supported for backward compatibility.
Matrix product that behaves like numpy.matmul: https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.matmul.html
inputs are expected to have tensor type and output type should not be null.
both data and indices tensor need to have rank larger than zero.
Matrix product that behaves like numpy.matmul: https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.matmul.html
'pads' input must be a 1D (shape: [input_rank]) or 2D tensor (shape: [1, input_rank]) of type int64
last dimension of indices must not be larger and rank of data tensor
onnxruntime::contrib::RegisterContribSchemas::<lambda_273270d63d2edeeb9cb50e9432a42c00>::operator ()
!(isinf_only && isnan_only)
Value of alpha
      Given an `input` and a flow-field `grid`, computes the `output` using `input` values and pixel locations from `grid`.
      Currently, only spatial (4-D) inputs are supported. For `input` with shape (N, C, H, W) and `grid` with shape (N, H_out, W_out, 2),
      the `output` will have shape (N, C, H_out, W_out).
      For each output location `output[n, :, h, w]`, the size-2 vector `grid[n, h, w]` specifies `input` pixel locations `x` and `y`,
      which are used to interpolate the output value `output[n, :, h, w]`.
      The GridSample operator is often used in doing grid generator and sampler in the [Spatial Transformer Networks](https://arxiv.org/abs/1506.02025).
      See also in [torch.nn.functional.grid_sample](https://pytorch.org/docs/master/generated/torch.nn.functional.grid_sample.html#torch-nn-functional-grid-sample).
      
1D input tensor
Value of beta
Bias applied to each channel, same size as C.
1D output tensor
 != mat_h:
Input rank must be >= 2.
      Based on Torch operator Embedding, creates a lookup table of embedding vectors of fixed size,
       for a dictionary of fixed size.
      
The inner-most 2 dimensions must have the same size (mat_w:
isinf_only
      Returns the upper or lower triangular part of a 2-D matrix, or batches of 2-D matrices. If the attribute "upper" is set to true,
      the upper triangular matrix is retained. Lower triangular matrix is retained otherwise. Default value for upper is true.
      Trilu takes one input tensor of shape [*, N, M], where * is zero or more batch dimensions. The upper triangular part consists
      of the elements on and above the given diagonal (k). The lower triangular part consists of elements on and below the diagonal.
      All other elements in the matrix are set to zero.
      If k = 0, the triangular part on and above/below the main diagonal is retained.
      If upper is set to true, a positive k retains the upper triangular matrix excluding k diagonals above
      the main diagonal. A negative k value includes as many diagonals below the main diagonal.
      If upper is set to false, a positive k retains the lower triangular matrix including k diagonals above
      the main diagonal. A negative k value excludes as many diagonals below the main diagonal.
      
Both attributes isinf_only and isnan_only cannot be set. Unset both to check for both conditions.
isnan_only
Mean = Reshape (Mean2D, ReducedShape)
InvStdDev2D = Reciprocal (StdDev)
Gaussian Error Linear Unit.
A high-performing neural network activation function.The GELU nonlinearity is
the expected transformation of a stochastic regularizer which randomly applies
the identity or zero map to a neuron's input. The GELU nonlinearity weights
inputs by their magnitude, rather than gates inputs by their sign as in ReLUs.
InvStdDev = Reshape (InvStdDev2D, ReducedShape)
Bias Gelu.
It's an extension of Gelu. It takes the sum of input A and bias input B as the input of Gelu activation. 
                CX = Mul (C, X)
                ERFCX = Erf (CX)
                ERFCXPlus1 = Add (ERFCX, One)
                PhiX = Mul (ERFCXPlus1, Half)
                Y = Mul (X, PhiX)
            
NormalizedT = Cast (Normalized)
Normalized = Div (Deviation, StdDev)
Scaled = Mul (NormalizedT, Scale2D)
Scale2D = Flatten <axis = 0> (Scale)
Biased = Add (Scaled, B2D)
B2D = Flatten <axis=0> (B)
Y = Reshape (Biased, XShape)
Biased = Identity (Scaled)
The previous GRU hidden state.
drop_states
Unactivated gate outputs from forget, update, and output gates, pre-activation.
hidden_prev
Array of sequence lengths.  len(seq_lengths) should equal batch size N.
gates
The timestep for this operation.
seq_lengths
The shape of filled tensor
GivenTensorFill
values
The filled tensor
Output data after scaling
Input data to be scaled
Bool to determine if hidden state is zeroes or passed along for timesteps past the given sequence_length.
GRUUnit
Tensor of data to extract slices from.
Output tensor
1-D tensor of starting indices of corresponding axis in `axes`
1-D tensor of axes that `starts` and `ends` apply to.
1-D tensor of ending indices (exclusive) of corresponding axis in axes
Constrain input and output types to all tensor types.
Sliced data tensor.
Input tensor of shape [N,C,H,W]
The scale to apply.
A 1-D values of (leftBorder, topBorder, rightBorder, bottomBorder).
Result, has same shape and type as input
Result, has same type as input, with H and W dimensions reduced.
A 1-D values of (height, width).
Input tensor
Threshold value
Input tensor A. The shape of A should be (M, K) if transA is 0, or (K, M) if transA is non-zero.
The FusedGemm operator schema is the same as Gemm besides it includes attributes
activation and leaky_relu_alpha.
Input tensor C. The shape of C should be unidirectional broadcastable to (M, N).
Input tensor B. The shape of B should be (K, N) if transB is 0, or (N, K) if transB is non-zero.
Constrain input and output types to float/int tensors.
Output tensor of shape (M, N).
Whether B should be transposed
Whether A should be transposed
input_0
ComplexMul
ComplexMulConj
input_1
Constrain input and output types to float tensors
The fused convolution operator schema is the same as Conv besides it includes an attribute
activation.
For internal use.
Sample echo operator.
Constrain input0 and output types to float tensors
normalized
signal_ndim
Irfft
onesided
hidden
The new GRU hidden state calculated by this op.
If 1, mean and variance are computed across channels. Default is 0.
Perform mean variance normalization.
Scaling value
If 0, normalize the mean only.  Default is 1.
Constrain to any tensor type. If the dtype attribute is not provided this must be a valid output type.
The scaled hyperbolic tangent values of the input tensor computed element-wise
N-dimensional dense matrix B
2-dimensional sparse matrix A. Either COO or CSR format
sparse_tensor(double)
sparse_tensor(float)
sparse_tensor(int32)
sparse_tensor(int64)
sparse_tensor(uint32)
sparse_tensor(uint64)
Constrain input A data types as 16-bit integer tensor
Matrix multiply results from A * B
Constrain output Y data types as 32-bit integer tensor.T3 must be tensor(uint32) when both T1 and T2 are tensor(uint16),or must be tensor(int32) when either T1 or T2 is tensor(int16).
Constrain input B data types as 16-bit integer tensor
Whether A should be transposed on the last two dimensions before doing multiplication
Scalar multiplier for the product of the input tensors.
Matrix multiply results
Whether B should be transposed on the last two dimensions before doing multiplication
The string used to pad output tensors when the tokens extracted doesn't match the maximum number of tokens found. If start/end markers are needed, padding will appear outside the markers.
Boolean whether to mark the beginning/end character with start of text character (0x02)/end of text character (0x03).
an optional list of strings attribute that contains a list of separators - regular expressions to match separators Two consecutive segments in X connected by a separator would be divided into two tokens. For example, if the input is "Hello World!" and this attribute contains only one space character, the corresponding output would be ["Hello", "World!"]. To achieve character-level tokenization, one should set the 'separators' to [""], which contains an empty string.
An optional string. Token's regular expression in basic POSIX format (pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap09.html#tag_09_03). If set, tokenizer may produce tokens matching the specified pattern. Note that one and only of 'tokenexp' and 'separators' should be set.
Matrix product that behaves like numpy.matmul: https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.matmul.html.
 The production MUST never overflow. The accumulation may overflow if and only if in 32 bits.
Minimum number of characters allowed in the output. For example, if mincharnum is 2, tokens such as "A" and "B" would be ignored
N-dimensional matrix B
N-dimensional matrix A
Scalar multiplier for input tensor C.
Scalar multiplier for the product of input tensors A * B.
Specified axis to insert a dimension
activation_gamma
Strings to tokenize
ExpandDims echo operator.
Input/Output is a string tensor
Tokenized strings
Three modes: `constant`(default) - pads with a given constant value, `reflect` - pads with the reflection of the vector mirrored on the first and last values of the vector along each axis, `edge` - pads with the edge values of array
The WordConvEmbedding takes in a batch of sequence words and embed each word to a vector.
Tensor of integers indicating the number of padding elements to add or remove (if negative) at the beginning and end of each axis. For 2D input tensor, it is the number of pixels. `pads` should be a 1D tensor of shape [2 * input_rank] or a 2D tensor of shape [1, 2 * input_rank]. `pads` format (1D example) should be as follow [x1_begin, x2_begin,...,x1_end, x2_end,...], where xi_begin is the number of pixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`.
Input tensor.
Tensor after padding.
(Optional) A scalar or rank 1 tensor containing a single value to be filled if the mode chosen is `constant` (by default it is 0.0).
A 1-D input tensor that is to be processed.
            Given `data` tensor, pads, mode, and value.
            Example:
            Insert 0 pads to the beginning of the second dimension.
            data = [
                    [1.0, 1.2],
                    [2.3, 3.4],
                    [4.5, 5.7],
                    ]
            pads = [0, 2, 0, 0]
            output = [
                    [
                    [0.0, 0.0, 1.0, 1.2],
                    [0.0, 0.0, 2.3, 3.4],
                    [0.0, 0.0, 4.5, 5.7],
                    ],
                    ]
            
Specify batchs of sequence words to embedding
Integer representing the embedding vector size for each char.If not provide, use the char embedding size of embedding vector.
Specify weights of conv
Sequence
Specify embedding vector of char
Specify bias of conv
Constrain to tensor(float).
Constrain to tensor(int32).
indices
Tensor of rank q >= 1.
Constrain input and output types to any tensor type.
Tensor of rank q-1+r-indices[-1].
Given `data` tensor of rank r >= 1, and `indices` tensor of rank q >= 1, gather
slices of `data` into an output tensor of rank q - 1 + r - indices[-1].
Example 1:
  data    = [[0,1],[2,3]]
  indices = [[0,0],[1,1]]
  output  = [0,3]
Example 2:
  data    = [[0,1],[2,3]]
  indices = [[1],[0]]
  output  = [[2,3],[0,1]]
Example 3:
  data    = [[[0,1],[2,3]],[[4,5],[6,7]]]
  indices = [[0,1],[1,0]]
  output  = [[2,3],[4,5]]
Example 4:
  data    = [[[0,1],[2,3]],[[4,5],[6,7]]]
  indices = [[[0,1]],[[1,0]]]
  output  = [[[2,3]],[[4,5]]]
Constrain indice type to int32 or int64
This operator applies convolution to word from left to right with window equal to conv_window_size and stride to 1.Take word 'example' for example, with conv_window_size equal to 2, conv is applied to [ex],[xa], [am], [mp]...If not provide, use the first dimension of conv kernal shape.
Integer representing the embedding vector size for each word.If not provide, use the fileter size of conv weight
An input tensor to hash.
The underlying implementation is MurmurHash3_x86_32 generating low latency 32bits hash suitable for implementing lookup tables, Bloom filters, count min sketch or feature hashing.
Constrain input type to unsigned or signed 32-bit integer tensor, or string tensor. It should be utf-8 encoded if using unicode.
32-bit hash value.
Seed for the hashing algorithm, unsigned 32-bit integer, default to 0.
Constrain output type to unsigned and signed 32-bit integer tensor.
Tensor of rank r >= 1.
If value is 1, output type is uint32_t, else int32_t. Default value is 1.
        Extracts crops from the input image tensor and resizes them using bilinear sampling or nearest neighbor sampling
        (possibly with aspect ratio change) to a common output size specified by crop_height and crop_width.
        Returns a tensor with crops from the input image at positions defined at the bounding box locations in boxes.
        The cropped boxes are all resized (with bilinear or nearest neighbor interpolation) to
        a fixed size = [crop_height, crop_width]. The result is a 4-D tensor [num_boxes, crop_height, crop_width, depth].
        The resizing is corner aligned.
Constrain types to int tensors.
type used for stash mean/inv_std_var
The first normalization dimension: normalization will be performed along dimensions axis : rank(inputs).
Scale tensor.
Input data tensor from the previous layer.
Output data tensor.
Bias tensor.
RoIs (Regions of Interest) to pool over; rois is 2-D input of shape (num_rois, 4) given as [[y1, x1, y2, x2], ...]. The RoIs' coordinates are normalized in the coordinate system of the input image. Each coordinate set has a 1:1 correspondence with the 'batch_indices' input.
batch_indices
1-D tensor of shape (num_rois,) with each element denoting the index of the corresponding image in the batch.
crop_size
1-D tensor of 2 elements: [crop_height, crop_width]. All cropped image patches are resized to this size. Both crop_height and crop_width need to be positive.
Constrain types to float tensors.
RoI pooled output, 4-D tensor of shape (num_rois, C, crop_height, crop_width). The r-th batch element Y[r-1] is a pooled feature map corresponding to the r-th RoI X[r-1].
2D matrix with shape (M,N)
The distance metric to use. If a string, the distance function can be "braycurtis", "canberra", "chebyshev", "cityblock", "correlation", "cosine", "dice", "euclidean", "hamming", "jaccard", "jensenshannon", "kulsinski", "mahalanobis", "matching", "minkowski", "rogerstanimoto", "russellrao", "seuclidean", "sokalmichener", "sokalsneath", "sqeuclidean", "wminkowski", "yule".
A 2D Matrix that represents the distance between each pair of the two collections of inputs.
2D matrix with shape (K,N)
The pooling method. Two modes are supported: 'bilinear' and 'nearest'. Default is 'bilinear'.
Constrains input to only numeric types.
Input data tensor from the previous operator; 4-D feature map of shape (N, C, H, W), where N is the batch size, C is the number of channels, and H and W are the height and the width of the data.
Value used for extrapolation, when applicable. Default is 0.0f. 
A 1-D tensor of the same type as 'x' containing all the unique values in 'x' sorted in the same order that they occur in the input 'x'
A 1-D INT64 tensor of the same size as 'x' containing the indices for each value in 'x' in the output 'uniques'
counts
A 1-D INT64 tensor containing the the count of each element of 'uniques' in the input 'x'
              Finds all the unique values (deduped list) present in the given input tensor.
              This operator returns 3 outputs.
              The first output tensor 'uniques' contains all of the unique elements of the input,
              sorted in the same order that they occur in the input.
              The second output tensor 'idx' is the same size as the input and it contains the index
              of each value of the input in 'uniques'.
              The third output tensor 'counts' contains the count of each element of 'uniques' in the input.
              Example:
                input_x = [2, 1, 1, 3, 4, 3]
                output_uniques = [2, 1, 3, 4]
                output_idx = [0, 1, 1, 2, 3, 2]
                output_counts = [1, 2, 2, 1]
              
Input can be of any tensor type.
Y = softmax(scores + bias)) with simple broadcast on bias. Intended to specialize softmax(scores + additive_mask) commonly found in transformer models.
Constrain input and output types to all numeric tensors and bool tensors.
broadcast bias across input for dimensions broadcast_axis to softmax_axis-1
apply softmax to elements for dimensions softmax_axis or higher
output, dropout_mask = Dropout(data + bias, ratio) + residual, Intended to specialize the dropout pattern commonly found in transformer models.
The bias (or mask) as Tensor.
The bias input, a vector with the same shape as last dim of data OR same shape with data
(Optional) Seed to the random generator, if not specified we will auto generate one.
A 0-D bool tensor. If given, this will scale gradients by the inverse of frequency of the indices (words) in the mini-batch. Default  is ``False``
padding_idx
Output tensor of the same type as the input tensor. Shape of the output is * x M, where '*' is the shape of input indices, and 'M' is the embedding size.
scale_grad_by_freq
Boolean. Indicates whether upper or lower part of matrix is retained. Default is true.
Constrain input and output types to all numeric tensors.
A 0-D tensor containing a single value corresponding to the number diagonals above or the main diagonal to exclude or include.Default value is 0 if it's not specified.
Input tensor of rank 2 or higher.
The bias input data that is a 1D tensor.
The normal input data.
Output tensor of the same type and shape as the input tensor.
Input tensor. Every matrix in the batch must be invertible.
The embedding matrix of size N x M. 'N' is equal to the maximum possible index + 1, and 'M' is equal to the embedding size
TorchEmbedding
A 0-D scalar tensor. If specified, the entries at `padding_idx` do not contribute to the gradient; therefore, the embedding vector at `padding_idx` is not updated during training, i.e. it remains as a fixed pad.
Long tensor containing the indices to extract from embedding matrix.
InvStdDev
Saved inverse standard deviation used during training to speed up gradient computation.
Type of Mean and InvStdDev tensors.
Constrain input types and output Y type to float tensors.
Constrain mean and inv_std_var to be float tensors.
Constrain input and output types (except mean and inv_std_var) to float tensors.
The output.
The input data as Tensor.
4-D tensor of shape (N, C, H, W), where N is the batch size, C is the numbers of channels, H and W are the height and width of the input data.
If align_corners=1, the extrema (-1 and 1) are considered as referring to the center points of the input's corner pixels. If align_corners=0, they are instead considered as referring to the corner points of the input's corner pixels, making the sampling more resolution agnostic.
Input offset, 4-D tensor of shape (N, H_out, W_out, 2), where H_out and W_out are the height and width of grid and output, Grid specifies the sampling pixel locations normalized by the input spatial dimensions. Therefore, it should have most values in the range of [-1, 1]. If grid has values outside the range of [-1, 1], the corresponding outputs will be handled as defined by padding_mode.
Constrain input types to all tensor types.
4-D tensor of shape (N, C, H_out, W_out).
Constrain output types to float tensors.
If true, check only for Inf, -Inf.
IsAllFinite
Constrain the output to a boolean tensor.
If true, check only for NaN.
The output scalar. Its value is true if all input tensors are finite. Otherwise, the output value would be false.
Input tensors to check.
Support padding modes for outside grid values: `zeros`(default), `border`, `reflection`. zeros: use 0 for out-of-bound grid locations, border: use border values for out-of-bound grid locations, reflection: use values at locations reflected by the border for out-of-bound grid locations.
Three interpolation modes: bilinear (default), nearest and bicubic.
residual
The residual input, must have the same shape as data
ratio
The ratio of random dropout, with value in [0, 1). If this input was not set, or if it was set to 0, the output would be a simple copy of the input. If it's non-zero, output will be a random dropout of input, which is typically the case during training.
The output mask of dropout.
If set to true then it indicates dropout is being used for training. It is an optional value hence unless specified explicitly, it is false. If it is false, ratio is ignored and the operation mimics inference mode where nothing will be dropped from the input data and if mask is requested as output it will contain all ones.
Constrain output 'mask' types to boolean tensors.
Constrain input 'ratio' types to float tensors.
+ cannot be safely updated to 
 in one of the subgraphs.
onnxruntime::graph_utils::CanUpdateImplicitInputNameInSubgraphs
 Implicit input name 
std::all_of(output_edges.cbegin(), output_edges.cend(), [&src_idx](const GraphEdge& edge) { return edge.src_arg_index == src_idx; })
Node must only have one used output
for node: 
onnxruntime::graph_utils::RemoveNodeWithSingleNodeInSingleUsedOutput
std::count_if(subgraph_node.InputEdgesBegin(), subgraph_node.InputEdgesEnd(), [input_slot_index](const Node::EdgeEnd& entry) { return entry.GetDstArgIndex() == input_slot_index; }) == 0
D:\a\_work\1\s\engine\lotus\onnxruntime\core\graph\graph_utils.cc
onnxruntime::graph_utils::UpdateImplicitInputNameInSubgraph
onnxruntime::graph_utils::GetNodeOutputName
Failed since multiple edges matched:
Attempting to get an input that does not exist.
onnxruntime::graph_utils::FindPath
onnxruntime::graph_utils::GetNodeInputName
index >= 0 && static_cast<size_t>(index) < inputs.size()
index >= 0 && static_cast<size_t>(index) < outputs.size()
Attempting to get an output that does not exist.
 ExplicitInputs:
 ImplicitInputs:
Invalid input index for node 
. Index:
Can only add a new input at the end of the current ones.
onnxruntime::graph_utils::ReplaceNodeInput
onnxruntime::graph_utils::AddNodeInput
num_explicit_inputs == static_cast<size_t>(target_input_idx)
itr != node_args.end()
Attempting to get index by a name which does not exist:
Should be unreachable if CanRemoveNodeAndMergeEdges is in sync with the logic here.
onnxruntime::graph_utils::GetIndexFromName
Initializer with same name exists. Name:
onnxruntime::graph_utils::RemoveNode
onnxruntime::graph_utils::AddInitializer
!graph.GetInitializedTensor(new_initializer.name(), existing)
TENSOR
FLOAT
UNDEFINED
SPARSE_TENSORS
SPARSE_TENSOR
STRING
FLOATS
GRAPH
STRINGS
GRAPHS
TENSORS
Null string in strings attribute. Invalid ORT format model.
Null strings attribute. Invalid ORT format model.
Null tensor in tensors attribute. Invalid ORT format model.
Null tensors attribute. Invalid ORT format model.
Null string attribute. Invalid ORT format model.
Missing dims for sparse initializer: 
Null tensor attribute. Invalid ORT format model.
onnxruntime::fbs::utils::LoadAttributeOrtFormat
Empty graph proto from deserialization of ORT format model
Null graph attribute. Invalid ORT format model.
Null ints attribute. Invalid ORT format model.
Null floats attribute. Invalid ORT format model.
Missing string data for initializer. Invalid ORT format model.
onnxruntime::fbs::utils::LoadInitializerOrtFormat
Missing values for sparse initializer. Invalid ORT format model.
Missing raw data for initializer. Invalid ORT format model.
Missing name for SparseTensor initializer. Invalid ORT format model.
onnxruntime::fbs::utils::LoadSparseInitializerOrtFormat
Missing indicies for sparse initializer: 
Invalid ORT format model.
onnxruntime::fbs::utils::SaveInitializerOrtFormat
onnxruntime::fbs::utils::SaveSparseInitializerOrtFormat
D:\a\_work\1\s\engine\lotus\onnxruntime\core\graph\graph_flatbuffers_utils.cc
Graph attribute value was null. Invalid ORT format model.
onnxruntime::fbs::utils::SaveAttributeOrtFormat
Missing dimensions for initializer. Invalid ORT format model.
SaveAttributeOrtFormat: Unsupported attribute type: 
D:\a\_work\1\s\engine\lotus\onnxruntime\core/graph/function_impl.h
onnxruntime::ViewerFunctionImpl::Body
 is out of bounds.
onnxruntime::ViewerFunctionImpl::MutableBody
 does not contain a graph.
GraphProto attribute inferencing is not enabled in this InferenceContextImpl instance.
Not supported
Resolve subgraph failed:
_dummy
node_in_parent_graph->InputDefs().size() == function_body_graph.GetInputsIncludingInitializers().size()
's number of inputs is different from function body graph's number of input.
node_in_parent_graph->OutputDefs().size() == function_body_graph.GetOutputs().size()
's number of outputs is different from function body graph's number of outputs.
onnxruntime::InitNestedModelLocalFunction
Function body initialization failed for Function '
onnxruntime::UpdateSubgraphsWithinFunctionBody
A node with a function body within a subgraph within another function body is currently not supported in ORT
onnxruntime::CreateSchema
input_arg->Type() != nullptr
onnxruntime::FunctionImpl::FunctionImpl
fused_function_subgraph
D:\a\_work\1\s\engine\lotus\onnxruntime\core\graph\function.cc
onnxruntime::IOTypeConstraintHelper
'. Error message 
 referenced by function body node 
 optype 
Cannot infer type and shape for function
. No opset import for domain
 in function opset imports.
domain_version != -1
No opset registered for domain 
B_scale
B_zero_point
C_scale
C_zero_point
{additionalDocumentation}
{name}
A_scale
A_zero_point
Input data type does not match the expected data type
Scale and Zero-point must be a scalar
Input data type does not match the expected data type. Current data type is 
Performs element-wise binary {name} on 8 bit data types (with Numpy-style broadcasting support).
{additionalDocumentation}
inputs are expected to have tensor type.
Constrain 'y', 'x_scale' to float tensors.
Constrain 'x' and 'x_zero_point' to 8-bit integer tensors.
ReduceSumInteger
Constrain input type to 8-bit integer tensor.
reduced
Constrain 'y_zero_point' and 'y' to 8-bit integer tensors.
Constrain 'x', 'y_scale' to float tensors.
x_scale
x_zero_point
The axis along which same quantization parameters are applied. It's optional.If it's not specified, it means per-tensor quantization and input 'x_scale' and 'x_zero_point' must be scalars.If it's specified, it means per 'axis' quantization and input 'x_scale' and 'x_zero_point' must be 1-D tensors.
D:\a\_work\1\s\engine\lotus\onnxruntime\core\graph\contrib_ops\quantization_defs.cc
y_scale
y_zero_point
Constrain input and output types to 8 bit signed and unsigned tensors.
Required attribute axis is missing
axis must be in [-rank, rank-1]. input rank was 
All inputs to Concat must have same rank
axis must be in [-rank, rank)
QLinearReduceMean
multiplication
data_scale
reduced_scale
data_zero_point
a_scale
Constrain input A data type to 8-bit integer tensor.
a_zero_point
C = (A_scale * (A - A_zero_point) + B_scale * (B - B_zero_point))/C_scale + C_zero_point
Constrain input a_scale, b_scale and output Y data type as float tensor.
C = ((A - A_zero_point) * (B - B_zero_point)) * (A_scale * B_scale)/C_scale + C_zero_point
addition
b_scale
b_zero_point
Constrain input A, b_scale and output Y data type as float tensor.
Constrain input B data type to 8-bit integer tensor.
A list of integers, along which to reduce. The default is to reduce over all the dimensions of the input tensor.
Constrain output data type to 32-bit integer tensor.T2 must be tensor(uint32) when T1 is tensor(uint8),or must be tensor(int32) when T1 is tensor(int8).
MulInteger
Keep the reduced dimension or not, default 1 mean keep reduced dimension.
Constrain output types to 32 bit tensors.
Constrain input types to 8 bit signed and unsigned tensors.
sequence_lens
initial_h
initial_c
Cell clip threshold. Clipping bounds the elements of a tensor in the range of [-threshold, +threshold] and is applied to the input of activations. No clip if not specified.
Optional scaling values used by some activation functions. The values are consumed in the order of activation functions, for example (f, g, h) in LSTM. Default values are the same as of corresponding ONNX operators.
Couple the input and forget gates if 1.
A list of 3 (or 6 if bidirectional) activation functions for input, output, forget, cell, and hidden. The activation functions must be one of the activation functions specified above. Optional: See the equations for default if not specified.
Y_scale
Constrain input and output types to 8 bit tensors.
Y_zero_point
Specify if the RNN is forward, reverse, or bidirectional. Must be one of forward (default), reverse, or bidirectional.
Optional scaling values used by some activation functions. The values are consumed in the order of activation functions, for example (f, g, h) in LSTM. Default values are the same as of corresponding ONNX operators.For example with LeakyRelu, the default alpha is 0.01.
Number of neurons in the hidden layer
reduced_zero_point
Coefficient of leakage.
X_scale
X_zero_point
Constrain input A and its zero point types to 8 bit tensors.
Constrain scale types to float tensors.
Constrain input C to 32 bit integer tensors.
Constrain input B and its zero point types to 8 bit tensors.
inputs
Sequence of (Tensor, Scale, ZeroPoint) tuples. The type is sequence of (T8, TF, T8).
Constrain scale types to any float tensor type.
Constrain seq_lens to integer tensor.
Which axis to concat on
Constrain weights types to 8 bit tensors.
Constrain output type to float32 or 8 bit tensors.
Constrain output zero point types to 8 bit tensors.
limit
delta
Can not get shape initializer data!
Constrain input and output types.
Unsupported type:
Unsupported non-raw-data data type!
Constrain input and output types to singed/unsigned int8 tensors.
Whether include pad pixels when calculating values for the edges. Default is 0, doesn't count include pad.
auto_pad must be either NOTSET, SAME_UPPER, SAME_LOWER or VALID. Where default value is NOTSET, which means explicit padding is used. SAME_UPPER or SAME_LOWER mean pad the input so that the output spatial size match the input.In case of odd number add the extra padding at the end for SAME_UPPER and at the beginning for SAME_LOWER. VALID mean no padding.
Padding for the beginning and ending along each spatial axis, it can take any value greater than or equal to 0. The value represent the number of pixels added to the beginning and end part of the corresponding axis. `pads` format should be as follow [x1_begin, x2_begin...x1_end, x2_end,...], where xi_begin the number of pixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`. This attribute cannot be used simultaneously with auto_pad attribute. If not present, the padding defaults to 0 along start and end of each spatial axis.
D:\a\_work\1\s\engine\lotus\onnxruntime\core\graph\contrib_ops\nhwc_schema_defs.cc
w_zero_point
w_scale
Output tensor must have at least 2 dimensions
Input tensor must have at least 2 dimensions
weight and zero_point pair is expected to have same type.
input and zero_point pair is expected to have be same type.
Stride along each spatial axis. If not present, the stride defaults to 1 along each spatial axis.
The size of the kernel along each axis.
Works on NHWC layout or not? Default not.
Whether to use ceil or floor (default) to compute the output shape.
memory_seq_lens
Couple the input and forget gates if 1, default 0.
Constrain seq_lens to integral tensors.
Number of neurons in the hidden layer.
bad dimensions
@@QLinearGlobalAveragePool ImageSize too large!
QLinearGlobalAveragePool parameter out of computation range!
code != static_cast<int>(common::OK)
EP_FAIL
GENERAL ERROR
SystemError
[ONNXRuntimeError]
onnxruntime::common::Status::Status
D:\a\_work\1\s\engine\lotus\onnxruntime\core\common\status.cc
ENGINE_ERROR
RUNTIME_EXCEPTION
NO_SUCHFILE
NO_MODEL
NOT_IMPLEMENTED
INVALID_GRAPH
INVALID_PROTOBUF
MODEL_LOADED
SUCCESS
 fail, errcode = 
open file 
Fatal error: 0 count processors from GetLogicalProcessorInformation
Invalid fd was supplied: 
Received negative size from stat call
GetFileSizeEx 
GetFileLength: File is too large
custom_create_thread_fn returned invalid handle.
onnxruntime::`anonymous-namespace'::WindowsThread::WindowsThread
Fatal error: 0 count processors from GetSystemInfo
onnxruntime::`anonymous-namespace'::WindowsEnv::GetNumCpuCores
D:\a\_work\1\s\engine\lotus\onnxruntime\core\platform\windows\env.cc
onnxruntime
system
D:\a\_work\1\s\engine\lotus\onnxruntime\core/platform/path_lib.h
onnxruntime::LoopDir
LoadLibrary failed with error 
" when trying to load "
onnxruntime::`anonymous-namespace'::WindowsEnv::FormatLibraryFileName
FreeLibrary failed with error 
Failed to find symbol in library, error code: 
, error code: 
DeleteFile() failed - path: 
MapFileIntoMemory is not implemented on Windows.
GetFinalPathNameByHandle() failed: 
onnxruntime::`anonymous-namespace'::WindowsEnv::GetCanonicalPath
onnxruntime::`anonymous-namespace'::WindowsEnv::DeleteFolder
RemoveDirectory() failed - path: 
onnxruntime::`anonymous-namespace'::WindowsEnv::ReadFileIntoBuffer
offset < 0
File is too large.
file_path == nullptr
ReadFile 
 fail: unexpected end
length > buffer.size()
SetFilePointerEx 
default_logger_id must be provided if instance_type is InstanceType::Default
Only one instance of LoggingManager created with InstanceType::Default can exist at any point in time.
onnxruntime::logging::LoggingManager::LoggingManager
D:\a\_work\1\s\engine\lotus\onnxruntime\core\common\logging\logging.cc
Default logger already set. 
onnxruntime::logging::LoggingManager::CreateDefaultLogger
ISink must be provided.
onnxruntime
onnxruntime::Path::Parse
onnxruntime::`anonymous-namespace'::ParsePathRoot
D:\a\_work\1\s\engine\lotus\onnxruntime\core\common\path.cc
Failed to parse path root: 
unnamed_thread_pool
Profiler not started yet
"core": 
"num_run": 
WaitRevoke
UnknownEvent
!current_parallel_section
onnxruntime::concurrency::ThreadPool::ParallelSection::ParallelSection
Nested parallelism not supported
], "core": 
", "block_size": [
"thread_id": "
Distribution
DistributionEnqueue
LogStart must pair with LogEnd
!points_.empty()
points_.empty()
onnxruntime::concurrency::ThreadPoolProfiler::MainThreadStat::Reset
onnxruntime::concurrency::ThreadPoolProfiler::MainThreadStat::LogEnd
onnxruntime::concurrency::ThreadPoolProfiler::MainThreadStat::LogEndAndStart
D:\a\_work\1\s\engine\lotus\onnxruntime\core\common\threadpool.cc
enabled_
onnxruntime::concurrency::ThreadPoolProfiler::Stop
"thread_pool_name": "
{"main_thread": {
}, "sub_threads": {
onnxruntime::concurrency::ThreadPool::ParallelFor
More work items than threads
n >= 0
D:\a\_work\1\s\engine\lotus\include\onnxruntime\core/platform/EigenNonBlockingThreadPool.h
onnxruntime::concurrency::ThreadPoolTempl<class onnxruntime::Env>::RunInParallelSection
n <= num_threads_+1
onnxruntime::concurrency::ThreadPoolTempl<class onnxruntime::Env>::RunInParallel
onnxruntime::ToWideString
onnxruntime::ToMBString
D:\a\_work\1\s\engine\lotus\onnxruntime\core\common\helper.cc
length overflow
onnxruntime::profiling::Profiler::Initialize
custom_logger != nullptr
D:\a\_work\1\s\engine\lotus\onnxruntime\core\common\profiler.cc
session_logger != nullptr
Maximum number of events reached, could not record profile event.
onnxruntime::profiling::Profiler::EndTimeAndRecordEvent
onnxruntime::profiling::Profiler::StartProfiling
onnxruntime::profiling::Profiler::Start
"name" :"
"args" : {
"ts" :
"ph" : "X",
" : "
Writing profiler data to file 
onnxruntime::profiling::Profiler::EndProfiling
"tid" :
"dur" :
{"cat" : "
"pid" :
unexpected failure
illegal input path:
 is not a valid year
 is not a valid day
 is not a valid date
sequence_type
map_type
tensor_type
onnxruntime::fbs::utils::SaveMapTypeOrtFormat
onnxruntime::fbs::utils::SaveTensorTypeAndShapeOrtFormat
onnxruntime::fbs::utils::SaveSequenceTypeOrtFormat
D:\a\_work\1\s\engine\lotus\onnxruntime\core\flatbuffers\flatbuffers_utils.cc
We do not support type [
onnxruntime::fbs::utils::SaveValueInfoOrtFormat
onnxruntime::fbs::utils::SaveTypeInfoOrtFormat
] for now
onnxruntime::fbs::utils::LoadValueInfoOrtFormat
Model must have opset imports. Invalid ORT format model.
Type:
Null type info for 
opset import domain is null. Invalid ORT format model.
onnxruntime::fbs::utils::LoadOpsetImportOrtFormat
opset id is null. Invalid ORT format model.
onnxruntime::fbs::utils::LoadMapTypeOrtFormat
Null tensor type info. Invalid ORT format model.
onnxruntime::fbs::utils::LoadSequenceTypeOrtFormat
Null value type info in fbs::MapType. Invalid ORT format model.
Null map type info. Invalid ORT format model.
 is not supported currently
onnxruntime::fbs::utils::LoadTypeInfoOrtFormat
Null sequence type info. Invalid ORT format model.
dim_param value with no name. Invalid ORT format model.
onnxruntime::fbs::utils::LoadTensorDimensionOrtFormat
 is missing type info.
SaveValueInfoOrtFormat: value_info_proto for 
onnxruntime::fbs::utils::LoadTensorTypeAndShapeOrtFormat
Null value type info in fbs::SequenceType. Invalid ORT format model.
Null entry in dimensions. Invalid ORT format model.
onnxruntime::fbs::utils::LoadTensorShapeOrtFormat
) in op definition.
) than declared (
 not in allowed output sizes.
has output size 
)'s input 
) has more outputs (
) has more inputs (
 is marked single but has an empty string in the graph
Operator '
, max=
 has unsupported type 
' has been deprecated since version 
 not in allowed input sizes.
) has output size 
 not in range [min=
) has input size 
 typestr: 
 has inconsistent type 
, has unsupported type: 
(outputs_.size() - 1) == i
!(it.GetName().empty())
: failed validating the check: 
ONNX Schema 
Required attribute '
Attribute specification type mismatch.
 has unknown expected type
' is missing.
Extra unparsed input unexpected.
(inputs_.size() - 1) == i
Duplicate type constraint name
Error parsing function body:
' is expected to have field 'floats'
' is expected to have field 'ints'
' is expected to have field 'g'
' is expected to have field 'type_proto'
' is expected to have field 'graphs'
' is expected to have field 'type_protos'
' is expected to have field 'strings'
' is expected to have field 'tensors'
Attribute '
 for operator 
)'s output 
' appeared multiple times.
' is expected to have field 't'
' is expected to have field 'sparse_tensor'
Unrecognized attribute: 
Mismatched attribute type in '
opaque
Invalid tensor data type 
optional
complex128
DataTypeUtils::FromDataTypeString - Received invalid data type string 
complex64
tensor(
Invalid data type 
sparse_tensor(
Unsuported type proto value case.
optional(
Values greater than this are mapped to 1, others to 0.
The input must be a tensor of a numeric type. The output will be of the same tensor type.
The input must be an integer map to either string or float.
map(int64, string)
The input must be a tensor of a numeric type or string. The output will be of the same tensor type.
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\traditionalml\defs.cc
An integer vocabulary array.<br>One and only one of the vocabularies must be defined.
The input type must be a tensor of a numeric type.
The size of each input in the input list
map(string, int64)
map(int64, double)
The input must be a map from strings or integers to either strings or a numeric type. The key and value types cannot be the same.
The output will be a tensor of the value type of the input map. It's shape will be [1,C], where C is the length of the input dictionary.
A string vocabulary array.<br>One and only one of the vocabularies must be defined.
map(string, float)
map(string, double)
The strings of the map. This sequence must be the same length as the 'cats_int64s' sequence
The integers of the map. This sequence must be the same length as the 'cats_strings' sequence.
The input must be a tensor of strings or integers, either [N,C] or [C].
The output is a tensor of strings or integers. Its shape will be the same as the input shape.
A string to use when an input integer value is not found in the map.<br>One and only one of the 'default_*' attributes must be defined.
An integer to use when an input string value is not found in the map.<br>One and only one of the 'default_*' attributes must be defined.
A string indicating the desired element type of the output tensor, one of 'TO_FLOAT', 'TO_STRING', 'TO_INT64'.
Indicates whether to only output as many values as are in the input (dense), or position the input based on using the key of the map as the index of the output (sparse).<br>One of 'DENSE', 'SPARSE'.
map(int64, float)
The output is a 1-D tensor of string, float, or integer.
If the value of map_form is 'SPARSE,' this attribute indicates the total length of the output tensor.
Input's shape should be 1D or 2D
The output will be a tensor of strings or integers.
A collection of weights of the model(s).
The input must be a tensor of a numeric type, and of of shape [N,C] or [C]. In the latter case, it will be treated as [1,C]
A list of ints.
A list of floats.
Output type is determined by the specified 'values_*' attribute.
A list of strings. One and only one of 'keys_*'s should be set.
An integer.
A float.
A list of strings. One and only one of 'value_*'s should be set.
A string.
Input type is not int64 tensor but keys_int64s is set
Input type is not float tensor but keys_floats is set
Only one of keys_*'s can be set in label encoder.
Input type is not string tensor but key_strings is set
The input type is a tensor of any shape.
Only one of values_*'s can be set in label encoder.
Value(s) to change to
A value that needs replacing.
The input type must be a tensor of a numeric type, either [N,C] or [C]. The output type will be of the same tensor type and shape.
Label encoder has only one input.
Label encoder has only one output.
Value(s) to change to.
First, offset by this.<br>Can be length of features in an [N,F] tensor or length 1, in which case it applies to all features, regardless of dimension count.
Second, multiply by this.<br>Can be length of features in an [N,F] tensor or length 1, in which case it applies to all features, regardless of dimension count.<br>Must be same length as 'offset'
The input must be a tensor of a numeric type, either [C] or [N,C].
The output type will be a tensor of strings or integers, depending on which of the the classlabels_* attributes is used. Its size will match the bactch size of the input.
One of 'MAX,' 'L1,' 'L2'
If true and category is not present, will return all zeros; if false and a category if not found, the operator will fail.
List of categories, ints.<br>One and only one of the 'cats_*' attributes must be defined.
List of categories, strings.<br>One and only one of the 'cats_*' attributes must be defined.
Weights of the model(s).
Weights of the intercepts, if used.
The input must be a tensor of a numeric type.
Indicates the transform to apply to the regression output vector.<br>One of 'NONE,' 'SOFTMAX,' 'LOGISTIC,' 'SOFTMAX_ZERO,' or 'PROBIT'
The total number of regression targets, 1 if not defined.
Class labels when using string labels. One and only one 'classlabels' attribute must be defined.
Class labels when using integer labels. One and only one 'classlabels' attribute must be defined.
A collection of intercepts.
Indicates whether to do OvR or multinomial (0=OvR is the default).
Indicates the transform to apply to the scores vector.<br>One of 'NONE,' 'SOFTMAX,' 'LOGISTIC,' 'SOFTMAX_ZERO,' or 'PROBIT'
Child node if expression is true.
Child node if expression is false.
Popularity of each node, used for performance and may be omitted.
The node kind, that is, the comparison to make at the node. There is no comparison to make at a leaf node.<br>One of 'BRANCH_LEQ', 'BRANCH_LT', 'BRANCH_GTE', 'BRANCH_GT', 'BRANCH_EQ', 'BRANCH_NEQ', 'LEAF'
node id that this weight is for.
The index of the class list that each weight is for.
For each node, define what to do in the presence of a missing value: if a value is missing (NaN), use the 'true' or 'false' branch based on the value in this array.<br>This attribute may be left undefined, and the defalt value is false (0) for all nodes.
The id of the tree that this node is in.
The output type will be a tensor of strings or integers, depending on which of the the classlabels_* attributes is used.
Feature id for each node.
Thresholds to do the splitting on for each node.
Tree id for each node.
Node id for each node. Ids may restart at zero for each tree, but it not required to.
Chosen support vectors
Flag indicating whether the regression is a one-class SVM or not.
The input type must be a tensor of a numeric type, either [C] or [N,C].
Indicates the transform to apply to the score. <br>One of 'NONE,' 'SOFTMAX,' 'LOGISTIC,' 'SOFTMAX_ZERO,' or 'PROBIT.'
Support vector coefficients.
The number of support vectors.
First set of probability coefficients.
Second set of probability coefficients. This array must be same size as prob_a.<br>If these are provided then output Z are probability estimates, otherwise they are raw scores.
The kernel type, one of 'LINEAR,' 'POLY,' 'RBF,' 'SIGMOID'.
List of 3 elements containing gamma, coef0, and degree, in that order. Zero if unused for the kernel.
Class labels if using integer labels.<br>One and only one of the 'classlabels_*' attributes must be defined.
Indicates the transform to apply to the score. <br>One of 'NONE,' 'SOFTMAX,' 'LOGISTIC,' 'SOFTMAX_ZERO,' or 'PROBIT'
Class labels if using string labels.<br>One and only one of the 'classlabels_*' attributes must be defined.
The output will be a sequence of string or integer maps to float.
seq(map(string, float))
The keys when using int keys.<br>One and only one of the 'classlabels_*' attributes must be defined.
seq(map(int64, float))
The keys when using string keys.<br>One and only one of the 'classlabels_*' attributes must be defined.
The node id of each weight
The index of the target that each weight is for
For each node, define what to do in the presence of a NaN: use the 'true' (if the attribute value is 1) or 'false' (if the attribute value is 0) branch based on the value in this array.<br>This attribute may be left undefined and the defalt value is false (0) for all nodes.
The id of the tree that each node is in.
Defines how to aggregate leaf values within a target. <br>One of 'AVERAGE,' 'SUM,' 'MIN,' 'MAX.'
The weight for each target
The total number of targets.
Base values for classification, added to final class score; the size must be the same as the classes or can be left unassigned (assumed 0)
The weight for the class in class_id.
Indicates the transform to apply to the score. <br> One of 'NONE,' 'SOFTMAX,' 'LOGISTIC,' 'SOFTMAX_ZERO,' or 'PROBIT.'
Child node if expression is true
Child node if expression is false
Node id for each node. Node ids must restart at zero for each tree and increase sequentially.
The input type must be a tensor of integers or strings, of any shape.
The output type will be a tensor of strings or integers, and will have the same shape as the input.
A list of labels.
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\traditionalml\old.cc
 expected to have tensor or sparse tensor type
 and Output 
 does not match type of output: 
Input: 
 should be of integer type and specify a type.
 does not specify a valid type.
 not specified
Value of attribute 
type: 
optional(tensor(uint16))
optional(tensor(uint32))
optional(seq(tensor(complex128)))
optional(tensor(uint8))
optional(tensor(int16))
optional(tensor(int32))
optional(tensor(uint64))
optional(tensor(int8))
optional(seq(tensor(float16)))
optional(seq(tensor(float)))
optional(seq(tensor(int32)))
optional(seq(tensor(int64)))
optional(seq(tensor(bool)))
optional(seq(tensor(complex64)))
optional(seq(tensor(double)))
optional(seq(tensor(string)))
optional(seq(tensor(uint8)))
optional(seq(tensor(uint16)))
optional(seq(tensor(int8)))
optional(seq(tensor(int16)))
optional(seq(tensor(uint32)))
optional(seq(tensor(uint64)))
axis must be in [-rank, rank-1].
target_type
Invalid position of 0.
Invalid dimension value: 
Target shape may not have multiple -1 dimensions.
The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto
Constrain output types. Casting to complex is not supported.
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\tensor\defs.cc
Constrain input types. Casting from complex is not supported.
optional(tensor(float))
optional(tensor(double))
optional(tensor(int64))
optional(tensor(float16))
optional(tensor(complex64))
optional(tensor(complex128))
optional(tensor(string))
optional(tensor(bool))
Mismatch between the sum of 'split' (
The input is not evenly splittable
Mismatch between number of splits (
) and the split dimension of the input (
outputs
concat_result
Constrain output types to any tensor type.
Invalid value of attribute 'axis'. Rank=
) and outputs (
 Value=
Input tensor can be of arbitrary type.
Constrain output to int64 tensor.
(Optional) Starting axis for slicing the shape. Default value is 0.Negative value means counting dimensions from the back.
(Optional) Ending axis for slicing the shape. Negative value means counting dimensions from the back. If omitted, sizes of all axes upto (including) the last one will be included.
Constrain output to int64 tensor, which should be a scalar though.
Which axis to concat on. A negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(inputs)..
(Optional) By default, when any value in the 'shape' input is equal to zero the corresponding dimension value is copied from the input tensor dynamically. allowzero=1 indicates that if any value in the 'shape' input is set to zero, the zero value is honored, similar to NumPy.
Invalid Target shape product of 0. Product cannot be 0 in combination with -1
Dimension could not be inferred: incompatible shapes
reshaped
Input and output types can be of any tensor type.
updates
Type of reduction to apply: none (default), add, mul. 'none': no reduction applied. 'add':  reduction using the addition operation. 'mul': reduction using the multiplication operation.
reduction
A list of integers. By default, reverse the dimensions, otherwise permute the axes according to the values given.
Invalid attribute perm {
}, input shape = {
Which axis to scatter on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data).
transposed
Input rank for starts and ends should be the same: (
Input axes has invalid data
) vs (
steps
'step' cannot be 0 for Slice
Only supports `int32_t` or `int64_t` inputs for starts/ends/axes/steps
Which axis to split on. A negative value means counting dimensions from the back. Accepted range is [-rank, rank-1] where r = rank(input).
Input axes has incorrect length
Input steps has incorrect length
Slice op must have either three, four or five inputs.
Incorrect or missing input value for starts and ends
Input tensor must be 4-dimensional
Blocks of [blocksize, blocksize] are moved.
DCR (default) for depth-column-row order re-arrangement. Use CRD for column-row-depth order.
values in 'axes' are beyond the bounds of the computed output shape
'axes' attribute must not contain any duplicates
Blocksize must be positive
List of integers indicating the dimensions to be inserted. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(expanded).
expanded
squeezed
List of integers indicating the dimensions to squeeze. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data).
data tensor must have rank >= 1
axis must be in [-r, r-1]
Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data).
sizes
Constrain roi type to float or double.
Indices tensor must have rank >= 1
'axis' must be in [-rank(indices), rank(indices)-1]
Constrain input and output types to all tensor, sequence, and optional types.
Four modes: round_prefer_floor (default, as known as round half down), round_prefer_ceil (as known as round half up), floor, ceil. Only used by nearest interpolation. It indicates how to get "nearest" pixel in input tensor from x_original, so this attribute is valid only if "mode" is "nearest".
When coordinate_transformation_mode is "tf_crop_and_resize" and x_original is outside the range [0, length_original - 1], this value is used as the corresponding output value. Default is 0.0f.
The coefficient 'a' used in cubic interpolation. Two common choice are -0.5 (in some cases of TensorFlow) and -0.75 (in PyTorch). Check out Equation (4) in https://ieeexplore.ieee.org/document/1163711 for the details. This attribute is valid only if "mode" is "cubic".
If set to 1, the weight of sampling locations outside the tensor will be set to 0 and the weight will be renormalized so that their sum is 1.0. The default value is 0.
The scale array along each dimension. It takes value greater than or equal to 1. The number of elements of 'scales' should be the same as the rank of input 'X'.
Two interpolation modes: nearest (default), and linear (including bilinear, trilinear, etc)
This attribute describes how to transform the coordinate in the resized tensor to the coordinate in the original tensor. <br/>
The coordinate of each dimension is transformed individually. Let's describe a case using axis x as an example.
Denote x_resized as the coordinate of axis x in the resized tensor, x_original as the coordinate of axis x in the original tensor, length_original as the length of the original tensor in axis x, length_resized as the length of the resized tensor in axis x, roi_x = (start_x, end_x) of the axis x in input "roi", scale = length_resized / length_original, <br/>
if coordinate_transformation_mode is "half_pixel", <br/>
x_original = (x_resized + 0.5) / scale - 0.5, <br/>
if coordinate_transformation_mode is "pytorch_half_pixel", <br/>
x_original = length_resized > 1 ? (x_resized + 0.5) / scale - 0.5 : 0, <br/>
if coordinate_transformation_mode is "align_corners", <br/>
x_original = x_resized * (length_original - 1) / (length_resized - 1), <br/>
if coordinate_transformation_mode is "asymmetric", <br/>
x_original = x_resized / scale, <br/>
if coordinate_transformation_mode is "tf_crop_and_resize", <br/>
x_original = length_resized > 1 ? start_x * (length_original - 1) + x_resized * (end_x - start_x) * (length_original - 1) / (length_resized - 1) : 0.5 * (start_x + end_x) * (length_original - 1).
Three interpolation modes: nearest (default), linear and cubic. The "linear" mode includes linear interpolation for 1D tensor and N-linear interpolation for N-D tensor (for example, bilinear interpolation for 2D tensor). The "cubic" mode includes cubic interpolation for 1D tensor and N-cubic interpolation for N-D tensor (for example, bicubic interpolation for 2D tensor).
Constrain input 'X' and output 'Y' to all tensor types.
'Repeats' input must be 1D tensor of type int64
'Repeats' input has incorrect number of values. The number of values in 'repeats' must be equal to the number of input dimensions.
Constrain repeat's type to int64 tensors.
repeats
Constrain input and output types to all tensor types (including bfloat).
Constrain to boolean tensors.
Constrain input types to float tensors.
Constrain to any tensor type.
(Optional) Whether map positive infinity to true. Default to 1 so that positive infinity induces true. Set this attribute to 0 if positive infinity should be mapped to false.
(Optional) Whether map negative infinity to true. Default to 1 so that negative infinity induces true. Set this attribute to 0 if negative infinity should be mapped to false.
Constrain output types to boolean tensors.
Input 'values' must have exactly two elements.
(Optional) Axis along which one-hot representation in added. Default: axis=-1. axis=-1 means that the additional dimension will be inserted as the innermost/last dimension in the output tensor. Negative value means counting dimensions from the back. Accepted range is [-r-1, r] where r = rank(indices).
Input 'depth' must have exactly one element.
Input 'values' must be rank 1 tensor.
depth
condition
(Optional) Axis along which to take slices. If not specified, input is flattened before elements being selected. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input).
OneHot node must have three inputs.
Input 'depth' must be a scalar or rank 1 tensor.
Constrains to boolean tensors.
Supported modes: `constant`(default), `reflect`, `edge`
'pads' input must be a 1D (shape: [2 * input_rank]) tensor of type int64
constant_value
inverse_indices
The number of batch dimensions. The gather of indexing starts from dimension of data[batch_dims:]
Both `data` and `indices` input tensors in GatherND op need to have rank larger than 0.
Last dimension of `indices` input tensor in GatherND op must not be larger than the rank of `data` tensor
Invalid value for attribute axis
(Optional) Whether to sort the unique elements in ascending order before returning as output. Must be one of 0, or 1 (default).
(Optional) The dimension to apply unique. If not specified, the unique elements of the flattened input are returned. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input).
'input' must have rank >= 2
'sequence_lens' must have rank of 1
Constrain to all tensor types.
(Optional) Specify which axis is time axis. Must be one of 0 (default), or 1.
(Optional) Specify which axis is batch axis. Must be one of 1 (default), or 0.
All Tensor, Sequence, and optional types
Only bool
Graph to run if condition is true. Has N outputs: values you wish to be live-out to the enclosing scope. The number of outputs must match the number of outputs in the else_branch.
Graph to run if condition is false. Has N outputs: values you wish to be live-out to the enclosing scope. The number of outputs must match the number of outputs in the then_branch.
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\controlflow\defs.cc
Loop 'body' subgraph outputs should all be tensors or sequences or optionals, but output 
Loop 'body' subgraph scan outputs should all be tensors but output 
If node has 
 was 
 outputs. Expected 
Graph attribute inferencing returned type information for 
 was not a tensor.
Scan input 
then_branch and else_branch produce different number of outputs. 
 but subgraphs produce 
 was not
Scan 'body' subgraph outputs should all be tensors but output 
 is invalid for a tensor of rank 
 axis value 
) is not equal to number of scan outputs (
Number of scan output axes specified (
) is not equal to number of scan inputs (
Number of scan input axes specified (
An optional list of K flags. The i-th element of the list specifies the axis for the i-th scan_output. The scan outputs are accumulated along the specified axis. If omitted, 0 will be used as the scan axis for every scan_output. Negative value for an axis means counting dimensions from the back. Accepted range is [-r, r-1].
Int64 tensor
An optional list of K flags, one for each scan_output. The i-th element of the list specifies whether the i-th scan_output should be constructed by appending or prepending a new value in each iteration: 0 indicates appending and 1 indicates prepending. If omitted, all scan_output tensors will be produced by appending a value in each iteration.
An optional list of M flags. The i-th element of the list specifies the axis to be scanned (the sequence axis) for the i-th scan_input. If omitted, 0 will be used as the scan axis for every scan_input. Negative value for an axis means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input).
All Tensor types
initial_state_and_scan_inputs
An attribute specifying the number of scan_inputs M. 
An optional list of M flags. The i-th element of the list specifies the direction to be scanned for the i-th scan_input tensor: 0 indicates forward direction and 1 indicates reverse direction. If omitted, all scan_input tensors will be scanned in the forward direction.
final_state_and_scan_outputs
The graph run each iteration. It has N+M inputs: (loop state variables..., scan_input_elts...). It has N+K outputs: (loop state variables..., scan_output_elts...). Each scan_output is created by concatenating the value of the specified scan_output_elt value at the end of each iteration of the loop. It is an error if the dimensions of these values change across loop iterations.
v_final_and_scan_outputs
v_initial
tensor of int64, which should be a scalar.
tensor of bool, which should be a scalar.
The graph run each iteration. It has 2+N inputs: (iteration_num, condition, loop carried dependencies...). It has 1+N+K outputs: (condition, loop carried dependencies..., scan_outputs...). Each scan_output is created by concatenating the value of the specified output value at the end of each iteration of the loop. It is an error if the dimensions or data type of these scan_outputs change across loop iterations.
All Tensor and Sequence types
boxes
iou_threshold
max_output_boxes_per_class
Number of sampling points in the interpolation grid used to compute the output value of each pooled output bin. If > 0, then exactly sampling_ratio x sampling_ratio grid points are used. If == 0, then an adaptive number of grid points are used (computed as ceil(roi_width / output_width), and likewise for height). Default is 0.
The pooling method. Two modes are supported: 'avg' and 'max'. Default is 'avg'.
default 1; Pooled output Y's height.
default 1; Pooled output Y's width.
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\object_detection\defs.cc
Allowed values are 'half_pixel' and 'output_half_pixel'. Use the value 'half_pixel' to pixel shift the input coordinates by -0.5 (the recommended behavior). Use the value 'output_half_pixel' to omit the pixel shift for the input (use this for a backward-compatible behavior).
Multiplicative spatial scale factor to translate ROI coordinates from their input spatial scale to the scale used when pooling, i.e., spatial scale of the input feature map X relative to the input image. E.g.; default is 1.0f. 
selected_indices
score_threshold
Integer indicate the format of the box data. The default is 0. 0 - the box data is supplied as [y1, x1, y2, x2] where (y1, x1) and (y2, x2) are the coordinates of any diagonal pair of box corners and the coordinates can be provided as normalized (i.e., lying in the interval [0, 1]) or absolute. Mostly used for TF models. 1 - the box data is supplied as [x_center, y_center, width, height]. Mostly used for Pytorch models.
Constrain output type to all tensor or sequence types.
Constrains input type to optional tensor and optional sequence types.
Constrains output to a boolean tensor.
Input type is null. Input must have Type information.
Input must be an optional-type value containing an element with type information.
OptionalGetElement must have an input element.
Constrains input type to all tensor and sequence types.
Constrains output type to all optional tensor or optional sequence types.
Type of the element in the optional output
OptionalHasElement is expected to have 1 input.
OptionalHasElement is expected to have 1 output.
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\optional\defs.cc
Optional is expected to have an output.
Optional is expected to have either an input or the type attribute set.
Attribute 'type' should be a TypeProto and it should specify a type.
Input type is null. Type information is expected for the input.
Whether the operator should behave like fmod (default=0 meaning it will do integer mods); Set this to 1 to force fmod treatment
Constrain input and output types to high-precision numeric tensors.
Invalid rank for 
]. Its actual value is: 
Wrong op_type name for running propagation: 
 broadcasting: (
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\math\defs.cc
'axis' must be in [
Pow takes input data (Tensor<T>) and exponent Tensor, and
produces one output data (Tensor<T>) where the function `f(x) = x^exponent`,
is applied to the data tensor elementwise.
Constrain input X and output types to float/int tensors.
Constrain input Y types to float/int tensors.
The Alpha value in Celu formula which control the shape of the unit. The default value is 1.0.
X_alpha
Elu_Result
Coefficient of SELU default to 1.67326319217681884765625 (i.e., float32 approximation of 1.6732632423543772848170429916717).
Coefficient of ELU.
Coefficient of SELU default to 1.05070102214813232421875 (i.e., float32 approximation of 1.0507009873554804934193349852946).
Constrain input and output types to signed numeric tensors.
X_Exp
X_ReduceSum
X_ReduceMax
X_Sub
X_Log
Constrain input and output types to numeric tensors.
List of tensors for 
slope
          {
            HS_X = HardSigmoid<alpha = 0.16666667163372, beta = 0.5>(X) 
            Y = Mul (X, HS_X)
          }
        
Value of alpha.
Value of beta.
'shape' input must be 1D tensor of type INT64
Constrain input and output types to all tensors.
Constrain index tensor to int64
Dimension on which to do the sort. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input).
Indices
Whether to return the top-K largest or smallest elements.
Whether to return the elements in sorted order.
K input must be of type int64.
Axis has less than the requested k elements.
K input must be a one-dimensional tensor of size 1.
Values
Constrain input b and its zero point data type to 8-bit integer tensor.
Constrain output y and its zero point data type to 8-bit integer tensor.
If set to 1 will return exclusive sum in which the top element is not included. In other terms, if set to 1, the j-th output element would be the sum of the first (j-1) elements. Otherwise, it would be the sum of the first j elements.
Constrain output Y data type as 32-bit integer tensor.
Constrain input a and its zero point data type to 8-bit integer tensor.
input and zero_point pair is expected to have same type.
transform_targets
const_zero_float
const_zero_target_typed
expanded_target_int64
squeeze_mask
const_one_float
const_zero_casted
input_gather_element_transform
loss_Ndd
weight_gather
loss_N1dd
weight_gather_sum
const_ignore_index
loss_unweighted
loss_sum
const_one
target
const_zero
input_gather_element
loss_NCdd
expanded_target
ignore_index
If set to 1 will perform the sums in reverse direction.
Constrain input and output types to floating-point tensors.
axis tensor can be int32 or int64 only
Shape3D
X_NCD
Type of reduction to apply to loss: none, sum, mean(default). 'none': no reduction will be applied, 'sum': the output will be summed. 'mean': the sum of the output will be divided by the number of elements in the output.
X_LogSM_NCD
X_shape
X_NDC
X_LogSM
Rank of input 
Ellipsis represents incompatible dimensions.
 does not match the equation indices.
Inputs
Constrain input and output types to all numerical tensor types.
Einsum expression string.
Type of reduction to apply to loss: none, sum, mean (default). 'none': the output is the loss for each sample. 'sum': the output will be summed. 'mean': the sum of the output will be divided by the sum of applied weights.
Specifies a target value that is ignored and does not contribute to the input gradient. It's an optional value.
NegativeLogLikelihoodLoss
Number of input tensors does not match the operands in the equation.
Constrain input, weight, and output types to floating-point tensors.
Constrain target to integer types
weight_gather_temp_1
Target rank must be 1 less than the input rank.
const_one_casted
weight_gather_temp
Input and target dimension value mismatch.
Weight rank must be 1.
log_prob
labels
SoftmaxCrossEntropyLoss
Attribute 'value_floats' expect a list of floats.
Attribute 'value_string' expect a string.
Attribute 'value_ints' expect a list of integers.
Attribute 'value_float' expect a float.
The value for the elements of the output tensor.
The value for the elements of the output tensor in sparse format.
Attribute 'value_strings' expect a list of strings.
TypeAndShapeInferenceFunction implementation incomplete: this line should never be reached.
value_float
value_floats
value_int
value_ints
One and only one of the attributes 'value', 'value_*' or 'sparse_value' must be specified for a Constant node.
Attribute 'value_int' expect an integer.
value_string
value_strings
sparse_value
Attribute expected to have a one-dim tensor
Attribute expected to have a one-dim sparse tensor
 or UNDEFINED. Got: 
 expected to have: 
Upper boundary of the output values.
The data type for the elements of the output tensor. If not specified, default is TensorProto::FLOAT.
Lower boundary of the output values.
The mean of the normal distribution.
The shape of the output tensor.
(Optional) Index of the diagonal to be populated with ones. Default is 0. If T2 is the output, this op sets T2[i, i+k] = 1. k = 0 populates the main diagonal, k > 0 populates an upper diagonal,  and k < 0 populates a lower diagonal.
(Optional) The data type for the elements of the output tensor. If not specified,the data type of the input tensor T1 is used. If input tensor T1 is also notspecified, then type defaults to 'float'.
Input tensor must be 2-dimensional
Constrain input types. Strings and complex are not supported.
Constrain output types. Strings and complex are not supported.
Invalid shape value: 
(Optional) The value of the output elements.Should be a one-element tensor. If not specified, it defaults to a tensor of value 0 and datatype float32
Shape input must be a one-dimensional tensor.
Constrain input types.
Constrain output types to be numerics.
The value for the sole element for the scalar, float32, output tensor.
The values for the elements for the 1D, float32, output tensor.
The value for the sole element for the scalar, int64, output tensor.
The values for the elements for the 1D, int64, output tensor.
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\generator\defs.cc
The value for the sole element for the scalar, UTF-8 string, output tensor.
The values for the elements for the 1D, UTF-8 string, output tensor.
The data type for the elements of the output tensor. if not specified, we will use the data type of the input tensor.
X_random
X_greater
Bernoulli
Constrain output types to all numeric tensors and bool tensors.
All inputs to 'Range' op must be of the same type
          {
            sub_result = Sub (limit, start)
            sub_result_casted = Cast <to = 1> (sub_result)
            delta_casted = Cast <to = 1> (delta)
            div_result = Div (sub_result_casted, delta_casted)
            ceil_result = Ceil (div_result)
            ceil_result_relu = Relu (ceil_result)
            ceil_result_relu_int = Cast <to = 7> (ceil_result_relu)
            ceil_result_relu_bool = Cast <to = 9> (ceil_result_relu)
            variadic_output, output = Loop (ceil_result_relu_int, ceil_result_relu_bool, start)
              <body = loop_body_attribute (int64 i, bool cond, prev) => (cond_out, current, range) {
                cond_out = Identity (cond)
                current = Add (prev, delta)
                range = Identity (prev)
              }>
          }
        
Constrain input types to common numeric type tensors.
Number of times to sample.
(Optional) The data type for the elements of the output tensor, if not specified, we will use int32.
Output type must be int32 or int64
Input tensor must have rank 2
Constrain output types to integral tensors.
The standard deviation of the normal distribution.
The data type for the elements of the output tensor. Default is TensorProto::FLOAT.
(Optional) The data type for the elements of the output tensor, if not specified, we will use the data type of the input tensor.
Input to 'Range' op should be scalars (Tensor with only one element and shape empty)
auto_pad must be either NOTSET, SAME_UPPER, SAME_LOWER or VALID. Where default value is NOTSET, which means explicit padding is used. SAME_UPPER or SAME_LOWER mean pad the input so that `output_shape[i] = input_shape[i] * strides[i]` for each axis `i`. The padding is split between the two sides equally or almost equally (depending on whether it is even or odd). In case the padding is an odd number, the extra padding is added at the end for SAME_UPPER and at the beginning for SAME_LOWER.
Input tensor must have atleast 2 dimensions
auto_pad must be either NOTSET, SAME_UPPER, SAME_LOWER or VALID. Where default value is NOTSET, which means explicit padding is used. SAME_UPPER or SAME_LOWER mean pad the input so that `output_shape[i] = ceil(input_shape[i] / strides[i])` for each axis `i`. The padding is split between the two sides equally or almost equally (depending on whether it is even or odd). In case the padding is an odd number, the extra padding is added at the end for SAME_UPPER and at the beginning for SAME_LOWER.
Attribute kernel_shape has incorrect size
Attribute kernel_shape must be specified
Attribute dilations has incorrect size
Attribute strides has incorrect size
This operator has **optional** inputs/outputs. See [the doc](IR.md) for more details about the representation of optional arguments. An empty string may be used in the place of an actual argument's name to indicate a missing argument. Trailing optional arguments (those not followed by an argument that is present) may also be simply omitted.
RoIs tensor must have 2 dimensions
Attribute pooled_shape has incorrect length
Multiplicative spatial scale factor to translate ROI coordinates from their input scale to the scale used when pooling.
Attribute pooled_shape must be specified
ROI pool output shape (height, width).
'output_shape' must be rank 1 tensor.
'output_shape' must have same number of elements as the shape of input tensor X.
p value of the Lp norm used to pool over the input data.
MaxUnpool op must have either two or three inputs.
Input tensor X must have atleast 2 dimensions.
Dilation value along each spatial axis of filter. If not present, the dilation defaults to 1 along each spatial axis.
Attribute kernel_shape has incorrect size.
Attribute kernel_shape must be specified.
Attribute pads has incorrect size.
Attribute strides has incorrect size.
Constrain input and output types to float and 8 bit tensors.
Attribute pads has incorrect size
Second input tensor has wrong dimension
The storage order of the tensor. 0 is row major, and 1 is column major.
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\nn\defs.cc
number of groups input channels and output channels are divided into. default is 1.
dilation value along each spatial axis of the filter. If not present, the dilation defaults to 1 along each spatial axis.
Padding for the beginning and ending along each spatial axis, it can take any value greater than or equal to 0.The value represent the number of pixels added to the beginning and end part of the corresponding axis.`pads` format should be as follow [x1_begin, x2_begin...x1_end, x2_end,...], where xi_begin the number ofpixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`.This attribute cannot be used simultaneously with auto_pad attribute. If not present, the padding defaultsto 0 along start and end of each spatial axis.
Constrain input x and its zero point data type to 8-bit integer tensor.
Constrain input w and its zero point data type to 8-bit integer tensor.
Constrain bias type to 32-bit integer tensor.
The shape of the convolution kernel. If not present, should be inferred from input 'w'.
Constrain filter type to 8-bit integer tensor.
Constrain output type to 8-bit integer tensor.
Stride along each spatial axis. If not present, the stride defaults is 1 along each spatial axis.
number of groups input channels and output channels are divided into.
The shape of the convolution kernel. If not present, should be inferred from input W.
dilation value along each spatial axis of the filter. If not present, the dilation defaults is 1 along each spatial axis.
running_var
running_mean
Constrain scale and bias types to float tensors.
Constrain mean and variance types to float tensors.
input_var
input_mean
Carries out batch normalization as described in the paper
https://arxiv.org/abs/1502.03167. Depending on the mode it is being run,
There are five required inputs 'X', 'scale', 'B', 'input_mean' and
'input_var'.
Note that 'input_mean' and 'input_var' are expected to be the estimated
statistics in inference mode (training_mode=False, default),
and the running statistics in training mode (training_mode=True).
There are multiple cases for the number of outputs, which we list below:
Output case #1: Y, running_mean, running_var (training_mode=True)
Output case #2: Y (training_mode=False)
When training_mode=False, extra outputs are invalid.
The outputs are updated as follows when training_mode=True:
running_mean = input_mean * momentum + current_mean * (1 - momentum)
running_var = input_var * momentum + current_var * (1 - momentum)
Y = (X - current_mean) / sqrt(current_var + epsilon) * scale + B
where:
current_mean = ReduceMean(X, axis=all_except_channel_index)
current_var =  ReduceVar(X, axis=all_except_channel_index)
Notice that ReduceVar refers to the population variance, and it equals to
sum(sqrd(x_i - x_avg)) / N
where N is the population size (this formula does not use sample size N - 1).
The computation of ReduceMean and ReduceVar uses float to avoid overflow for float16 inputs.
When training_mode=False:
Y = (X - input_mean) / sqrt(input_var + epsilon) * scale + B
For previous (depreciated) non-spatial cases, implementors are suggested
to flatten the input shape to (N x C * D1 * D2 * ... * Dn) before a BatchNormalization Op.
Additional elements added to the side with higher coordinate indices in the output. Each padding value in "output_padding" must be less than the corresponding stride/dilation dimension. By default, this attribute is a zero vector. Note that this attribute doesn't directly affect the computed output values. It only controls the selection of the computed values, so changing this attribute only adds or removes output elements. If "output_shape" is explicitly provided, "output_padding" does not contribute additional size to "output_shape" but participates in the computation of the needed padding amount. This is also called adjs or adjustment in some frameworks.
Factor used in computing the running mean and variance.e.g., running_mean = running_mean * momentum + mean * (1 - momentum).
If set to true, it indicates BatchNormalization is being used for training, and outputs 1, 2, 3, and 4 would be populated.
This number of op outputs should be 3 when Training_mode = True, but it is not.
This number of op outputs should be 1 when Training_mode = False, but it is not.
Stride along each spatial axis. If not present, the stride defaults to 1 along each axis.
The pads attribute cannot be used simultaneously with auto_pad attribute
Constrain output y data type to 32-bit integer tensor.
dilation value along each spatial axis of the filter. If not present, the dilation defaults to 1 along each axis.
The shape of the output can be explicitly set which will cause pads values to be auto generated. If output_shape is specified pads values are ignored. See doc for details for equations to generate pads
Constrain input and output  types to float tensors.
Scaling parameter.
The exponent.
Input tensor must have rank 1 or 2
ngram_indexes must be non-empty with no negative values
) for attribute 'axis'
Invalid value(
The number of channels to sum over
Constrain input and output to all tensor types.
Indicate up to which input dimensions (exclusive) should be flattened to the outer dimension of the output. The value for axis must be in the range [-r, r], where r is the rank of the input tensor. Negative value means counting dimensions from the back. When axis = 0, the shape of the output tensor is (1, (d_0 X d_1 ... d_n), where the shape of the input tensor is (d_0, d_1, ... d_n). 
Ratio of Dropout must be a scalar.
training_mode of Dropout must be a scalar.
The bias value added to output. Default is 0.
The lambd value for the Shrink formulation. Default is 0.5.
The order of the normalization, only 1 or 2 are supported.
The axis on which to apply normalization, -1 mean last axis.
List of stop words. If not set, no word would be removed from X.
Environment dependent string that denotes the locale according to which output strings needs to be upper/lowercased.Default en_US or platform specific equivalent as decided by the implementation.
string enum that cases output to be lowercased/uppercases/unchanged. Valid values are "LOWER", "UPPER", "NONE". Default is "NONE"
Boolean. Whether the identification of stop words in X is case-sensitive. Default is false
        {
          Exponent = Constant <value = float {2.0}>()
          Epsilon = Constant <value = float {1e-9}>()
          X_RM = ReduceMean <axes : ints = @axes> (X)
          EX_squared = Pow (X_RM, Exponent)
          X_squared = Pow (X, Exponent)
          E_Xsquared = ReduceMean <axes : ints = @axes> (X_squared)
          Variance = Sub (E_Xsquared, EX_squared)
          STD = Sqrt (Variance)
          X_variance = Sub (X, X_RM)
          Processed_STD = Add (STD, Epsilon)
          Y = Div (X_variance, Processed_STD)
        }
        
A list of integers, along which to reduce. The default is to caculate along axes [0,2,3] for calculating mean and variance along each channel. Two variables with the same C-coordinate are associated with the same mean and variance.
list of floats. This attribute stores the weight of each n-gram in pool. The i-th element in weights is the weight of the i-th n-gram in pool. Its length equals to the size of ngram_indexes. By default, weights is an all-one tensor.This attribute is used when mode is "IDF" or "TFIDF" to scale the associated word counts.
The weighting criteria. It can be one of "TF" (term frequency), "IDF" (inverse document frequency), and "TFIDF" (the combination of TF and IDF)
The starting indexes of 1-grams, 2-grams, and so on in pool. It is useful when determining the boundary between two consecutive collections of n-grams. For example, if ngram_counts is [0, 17, 36], the first index (zero-based) of 1-gram/2-gram/3-gram in pool are 0/17/36. This format is essentially identical to CSR (or CSC) sparse matrix format, and we choose to use this due to its popularity.
list of int64s (type: AttributeProto::INTS). This list is parallel to the specified 'pool_*' attribute. The i-th element in ngram_indexes indicate the coordinate of the i-th n-gram in the output tensor.
Input shape must have either [C] or [1,C] dimensions where C > 0
1-D tensor of floats
Maximum n-gram length. If this value is 3, 3-grams will be used to generate the output.
Input is ether string UTF-8 or int32/int64
List of strings n-grams learned from the training set. Either this or pool_int64s attributes must be present but not both. It's an 1-D tensor starting with the collections of all 1-grams and ending with the collections of n-grams. The i-th element in pool stores the n-gram that should be mapped to coordinate ngram_indexes[i] in the output vector.
List of int64 n-grams learned from the training set. Either this or pool_strings attributes must be present but not both. It's an 1-D tensor starting with the collections of all 1-grams and ending with the collections of n-grams. The i-th element in pool stores the n-gram that should be mapped to coordinate ngram_indexes[i] in the output vector.
Minimum n-gram length. If this value is 2 and max_gram_length is 3, output may contain counts of 2-grams and 3-grams.
Maximum number of items (integers/strings) to be skipped when constructing an n-gram from X. If max_skip_count=1, min_gram_length=2, max_gram_length=3, this operator may generate 2-grams with skip_count=0 and skip_count=1, and 3-grams with skip_count=0 and skip_count=1
Invalid position of 0
Invalid Target shape product of 0
length of each output. Values should be >= 0.
'step' cannot be 0
Target shape may not have multiple -1 dimensions
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\tensor\old.cc
New shape
legacy optimization attribute.
One float, indicates the value to be filled, default is 0
consumed_inputs
Which axis to split on
length of each output
outputs...
paddings
Three modes: constant(default), reflect, edge
List of integers indicate the padding element count at the beginning and end of each axis, for 2D it is the number of pixel. `paddings` rank should be double of the input's rank. `paddings` format should be as follow [x1_begin, x2_begin...x1_end, x2_end,...], where xi_begin the number of pixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`.
Constrain input types. Casting from strings and complex are not supported.
Constrain output types. Casting to strings and complex are not supported.
Constrains input and output to only numeric types.
rank must be greater than axis
Which axis to concat on.  Default value is 1.
This attribute describes how to transform the coordinate in the resized tensor to the coordinate in the original tensor. <br/>
The coordinate of each dimension is transformed individually. Let's describe a case using axis x as an example.
Denote x_resized as the coordinate of axis x in the resized tensor, x_original as the coordinate of axis x in the original tensor, length_original as the length of the original tensor in axis x, length_resized as the length of the resized tensor in axis x, roi_x = (start_x, end_x) of the axis x in input "roi", scale = length_resized / length_original, <br/>
if coordinate_transformation_mode is "half_pixel", <br/>
x_original = (x_resized + 0.5) / scale - 0.5, <br/>
if coordinate_transformation_mode is "pytorch_half_pixel", <br/>
x_original = length_resized > 1 ? (x_resized + 0.5) / scale - 0.5 : 0, <br/>
if coordinate_transformation_mode is "align_corners", <br/>
x_original = x_resized * (length_original - 1) / (length_resized - 1), <br/>
if coordinate_transformation_mode is "asymmetric", <br/>
x_original = x_resized / scale, <br/>
if coordinate_transformation_mode is "tf_half_pixel_for_nn", <br/>
x_original = (x_resized + 0.5) / scale, <br/>
if coordinate_transformation_mode is "tf_crop_and_resize", <br/>
x_original = length_resized > 1 ? start_x * (length_original - 1) + x_resized * (end_x - start_x) * (length_original - 1) / (length_resized - 1) : 0.5 * (start_x + end_x) * (length_original - 1).
Ending indices (exclusive) of corresponding axis in axes`
Which axis to scatter on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1]
Attribute 'scales' is required.
Axes that `starts` and `ends` apply to. It's optional. If not present, will be treated as [0, 1, ..., len(`starts`) - 1].
Starting indices of corresponding axis in `axes`
Incorrect or missing attribute value for starts and ends
Attribute axes has incorrect length
Constrain output types to bool, int32, int64, float16, float, double tensors.
Two interpolation modes: nearest(default), bilinear
Number of elements of attribute 'scales' must be same as rank of input 'X'
Attribute 'scales' must have floats type.
) is not equal to the existing rank value (
Ranks inferred (
Constrain tiles and axis's type to int64 tensors.
tiles
The scale along height dimension. It takes value greater than or equal to 1.
height_scale
The scale along width dimension. It takes value greater than or equal to 1.
width_scale
Constrain input and output types to all tensor and sequence types.
List of integers indicating the number of padding elements to add or remove (if negative) at the beginning and end of each axis. For 2D it is the number of pixels. `pads` rank should be double of the input's rank. `pads` format should be as follow [x1_begin, x2_begin...x1_end, x2_end,...], where xi_begin the number of pixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`.
One float, indicates the value to be filled.
(Optional) Axis along which to take slices. If not specified, input is flattened before elements being selected.
Attribute value for pads is required
Attribute pads has incorrect length
Which axis to split on. 
List of non-negative integers, indicate the dimensions to squeeze.
Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1]
'axis' must be in [-rank(indices)-1, rank(indices)]
(Optional) Axis along which one-hot representation in added. Default: axis=-1. axis=-1 means that the additional dimension will be inserted as the innermost/last dimension in the output tensor.
List of non-negative integers, indicate the dimensions to be inserted
EX_squared
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\nn\old.cc
Variance
X_squared
E_Xsquared
If set to nonzero, run spatial batch normalization in test mode, default is 0.
is_test
If true, compute the mean and variance across all spatial elements If false, compute the mean and variance across per feature.Default is 1.
The epsilon value to use to avoid division by zero, default is 1e-5f.
Factor used in computing the running mean and variance.e.g., running_mean = running_mean * momentum + mean * (1 - momentum), default is 0.9f.
p value of the Lp norm used to pool over the input data, default is 2.0.
Stride along each axis.
The zero-padding added to one side of the output. This is also called adjs/adjustment in some frameworks.
dilation value along each spatial axis of the filter.
Stride along each spatial axis.
X_variance
Processed_STD
Padding for the beginning and ending along each axis, it can take any value greater than or equal to 0. The value represent the number of pixels added to the beginning and end part of the corresponding axis. `pads` format should be as follow [x1_begin, x2_begin...x1_end, x2_end,...], where xi_begin the number of pixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`. This attribute cannot be used simultaneously with auto_pad attribute.
auto_pad must be either NOTSET, SAME_UPPER, SAME_LOWER or VALID. Where default value is NOTSET, which means explicit padding is used. SAME_UPPER or SAME_LOWER mean pad the input so that the output size match the input.In case of odd number add the extra padding at the end for SAME_UPPER and at the beginning for SAME_LOWER. VALID mean no padding. DEPRECATION NOTE: auto_pad is only intended to support legacy uses, and for framework authors, one is explicitly encouraged to use explicit padding specified in the pads attribute.
Dilation value along each spatial axis of filter.
If true, compute the mean and variance across per activation. If false, compute the mean and variance across per feature over each mini-batch.
Indicate up to which input dimensions (exclusive) should be flattened to the outer dimension of the output. The value for axis must be in the range [0, R], where R is the rank of the input tensor. When axis = 0, the shape of the output tensor is (1, (d_0 X d_1 ... d_n), where the shape of the input tensor is (d_0, d_1, ... d_n). 
The ratio of random dropout
(float, default 0.5) the ratio of random dropout
(int, default 0) if nonzero, run dropout in test mode where the output is simply Y = X.
Constrain output mask types to boolean tensors.
Carries out batch normalization as described in the paper
https://arxiv.org/abs/1502.03167. Depending on the mode it is being run,
there are multiple cases for the number of outputs, which we list below:
Output case #1: Y, mean, var, saved_mean, saved_var (training mode)
Output case #2: Y (test mode)
For previous (depreciated) non-spatial cases, implementors are suggested
to flatten the input shape to (N x C*D1*D2 ..*Dn) before a BatchNormalization Op.
Carries out batch normalization as described in the paper
https://arxiv.org/abs/1502.03167. Depending on the mode it is being run,
There are five required inputs 'X', 'scale', 'B', 'input_mean' and
'input_var'.
Note that 'input_mean' and 'input_var' are expected to be the estimated
statistics in inference mode (training_mode=False, default),
and the running statistics in training mode (training_mode=True).
There are multiple cases for the number of outputs, which we list below:
Output case #1: Y, running_mean, running_var (training_mode=True)
Output case #2: Y (training_mode=False)
When training_mode=False, extra outputs are invalid.
The outputs are updated as follows when training_mode=True:
running_mean = input_mean * momentum + current_mean * (1 - momentum)
running_var = input_var * momentum + current_var * (1 - momentum)
Y = (X - current_mean) / sqrt(current_var + epsilon) * scale + B
where:
current_mean = ReduceMean(X, axis=all_except_channel_index)
current_var =  ReduceVar(X, axis=all_except_channel_index)
Notice that ReduceVar refers to the population variance, and it equals to
sum(sqrd(x_i - x_avg)) / N
where N is the population size (this formula does not use sample size N - 1).
When training_mode=False:
Y = (X - input_mean) / sqrt(input_var + epsilon) * scale + B
For previous (depreciated) non-spatial cases, implementors are suggested
to flatten the input shape to (N x C * D1 * D2 * ... * Dn) before a BatchNormalization Op.
Constrain mean and variance types to float tensors. It allows all float type for U.
Computes an one-layer LSTM. This operator is usually supported via some
custom implementation such as CuDNN.
Notations:
`X` - input tensor
`i` - input gate
`o` - output gate
`f` - forget gate
`c` - cell gate
`t` - time step (t-1 means previous time step)
`W[iofc]` - W parameter weight matrix for input, output, forget, and cell gates
`R[iofc]` - R recurrence weight matrix for input, output, forget, and cell gates
`Wb[iofc]` - W bias vectors for input, output, forget, and cell gates
`Rb[iofc]` - R bias vectors for input, output, forget, and cell gates
`P[iof]`  - P peephole weight vector for input, output, and forget gates
`WB[iofc]` - W parameter weight matrix for backward input, output, forget, and cell gates
`RB[iofc]` - R recurrence weight matrix for backward input, output, forget, and cell gates
`WBb[iofc]` - W bias vectors for backward input, output, forget, and cell gates
`RBb[iofc]` - R bias vectors for backward input, output, forget, and cell gates
`PB[iof]`  - P peephole weight vector for backward input, output, and forget gates
`H` - Hidden state
`num_directions` - 2 if direction == bidirectional else 1
Activation functions:
  Relu(x)                - max(0, x)
  Tanh(x)                - (1 - e^{-2x})/(1 + e^{-2x})
  Sigmoid(x)             - 1/(1 + e^{-x})
  (NOTE: Below are optional)
  Affine(x)              - alpha*x + beta
  LeakyRelu(x)           - x if x >= 0 else alpha * x
  ThresholdedRelu(x)     - x if x >= alpha else 0
  ScaledTanh(x)          - alpha*Tanh(beta*x)
  HardSigmoid(x)         - min(max(alpha*x + beta, 0), 1)
  Elu(x)                 - x if x >= 0 else alpha*(e^x - 1)
  Softsign(x)            - x/(1 + |x|)
  Softplus(x)            - log(1 + e^x)
Equations (Default: f=Sigmoid, g=Tanh, h=Tanh):
  - it = f(Xt*(Wi^T) + Ht-1*(Ri^T) + Pi (.) Ct-1 + Wbi + Rbi)
  - ft = f(Xt*(Wf^T) + Ht-1*(Rf^T) + Pf (.) Ct-1 + Wbf + Rbf)
  - ct = g(Xt*(Wc^T) + Ht-1*(Rc^T) + Wbc + Rbc)
  - Ct = ft (.) Ct-1 + it (.) ct
  - ot = f(Xt*(Wo^T) + Ht-1*(Ro^T) + Po (.) Ct + Wbo + Rbo)
  - Ht = ot (.) h(Ct)
The shape format of inputs X, initial_h, initial_c and outputs Y, Y_h, Y_c. If 0, the following shapes are expected: X.shape = [seq_length, batch_size, input_size], Y.shape = [seq_length, num_directions, batch_size, hidden_size], initial_h.shape = Y_h.shape = initial_c.shape = Y_c.shape = [num_directions, batch_size, hidden_size]. If 1, the following shapes are expected: X.shape = [batch_size, seq_length, input_size], Y.shape = [batch_size, seq_length, num_directions, hidden_size], initial_h.shape = Y_h.shape = initial_c.shape = Y_c.shape = [batch_size, num_directions, hidden_size].
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\rnn\defs.cc
When computing the output of the hidden gate, apply the linear transformation before multiplying by the output of the reset gate.
Computes an one-layer GRU. This operator is usually supported via some custom
implementation such as CuDNN.
Notations:
`X` - input tensor
`z` - update gate
`r` - reset gate
`h` - hidden gate
`t` - time step (t-1 means previous time step)
`W[zrh]` - W parameter weight matrix for update, reset, and hidden gates
`R[zrh]` - R recurrence weight matrix for update, reset, and hidden gates
`Wb[zrh]` - W bias vectors for update, reset, and hidden gates
`Rb[zrh]` - R bias vectors for update, reset, and hidden gates
`WB[zrh]` - W parameter weight matrix for backward update, reset, and hidden gates
`RB[zrh]` - R recurrence weight matrix for backward update, reset, and hidden gates
`WBb[zrh]` - W bias vectors for backward update, reset, and hidden gates
`RBb[zrh]` - R bias vectors for backward update, reset, and hidden gates
`H` - Hidden state
`num_directions` - 2 if direction == bidirectional else 1
Activation functions:
  Relu(x)                - max(0, x)
  Tanh(x)                - (1 - e^{-2x})/(1 + e^{-2x})
  Sigmoid(x)             - 1/(1 + e^{-x})
  (NOTE: Below are optional)
  Affine(x)              - alpha*x + beta
  LeakyRelu(x)           - x if x >= 0 else alpha * x
  ThresholdedRelu(x)     - x if x >= alpha else 0
  ScaledTanh(x)          - alpha*Tanh(beta*x)
  HardSigmoid(x)         - min(max(alpha*x + beta, 0), 1)
  Elu(x)                 - x if x >= 0 else alpha*(e^x - 1)
  Softsign(x)            - x/(1 + |x|)
  Softplus(x)            - log(1 + e^x)
Equations (Default: f=Sigmoid, g=Tanh):
  - zt = f(Xt*(Wz^T) + Ht-1*(Rz^T) + Wbz + Rbz)
  - rt = f(Xt*(Wr^T) + Ht-1*(Rr^T) + Wbr + Rbr)
  - ht = g(Xt*(Wh^T) + (rt (.) Ht-1)*(Rh^T) + Rbh + Wbh) # default, when linear_before_reset = 0
  - ht = g(Xt*(Wh^T) + (rt (.) (Ht-1*(Rh^T) + Rbh)) + Wbh) # when linear_before_reset != 0
  - Ht = (1 - zt) (.) ht + zt (.) Ht-1
A list of 2 (or 4 if bidirectional) activation functions for update, reset, and hidden gates. The activation functions must be one of the activation functions specified above. Optional: See the equations for default if not specified.
Computes an one-layer simple RNN. This operator is usually supported
via some custom implementation such as CuDNN.
Notations:
`X` - input tensor
`i` - input gate
`t` - time step (t-1 means previous time step)
`Wi` - W parameter weight matrix for input gate
`Ri` - R recurrence weight matrix for input gate
`Wbi` - W parameter bias vector for input gate
`Rbi` - R parameter bias vector for input gate
`WBi` - W parameter weight matrix for backward input gate
`RBi` - R recurrence weight matrix for backward input gate
`WBbi` - WR bias vectors for backward input gate
`RBbi` - RR bias vectors for backward input gate
`H` - Hidden state
`num_directions` - 2 if direction == bidirectional else 1
Activation functions:
  Relu(x)                - max(0, x)
  Tanh(x)                - (1 - e^{-2x})/(1 + e^{-2x})
  Sigmoid(x)             - 1/(1 + e^{-x})
  (NOTE: Below are optional)
  Affine(x)              - alpha*x + beta
  LeakyRelu(x)           - x if x >= 0 else alpha * x
  ThresholdedRelu(x)     - x if x >= alpha else 0
  ScaledTanh(x)          - alpha*Tanh(beta*x)
  HardSigmoid(x)         - min(max(alpha*x + beta, 0), 1)
  Elu(x)                 - x if x >= 0 else alpha*(e^x - 1)
  Softsign(x)            - x/(1 + |x|)
  Softplus(x)            - log(1 + e^x)
Equations (Default: f=Tanh):
  - Ht = f(Xt*(Wi^T) + Ht-1*(Ri^T) + Wbi + Rbi)
One (or two if bidirectional) activation function for input gate. The activation function must be one of the activation functions specified above. Optional: Default `Tanh` if not specified.
First input tensor must have rank 3
The shape format of inputs X, initial_h and outputs Y, Y_h. If 0, the following shapes are expected: X.shape = [seq_length, batch_size, input_size], Y.shape = [seq_length, num_directions, batch_size, hidden_size], initial_h.shape = Y_h.shape = [num_directions, batch_size, hidden_size]. If 1, the following shapes are expected: X.shape = [batch_size, seq_length, input_size], Y.shape = [batch_size, seq_length, num_directions, hidden_size], initial_h.shape = Y_h.shape = [batch_size, num_directions, hidden_size].
Loop 'body' subgraph outputs should all be tensors or sequences but output 
An optional list of M flags. The i-th element of the list specifies the axis to be scanned (the sequence axis) for the i-th scan_input. If omitted, 0 will be used as the scan axis for every scan_input.
An optional list of K flags. The i-th element of the list specifies the axis for the i-th scan_output. The scan outputs are accumulated along the specified axis. If omitted, 0 will be used as the scan axis for every scan_output.
Mismatched type for output 
Mismatched tensor element type for output 
 else=
 then=
Loop 'body' subgraph outputs should all be tensors but output 
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\controlflow\old.cc
(Optional) The axis of the dequantizing dimension of the input tensor. Ignored for per-tensor quantization. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input).
Constrain 'x_zero_point' and 'x' to 8-bit/32-bit integer tensor.
(Optional) The axis of the quantization dimension of the input tensor. Ignored for per-tensor quantization. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input).
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\quantization\defs.cc
Constrain 'y_zero_point' and 'y' to 8-bit integer tensor.
Constrain 'x' to float or int32 tensor.
        {
           Q_Min = Constant<value = float {0.0}>()
           Q_Max = Constant<value = float {255.0}>()
           X_Min = ReduceMin <keepdims = 0> (x)
           X_Min_Adjusted = Min (X_Min, Q_Min)
           X_Max = ReduceMax <keepdims = 0> (x)
           X_Max_Adjusted = Max (X_Max, Q_Min)
           X_Range = Sub (X_Max_Adjusted, X_Min_Adjusted)
           Scale = Div (X_Range, Q_Max)
           Min_Scaled = Div (X_Min_Adjusted, Scale)
           Initial_ZeroPoint_FP = Sub (Q_Min, Min_Scaled)
           Clipped_ZeroPoint_FP = Clip (Initial_ZeroPoint_FP, Q_Min, Q_Max)
           Rounded_ZeroPoint_FP = Round (Clipped_ZeroPoint_FP)
           Zeropoint = Cast <to = 2> (Rounded_ZeroPoint_FP)
           y_scale = Identity (Scale)
           y_zero_point = Identity (Zeropoint)
           y = QuantizeLinear (x, Scale, Zeropoint)
        }
        
Constrain 'y_zero_point' and 'y' to 8-bit unsigned integer tensor.
Constrain 'x' to float tensor.
A list of integers, along which to reduce. The default is to reduce over all the dimensions of the input tensor. Accepted range is [-r, r-1] where r = rank(data).
Defines behaviour if 'axes' is empty. Default behaviour with 'false' is to reduce all axes. When axes is empty and this attribute is set to true, input tensor will not be reduced,and the output tensor would be equivalent to input tensor.
axes as an input and attribute cannot be specified at the same time.
Whether to select the last index or the first index if the {name} appears in multiple indices, default is False (first index).
The axis in which to compute the arg indices. Accepted range is [-r, r-1] where r = rank(data).
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\reduction\defs.cc
Constrain input and output types to high-precision and 8 bit numeric tensors.
This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).
Coefficient of ELU default to 1.0.
Coefficient of SELU default to 1.0507.
Value of beta default to 0.5
Value of alpha default to 0.2
If set, defines the broadcast dimensions. See doc for details.
Coefficient of SELU default to 1.6732.
Coefficient of leakage default to 0.01.
Pass 1 to enable broadcasting
If necessary the right-hand-side argument will be broadcasted to match the
shape of left-hand-side argument. When broadcasting is specified, the second
tensor can either be of element size 1 (including a scalar tensor and any
tensor with rank equal to or smaller than the first tensor), or having its
shape as a contiguous subset of the first tensor's shape. The starting of the
mutually equal shape is specified by the argument "axis", and if it is not set,
suffix matching is assumed. 1-dim expansion doesn't work yet.
For example, the following tensor shapes are supported (with broadcast=1):
  shape(A) = (2, 3, 4, 5), shape(B) = (,), i.e. B is a scalar tensor
  shape(A) = (2, 3, 4, 5), shape(B) = (1, 1), i.e. B is an 1-element tensor
  shape(A) = (2, 3, 4, 5), shape(B) = (5,)
  shape(A) = (2, 3, 4, 5), shape(B) = (4, 5)
  shape(A) = (2, 3, 4, 5), shape(B) = (3, 4), with axis=1
  shape(A) = (2, 3, 4, 5), shape(B) = (2), with axis=0
Attribute `broadcast=1` needs to be passed to enable broadcasting.
Describes the axis of the inputs when coerced to 2D; defaults to one because the 0th axis most likely describes the batch_size
Describes the axis of the inputs when coerced to 2D; defaults to one because the 0th axis most likely describes the batch_size. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input).
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\math\old.cc
Scalar multiplier for input tensor C, the default value is 1.0.
Scalar multiplier for the product of input tensors A * B, the default value is 1.0.
Dimension on which to do the sort.
Number of top elements to retrieve
Invalid value for attribute k
Maximum value, above which element is replaced by max
Minimum value, under which element is replaced by min
Whether C should be broadcasted
Constrains input/output to boolean tensors.
Constrain input and output types to integer tensors.
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\logical\defs.cc
Constrains output to boolean tensor.
Constrains input to boolean tensor.
Constrains input types to all numeric tensors.
        {
            O1 = Less (A, B)
            O2 = Equal (A, B)
            C = Or (O1, O2)
        }
        
Direction of moving bits. It can be either "RIGHT" (for right shift) or "LEFT" (for left shift).
Attribute 'value' of Constant node must exist with 'Tensor' data.
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\generator\old.cc
One of the attributes 'value' or 'sparse_value' must be specified for a Constant node.
Only one of the attributes 'value' or 'sparse_value' must be specified for a Constant node.
Computes the indices of the {name} elements of the input tensor's element along the
provided axis. The resulting tensor has the same rank as the input if keepdims equal 1.
If keepdims equal 0, then the resulting tensor have the reduced dimension pruned.
The type of the output tensor is integer.
The axis in which to compute the arg indices.
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\reduction\old.cc
position
Input type for input at index 0 is null. Type info is expected.
Constrain position to integral tensor. It must be a scalar(tensor of empty shape).
Input Sequence and Tensor are expected to have type info. Current type is null.
Constrain input types to any tensor type.
output_sequence
input_sequence
Input Sequence and Tensor are expected to have the same elem type. Sequence=
 Tensor=
 is null. Type info is expected.
SequenceConstruct is expected to have at least 1 input.
Element type of inputs are expected to be the same.
Input type for input at index 
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\sequence\defs.cc
(Optional) The data type of the tensors in the output sequence. The default type is 'float'.
Attribute dtype should be of integer type and specify a type.
], Value=
new_axis must be either 0 or 1
Keep the split dimension or not. Default 1, which means we keep split dimension. If input 'split' is specified, this attribute is ignored.
Insert and concatenate on a new axis or not, default 0 means do not insert new axis.
Which axis to concat on. Accepted range in `[-r, r - 1]`, where `r` is the rank of input tensors. When `new_axis` is 1, accepted range is `[-r - 1, r]`. 
Invalid value of attribute 'axis'. Accepted range=[
Sum of split values not equal to 'input' dim size on 'axis'. 'axis' dim size=
 sum of split values=
Input 'split' can not be empty.
Which axis to split on. A negative value means counting dimensions from the back. Accepted range is [-rank, rank-1].
Constrain output types to all tensor types.
Constrain split size to integral tensor.
Only supports `int32_t` or `int64_t` inputs for split
Constrain output to integral tensor. It must be a scalar(tensor of empty shape).
Constrains input to integral tensors.
Constrains input to float tensors.
If set, defines the broadcast dimensions.
Enable broadcasting
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\logical\old.cc
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\object_detection\old.cc
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\quantization\old.cc
Optional scaling values used by some activation functions. The values are consumed in the order of activation functions, for example (f, g, h) in LSTM.
foward
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\rnn\old.cc
The sequence output for the hidden is optional if 0. Default 0.
Data size mismatch. Tensor: 
 expected size 
 does not match the actual size
Cannot parse data from external tensors. Please 
The type of tensor: 
 is undefined so it cannot be parsed.
load external data into raw data for tensor: 
ParseData type mismatch for tensor: 
. Expected:
 Actual:
 source=
 target=
Output was expected to have tensor type. Got 
Input element type of 
 does not match existing output type of 
Input was expected to have tensor or sparse tensor type. Got 
Input was expected to have either tensor, sequence, or optional type. Got 
Input was expected to have optional type. Got 
Element type of input was unknown
Input was expected to have sequence type. Got 
source sequence type missing element type.
Mismatched sparse tensor element type:
Mismatched tensor element type:
Mismatched type:
Element type of tensor or sparse tensor input was unknown
target optional type missing element type.
source optional type missing element type.
target sequence type missing element type.
==> Context: 
Data of TensorProto ( tensor name: 
) is stored externally and should not have data field.
uint64_data
double_data
TensorProto ( tensor name: 
) is stored externally but doesn't have a location.
) should be stored in 
, but it doesn't exist or is not accessible.
float_data
setting data_type field (tensor name: 
) to UNDEFINED is not allowed
data_type
raw_data
int64_data
string_data
int32_data
elem_type
 is required but missing.
Field '
Unrecognized type value case (value_info name: 
value_type
key_type
' of 
value_info
 is required to be non-empty.
) first dimension size does not equal NNZ.
] not in sorted order.
Sparse tensor (
) index value at position [
sparse_tensor_proto
] not in lexicographic sorted order.
] out of range.
) second dimension size does not match rank of tensor.
] out of range [0, 
Sparse tensor indices (
) has 
 values, but NNZ is 
Unrecognized data_type (tensor name: 
) should not be stored in raw_data field
) should contain one and only one value field.
TensorProto (tensor name: 
) is 0-element but contains data!
values of data_type '
' should be stored in field '
' instead of '
STRING data (tensor name: 
Graph must be in single static assignment (SSA) form, however '
' has been used as graph input names multiple times.
graph
Sparse tensor initializers must have a non-empty name
 in initializer but not in graph input
 initializer name is not unique
Tensor initializers must have a non-empty name
No Op registered for 
 with domain_version of 
No opset import for domain '
Warning: Checker does not support models with experimental ops: 
Bad node spec for node. Name: 
 OpType: 
Op registered for 
 is deprecated in domain_version of 
) should refer to attribute in parent node.
Attribute (name: 
) should not contain more than one value field.
type field and data field mismatch in attribute 
NodeProto (name: 
, type: 
) has zero input and zero output.
op_type
) dimensions are not positive.
) must have a dense-rank > 0
Sparse tensor values (
) must have rank 1.
) has no index values.
) must have rank 1 or 2.
) must have INT64 type.
ConstantFill
' of node: 
name: 
 is not output of any previous nodes.
 sparse initializer name is not unique across initializers and sparse_initializers
' has been used as output names multiple times.
Nodes in a graph must be topologically sorted, however input '
stol argument out of range
invalid stol argument
floats
type_proto
Unexpected attribute type.
Unhandled type: %d
Error parsing TensorProto shape (expected numeric dimension).
String value expected, but not found.
Unexpected literal type.
Integer value expected, but not found.
Value expected but not found.
Error parsing TensorProto (expected a tensor shape).
Error parsing TensorProto (expected a tensor type).
Unexpected type.
Identifier expected but not found.
Expected character 
(line: 
 column: 
graphs
tensors
strings
type_protos
sparse_tensors
[ParseError at position 
Error context: 
Data for input  
 already exists.
optional_type
, node name: 
Cannot find missing input: 
in initializers. 
Graph initializer names must appear after the actual inputs: 
(op_type:
 inputs but 
 were provided
Container for generated shape data cannot be nullptr when enable_data_propagation option is set.
 were provided.
The number of graph input cannot be smaller than the number of node input
Cannot use the same name as both a subgraph initializer and subgraph input: 
Graph has 
Shape inference error(s): 
Warning: Unsupported operator 
. No schema registered for this operator.
Warning: Shape inference does not support
 inferred=
NOT_SET
sparse_tensor_type
opaque_type
 models with experimental operators: 
type case unsupported for symbolic shape inference. inferred=
type case unsupported. existing=
type case mismatch. existing=
Inferred shape and existing shape differ in rank: (
Inferred elem type differs from existing elem type: (
unk__
Inferred shape and existing shape differ in dimension 
) is not equal to the existing dim value (
Number of elements of input 'scales' must be same as rank of input 'X'
Input 'sizes' must have int64 element type.
Number of elements of input 'sizes' must be same as rank of input 'X'
Dimension value inferred (
Input 'scales' must have float element type.
onnx.TypeProto.Opaque
onnx.TypeProto.SparseTensor
onnx.TypeProto.Optional
onnx.TypeProto.Map
onnx.FunctionProto
onnx.OperatorSetIdProto
onnx.TypeProto
onnx.SparseTensorProto
onnx.TensorProto
onnx.TensorProto.Segment
onnx.GraphProto
onnx.TypeProto.Sequence
onnx.TypeProto.Tensor
onnx.TensorShapeProto
onnx.TensorShapeProto.Dimension
onnx.NodeProto
onnx.ValueInfoProto
onnx.AttributeProto
onnx.TensorAnnotation
onnx.StringStringEntryProto
onnx.ModelProto
onnx.TrainingInfoProto
FLOATFLOATSGRAPHGRAPHSINTINTSSPARSE_TENSORSPARSE_TENSORSSTRINGSTRINGSTENSORTENSORSTYPE_PROTOTYPE_PROTOSUNDEFINED
SearchOnePass inconsistency
SearchDFA inconsistency
DFA out of memory: 
pattern length 
SearchNFA inconsistency
SearchBitState inconsistency
RE2: invalid startpos, endpos pair. [
startpos: 
endpos: 
text size: 
program size 
list count 
bytemap range 
Unexpected re_anchor value: 
Error compiling '
Error parsing '
D:\a\_work\1\s\engine\lotus\cmake\external\re2\re2\re2.cc
Invalid RE2: 
Error reverse compiling '
pattern too large - compile failed
Unknown encoding 
NumCapturesWalker::ShortVisit called
invalid named capture group
no argument for repetition operator
trailing \
unexpected )
missing )
invalid UTF-8
invalid perl operator
bad repetition operator
invalid repetition size
unexpected error
no error
Unexpected op in Regexp::Equal: 
Bad reference count 
missing ]
invalid character class range
invalid character class
invalid escape sequence
D:\a\_work\1\s\engine\lotus\cmake\external\re2\re2\regexp.cc
Regexp not destroyed.
Walk NULL
D:\a\_work\1\s\engine\lotus\cmake\external\re2\re2/walker-inl.h
Stack not empty.
Compiler::Copy called!
D:\a\_work\1\s\engine\lotus\cmake\external\re2\re2\compile.cc
should never happen
Missing case in Compiler: 
No ranges in char class
Bad hex digit 
RE2: unexpected op: 
Bad call to ParseState::ParsePerlFlags
RepetitionWalker::ShortVisit called
D:\a\_work\1\s\engine\lotus\cmake\external\re2\re2\parse.cc
AddFoldedRange recurses too much.
unknown round: 
Concat of 
Bad args: nsubmatch=
context does not contain text
Unexpected opcode in short circuit: 
D:\a\_work\1\s\engine\lotus\cmake\external\re2\re2\nfa.cc
unhandled 
 in AddToThreadq
Unhandled 
 in step
Failed to analyze start state.
StateSaver failed to restore state.
Unexpected special state in RunStateOnByte
NULL state in RunStateOnByte
DeadState in RunStateOnByte
D:\a\_work\1\s\engine\lotus\cmake\external\re2\re2\dfa.cc
unhandled opcode: 
RunStateOnByteUnlocked failed after Reset
RunStateOnByteUnlocked failed after ResetCache
job_.size() = 
Unexpected opcode: 
D:\a\_work\1\s\engine\lotus\cmake\external\re2\re2\bitstate.cc
GrowStack() failed: 
njob_ = 
D:\a\_work\1\s\engine\lotus\cmake\external\re2\re2\onepass.cc
Cannot use SearchOnePass for unanchored matches.
SimplifyWalker::ShortVisit called
DoCoalesce failed: r2->op() is 
DoCoalesce failed: r1->op() is 
CoalesceWalker::ShortVisit called
Malformed repeat 
Simplify case not handled: 
D:\a\_work\1\s\engine\lotus\cmake\external\re2\re2\simplify.cc
Case not handled in ComputeSimple: 
[:ascii:]
[:^alpha:]
[:alpha:]
[:^alnum:]
[:cntrl:]
[:^blank:]
[:blank:]
[:^ascii:]
[:alnum:]
!~09AZ__az ~09AZ__az09AZaz!/:@[`{~
  AZaz
[:^xdigit:]
[:xdigit:]
[:^word:]
  09AFafAZ
[:space:]
[:^punct:]
[:punct:]
[:^print:]
[:word:]
[:^upper:]
[:upper:]
[:^space:]
[:graph:]
[:^digit:]
[:digit:]
[:^cntrl:]
[:print:]
[:^lower:]
[:lower:]
[:^graph:]
  09az
Caucasian_Albanian
Carian
Canadian_Aboriginal
Buhid
Chakma
Bhaiksuki
Bengali
Batak
Bassa_Vah
Buginese
Braille
Brahmi
Bopomofo
Arabic
Anatolian_Hieroglyphs
Adlam
Bamum
Balinese
Avestan
Armenian
Hangul
Gurmukhi
Gunjala_Gondi
Hebrew
Hatran
Hanunoo
Hanifi_Rohingya
Glagolitic
Georgian
Ethiopic
Elymaic
Gujarati
Greek
Grantha
Gothic
Dives_Akuru
Devanagari
Deseret
Cyrillic
Elbasan
Egyptian_Hieroglyphs
Duployan
Dogra
Common
Chorasmian
Cherokee
Cypriot
Cuneiform
Coptic
Lycian
Malayalam
Makasar
Mahajani
Lydian
Limbu
Lepcha
Latin
Linear_B
Linear_A
Khitan_Small_Script
Kharoshthi
Kayah_Li
Katakana
Khudawadi
Khojki
Khmer
Inscriptional_Pahlavi
Inherited
Imperial_Aramaic
Hiragana
Kannada
Kaithi
Javanese
Inscriptional_Parthian
Nushu
Old_Hungarian
Ol_Chiki
Ogham
Nyiakeng_Puachue_Hmong
Nabataean
Myanmar
Multani
New_Tai_Lue
Nandinagari
Meroitic_Hieroglyphs
Meroitic_Cursive
Mende_Kikakui
Mongolian
Masaram_Gondi
Marchen
Manichaean
Mandaic
Meetei_Mayek
Medefaidrin
Saurashtra
Samaritan
Runic
SignWriting
Siddham
Shavian
Sharada
Phoenician
Phags_Pa
Rejang
Psalter_Pahlavi
Palmyrene
Pahawh_Hmong
Osmanya
Osage
Pau_Cin_Hau
Old_Persian
Old_Permic
Old_North_Arabian
Old_Italic
Oriya
Old_Turkic
Old_South_Arabian
Old_Sogdian
Warang_Citi
Wancho
Ugaritic
Zanabazar_Square
Yezidi
Thaana
Telugu
Tangut
Tamil
Tirhuta
Tifinagh
Tibetan
Tagbanwa
Tagalog
Syriac
Syloti_Nagri
Takri
Tai_Viet
Tai_Tham
Tai_Le
Sinhala
Sundanese
Soyombo
Sora_Sompeng
Sogdian
0!0)080:0
,.,0,^,`
 : : 
.!.!.0
? @ T T 3
&!&!e
!$!$!&!&!(!(!*!-!0!3!>!?!E!E!
,.,`,`,b,d,g,g,i,i,k,k,m,p,r,r,u,u,~,
.0/011
2`2~2`
!#!%!%!'!'!)!)!.!.!:!;!J!J!L!M!O!O!
#"#(#+#{#}#
#&$@$J$
&n&p&g'
+/+E+F+M+s+v+
,P.Q.
0 0 06070>0?0
2*2G2P2P2`2
!$!$!&!&!(!(!*!-!/!9!<!?!E!I!N!N!
,.,0,^,`,
-%-'-'-----0-g-o-o-
-/./.
01050;0<0A0
1/111
.:.;.@.@.
00000
 d f p t ~ 
!%!'!)!,!1!3!M!O!_!
!&$@$J$`$
)s+v+
0 00070<0?0
1 2_2
-*0/0
 ( ) / / _ _ 
))]]}}
F F ~ ~ 
#*#*#i'i'k'k'm'm'o'o'q'q's's'u'u'
)#.#.%.%.'.'.).).
(([[{{
 E E } } 
#)#)#h'h'j'j'l'l'n'n'p'p'r'r't't'
)".".$.$.&.&.(.(.B.B.
 9 9 
. . .
$$++<>^^``||~~
D D R R z | 
!#!%!%!'!'!)!)!.!.!:!;!@!D!J!M!O!O!
#(#+#&$@$J$
)s+v+
,P.Q.
0 0 06070>0?0
2*2G2P2P2`2
 *!+!2!2!N!N!`!
 * . ` d f o 
!#%*,/:;?@[]__{{}}
 ' 0 C E Q S ^ } ~ 
#)#*#h'u'
,p-p-
...0.O.R.R.
00000=0=0
++<>||~~
D D R R z | 
!@!D!K!K!
" #!#|#|#
%o&o&
*0+D+G+L+)
 / / _ _ 
 * . ` d f o 
-*0-0
-%-'-'-----
0!0)080;0
.0/0#
0-g-o-p-
 *0-0
!#%'**,,./:;?@\\
   ' 0 8 ; > A C G Q S S U ^ 
,p-p-
.*...0.9.<.?.A.A.C.O.R.R.
0=0=0
5!8!0-g-
0<0<0A0
1/111
 |,},o-o-/./.
01050;0;0
!/!/!4!4!9!9!<!=!F!I!N!N!
!0,^,a,a,e,f,h,h,j,j,l,l,q,q,s,t,v,{,
-%-'-'-----A
p p t y 
0!0)080:0
1 2)2H2O2Q2_2
p p t y 
 P!_!
1 2)2H2O2Q2_2
Bad final char: 
[^\x00-\x{10ffff}]
(){}[]*+?|.^$\
{%d,%d}
{%d,}
 [truncated]
D:\a\_work\1\s\engine\lotus\cmake\external\re2\re2\tostring.cc
kRegexpCapture cap() == 0
[]^-\
(?HaveMatch:%d)
\x{%x}
(?-m:^)
(?-m:$)
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\GraphTransformers\GraphTransformerHelpers.cc
GraphTransformerHelpers::RegisterGraphTransformers
WinmlRuleTransformer
BatchNormalizationAddFusion
BatchNormalizationMulFusion
Internal error in BatchNormalizationMulFusion. BatchNormalization_B_tensor_proto is NULL
%~3a*
++Q5@.
'7JCy7
L>2?~>p
5?*BL?fff?
l?4?~?
?&TpxAD
?333333
t@33{@
raB3G
~3a*~3a*~3a*~3a*
++Q5@.Q5@.Q5@.Q5@.
6JCy7JCy7JCy7JCy7
8giP9giP9giP9giP9
L>2?~>2?~>2?~>2?~>
l?4?~?4?~?4?~?4?~?
`@33{@33{@33{@33{@
=2>l>
?"@x@
ProcessInfo
UTCReplace_AppSessionGuid
PartA_PrivTags
schemaVersion
runtimeVersion
isRedist
RuntimePerf
UTCReplace_AppSessionGuid
PartA_PrivTags
schemaVersion
sessionId
totalRuns
totalRunDuration
EvaluationStop
ExecutionProviderEvent
UTCReplace_AppSessionGuid
PartA_PrivTags
adapterLuidLowPart
adapterLuidHighPart
SessionCreation
UTCReplace_AppSessionGuid
PartA_PrivTags
schemaVersion
sessionId
irVersion
OrtProgrammingProjection
modelProducerName
modelProducerVersion
modelDomain
usefp16
domainToVersionMap
modelGraphName
modelMetaData
loadedFrom
executionProviderIds
SessionCreationStart
UTCReplace_AppSessionGuid
PartA_PrivTags
EvaluationStart
RuntimeError
UTCReplace_AppSessionGuid
PartA_PrivTags
schemaVersion
hResult
sessionId
errorCode
errorCategory
errorMessage
function
Microsoft.ML.ONNXRuntime
onnxruntime.pdb
.text
.text$di
.text$mn
.text$x
.text$yd
.idata$5
.00cfg
.CRT$XCA
.CRT$XCC
.CRT$XCL
.CRT$XCU
.CRT$XCZ
.CRT$XDA
.CRT$XDZ
.CRT$XIA
.CRT$XIC
.CRT$XIZ
.CRT$XLA
.CRT$XLC
.CRT$XLD
.CRT$XLZ
.CRT$XPA
.CRT$XPZ
.CRT$XTA
.CRT$XTZ
.gfids
.giats
.rdata
.rdata$T
.rdata$r
.rdata$sxdata
.rdata$zETW0
.rdata$zETW1
.rdata$zETW2
.rdata$zETW9
.rdata$zzzdbg
.rtc$IAA
.rtc$IZZ
.rtc$TAA
.rtc$TZZ
.tls$
.tls$ZZZ
.xdata$x
.didat$2
.didat$3
.didat$4
.didat$6
.didat$7
.edata
.idata$2
.idata$3
.idata$4
.idata$6
.data
.data$r
.data$rs
.didat$5
.rsrc$01
.rsrc$02
CreateDXGIFactory2
CoCreateFreeThreadedMarshaler
CoTaskMemAlloc
onnxruntime.dll
OrtGetWinMLAdapter
OrtGetApiBase
OrtSessionOptionsAppendExecutionProviderEx_DML
OrtSessionOptionsAppendExecutionProvider_CPU
OrtSessionOptionsAppendExecutionProvider_DML
FindFirstFileW
SetLastError
FindNextFileW
FindClose
MultiByteToWideChar
GetLastError
WideCharToMultiByte
api-ms-win-core-file-l1-1-0.dll
api-ms-win-core-errorhandling-l1-1-0.dll
api-ms-win-core-string-l1-1-0.dll
api-ms-win-core-processenvironment-l1-1-0.dll
strcspn
_unlock_locales
_lock_locales
__strncnt
wcsnlen
_o____lc_codepage_func
_o____lc_collate_cp_func
_o____lc_locale_name_func
_o____mb_cur_max_func
_o___acrt_iob_func
_o___pctype_func
_o___std_exception_copy
_o___std_exception_destroy
_o___stdio_common_vfprintf
_o___stdio_common_vsnprintf_s
_o___stdio_common_vsprintf
_o___stdio_common_vsprintf_s
_o___stdio_common_vswprintf
_o__aligned_free
_o__aligned_malloc
_o__beginthreadex
_o__callnewh
_o__calloc_base
_o__cexit
_o__CIcosh
_o__CIfmod
_o__CIpow
_o__CIsinh
_o__CItanh
_o__close
_o__configure_narrow_argv
_o__create_locale
_o__crt_atexit
_o__dclass
_o__difftime64
_o__errno
_o__execute_onexit_table
_o__fdclass
_o__fdsign
_o__free_base
_o__free_locale
_o__fseeki64
_o__fstat64i32
_o__get_errno
_o__get_stream_buffer_pointers
_o__Getdays
_o__Getmonths
_o__Gettnames
_o__gmtime64_s
_o__initialize_narrow_environment
_o__initialize_onexit_table
_o__invalid_parameter_noinfo
_o__invalid_parameter_noinfo_noreturn
_o__libm_sse2_acos_precise
_o__libm_sse2_asin_precise
_o__libm_sse2_atan_precise
_o__libm_sse2_cos_precise
_o__libm_sse2_exp_precise
_o__libm_sse2_log_precise
_o__libm_sse2_pow_precise
_o__libm_sse2_sin_precise
_o__libm_sse2_sqrt_precise
_o__libm_sse2_tan_precise
_o__localtime64_s
_o__lock_file
_o__malloc_base
_o__mktime64
_o__purecall
_o__read
_o__realloc_base
_o__register_onexit_function
_o__seh_filter_dll
_o__set_errno
_o__sopen_s
_o__stat64i32
_o__Strftime
_o__strnicmp
_o__strtoi64
_o__towlower_l
_o__towupper_l
_o__unlock_file
_o__W_Getdays
_o__W_Getmonths
_o__W_Gettnames
_o__wcsdup
_o__Wcsftime
_o__wfsopen
_o__write
_o__wsopen_s
_o_abort
_o_acoshf
_o_asinhf
_o_atanhf
_o_bsearch
_o_calloc
_o_ceil
_o_fclose
_o_fflush
_o_fgetc
_o_fgetpos
_o_floor
_o_fputc
_o_fread
_o_free
_o_frexp
_o_fseek
_o_fsetpos
_o_fwrite
_o_isalnum
_o_isalpha
_o_isdigit
_o_islower
_o_isspace
_o_isupper
_o_iswspace
_o_ldexp
_o_localeconv
_o_log2
_o_log2f
_o_malloc
_o_nearbyintf
_o_remainderf
_o_rint
_o_rintf
_o_roundf
_o_scalbn
_o_scalbnf
_o_setlocale
_o_setvbuf
_o_strerror
_o_strncpy_s
_o_strtod
_o_strtof
_o_strtol
_o_strtoll
_o_strtoull
_o_terminate
_o_tolower
_o_ungetc
_o_wcsftime
api-ms-win-crt-string-l1-1-0.dll
api-ms-win-crt-locale-l1-1-0.dll
api-ms-win-crt-private-l1-1-0.dll
InitOnceComplete
InitOnceBeginInitialize
InitializeSRWLock
TryAcquireSRWLockExclusive
GetCurrentThreadId
ReleaseSRWLockExclusive
AcquireSRWLockExclusive
FormatMessageA
GetStringTypeW
QueryPerformanceFrequency
QueryPerformanceCounter
GetSystemTimePreciseAsFileTime
EnterCriticalSection
LeaveCriticalSection
InitializeCriticalSectionEx
DeleteCriticalSection
EncodePointer
DecodePointer
LocalFree
LCMapStringEx
GetLocaleInfoEx
GetCPInfo
CompareStringEx
CloseHandle
InitializeCriticalSectionAndSpinCount
SetEvent
ResetEvent
WaitForSingleObjectEx
CreateEventW
GetModuleHandleW
GetProcAddress
UnhandledExceptionFilter
SetUnhandledExceptionFilter
GetCurrentProcess
TerminateProcess
IsProcessorFeaturePresent
GetCurrentProcessId
GetSystemTimeAsFileTime
InitializeSListHead
IsDebuggerPresent
RtlUnwind
RaiseException
FlsAlloc
FlsGetValue
FlsSetValue
FlsFree
InterlockedFlushSList
api-ms-win-core-synch-l1-2-0.dll
api-ms-win-core-synch-l1-1-0.dll
api-ms-win-core-processthreads-l1-1-0.dll
api-ms-win-core-localization-l1-2-0.dll
api-ms-win-core-profile-l1-1-0.dll
api-ms-win-core-sysinfo-l1-2-0.dll
api-ms-win-core-util-l1-1-0.dll
api-ms-win-core-heap-l2-1-0.dll
api-ms-win-core-handle-l1-1-0.dll
api-ms-win-core-libraryloader-l1-2-0.dll
api-ms-win-core-processthreads-l1-1-1.dll
api-ms-win-core-sysinfo-l1-1-0.dll
api-ms-win-core-interlocked-l1-1-0.dll
api-ms-win-core-debug-l1-1-0.dll
api-ms-win-core-rtlsupport-l1-1-0.dll
api-ms-win-core-fibers-l1-1-0.dll
_initterm
_initterm_e
strncmp
api-ms-win-crt-runtime-l1-1-0.dll
PathCchRemoveFileSpec
PathCchRemoveBackslash
api-ms-win-core-path-l1-1-0.dll
SleepConditionVariableSRW
GetModuleFileNameA
CreateSemaphoreExW
HeapFree
ReleaseSemaphore
GetModuleHandleExW
WaitForSingleObject
ReleaseMutex
FormatMessageW
OutputDebugStringW
OpenSemaphoreW
HeapAlloc
CreateMutexExW
GetProcessHeap
DebugBreak
VirtualFree
VirtualAlloc
GetModuleFileNameW
GetFullPathNameW
LoadLibraryExW
FreeLibrary
WakeAllConditionVariable
CreateDirectoryW
SetThreadAffinityMask
ReadFile
GetFileSizeEx
SetThreadDescription
RemoveDirectoryW
GetFinalPathNameByHandleW
LoadLibraryExA
GetEnvironmentVariableA
GetFileAttributesW
CreateFile2
Sleep
GetFileAttributesA
GetLogicalProcessorInformation
GetCurrentThread
DeleteFileW
GetSystemInfo
SetFilePointerEx
CreateDirectoryA
WakeConditionVariable
GetCurrentProcessorNumber
EventUnregister
EventSetInformation
EventRegister
EventWriteTransfer
ReleaseSRWLockShared
AcquireSRWLockShared
api-ms-win-core-heap-l1-1-0.dll
api-ms-win-core-memory-l1-1-0.dll
api-ms-win-core-processtopology-obsolete-l1-1-0.dll
api-ms-win-core-processthreads-l1-1-3.dll
api-ms-win-core-file-l1-2-0.dll
api-ms-win-eventing-provider-l1-1-0.dll
VirtualProtect
VirtualQuery
ilogbf
ilogb
strnlen
api-ms-win-crt-math-l1-1-0.dll
GetNativeSystemInfo
LoadLibraryW
api-ms-win-core-libraryloader-l1-2-1.dll
.?AVbad_exception@std@@
.?AVbad_alloc@std@@
.?AVexception@std@@
.?AVbad_array_new_length@std@@
.?AVfailure@ios_base@std@@
.?AVruntime_error@std@@
.?AVsystem_error@std@@
.?AVbad_cast@std@@
.?AV_System_error@std@@
.?AVFatalException@protobuf@google@@
.?AVlength_error@std@@
.?AVlogic_error@std@@
.?AVout_of_range@std@@
.?AVinvalid_argument@std@@
.?AVbad_function_call@std@@
.?AV<lambda_59ed2dcfa7c0d744c558df76076fb0aa>@@
.?AV<lambda_301279dcdc0bb6da9000a75437e1d615>@@
.?AVOnnxRuntimeException@onnxruntime@@
.P6AXPAX@Z
.?AU?$default_delete@VIExecutionProvider@onnxruntime@@@std@@
.?AV<lambda_12065ee63fda2ae25d17bc16a002c579>@@
.?AVNotImplementedException@onnxruntime@@
.?AV<lambda_a4fe3bd39dcd0107f90455cb45c21429>@@
.?AV<lambda_ca87e505bf8a7e1f26579083c3683d94>@@
.?AV<lambda_e87e8efc1f18e44e772bf580822fd15c>@@
.?AUException@Ort@@
.?AV<lambda_31da21da588bb3698a015fc212bb17cf>@@
.?AU?$default_delete@VCPUExecutionProvider@onnxruntime@@@std@@
.?AVother_error@detail@nlohmann@@
.?AV<lambda_e6ae48ddd96d9e98a83433a4d595515b>@@
.?AVexception@detail@nlohmann@@
.?AV<lambda_9e6211c87401ff34ec951ed3f9d92212>@@
.?AV<lambda_9c73eb49aba39bc780e6a604a9602ad6>@@
.?AV<lambda_a875e26a77cffbb176a8ef549b2a14b9>@@
.?AV<lambda_15ec4af4e71787ba502de1205d84eea4>@@
.?AU?$default_delete@VModel@onnxruntime@@@std@@
.?AV<lambda_590e59531275857afafc7e97ec0dad13>@@
.?AV<lambda_de00575298e9c7ead3b243890c596395>@@
.?AV<lambda_daa97a08fba16356a015278e0f3ca1ed>@@
.?AV<lambda_6922b57be4edd39a89a7cea9e2ebb658>@@
.?AV<lambda_0fb4e93f4014e9a9ed52287648f86c91>@@
.?AV<lambda_33c11271a908f4f215dd361ca0be165e>@@
.?AVInferenceError@onnx@@
.?AV<lambda_8fef7dbb6b3e81cda489ebe45ab449bb>@@
.?AV<lambda_c9d29aed4526d430f521ffb38e7c1059>@@
.?AV<lambda_b35f6304ab9b91e067de16d773b507fb>@@
.?AV<lambda_888c8332848e83c225ef5f5587c3b38e>@@
.?AV<lambda_598c6d249dbd14226af8e69fe738f910>@@
.?AV<lambda_ab7cc530eb49a3a15082dbc27eb613da>@@
.?AV<lambda_dcbbb9aee7454d5a0feb975df66a7001>@@
.?AVSchemaError@onnx@@
.?AV<lambda_18dbcf0a6112b471cd3435637c32f0f8>@@
.?AV<lambda_d522df09ab9335db57f0cfc8c09d6630>@@
.?AV<lambda_3a80fca790cdb0fce14ddaedefa20351>@@
.P6AXAAUInferenceContext@onnx@@@Z
.?AV<lambda_73cc642ed8afaa2889c4f09584fca345>@@
.?AV<lambda_519fe9cae569b1ad1c3f73288fa352d4>@@
.?AV<lambda_7965ceabe5f45db89655ad0a7fd04cd6>@@
.?AV<lambda_716d92b2455b6dca0a3e195a50699167>@@
.?AVinvalid_iterator@detail@nlohmann@@
.?AVtype_error@detail@nlohmann@@
.?AVout_of_range@detail@nlohmann@@
.?AVparse_error@detail@nlohmann@@
.?AVResultException@wil@@
.?AV<lambda_55e8e059d201c280196491784367b3c1>@@
.?AV<lambda_199eec5d9457d35704c78b6cd8a09f57>@@
.?AV<lambda_45f451203469e52634465d9e0804de20>@@
.?AV<lambda_ac9e5c5dae7b58201a89b0e1484eabd5>@@
.?AV<lambda_0b7b88c72b3c6e2d1499af326149f2ac>@@
.?AV<lambda_e9a129094f8ce9402077e710005a387c>@@
.?AV<lambda_de8b881f6b63dfc498a2c66c13a87c6a>@@
.?AV<lambda_cd4f9515fd1d18f38ab6ba0038b1c96b>@@
.?AV<lambda_766375b35334ae7ebdda42fcdeada652>@@
.?AV<lambda_aff3a47ffdeb0107e2635883ea459138>@@
.?AVbad_variant_access@std@@
.?AVbad_optional_access@std@@
.?AV<lambda_b588eebdb691c2e62a30a6a9e67054ef>@@
.?AV<lambda_afc7894d5145143e12a5c0e655a064d4>@@
.?AV<lambda_4098378d7ad3a9be3cb392d1987f5a82>@@
.?AV<lambda_f95b3cba5a1c25b0ed905809853fb736>@@
.?AV<lambda_c7a086a1b0574ca822082c2b20b57abb>@@
.?AV<lambda_8a5db151b7b360081c3c199e43485789>@@
.?AV<lambda_d6ca9c94cfca15f9aa33f45058a1fcd2>@@
.?AV<lambda_7263948f9424947480bb5df404fee515>@@
.?AV<lambda_36dfc92b17b40b8d41cb63bf3c56bb44>@@
.?AV<lambda_0f8bfbc63e2fcc8494bd65f666c7791b>@@
.?AV<lambda_1eb06dcafd75d6cff15c185443e559ed>@@
.?AV<lambda_d222db6c7292e4a7be3d95ab9d5f696d>@@
.?AUhresult_illegal_state_change@winrt@@
.?AUhresult_illegal_delegate_assignment@winrt@@
.?AUhresult_changed_state@winrt@@
.?AUhresult_illegal_method_call@winrt@@
.?AUhresult_class_not_available@winrt@@
.?AUhresult_class_not_registered@winrt@@
.?AUhresult_out_of_bounds@winrt@@
.?AUhresult_no_interface@winrt@@
.?AUhresult_canceled@winrt@@
.?AUhresult_not_implemented@winrt@@
.?AUhresult_invalid_argument@winrt@@
.?AUhresult_error@winrt@@
.?AUhresult_wrong_thread@winrt@@
.?AUhresult_access_denied@winrt@@
.?AV<lambda_339aa899e5780231e41cee84a9ac8a33>@@
.?AV<lambda_4a8ce7446699411c4438ae92f5cf818a>@@
.?AV<lambda_19ac80b5fbb34feb84f95acd76dbb931>@@
.?AV<lambda_a4f3d40f4d5a0b2abf807cb8d9827633>@@
.?AV<lambda_c01406ec4a694ccc7628eb9a89481150>@@
.?AV<lambda_35452b6fa29b4a84e3a3d83c879b06cd>@@
.?AV<lambda_c193fdd8c0f883adaf2de3acc3f64e63>@@
.P6APAVOpKernel@onnxruntime@@ABVOpKernelInfo@1@@Z
.?AV<lambda_14cf9ddcdab5183a36b1158dec720c25>@@
.?AV<lambda_86574d63891f8965457453a282e51ac5>@@
.P6A?AVStatus@common@onnxruntime@@PAXAAV?$vector@UOrtValue@@V?$allocator@UOrtValue@@@std@@@std@@0I@Z
.?AV<lambda_f05d87f4b55c6f94a6e4ce358c6d246e>@@
.?AU?$Exp@N@functors@onnxruntime@@
.?AU?$Abs@N@functors@onnxruntime@@
.?AU?$Abs@E@functors@onnxruntime@@
.?AU?$Floor@M@functors@onnxruntime@@
.?AU?$Neg@H@functors@onnxruntime@@
.?AV<lambda_ec1910e0e2f828f423168f93cf5ec05e>@@
.?AU?$Abs@I@functors@onnxruntime@@
.?AU?$Abs@_J@functors@onnxruntime@@
.?AU?$Sqrt@N@functors@onnxruntime@@
.?AU?$Reciprocal@M@functors@onnxruntime@@
.?AU?$Exp@M@functors@onnxruntime@@
.?AU?$Reciprocal@N@functors@onnxruntime@@
.?AU?$Log@N@functors@onnxruntime@@
.?AU?$Abs@H@functors@onnxruntime@@
.?AU?$Neg@N@functors@onnxruntime@@
.?AU?$Neg@_J@functors@onnxruntime@@
.?AV<lambda_4276ac6945fda74461cbbc3f55ea202e>@@
.?AV<lambda_b56eca9e5f17216a19fa025aad3fbc12>@@
.?AU?$Neg@M@functors@onnxruntime@@
.?AU?$Sqrt@M@functors@onnxruntime@@
.?AU?$Abs@G@functors@onnxruntime@@
.?AU?$Abs@C@functors@onnxruntime@@
.?AU?$Log@M@functors@onnxruntime@@
.?AV<lambda_331d7506767c1ed93515cbb23d39b0d9>@@
.?AU?$Abs@F@functors@onnxruntime@@
.?AU?$Ceil@M@functors@onnxruntime@@
.?AU?$Abs@_K@functors@onnxruntime@@
.?AU?$Neg@C@functors@onnxruntime@@
.?AU?$Abs@M@functors@onnxruntime@@
.P6AMMMM@Z
.?AV<lambda_fd3f60c5d0df63667d14dc08824a1b73>@@
.?AV<lambda_cb8e619381015eaa09861c60b8150125>@@
.?AU?$Relu@M@functors@onnxruntime@@
.?AU?$Selu@M@functors@onnxruntime@@
.?AU?$Sigmoid@M@functors@onnxruntime@@
.?AU?$Celu@M@functors@onnxruntime@@
.?AU?$Elu@M@functors@onnxruntime@@
.?AU?$Softplus@M@functors@onnxruntime@@
.?AU?$HardSigmoid@M@functors@onnxruntime@@
.?AU?$Softsign@M@functors@onnxruntime@@
.?AU?$Sigmoid@N@functors@onnxruntime@@
.?AU?$ThresholdedRelu@M@functors@onnxruntime@@
.?AU?$Relu@N@functors@onnxruntime@@
.?AU?$Relu@C@functors@onnxruntime@@
.?AU?$Tanh@M@functors@onnxruntime@@
.?AU?$LeakyRelu@M@functors@onnxruntime@@
.?AU?$Relu@H@functors@onnxruntime@@
.?AU?$Tanh@N@functors@onnxruntime@@
.?AV<lambda_4d3103e3f4278a6d0b0a41cd1fe918ee>@@
.?AV<lambda_92ea355126c2f4fbe475cb5acb7184bb>@@
.?AV<lambda_786ab6644ee6b69320106188cb9d35b2>@@
.?AV<lambda_d492ab087615483c649f2fbeb7c7afb8>@@
.?AV<lambda_7413de4275d15476d9100c3e64569703>@@
.?AV<lambda_3e9031054352b07bad13c7fb5c2ce7bb>@@
.?AV<lambda_d45414743c2d05f7edcf6eb84b1c5376>@@
.?AV<lambda_e64d6f02466f94741deccd76032ec2f7>@@
.?AV<lambda_eb7a0c3b4f55a6896582386a718283be>@@
.?AV<lambda_7e705c133fb64792e116b9bec62de367>@@
.?AV<lambda_9e70336b15c8be3003ecde8d0dd2702d>@@
.?AV<lambda_70a4e4f9104009b11570775cde48242b>@@
.?AV<lambda_604acd6b059bd301f8025e011790c72a>@@
.?AV<lambda_8f9d3b0d9ea77fd034ca21fb6a734c46>@@
.?AV<lambda_d698068c9068e0c2606c104c4b724c40>@@
.?AV<lambda_66b8e949c8c097783546f36f94506804>@@
.?AV<lambda_5e6f4957377cd9027386cbe3c8840003>@@
.?AV<lambda_f491f0d15366e04e0edf3bf2753174a5>@@
.?AV<lambda_f12636f558763d225cd192e9911d0ad0>@@
.?AV<lambda_466bfce1c88393bfc4930b50054f11e7>@@
.?AV<lambda_87e032e72109af3068317fff82549670>@@
.?AV<lambda_bb011bad1a611a9fc5e45821394e55ea>@@
.?AV<lambda_43a24ff760813c6ddd87379f0ec23ef7>@@
.?AV<lambda_56724cedf7b3d45672d6aaa9b61c2b1a>@@
.?AV<lambda_651b32f5fa8d57f69a57602106f1e1ba>@@
.?AV<lambda_061ce113b6886684397fa36d116f6701>@@
.?AV<lambda_2047474ce4cc4fb3591bd2cb304e60de>@@
.?AV<lambda_a41c606239635631821142ba5046850a>@@
.?AV<lambda_7f77cbd32daf2cf04fdea2edf875f503>@@
.?AV<lambda_96332e6dc90d0b557fee4b084f8c74f6>@@
.?AV<lambda_7f850842e5126f5ee8e78b4d81ce1b94>@@
.?AV<lambda_95777b3f28fcd1bc04104997acb745bb>@@
.?AV<lambda_5996e886d07d4d6c6b33ab2939ab88dc>@@
.?AV<lambda_e0cbc3c78c2ca2e1945708d5effc57eb>@@
.?AV<lambda_f388454f75e91de2e1f91bb03ba26f7a>@@
.?AV<lambda_2e43ccf94d692d90fc25b9f3f15b1f3a>@@
.?AV<lambda_0d76ad475b70d10280a0f03bb12202c0>@@
.?AV<lambda_aa3d1936b4520347874ab4b1ff4df1a8>@@
.?AV<lambda_1d18bde1e76bce053ccc17b3ed08cad8>@@
.?AV<lambda_a2eeee047031406dbe388d0066fc2709>@@
.?AV<lambda_e2fce933a994c63cb8e5ceee1e0963b6>@@
.?AV<lambda_2b360603aec7588daf58b55d3e48bc2d>@@
.?AV<lambda_a0193b154f39aae0502793b45f34eecb>@@
.?AV<lambda_a582beb35c004ba9f7a0a06ee52a248a>@@
.?AV<lambda_b29f4ae43268c809ebc31a8ad9d86159>@@
.?AV<lambda_7291a02d034b9a7f0a27fdb5705bfe30>@@
.?AV<lambda_f7a998b3515df6d592f0862620c5de18>@@
.?AV<lambda_d6f808cdffda8618fc34f8b7c63dabf2>@@
.?AV<lambda_bf5d26f70475f0fe685bc8ad946c36b3>@@
.?AV<lambda_dcd647a35a2dbde7587f9217025bc168>@@
.?AV<lambda_5de0fe5ec3ad834b64888d20361865ce>@@
.?AV<lambda_dd543c4aa2344f0c721733cdff68d0b3>@@
.?AV<lambda_6f9292e3cd6b9f1b15fc272b7af94e1a>@@
.?AV<lambda_947e49898d3c63f075f4c68c9b06caa0>@@
.?AV<lambda_01a9f753d9bd01cb2bfad855ca7f10c7>@@
.?AV<lambda_077c87efaa9c726a84085e4b8a5f0dfc>@@
.?AV<lambda_b4032de8aabc62f3d8e14c9557b8e4c2>@@
.?AV<lambda_fdc34539e9b271a361a2a0c8195e907f>@@
.?AV<lambda_1e62541677783c96e5b936f0f682ca92>@@
.?AV<lambda_375dfab7231b0fed5cbc2d28f45f1de4>@@
.?AV<lambda_fb29648c107445cc8f675923b21825bb>@@
.?AV<lambda_23fc752a2fe808aaecaafb193b1a6fe4>@@
.?AV<lambda_6a264f227367afe63ba4921d698f9b11>@@
.?AV<lambda_687b6564a98631c4ae886f974f6a6448>@@
.?AV<lambda_132f150dbe2a6661cedd4acd928f9ad6>@@
.?AV<lambda_d7bfa7cc168368c290c2d7a0788cd723>@@
.?AV<lambda_7be8916f64e405639dec1d893b64c678>@@
.?AV<lambda_f513723b143f04ceb7a6dc1caaf8d647>@@
.?AV<lambda_0d228c1c046c6169ab9bdaf46172d4b8>@@
.?AV<lambda_6d1c642e24095d4ebbe2c39c89123bfc>@@
.?AV<lambda_ba8ab8ea5e43c631481b914bba906376>@@
.?AV<lambda_b437eb3a1b8ce8112ef34101e8422db3>@@
.?AV<lambda_3d55206f062b0cb6d4082a57d197185d>@@
.?AV<lambda_0118a8ab65a6f61a54e801a4c2f6a842>@@
.?AV<lambda_34f127269096bdecdd81b55565792ef7>@@
.?AV<lambda_810201540ffb706396f9dcc1175a3533>@@
.?AV<lambda_9838567c14a727b6e962c38ba47aee11>@@
.?AV<lambda_68c5430cbb13436022345da27dc88632>@@
.?AV<lambda_58a57dc152175dcefed537af0e4224e8>@@
.?AV<lambda_05b97968a36f0f68189b0f8fe2c5855e>@@
.?AV<lambda_b0ebe73dd875315ff8ad13e2b0e77e37>@@
.?AV<lambda_fbd17297dd38c3cf3535c4dd5c8397f9>@@
.?AV<lambda_eb3cc1469a265bbe9f2fbbb9b40f13a2>@@
.?AV<lambda_8063db1f151bd88e3ec79c09eea46da4>@@
.?AV<lambda_b629d0fd638b8e3cc3ff0e2e1fa99a79>@@
.?AV<lambda_10a89fcba82ae9af0fd4de45f400c57a>@@
.?AV<lambda_a7b64ccd3ccc5eb737481b0a45bc9e55>@@
.?AV<lambda_e1d13b2195468d926eb95765f5990099>@@
.?AV<lambda_457d048bb1398944d91bd06e08be9004>@@
.?AV<lambda_aa572ddb1cb313eeb2f9a7fe6780d39d>@@
.?AV<lambda_1f810ffbf9d1e55e1001a4c64ceea3d9>@@
.?AV<lambda_7523eb0c2ea9937bd2da9cb954f010c9>@@
.?AV<lambda_4775b089fde8e28dff85f398113525c6>@@
.?AV<lambda_3f231ead7bd5577794abe2ca66a64f06>@@
.?AV<lambda_26ee2a828034f17afc207c8ad30c609a>@@
.?AV<lambda_f749f947d6e043250dfdbe549acd74ee>@@
.?AV<lambda_3bcb9b903ba1cf4e300afd0f291b2ee4>@@
.?AV<lambda_932674f6b1528de9101a6cf1da2f3619>@@
.?AV<lambda_43719f9d8c83073f26885f37cf523fe1>@@
.?AV<lambda_56708f1224b2fa079c579dbcc99f5723>@@
.?AV<lambda_7a970010ad38e3a1508f7194da0afc42>@@
.?AV<lambda_dfdd1161f2f278e15b5873916c2b0ca7>@@
.?AV<lambda_0367bcb44eefe2d25755a8cf3194c733>@@
.?AV<lambda_f36a9f1162ce53394a7287b72f55835f>@@
.?AV<lambda_4682f43d0273e3d01e9224f8a5dcf803>@@
.?AV<lambda_414a43a198ea3ff0dfdcf4ed89cfe63a>@@
.?AV<lambda_ebc32470c7ba4b7c18e53e48be43fa56>@@
.?AV<lambda_2a6ee45563922231f230ab0594ec8ab5>@@
.?AV<lambda_deb528c4a03e167325d02f55697f57f9>@@
.?AV<lambda_6b8e74af1b2b937274f6d872f7931a4b>@@
.?AV<lambda_dada8d57643005b7e2297cabdd6baa97>@@
.?AV<lambda_3ec6b12ad4cd55e04d715e409cf94caa>@@
.?AV<lambda_f43464c7a9a747b50c25ae66c092bf51>@@
.?AV<lambda_8687679c83f6820abda420b1312e974a>@@
.?AV<lambda_a99d8d5e4f7ccd722937dd9eb05ffd73>@@
.?AV<lambda_22dae262e3744994c648a6a28f79603c>@@
.?AV<lambda_34078c7540b6602389f40bae3a86d150>@@
.?AV<lambda_31433bd7aba335c4dcfc2d21c7b0556b>@@
.?AV<lambda_e8aeb42bf0aa89a79a408edee79cf184>@@
.?AU?$Powx@M@functors@onnxruntime@@
.?AV<lambda_b0d291db2b118b82971a2961eee9c859>@@
.?AV<lambda_bc55cb0d4e1fc3c67aaad8faf5025983>@@
.?AV<lambda_51fc723ea39c4518ba8a40d8ce3fda32>@@
.?AV<lambda_d5f3ad82c6f06fa69e5f77344156b4ec>@@
.?AV<lambda_35b794944dfe770731cae452127f793f>@@
.?AV<lambda_a777aefbfa0ed449706f4a933e2595f0>@@
.?AV<lambda_c311c1ca35d5610174ed400a6355dd88>@@
.?AV<lambda_55a8c7f82471609cdeead8ac3d02d601>@@
.?AV<lambda_f65c61404cbe8a2665c2dfba4b8d6c08>@@
.?AV<lambda_7e64dc56260f5aa848b67008058a8451>@@
.?AV<lambda_3535579646aa11381d1f86e8f1dad915>@@
.?AV<lambda_227060e20de8b811822001a4d5e06e66>@@
.?AV<lambda_d252a66a2c07e9c22634d060e74c7bae>@@
.?AV<lambda_9671e9cce0def787c04a0ca1f741034c>@@
.?AV<lambda_9531d8dd38f435ce1e9840c5a24c7961>@@
.?AV<lambda_2529e063101eea511edf1238da0f8253>@@
.?AV<lambda_d028772904ecde58905f5707c0de32a5>@@
.?AV<lambda_c8c33a5265022502f864eaf2ce2649c7>@@
.?AV<lambda_850028fba9642b015a2c6ff81bf07107>@@
.?AV<lambda_8c32b6976971eb48d48ba6d5dd1a9ae2>@@
.?AV<lambda_a7ed49975b7832e11720c7c8bc70dba8>@@
.?AV<lambda_34f93c0ee84ed95033247b350e3e9c13>@@
.?AV<lambda_1dab84a38765585bcd86328d60f37519>@@
.?AV<lambda_f3fcc869c6c1998b84de6d82e27f5b75>@@
.?AV<lambda_af2b72ce93dfb4b4ab2e91dda701576d>@@
.?AV<lambda_3c57c4f415fca3790b9eb6cedd38e28f>@@
.?AV<lambda_7d7d52dd8f31ce0fc48036fb1081b283>@@
.?AV<lambda_1433cb9cd8f8490322ad74b6fa812120>@@
.P6A?AVStatus@common@onnxruntime@@ABV?$vector@IV?$allocator@I@std@@@std@@ABVTensor@2@AAV52@PBVTensorShape@2@PAX@Z
.P6A?AV?$unique_ptr@VTensor@onnxruntime@@U?$default_delete@VTensor@onnxruntime@@@std@@@std@@ABVTensor@onnxruntime@@V?$span@$$CB_J@gsl@@_NV?$shared_ptr@VIAllocator@onnxruntime@@@1@PBVTensorShape@3@PAVThreadPool@concurrency@3@PAX@Z
.P6A?AVStatus@common@onnxruntime@@PBH0PAHIIIIIIIPAVThreadPool@concurrency@2@PAX@Z
.P6A?AVStatus@common@onnxruntime@@PB_J0PA_JIIIIIIIPAVThreadPool@concurrency@2@PAX@Z
.P6A?AV?$unique_ptr@VTensor@onnxruntime@@U?$default_delete@VTensor@onnxruntime@@@std@@@std@@ABVTensor@onnxruntime@@_J1V?$shared_ptr@VIAllocator@onnxruntime@@@1@PAX@Z
.P6A?AVStatus@common@onnxruntime@@ABVTensor@2@AAV32@PAX@Z
.P6A?AVStatus@common@onnxruntime@@PBN0PANIIIIIIIPAVThreadPool@concurrency@2@PAX@Z
.P6A?AVStatus@common@onnxruntime@@PBM0PAMIIIIIIIPAVThreadPool@concurrency@2@PAX@Z
.?AU?$MaxPool1DTask@C@onnxruntime@@
.?AU?$MaxPool3DTask@C@onnxruntime@@
.?AU?$MaxPool2DTask@M@onnxruntime@@
.?AU?$Pool3DTask@MVLpPool@onnxruntime@@@onnxruntime@@
.?AU?$MaxPool1DTask@M@onnxruntime@@
.?AU?$MaxPool2DTask@N@onnxruntime@@
.?AU?$MaxPool3DTask@E@onnxruntime@@
.?AU?$MaxPool1DTask@E@onnxruntime@@
.?AU?$MaxPool2DTask@E@onnxruntime@@
.?AU?$MaxPool3DTask@M@onnxruntime@@
.?AU?$MaxPool2DTask@C@onnxruntime@@
.?AU?$MaxPool3DTask@N@onnxruntime@@
.?AU?$Pool2DTask@MVLpPool@onnxruntime@@@onnxruntime@@
.?AU?$MaxPool1DTask@N@onnxruntime@@
.?AU?$Pool1DTask@MVLpPool@onnxruntime@@@onnxruntime@@
.?AV<lambda_3e8fe92fc2a983eaaae8e23df9000d98>@@
.?AV<lambda_f331e8788a2dcbae240a86dcd0d7b3ca>@@
.?AV<lambda_0a9bedc68e3bb7f405a0cb52fae05ca5>@@
.?AV<lambda_41ba827606a0dc44b626761483b4bac9>@@
.?AV<lambda_f02097f693588c4875083cbaa36b78e4>@@
.?AV<lambda_c46cb457b677540589add8ea80c57d8e>@@
.?AV<lambda_007ddac2123b71f70c18b64d41e9be57>@@
.?AV<lambda_1f154a6fecf26a7ac5bcd91fb4939855>@@
.?AV<lambda_0a77da8549581038807d4f031ad4715d>@@
.?AV<lambda_b7be74f8e6a45391be9d480e0a0a34ff>@@
.?AV<lambda_a978e2da606089f22a9b81ea0013257b>@@
.?AV<lambda_f66636158d41c23d6c8741d8b8b758e4>@@
.?AV<lambda_a949a47209eded0ee39e3c26cfeb0e88>@@
.?AV<lambda_e0b99860c2bc4f26550c2e8d362d7aeb>@@
.?AV<lambda_7abe23a5198e897647d783cb391d2ac6>@@
.?AV<lambda_c3ba679072e8df17b11fd4f7af75d138>@@
.?AV<lambda_86895a13d56222c09289c706b79a6852>@@
.?AV<lambda_491a5ff9baf0c87e80ac72a0beb259ee>@@
.?AV<lambda_0579b41fc30dc7704e4037c901365f86>@@
.?AV<lambda_2fd33bbe7c4a2b258d4d665b8341f671>@@
.?AV<lambda_e31b0a3938d8cf9c535bd4f318233ee3>@@
.?AV<lambda_009e82d406f51c12618b944e70e2ad71>@@
.?AV<lambda_221cfc4539809ca9dda24743972ab18a>@@
.?AV<lambda_80600ba38f1578c0943cc3b4cc8d084c>@@
.P6A?AV?$OrtValueTensorSlicer@$$CBUOrtValue@@@onnxruntime@@ABUOrtValue@@_J1@Z
.P6A?AV?$OrtValueTensorSlicer@UOrtValue@@@onnxruntime@@AAUOrtValue@@_J1@Z
.?AV<lambda_cb4a1e251f906453321bd121147ccb84>@@
.?AV<lambda_d610c17b70b54be4f32b495a21be0c2e>@@
.?AV<lambda_cb775efbeda17e0717f38321c23d0575>@@
.?AV<lambda_de588006d8cdb99d9cfbe9ec58914e7c>@@
.?AV<lambda_22a56cf2842ab7e44f2594d178dc2c1e>@@
.?AV<lambda_1abd5511a80f7bfc6909f2fad522d987>@@
.?AVrange_error@std@@
.?AV<lambda_bc6c2ea8bb268799c4c254106b4aba77>@@
.?AV<lambda_e14efb45021ce055c5f707c6ecaf4d7e>@@
.?AV<lambda_59c51bd67b590a247c9762a18455397f>@@
.?AV<lambda_30bb158bcf23a957ab7bf46d0a465fb6>@@
.?AV<lambda_43f02a942b5a7f30668fcdafd5ea2fac>@@
.?AV<lambda_e6ed4e30f502254d17f91a8198ab3dff>@@
.?AV<lambda_0f9a7ecc0cde6f755170ea655764ab2c>@@
.?AV<lambda_d303bd53dadec643ee9070953471f748>@@
.?AV<lambda_67d57227949a925edd39b6db70853db7>@@
.?AV<lambda_5016737842528dd11679bfef0d0ab738>@@
.?AV<lambda_c2e380cf1102ff4d7d9bd2e215fde5b4>@@
.?AV<lambda_b9005de86b74a327b831744e992c382d>@@
.?AV<lambda_66b6761e17b1698f92d4664857f34eef>@@
.?AV<lambda_e7465666a1cce2f1be8b885d5e56a9d2>@@
.?AV<lambda_d523df6827c8f76e5f91e59ed710694a>@@
.?AV<lambda_a65213a64b99d0456231cd4b3dfd703b>@@
.?AV<lambda_fa21abf52748e92ccc4db121eff52d81>@@
.?AV<lambda_8ec99f105e4604958a8ba328f7e77b1f>@@
.?AV<lambda_4b1533207937d10b38e9bd484b05643c>@@
.?AV<lambda_f22ff450a42615719b4e6b964a52c793>@@
.?AV<lambda_ca50ed85a5b6a014480832a0283a2f4d>@@
.?AV<lambda_e36f361e8289b79d05cd119226887515>@@
.?AV<lambda_b8c456fe40447d4b39feffecfde76884>@@
.?AV<lambda_51ff266d4a509f82cc61194622d6aafc>@@
.?AV<lambda_976a37e7ee737ef6672be8fc72541275>@@
.?AV<lambda_c580670d4132814d3bd00ecba71eaf16>@@
.?AV<lambda_ba0868398d1cd25f6a2d11110301255b>@@
.?AV<lambda_c7c1c280bcb590aff0e32b0d2bffa767>@@
.?AV<lambda_ca1582b0d14abd4d03d9895812c8d107>@@
.?AV<lambda_7029d41d4a95ff2575c8227d099f78bc>@@
.?AV<lambda_d099cfc55508afaa7cb7103a60412ec4>@@
.?AV<lambda_a7688683053999985ad4b5ee1c5e7d55>@@
.?AV<lambda_404f128c43627cca26dd30079f566078>@@
.?AV<lambda_566e56833ba647bded2d1d52059227e2>@@
.?AV<lambda_7ec65dfaaf4dc75c3f7f2cbd42331128>@@
.?AV<lambda_09860caeb18fe5c6553c7cece043a6bf>@@
.?AV<lambda_5b19bcf90126dbac847fa4a3a03b8aab>@@
.?AV<lambda_0d37934dc4686e9f8cdc4b98c71d039f>@@
.?AV<lambda_d95ddce1a1fa4ff8b9f16e34e85c7615>@@
.?AV<lambda_c4a3869be068e7ba91c5160df08ada1c>@@
.?AV<lambda_0ec02d5276c6a16fd616d10b796f4d2c>@@
.?AV<lambda_159e0921f0d4eb52aba9cdc091e10630>@@
.?AV<lambda_125b77507bd9658139842e91e09bb467>@@
.?AV<lambda_9821a1e0b606be014cc2fca34ba53b1a>@@
.?AV<lambda_597c15d719a814d12541cbd2bbff60a5>@@
.?AV<lambda_2e2760691203ec6d385de10317d43eb8>@@
.?AV<lambda_5d86fa40c6f141e2f5308ab25a37373d>@@
.?AV<lambda_5de94ebf8ac34b72c76fcea7a63f223e>@@
.?AV<lambda_3cd6c3d57d7e8805d712028bde485ef8>@@
.?AV<lambda_caa048ef690674a871bcb383934123ad>@@
.?AV<lambda_6829271c72aa0e0367fc0ccf7a9e3edb>@@
.?AV<lambda_7c5eb1f068179d900c888fe3d5890426>@@
.?AV<lambda_e57b1a786ad480298008b53b2aee7436>@@
.?AV<lambda_637d27d6a01beef351cd6df162f9426e>@@
.?AV<lambda_3f2f36db15d672a75a701714494919da>@@
.?AV<lambda_d3bde8af1c4b2f7f5d32bc0631a76a47>@@
.?AV<lambda_ffd7c59dd1db3541219e1571e11a3c77>@@
.?AV<lambda_2f8e437cb9f95dd76b3982067268238d>@@
.?AV<lambda_730e393831d92a8b1623c0ee3e0e1a73>@@
.?AV<lambda_b904ddb979f839dff33d10b1e7d0bdbf>@@
.?AV<lambda_f0c2d149234417642d38bb42f3187940>@@
.?AV<lambda_d7733c1584aa3badfa0c0498f12b13b3>@@
.?AV<lambda_ee98d36ba3b6f304603cfa013effa1d0>@@
.?AV<lambda_0d1cea9b21275bcc722a263f1867aab0>@@
.?AV<lambda_67cab59cf9e4d6fff4a228b6f50a31f4>@@
.?AV<lambda_bb3d425e1c6940fb9e0c2d83749b4004>@@
.?AV<lambda_d51c06990161dc274f89c0ff046b1644>@@
.?AV<lambda_6d1a31765a629c9bc43dcea20434203a>@@
.?AV<lambda_4e6fa5fab57276f6c38287bd072060d0>@@
.?AV<lambda_a9804f77c3a977ad53df45464de9292b>@@
.?AV<lambda_4a6aeb4175d7888941bfb44f87a822c0>@@
.?AV<lambda_4782a3788b4d37949c863bd3b1d2db4e>@@
.?AV<lambda_077886625d25c359a62527fbaa92e49b>@@
.?AV<lambda_02f0fede11d4b9d7439baa125e06788b>@@
.?AV<lambda_4b40e91ae8e8c37fb63980fee637f885>@@
.?AV<lambda_d13d78c406485322a9ca312b3346960c>@@
.?AV<lambda_e742661535f51343e97c0bd1f594fd9e>@@
.?AV<lambda_ff5e6787b4748302677d41e07ba5b8c0>@@
.?AV<lambda_9b848ee398be1c2a596086e8c2c0124c>@@
.?AV<lambda_0adf0d24f2c35fa4a66c3d2e349117c6>@@
.?AV<lambda_0fe8093ffea6514602a81f131e8a213b>@@
.?AV<lambda_2e79487384136cbeb73bf5cd55164ae5>@@
.?AV<lambda_f20e48e7127186879f643480a395548e>@@
.?AV<lambda_7a6442793a842f584bc31790203846c7>@@
.?AV<lambda_a5f2de4a08d99e6253357934807ae733>@@
.?AV<lambda_02dbf81fbe0e79782741ce39a831bb2f>@@
.?AV<lambda_3199312d6fe856623dcfbc1d675d418f>@@
.?AV<lambda_6423e25c1122e8ea98faabe27ee57b24>@@
.?AV<lambda_b1dfe0352ec14503f6bc27c9ab7381cc>@@
.?AV<lambda_c5dec7e5184735ef54bbb3fa15124e26>@@
.?AV<lambda_f4ee709d6b6434a145be84c46551b589>@@
.?AV<lambda_5ca3b2fb2261ad467c7e40fda304e71e>@@
.?AV<lambda_d318dbc50159c2f1256019c88eaff40b>@@
.?AV<lambda_771974d945874fd2518ef4fcc7999ce0>@@
.?AV<lambda_fada670c8c8fb6ae388f5a9e4132a8cd>@@
.?AV<lambda_9e468aee1cda7bcda7bd21224cf0d16f>@@
.?AV<lambda_b7019bfd7941f33388d4d95b9daa50e8>@@
.?AV<lambda_bc73456ccef784a5af3834b4cd8faaf9>@@
.?AV<lambda_bd14764690ecfef9fa94612b0f5ef502>@@
.?AV<lambda_057831c89172b3e745c34953b854d875>@@
.?AV<lambda_175914cea4037e183fc735b60abe96c9>@@
.?AV<lambda_8516815e67f3594718fcfcc07116860d>@@
.?AV<lambda_74ee45c15b018f2c2709e09284f9b432>@@
.?AV<lambda_c7e777d5cfa84aa6006d2063a10ce1f4>@@
.?AV<lambda_5cc43c49af3b89fd43e2aa8a8afc2e1b>@@
.?AV<lambda_47109259520c4e793b98906acbd4073b>@@
.?AV<lambda_b6b6722f1e0f042c63804b4645002c54>@@
.?AV<lambda_034a872241814873b16b150e98786486>@@
.?AV<lambda_dfc50b5c75707c0e32bd4de87875de8d>@@
.?AV<lambda_359fe019cdce504d42df9f98cd05ca23>@@
.?AV<lambda_38e1bdb2070443b647c40452b6c90c15>@@
.?AV<lambda_975b91ffee199f0bfc6f725443e3d8f8>@@
.?AV<lambda_f1cd88210e6db5811994272c07493962>@@
.?AV<lambda_5241072fb302daf3d92fe8af4902fcf1>@@
.?AV<lambda_45e08ad47c9bff0006768a2c526864a3>@@
.?AV<lambda_772b291d0a2826994f823d005fbf5dd0>@@
.?AV<lambda_747d01ad8121cb89860d325b0c86de9f>@@
.?AV<lambda_3deb1638445ba1a8c860cf9e9ece0b32>@@
.?AV<lambda_59d9acea5faf21b910fd8b35ea3c81b3>@@
.?AV<lambda_2bb569fe2197271e2b1302ced7b75d4a>@@
.?AV<lambda_691026265f1e73c9b49221bbcaaf7e8d>@@
.?AV<lambda_609ecd802714b672c2c81632a57c73f4>@@
.?AV<lambda_c9e83474de578fe8631d32b8d8721959>@@
.?AV<lambda_811c61ef0fce38996d92306d6a5fd2ba>@@
.?AV<lambda_c8fa02c31283474933648634b584ab88>@@
.?AV<lambda_87082f934acdbb058c5a96c46c9520a5>@@
.?AU?$ParametricSoftplus@M@functors@onnxruntime@@
.?AV<lambda_9c9dbe7f2291bf7ad9d3c12ac8113947>@@
.?AU?$ScaledTanh@M@functors@onnxruntime@@
.?AV<lambda_41ab0f59dafb4a0526d00edffc434f5b>@@
.?AV<lambda_4177af3ea58d3ef77d743843af9cc9e4>@@
.?AV<lambda_fabc2f3ff1a13bd31a439cf6cb082152>@@
.?AV<lambda_2bc5f319d61b1e5d5273f7fc52f04186>@@
.?AV<lambda_5da63fe37738ec572cb4587955372cb4>@@
.?AU?$MaxpoolWithMask1DTask@M@contrib@onnxruntime@@
.?AU?$MaxpoolWithMask2DTask@M@contrib@onnxruntime@@
.?AU?$MaxpoolWithMask3DTask@M@contrib@onnxruntime@@
.?AV<lambda_f5cf874a75941a4df47924a1ea9ea866>@@
.?AV<lambda_d8d69ea69c6171a0cf8e50796541d588>@@
.?AV<lambda_d2d7cc7cd6283dbb6ec9d6d27a7f28ba>@@
.?AV<lambda_f86ec1fac20fee0d4eb5798f81c1da0c>@@
.?AV<lambda_2b438ab8008a3839c2ba42ed80d39f62>@@
.?AV<lambda_19294b02f6e15956b08baa928b37992a>@@
.?AV<lambda_3e4d0130fa8e6aaa56df4817dcf6c190>@@
.?AV<lambda_2615da15ba53b69982b830ee817c81ee>@@
.?AV<lambda_4d8164c41daa6332527fd7a024ba3da7>@@
.?AV<lambda_f0a5b0eb3f2f9f34c815ca05348f51fc>@@
.?AV<lambda_9d370f39b593a9c0408175efba97f3a7>@@
.?AV<lambda_c20626d04f0b55b1c8f4e6d26e7dbf7f>@@
.?AV<lambda_afcaa8d78bccbe99ca495a99e4eeaf71>@@
.?AV<lambda_f38e192139ba1cff9e8db37924678e9b>@@
.?AV<lambda_dc38421579cc359d17e1b6586ccd53c4>@@
.?AV<lambda_cc475f2c9c4a5e3bde84a6796d60e832>@@
.?AV<lambda_b9678ecfd54fea19e98c665d6b7111d8>@@
.?AV<lambda_ab99d16aa8824a5d2e1b2dc9090914c7>@@
.?AV<lambda_d1104bdf26eb9a3a2f36a00e0287055b>@@
.?AV<lambda_9b8516a2c0a6d92f94f78080c5bc3b53>@@
.?AV<lambda_4ed1fe6920001affda0cd1722766e0e3>@@
.?AV<lambda_a6ed0d25733f7001ea91db6552d588cd>@@
.?AU?$QLinearPool3DTask@EVAveragePool@onnxruntime@@@contrib@onnxruntime@@
.?AU?$QLinearPool2DTask@EVAveragePool@onnxruntime@@@contrib@onnxruntime@@
.?AU?$QLinearPoolNhwc1DTask@EVAveragePool@onnxruntime@@@contrib@onnxruntime@@
.?AV<lambda_9f09f893fd25c5f3e434b7471f6ab3a6>@@
.?AU?$QLinearPoolNhwc2DTask@EVAveragePool@onnxruntime@@@contrib@onnxruntime@@
.?AU?$QLinearPool1DTask@EVAveragePool@onnxruntime@@@contrib@onnxruntime@@
.?AU?$QLinearPoolNhwc3DTask@EVAveragePool@onnxruntime@@@contrib@onnxruntime@@
.?AV<lambda_c225c1d25bbd93cb1adc0485c77bcd32>@@
.?AV<lambda_e5e6dbc489066f0491275014df31d396>@@
.?AV<lambda_52a86257ba105ca00a347edc7c75852c>@@
.?AV<lambda_788dd9ac40e758b51e6e5062cd2232df>@@
.?AV<lambda_99c04f24fb37c840416c3031e266a935>@@
.?AV<lambda_97b64e3cab478b2173a15dcda1845df3>@@
.?AV<lambda_16be1195897e7eadc7afc6c2ac4f5c83>@@
.?AV<lambda_69fb29d342558f2407dd09c009c85823>@@
.?AV<lambda_a1e22134fbe96254ce6d7a539c26c203>@@
.?AV<lambda_c332dd4138e096b0ec5e0eae392e23e1>@@
.?AV<lambda_62479ee4cba0a1701384406c477dee66>@@
.?AV<lambda_f72360673da28b091dd40b649eb46e9c>@@
.?AV<lambda_d60ebcb8fb80278504251155552d9189>@@
.?AV<lambda_95a0740ad14942e833a8456dd992b6d9>@@
.?AV<lambda_a2fde83c21b5ed7bbde18c0d05ee467a>@@
.?AV<lambda_192a5a4ddd61389a8d1be3b6ca8c764b>@@
.?AU?$default_delete@VIAllocator@onnxruntime@@@std@@
.?AU?$default_delete@VBFCArena@onnxruntime@@@std@@
.?AV<lambda_295cf861cf0e14c55f05d3a6f16e5476>@@
.?AV<lambda_a8e944132f86054019a0dd06ad885a3f>@@
.?AV<lambda_07c226046fe685e37979217c354766b5>@@
.?AV<lambda_3e31c198cbd0b452ff5ea25fd603eaa6>@@
.?AV<lambda_ee881615ba5030f55f651fb9e91a2637>@@
.?AV<lambda_42fc8c33eb035f61f8d39bab0f514aa0>@@
.?AV<lambda_36c84409ed98093c11b3d0d02c089264>@@
.?AV<lambda_d41a0a7dd6ff244599c8f5fcf6aa6a8e>@@
.?AV<lambda_dc786c9bfe42f2772e18a10475fce9cf>@@
.?AV<lambda_68f4cdf20c40d247dba9fe0156c932f5>@@
.?AV<lambda_d5280148c0688c7d0bac28aeda9b34e8>@@
.?AV<lambda_4f86db608486d9d830b4d71843ace9e1>@@
.?AV<lambda_5949c53d8ff4275e91f18552fb52fd7c>@@
.?AV<lambda_043c78e85491bd0db92f03f63f8c56ae>@@
.?AV<lambda_a4eb294818c1d4196ae37e2aec2c57f3>@@
.?AV<lambda_4a61ff195999a8a104bf36ba131bf403>@@
.?AV<lambda_217727e3bc662f496961b1bb8f9e821c>@@
.?AV<lambda_27534cc625d5ab04dc11bfb2c47d616f>@@
.?AV<lambda_ad704c2931acf5ee8c141f89b8d0ba76>@@
.?AV<lambda_45afddba11e0bff90c21310e9f91b09f>@@
.?AV<lambda_98b53a1372ec257c871e8ef28d59be07>@@
.?AV<lambda_6d6f3d3230139a3d222e52865731387e>@@
.P6A?AVStatus@common@onnxruntime@@ABVNode@2@AAVGraph@2@ABV?$vector@PBVTypeProto@onnx@@V?$allocator@PBVTypeProto@onnx@@@std@@@std@@AAV56@ABUResolveOptions@42@@Z
.?AV<lambda_fbb10b34ab7c53fa4628c7de9c1d7ff5>@@
.?AUNodeCompare@onnxruntime@@
.?AUPriorityNodeCompare@onnxruntime@@
.?AV<lambda_1da5929b711c2681e91623a20d50a98f>@@
.?AV<lambda_76eec3e346e3038343d242b83e7ef7cf>@@
.?AV<lambda_8d52f2d53a04afb64fff4765a2f91d21>@@
.?AV<lambda_76f850960fe6ac327acd4613d41835f7>@@
.?AV<lambda_9a2786289a4d18fa33a8facc5fc736fa>@@
.?AV<lambda_22485ec429c0920bef998442ed4b0141>@@
.?AV<lambda_2f6217725d36a8fee3233a5a3a8acbc4>@@
.?AV<lambda_d8c2999bab01791ce81720c649701452>@@
.?AV<lambda_8ac5cfcccd347e512ac9419fd63346cb>@@
.?AV<lambda_245134a5955e4a8621bc7afd20ddbe70>@@
.?AV<lambda_26de9b5e95466f00708e3ae83a84c608>@@
.?AV<lambda_423c7f4514cb0f580d17c1969d94ab4d>@@
.?AV<lambda_860b5cff89ff5f292872db2aaa19c52a>@@
.?AV<lambda_a9ad92a14c7791713a13c8f77948b581>@@
.?AV<lambda_076f399d54601fd1059236c6a3813828>@@
.?AV<lambda_9d8771d050eb3774da949f76ff7b2f7a>@@
.?AV<lambda_e74921842af68f126d6a963647d9b662>@@
.?AV<lambda_d49bbfb4942fd85adb6441f849e70bf8>@@
.?AV<lambda_4ee11195e6ac34d8802828c9de8931d7>@@
.?AV<lambda_273270d63d2edeeb9cb50e9432a42c00>@@
.?AV<lambda_1c17b461588ff1b22a6f9497bff5b28b>@@
.?AV<lambda_f65b8c2fd23e2320c311133a80f471c4>@@
.?AV<lambda_1cd6b5a2cebd273f7225f466512c43d5>@@
.?AV<lambda_07364fb607ea89235f416789a046b92d>@@
.?AV<lambda_cc4bc3c6927d9351454759e8077d0a48>@@
.?AV<lambda_98f67966096a245ba60cb535f4355420>@@
.?AV<lambda_2d30aee68f6d7fc70a227ffb90481678>@@
.?AV<lambda_56c3ac63d8a5c9a2d5ed84f082ef3cf5>@@
.?AV<lambda_e7f381725b47e4a5cf7649db595fbdf3>@@
.?AV<lambda_ec14091c8d829910b5be28a833d6a3dd>@@
.?AV<lambda_29460ba72c9a9f61937f695b212c2385>@@
.?AV<lambda_5c90896dede9afea4c2bfc57d475c2a5>@@
.?AV<lambda_71ef653055b00e102f19330e0f78a252>@@
.?AV<lambda_7134bef299e596690440ce2c3fc8b019>@@
.?AV<lambda_c09531cdd6b30fb34e9285a274065481>@@
.?AV<lambda_814de60c7177af03b3ddc9e5c36d561a>@@
.?AV<lambda_104e5d4f3c346a5807a7db11389af990>@@
.?AV<lambda_7d629299b17d3241cda0057fa57cac3b>@@
.?AV<lambda_893ec3becb9cd41d9efbc7eea98d41c9>@@
.?AV<lambda_12dd4c4dcfd6c2d8effe6071c823bd43>@@
.?AV<lambda_9a36e00d72a5d737ec77f3dedc2d9072>@@
.?AV<lambda_e15d10c4385b9adab7c3b2967076a5a0>@@
.?AV<lambda_6a32611970906250c6b47d8292ad00e7>@@
.?AV<lambda_6107f589d79fa5fffbc0daf95ca9b592>@@
.P6A?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@ABV01@0@Z
.?AV<lambda_672376956558218714cb97c30f1f09fd>@@
.?AV<lambda_85740bb65c1a7256d972833e138b25dd>@@
.?AV<lambda_14407bcf8cfc66db9a85fd2745a31d94>@@
.?AV<lambda_7a5892277686c5fc70de8922cd945035>@@
.?AV<lambda_b3cbfd0276cf0ad6f3b58532d60f239f>@@
.?AV<lambda_001ec6ab06d68d1c890e5bf445b4d487>@@
.?AV<lambda_f9fcc66b14829c5756eb43c20bdf984b>@@
.?AV<lambda_8854c23c2fa70d516c73af69bc8b08fb>@@
.?AV<lambda_7361ba811d5ff92bf50e102c73312480>@@
.?AV<lambda_77bbe4874254ed84a312b5fa826a83d3>@@
.?AV<lambda_4895a69c6e2d4d3665d84240b52946d2>@@
.?AV<lambda_c7733fbf1e7357c5a2b00aac2ede4865>@@
.?AV<lambda_fc59cfd6180e7962f9dbfcf6d01970a3>@@
.?AV<lambda_2ed99b1b7f84cedcec6ea06ce6c01dc5>@@
.?AV<lambda_b58abd66dfb794261458d2caeeba2a75>@@
.?AV<lambda_87a4a0298034d28fa5baea8b9a5f4bfa>@@
.?AV<lambda_e12a5cc64ca47f52fabae1f759666c1b>@@
.?AV<lambda_afff46216fde1007fd3787aa1c6f64ac>@@
.?AV<lambda_2f25b8f6b95e7cf36d88a2facd4ae1ab>@@
.?AV<lambda_dc5b9bd79fd7f93771785e3c76ee3967>@@
.?AV<lambda_715d031581e8d9abada945eeadbab576>@@
.?AV<lambda_c898490d638c72402474e8c5b41a9926>@@
.?AV<lambda_5ead457e3eeae74d2baaa54a34ab876e>@@
.?AV<lambda_6e847de2b8df09456577c83c1abac749>@@
.?AV<lambda_d2a49f03e450ccce2f16f6b50c34ec41>@@
.?AV<lambda_9f3bde71d5b761f626949ad2b1feb96c>@@
.?AV<lambda_f981a1e78256ff9696dd76fdbfa999a8>@@
.?AVValidationError@checker@onnx@@
.?AV<lambda_73119717ae86d28cb548ac4543eb73ed>@@
.?AV<lambda_53958a524045125d538038a834c170f9>@@
.?AV<lambda_a45f274125d5aab3ada44bd6d91ed7f6>@@
.?AV<lambda_1476157c6a284e4f379191854345eea5>@@
.?AV<lambda_0aa1cbb10b7e27c8eaa9e1c4d936a012>@@
.?AV<lambda_9f22cccb787c8b0be66f7b40706128ae>@@
.?AV<lambda_62526b7eac31a4fea5091c2f348a5e23>@@
.?AV<lambda_d7d65f11678621cb189879800cf53faf>@@
.?AV<lambda_2997ea06f466f3cb7af7580139e0e9b1>@@
.?AV<lambda_0a786afdc494302a03a8347211af4f5e>@@
.?AV<lambda_0b84cd883df2cf8f5da7751da99be58c>@@
.?AV<lambda_20ce3835ea17537cf13d089eb1a443c1>@@
.?AV<lambda_8226f92ae14eb377c8291a622693e68d>@@
.?AV<lambda_348a4c4053fb0e4fb238401e94ac3418>@@
.?AV<lambda_ee988755aabf6d6b8cbe1162f42f9b60>@@
.?AV<lambda_3ad3f00f380cbc79acdede7031c2b35c>@@
.?AV<lambda_965a3b769c39b0bedf189a3ea0545ea9>@@
.?AV<lambda_221457049dd6e599b1e592be3898266a>@@
.?AV<lambda_bb1f7ff97f0a93bd0d62291fdf3e87e2>@@
.?AV<lambda_8a900b8b41180a41f1571bb2c4cf6f8e>@@
.?AV<lambda_19a25a0e0bced01a01388502a5fb897a>@@
.?AV<lambda_6d0547d7d9e564311a780ca9dc7db655>@@
.?AV<lambda_91fdf8e96bd0363c4307fa79933bf9ee>@@
.?AV<lambda_82b38e81208f4cf45ccff9cab1f1ab56>@@
.?AV<lambda_5169332acacd2cda6014329563a49097>@@
.?AV<lambda_1cdda74d195f4dc7bbdbaed3ee174bf7>@@
.?AV<lambda_58e45bcf81f2eb4f6ae6b3f124defb13>@@
.?AV<lambda_56054d16adda54f2046f2f8778fd36d1>@@
.?AV<lambda_217211e0b9216fbae93bbaf0026e78ee>@@
.?AV<lambda_1147aa6d38b42c0a8559813b3004180f>@@
.?AV<lambda_81de0b469ab0f1e7bc6d7cfef2665991>@@
.?AV<lambda_bb618da91ef45dc5293c089a74c35488>@@
.?AV<lambda_b24067cfb776b28c2465a46fad4c39cd>@@
.?AV<lambda_3e2860b55958cf532cc6672e843fd5e5>@@
.?AV<lambda_1642adc2d95a594ec2d1ca1c1679605d>@@
.?AV<lambda_939f4702fdcc80c80258913a08838422>@@
.?AV<lambda_c277816ed3e96f001ac0003fb11da190>@@
.?AV<lambda_b3ffb2b19f45444139b13fc0568f9537>@@
.?AV<lambda_d86f3bcde641b4c18df519c60bc10d09>@@
.?AV<lambda_28d76a4078a4a8dbd4ec44ed08d36b25>@@
.?AV<lambda_0a0326aaa0c17e1dc10459d8b6c3398c>@@
.?AV<lambda_88b1496ecc213c07fd24b30bb5c856e7>@@
.?AV<lambda_e6687d53b2ff8a73e462a3df53da446b>@@
.?AV<lambda_a6d62ed7a3aadce21a3a08f0b111f7e9>@@
.?AV<lambda_18e0b70d8d5a276d72055cc8661094dd>@@
.?AV<lambda_6bf7af48e7a4158e20b4f833c15de130>@@
.?AV<lambda_d8059ce711ce14c36533947d64fa5a34>@@
.?AV<lambda_8da27afb1f9e2f39230ce8504a3b22f1>@@
.?AV<lambda_3bf9d7b5239a137326d2c5fa821fa737>@@
.?AV<lambda_b0f9e3c706f87be7af57692823441e06>@@
.?AV<lambda_140ab31565a9f257f202ece6453c4bfd>@@
.?AV<lambda_cd453f5abbb4020fb3775475830cd8d1>@@
.?AV<lambda_aa831217bcc44892538d39aae8c330f6>@@
.?AV<lambda_9283763b81fbf1c4441399956127b6eb>@@
.?AV<lambda_2a976ac88d60c988f6c82762c8669484>@@
.?AV<lambda_5e0adb550519bb305a282a49bc052ff5>@@
.?AV<lambda_edcbc83438e6dd39d1d927db086748d9>@@
.?AV<lambda_10f3982d8c74497eebbba2b59c0e4116>@@
.?AV<lambda_3ce4407e5879111e6c1a6435d36077e4>@@
.?AV<lambda_54b4204c0ce4d49d0ae3ff6e8b9800a4>@@
.?AV<lambda_03c53d5c540d06d25e58298bd3e2675f>@@
.?AV<lambda_bcb80e95aca4245f759eaa0035e910aa>@@
.?AV<lambda_0d82943818680ad159348f56b6d25693>@@
.?AV<lambda_eb7de4d30a85c7b44c19b4f8197380ee>@@
.?AV<lambda_2dc16cf30de15202e0434934c8ec0571>@@
.?AV<lambda_b881ee6a3336741e3e3697374c374936>@@
.?AV<lambda_d0fdf4211aa9516cbe57f435cdbb747f>@@
.?AV<lambda_57db475e5df80ef36653d188ca9951ac>@@
.?AV<lambda_dcbbb46dcb2ccb38f1b7e3e89a0ca172>@@
.?AV<lambda_b6dee4d13517d8cf66ef1e240c3ec6a4>@@
.?AV<lambda_348952d4e8cb4d6ca40b91f67d9fe4cc>@@
.?AV<lambda_c9cc85b6189178098c01bbceb71ba127>@@
.?AV<lambda_1e6c58f2c47fbfdcb340ca8aa595e354>@@
.?AV<lambda_c6a4c758ec4a05da2df0a96bda1f195f>@@
.?AV<lambda_c765ee4079d8132e5b64fd41ad2c9a7d>@@
.?AV<lambda_16bdac8a01801cd7a5fcb1f21cc7a9f0>@@
.?AV<lambda_f08787ec2811763c406ae46c36fa9fd3>@@
.?AV<lambda_0031e1631721c75a1f7a0e71cb748ed7>@@
.P6A_NABUFunctionBodyBuildContext@onnx@@ABVOpSchema@1@AAVFunctionProto@1@@Z
.?AV<lambda_510b48fcc953c4af1dbcb4b8cf19756a>@@
.?AV<lambda_9c23f807f5b88acf4c74334fd51de3b9>@@
.?AV<lambda_874d535e153c7e2cc7bc8cf3f159ba49>@@
.?AV<lambda_1f77acb4e34034b00a5a2150384c9d77>@@
.?AV<lambda_65ea036252223d30a37862dfd4f7bc69>@@
.?AV<lambda_5e0f6565dc7aa7f6ebc79fe2fd37d8ea>@@
.?AV<lambda_d6d6a472b26fc7f192d382e93bba1d57>@@
.?AV<lambda_16aeacf7ad4b53ba550defbc9c03abde>@@
.?AV<lambda_3f849c6f51e31293c619565ac636bda7>@@
.?AV<lambda_f4d85bd911255cc25b789888dd092506>@@
.?AV<lambda_68094969648d2e0eeda4bdcc6834405e>@@
.?AV<lambda_b86aa8ef1de0593a5547f08d792c1565>@@
.?AV<lambda_712b3a1726a5ab1ce39d1b3e7f50d979>@@
.?AV<lambda_f78d8769dd40946ef61ad5956cbf10fe>@@
.?AV<lambda_8704e7031b6ab9a2ec7b63d5aee19048>@@
.?AV<lambda_e8a1f60a9605811126ddd13004e0920a>@@
.?AV<lambda_3354f944db7877754471d985be3fe29b>@@
.?AV<lambda_9f005c5385db773ebfdb522ad8aa0be6>@@
.?AV<lambda_dc838e8d8ebeda8eb60e5b42b6de163e>@@
.?AV<lambda_4a58e5f6034c78ef31ef5d51eb58949a>@@
.?AV<lambda_9a4c460641eeb4213d39e91aec522d90>@@
.?AV<lambda_53687b87e8fc26669694bb154ed06213>@@
.?AV<lambda_47f0845d7b66643be4c58515f34249ea>@@
.?AV<lambda_486023130535c54dc87259934f24df6e>@@
.?AV<lambda_2ce98da2a528969c49f46cb09a7b6eee>@@
.?AV<lambda_4ff3da6778c55d2ab99a53821a349232>@@
.?AV<lambda_253a3836eec6c1ddbcb7d28211e21bd6>@@
.?AV<lambda_ecbde622a6c4fa53412bd71f973ccdba>@@
.?AV<lambda_85b8c9439e9295f048404909acdf29d3>@@
.?AV<lambda_976990ca77e2d50b70bfeaa03811f4a4>@@
.?AV<lambda_754c2a248a4974a3df70d7c3cf8fcac0>@@
.?AV<lambda_1433794861c06b7d98f325651aa885c9>@@
.?AV<lambda_a7b5600ffd3fa627825fdc42ab8e2c48>@@
.?AV<lambda_abeea81c8db6fc4afe459674d5836b9e>@@
.?AV<lambda_b13b6f35685da4b80165c8f42fe46541>@@
.?AV<lambda_577697c65f7f30ab5c890fb5e643c3c6>@@
.?AV<lambda_64e20fef6bc734aa72d77b8028e3b077>@@
.?AV<lambda_048b899a3305cb2c0c070dd18e5b4d17>@@
.?AV<lambda_cc9e076beb57119667e5b07bb3c48707>@@
.?AV<lambda_6367df6aa050372a33bd7465b9bffebf>@@
.?AV<lambda_4c81a7b179f9e26e2d05ebefbb83c381>@@
.?AV<lambda_7ca26f875a5bd8018609c044fb5616a1>@@
.?AV<lambda_031e8dfd8d6a123502d9ec2194443e60>@@
.?AV<lambda_d30ba908d7a4df45178218a83c8b90aa>@@
.?AV<lambda_14ab4d68c965e23bff80a9edfde3b16e>@@
.?AV<lambda_f1490410ee527b42f3bd28e691dae2a7>@@
.?AV<lambda_acdb2664af6229da1e4c43d330c314d8>@@
.?AV<lambda_cc810076352af54d8ab1334d58ef2ac8>@@
.?AV<lambda_0ba48c1f3299a5f4228b96a5876e6f39>@@
.?AV<lambda_6dd6cf995614bc7c2e490d548728e197>@@
.?AV<lambda_af2c39467b4484bc2a2f611cf533daa7>@@
.?AV<lambda_95d4c31e5917c076b85b93f005fcb675>@@
.?AV<lambda_d125101b1e77d289ab0937ad814f16d4>@@
.?AV<lambda_f9b942c519c5abdf8c9e76770faf15a7>@@
.?AV<lambda_da7c0e243075d823e83aad5424aee4f1>@@
.?AV<lambda_45e1f6e47762148b282b3bb16964d9a7>@@
.?AV<lambda_c2634be0290355aa32c42903b822d942>@@
.?AV<lambda_7dbc984020fde247f30ce0431c9445e7>@@
.?AV<lambda_b090b097d440a4b2445143675bf63aa8>@@
.?AV<lambda_522944c70ac483ca59ad2373ac030c86>@@
.?AV<lambda_08b89006bb2cd45177aec966a5a64a25>@@
.?AV<lambda_845fb0d365668e61a65e09bae6280c09>@@
.?AV<lambda_dec8978adb68f88cd37c14b215792fcb>@@
.?AV<lambda_40f74427ec28f20a5d59b70a3f7ba162>@@
.?AV<lambda_3e5e22c83296c8f56ae7effebfec7009>@@
.?AV<lambda_36945aef9b69b8bc246158e60ded74ff>@@
.?AV<lambda_2bed918e0092d38de095a3dfcc39bb6a>@@
.?AV<lambda_5151cdb5e2d0e967b0ff9b516f4dde5e>@@
.?AV<lambda_a4fb1c1b5f905e04c31cbbe6c165878a>@@
.?AV<lambda_673e4ce19ce5833538c9ec8e56f2275c>@@
.?AV<lambda_bb245b6c588ba3a259962707ee90e1f2>@@
.?AV<lambda_40d7d549b7296d37bd8a375a6a32735f>@@
.?AV<lambda_42f0399bf2e271e6e951294a5aec23bb>@@
.?AV<lambda_8bbb4bb940b6248f0079a061cca5209e>@@
.?AV<lambda_0f569c3741dec4cbd25547f5cfa47a1f>@@
.?AV<lambda_13bf14abed62c4ddb7d24aa834c66cd3>@@
.?AV<lambda_6105a132d5626ddb9b2ed4958c88e1e2>@@
.?AV<lambda_d9adada7822ba67bc618dd6220c7e11e>@@
.?AV<lambda_6665c408830d0dff611752f43635ad2f>@@
.?AV<lambda_913ebb992bc3e09bdf2737529cebbf5a>@@
.?AV<lambda_7c393e0ced2515f63b0674de1e1229b1>@@
.?AV<lambda_fc1efac575c2b28ad2e40dacdc9d02c9>@@
.?AV<lambda_7825462dd39e55a2adc835dd480c65dc>@@
.?AV<lambda_140d9b670661cea14ac8b7667467087d>@@
.?AV<lambda_45c4687729fcbb9b32a11320d77112ec>@@
.?AV<lambda_7d7889780bcdcdf31ae68e0e6e2b6b04>@@
.?AV<lambda_9cc2379234b917d77614797746e75684>@@
.?AV<lambda_ef1b5ee9690bf60d6d8ba03dec70f0fc>@@
.?AV<lambda_43ec4bbe506690d5dbfb40f909da196a>@@
.?AV<lambda_006f042491572090b778a6887556c541>@@
.?AV<lambda_04111ef4993b4b7660410cb114680c48>@@
.?AV<lambda_5977a43abcce90501d97902d58330cfd>@@
.?AV<lambda_d2d693a490e9887da5a024c703dc8e3e>@@
.?AV<lambda_e8a6e880ee4bae18375748e5a9775705>@@
.?AV<lambda_747f5a5054cea5042105b3988bb8e54a>@@
.?AV<lambda_2cdbc5f873c2ce32c36481fab53d6870>@@
.?AV<lambda_946ede80a21d7425c1a0e51bfb4c4cf1>@@
.?AV<lambda_404d0fe71cc8d7867c9f1bc290b28bbd>@@
.?AV<lambda_af999ce2e03a2039dbf31c1ff13e0675>@@
.?AV<lambda_6aeba50936d3f6c7b11f3c24b58165e0>@@
.?AV<lambda_665f26c4f805d51279dd7cc3218f1cd8>@@
.?AV<lambda_d2aedfced5017e33ef2fd58df23b739b>@@
.?AV<lambda_96fe9f03daad80a6076c498e634c5f49>@@
.?AV<lambda_5dddc316c01bb02791c37b03fa523bb0>@@
.?AV<lambda_4c798c4f860875abb86117d8e4ba8ff1>@@
.?AV<lambda_9330135269578d8e01a33adf741ca665>@@
.?AV<lambda_35d59ef9b84b9c29d8badb1b46db4e70>@@
.?AV<lambda_9ec8ee75f7d0151bdad20dc55223ff7f>@@
.?AV<lambda_ab9a9c5c0feb1fa6529cfd9c464ffa82>@@
.?AV<lambda_be82f0d3082d1770e6fe9b6560ab180a>@@
.?AV<lambda_c6cc7a538d9c817426d2f4671032805a>@@
.?AV<lambda_458163864faccd230ccf1ce10d3e187d>@@
.?AV<lambda_43d76a454d7e446551a32b163679964a>@@
.?AV<lambda_a4015e490e3e9078f9108a810d677815>@@
.?AV<lambda_3c5ccc04791f73730d9ca40d92c258b4>@@
.?AV<lambda_33763db33c7430e0074fd25b65b4479f>@@
.?AV<lambda_e4ad6020d0e0535a8933e08ebcb686f6>@@
.?AV<lambda_27a631d2450c357a52927c1dfbd2efda>@@
.?AV<lambda_5c5ac1f6c71d812ad45802a5c8ea5757>@@
.?AV<lambda_d73505fcfab084acb258f01cd57039ac>@@
.?AV<lambda_a8ee5a68ff8f1273772c7d2c6e9affbe>@@
.?AV<lambda_2f4103e1c4a1773101a6c1f055d56df4>@@
.?AV<lambda_40a7ea59743a73ab67f43e4e9682b9a3>@@
.?AV<lambda_6f010deb824d03b6370449d6f782b9da>@@
.?AV<lambda_0af4c846dd5af6632beb77ffbd6469c9>@@
.?AV<lambda_2122617e64cb8af81fe74e4dbd5b36b4>@@
.?AV<lambda_fd266a9166d0b10d35a0d8f14994daa2>@@
.?AV<lambda_39d5079814e87d3ff2cd3537d46812cc>@@
.?AV<lambda_5e8795de6046262c724adaee055c9399>@@
.?AV<lambda_aec81523952d967a50c07470b5814993>@@
.?AV<lambda_d8ccbf6f2719267adef088325c832169>@@
.?AV<lambda_6f05a8b4f6851ad7c85a3a3bcb9b9104>@@
.?AV<lambda_0ce942d577225b307b9474951f72244f>@@
.?AV<lambda_9257e4c042146540822b621335e71112>@@
.?AV<lambda_9c03d71b3e2a72420d0112f6f3840dd4>@@
.?AV<lambda_1e1df0082e8fe003ce7e63ccb14a6aa2>@@
.?AV<lambda_4ad3f36a5fcdf1d188458be0a989ed3e>@@
.?AV<lambda_62f1ce9bc208e37c7e5739c8f36388ed>@@
.?AV<lambda_637049d8de28c0182bf4a2696729caaa>@@
.?AV<lambda_90e67d7cf9c8fca59bac4a84fc304da2>@@
.?AV<lambda_50bf1563c4f2a658d5a1f8997cf7d841>@@
.?AV<lambda_4ae79009ffed4baaf3b13205e614d6ec>@@
.?AV<lambda_4c0d5472c0e9893aea307d804bd55070>@@
.?AV<lambda_84e8a40ae9ede95d9e10e30d574bf4f6>@@
.?AV<lambda_7b0f2fd49a85118506333bcc9c82c163>@@
.?AV<lambda_9cbf79b94b2fe6cbcf57182cdc276d42>@@
.?AV<lambda_e6da43affed7c8fa1984fb0568cffe7d>@@
.?AV<lambda_3a5e9806f5a74aa166ab1dd78ca1a933>@@
.?AV<lambda_a0fb753c0036461099c9ffd5a5b954e2>@@
.?AV<lambda_2b4b545989d1d9ef9003a30b5e0ca43b>@@
.?AV<lambda_58ba1217675fabc528cd9f4eff671e5e>@@
.?AV<lambda_a81fb453624cd459713d464cbd39f40b>@@
.?AV<lambda_5ef99cfa5845f4c4d490717f4ebe278d>@@
.?AV<lambda_b1c0cff63caf505f6536baee30943a62>@@
.?AV<lambda_37fc46271b5a9d577e78557058b76819>@@
.?AV<lambda_c4dc4b2967ca7204e8ba49b0607e0250>@@
.?AV<lambda_06e5eb766e97cbbd1e9836fc044820b5>@@
.?AV<lambda_ddff9a77cb22aafff303a88b4705bee2>@@
.?AV<lambda_00b59445cb71ad9abff9006cdec65ab0>@@
.?AV<lambda_2d779c3a3c8b726adf3efb1bfd3cbb40>@@
.?AV<lambda_d73f309e1e17ba4988eef7d15e33e2b5>@@
.?AV<lambda_556d5dc9dce2234c43f6705a2f6c7701>@@
.?AV<lambda_7fdfed5ff1e8b275291a3713701dc711>@@
.?AV<lambda_dfa4f11b86420105767b17dba3f3e58c>@@
.?AV<lambda_ea55a0337c6548d84265ecdc39ed5e30>@@
.?AV<lambda_999155f75226cbdc950d71bcbf55a961>@@
.?AV<lambda_64826d400df5e2683a863a4dbb954602>@@
.?AV<lambda_5bd441e42294bfab33849ec1b22ce125>@@
.?AV<lambda_4e978ba75a8a0f3f4d2d1e75d74ac4de>@@
.?AV<lambda_c5677617c2b0074bdccd3f6596388b76>@@
.?AV<lambda_87df0aeb452bc224372e0c71d6a5bb70>@@
.?AV<lambda_d430b12f7c973c17dbb8b927fbda5d7d>@@
.?AV<lambda_446c3abfba59357df64608aa0ee4e290>@@
.P6AXAAUDataPropagationContext@onnx@@@Z
.?AVtype_info@@
.?AVerror_category@std@@
.?AV_Generic_error_category@std@@
.?AVstl_critical_section_win7@details@Concurrency@@
.?AVstl_critical_section_interface@details@Concurrency@@
.?AV_Locimp@locale@std@@
.?AV_Facet_base@std@@
.?AU_Crt_new_delete@std@@
.?AVfacet@locale@std@@
.?AV?$codecvt@DDU_Mbstatet@@@std@@
.?AV?$ctype@D@std@@
.?AVcodecvt_base@std@@
.?AV?$numpunct@D@std@@
.?AUctype_base@std@@
.?AV?$num_get@DV?$istreambuf_iterator@DU?$char_traits@D@std@@@std@@@std@@
.?AV?$num_put@DV?$ostreambuf_iterator@DU?$char_traits@D@std@@@std@@@std@@
.?AV?$moneypunct@D$0A@@std@@
.?AV?$collate@D@std@@
.?AV?$money_put@DV?$ostreambuf_iterator@DU?$char_traits@D@std@@@std@@@std@@
.?AV?$time_put@DV?$ostreambuf_iterator@DU?$char_traits@D@std@@@std@@@std@@
.?AV?$messages@D@std@@
.?AUmessages_base@std@@
.?AUtime_base@std@@
.?AV?$moneypunct@D$00@std@@
.?AV?$money_get@DV?$istreambuf_iterator@DU?$char_traits@D@std@@@std@@@std@@
.?AV?$_Mpunct@D@std@@
.?AUmoney_base@std@@
.?AV?$time_get@DV?$istreambuf_iterator@DU?$char_traits@D@std@@@std@@@std@@
.?AV?$messages@G@std@@
.?AV?$moneypunct@_W$00@std@@
.?AV?$ctype@G@std@@
.?AV?$num_put@_WV?$ostreambuf_iterator@_WU?$char_traits@_W@std@@@std@@@std@@
.?AV?$messages@_W@std@@
.?AV?$time_get@GV?$istreambuf_iterator@GU?$char_traits@G@std@@@std@@@std@@
.?AV?$time_put@_WV?$ostreambuf_iterator@_WU?$char_traits@_W@std@@@std@@@std@@
.?AV?$moneypunct@G$0A@@std@@
.?AV?$collate@G@std@@
.?AV?$num_get@GV?$istreambuf_iterator@GU?$char_traits@G@std@@@std@@@std@@
.?AV?$codecvt@GDU_Mbstatet@@@std@@
.?AV?$moneypunct@G$00@std@@
.?AV?$numpunct@_W@std@@
.?AV?$num_put@GV?$ostreambuf_iterator@GU?$char_traits@G@std@@@std@@@std@@
.?AV?$_Mpunct@G@std@@
.?AV?$num_get@_WV?$istreambuf_iterator@_WU?$char_traits@_W@std@@@std@@@std@@
.?AV?$money_get@GV?$istreambuf_iterator@GU?$char_traits@G@std@@@std@@@std@@
.?AV?$money_put@GV?$ostreambuf_iterator@GU?$char_traits@G@std@@@std@@@std@@
.?AV?$time_get@_WV?$istreambuf_iterator@_WU?$char_traits@_W@std@@@std@@@std@@
.?AV?$time_put@GV?$ostreambuf_iterator@GU?$char_traits@G@std@@@std@@@std@@
.?AV?$money_put@_WV?$ostreambuf_iterator@_WU?$char_traits@_W@std@@@std@@@std@@
.?AV?$codecvt@_WDU_Mbstatet@@@std@@
.?AV?$numpunct@G@std@@
.?AV?$money_get@_WV?$istreambuf_iterator@_WU?$char_traits@_W@std@@@std@@@std@@
.?AV?$_Mpunct@_W@std@@
.?AV?$collate@_W@std@@
.?AV?$ctype@_W@std@@
.?AV?$moneypunct@_W$0A@@std@@
.?AVios_base@std@@
.?AV?$basic_filebuf@DU?$char_traits@D@std@@@std@@
.?AV?$basic_streambuf@DU?$char_traits@D@std@@@std@@
.?AV?$_Iosb@H@std@@
.?AV_Iostream_error_category2@std@@
.?AV?$basic_ios@DU?$char_traits@D@std@@@std@@
.?AV?$basic_ostream@DU?$char_traits@D@std@@@std@@
VS_VERSION_INFO
StringFileInfo
040904E4
CompanyName
Microsoft Corporation
FileDescription
ONNX Runtime
FileVersion
1.10.220126-2359.1.dml-1.8.89dd732
InternalName
ONNX Runtime
LegalCopyright
 Microsoft Corporation. All rights reserved.
OriginalFilename
onnxruntime.dll
ProductName
Microsoft
 Windows
 Operating System
ProductVersion
1.10.220126-2359.1.dml-1.8.89dd732
VarFileInfo
Translation
<?xml version='1.0' encoding='UTF-8' standalone='yes'?>
<assembly xmlns='urn:schemas-microsoft-com:asm.v1' manifestVersion='1.0'>
  <trustInfo xmlns="urn:schemas-microsoft-com:asm.v3">
    <security>
      <requestedPrivileges>
        <requestedExecutionLevel level='asInvoker' uiAccess='false' />
      </requestedPrivileges>
    </security>
  </trustInfo>
</assembly>
1#1H1
2&2.2M2
6'7A7K7a7
8!8A8a8
9!9A9b9f9l9p9v9z9
:+:9:G:S:Y:b:h:n:t:
;1;S;l;
<,<I<j<
=>=`=
>@>i>
?+???S?c?w?
0'0@0Y0r0
1&1C1U1g1
2"2F2X2j2|2
454@4K4V4l4w4
5*5@5K5]5h5z5
5$696b6
7!737E7W7b7x7
8L8S8`8p8
9(9B9H9b9h9
:":(:B:H:b:h:
;";(;B;H;b;h;
<"<(<B<H<b<h<
="=(=B=H=b=h=
>">(>B>H>b>h>
?2?8?R?X?r?x?
02080R0X0r0x0
12181R1X1v1
6"7/7E7k7
8"8(8B8H8b8h8
9"9(9B9H9b9h9
:":(:B:H:b:h:
;";(;B;H;b;h;
<"<(<B<H<b<h<
===W=\=|=
>L>b>j>r>
>"?(?I?n?
060Y0
181e1
2*2C2H2R2q2
313Q3q3
414Q4q4
515Q5q5
61696?6D6M6a6k6
7-7S7}7
72888T8
:>:~:9;R;
;0<U<m<
=-=S=`=
>N>|>
@0Q0Y0f0q0}0
:&;q;
<5<{<
1$2Q2v2
323^3~3
5W5~5
656x6
9R9~9
: ;r;
<.<P<r<
?4?B?~?
0;1E1
262`2
3!4m4y5
636A6
6-7a7
:!:':r:
< =/=k=
>S>w>
>>?b?
20A0U0u0
0#1I1k1
2C2R2
223f3
566J6}6
737e7y7
<F>A?
1=1W1:2l2
2)3C3V3l3
6m7%8
0$1?1
6>8k8
8-9E9
4Y5[:|;w<
<F=y=
= >9>Z>
2R2d2
2b3t3
7b8s8|8
:F<z<
3)3s3
5<6b6
6<7_7
7E8U8.9
9v:+;k;
2$2*2u2
4 5*575t5
627E8
9@9g9v9
4D5S5b5s5{5
6"666J6\6
8&9<9I9V9
97:Z:}:
;2;b;
<)<N<
=#=Y=k=v=
> >)>A>I>P>Y>o>|>
?4?:?]?w?
1!1+1B1Y1d1m1r1
:2:=:]:
:/;O;
;0<]<q<
2C3S4C5
5!7a7
7+8u8z8
;e;1<
>4>U>o>t>
2W2\2g2
9-969<9
9!;R;W<d<y<
=+>N>
3R3f3
5"5C5N5|5
5%6c6z6
6Z7f7
222A2Y2
2+393Y3
6"6O6
6S738C9
011M1h1
7,9f9
:G;};
;:<|<
0j1q1
2-2>2G2M2
3y4i5
6'7_7
:=;J;v;
;(<J<j<
=2===k=
=$>1>]>~>
?1?Q?s?~?
0$0R0j0u0
0#191&222U2
3!4S4
;s<t=
>!>s>
>#?j?o?w?
0,1s1
4(5g5
8L9r9
9L:o:
:W;g;3<O=?>3?
4S5@6
=H>3?
40R0a0y0
0K1Y1y1
7$8c8
;L<r<
<L=o=
=W>g>1?q?
:Z;i;#<
2?3T3
3>4Z4u4
6t8/9
:1:`:
3.3@3e4
=X>b>h>
2 3^3
;!<B<
3.5v5
=9>r>
>"?'?2?
5<6R6
8O8T8_8
;\<>=
A0R0[0a0
1P1a1
2-2>2G2M2
2<3M3u3
4*43494
4(595a5u5
5'6L6b6
8!9;9
:-:P:
1+1H1
373R3Y3n3
434x4
4Y6c6
6X7l7
7B8V8
859I9y9
9+:?:o:
;6;J;h;z;
=;>O>
>X?g?
0"0R0f0
021F1v1
323F3W3
3J4i4s4
5*5&6/6
7#7,7
8(9f9z9
9%:H:Z:
;:<N<
='=1=V=x=
?Q?h?
0O0c0r0z0
2=2h2z2
3%3U3i3x3
4J4^4t4
5.5B5h5|5
5!656e6y6
667J7p7
8#8I8]8x8
8$9?9f9
:8:D:[:
183G3
494M4h4w4
5)5=5m5
7V7w7
8@9W9
; <4<H<W<
=1=x=
>G>[>x>
?L?`?x?
000`0t0
171e1
292t2
3(3[3o3
464a4
5K5_5(676{6
737H7W7
8?8S8
9C9W9}9
:>:R:x:
:!;5;[;o;
<H<W<
=(=7=
>(>7>
?!?Q?e?
0(0<0l0
1L1`1z1
2H2T2k2
2$383R3f3
4H4T4k4
515x5
5"666X6g6
6#777E7
7>8R8
8!959e9y9
:E:Y:x:
:%;9;X;g;
<8<G<
=:=N=h=w=
=J>^>
>(?:?[?x?
080L0r0
121F1h1z1
2(2:2_2u2
373K3q3
3E4_4
5(5c5
7@7x8
8%999O9c9i9
:*;k;u;
<m=t={=
>&><>P>V>
?4?L?V?k?
050I0O0g0q0
121H1q1
2@2T2Z2r2|2
3$3e3
4$4X4f4
5H5W5
6,7@7X7j7
8(848
969J9h9z9
:<:_:
:e;|;
<(=7=
=+>?>X>g>
?/?_?s?
080D0
011;1a1q1
1H2B3R3
576b6v6
6$7F8
:&:|:
<F=i=
D0<1F1[1p1
1!2D2
4*5&606E6Z6w6
;P<5=?=T=i=
1)2i2
333H3e3
3P4Z4o4
969@9U9i9
:";k;
<9<C<X<l<
=!=>=
=8>m>
?+?5?J?_?
3O5[6
9$999N9k9
9E:L:a:v:
;';m;t;
<2<O<
<)=0=E=Z=w=
>Q>X>m>
0(0=0R0o0
2%2:2O2q2
4%4:4O4q4o5
7a7k7
9 9=9
:h:r:
<0<[=\>
1U1z1
2!2:2
3N5[5
8Q8[8p8
909m9w9
9P:k:
2C3^3/5_5)?7?
3c485
5h6z6
7,7@7
808U8
9}:(;7;
;H<V<
2"2=2^3
6h7w7x8
8h9w9&:
=)=;=\=w=
>,>:>T>_>
0!141
142z2
3=4p4
8^9c9
> ?Z?
2\3o3
4t4$5"7
3*4A4j4
4B5`5
0+1F1
1L3)5
607?7
9!:I;
<1<N<
<'=1=F=[=x=
>I>S>h>}>
?$?9?N?k?
222W2s2}2
313>3n3
4n4M6S:
2C2s2
3s425
536c6
6v779&:
0*4c7
738C8I8Q8f8r8
:(;7;
1S3C6
809p9
737=7Z7}7
8j8p8
9]9x9
:;:E:b:
; ;r;
=3=X=g=
>.>^>r>
?J?^?
0&0:0`0t0
1A1U1x1
1:2N2~2
3"3j3~3
4H4T4e4
558k8
<4=B=
>0?f?
D0k0r0
10171\1y1
1!2>2~2
3\3f3{3
4;4x4
5%5:5W5
5"6,6A6V6s6
7>7H7]7r7
8Z8d8y8
999v9
:#:8:U:
: ;*;?;T;q;
;<<F<[<p<
=X=b=w=
>7>t>~>
?!?6?S?
50?0T0i0>2
7=8O8$9
9,:D:V:
<H=$>
262J2`2t2z2
4'4q4
5/5H5V5r5
6+6@6N6[6H7V7r7
9$9n9
:&;.;L;S;h;};
0B0G0
6 787R7r7
:*:=:
:1;^;v;
<><}<
=L=|=
0^0h0
3"363
="=>=E=Z=o=
>)>0>E>Z>
>:?H?q?~?
0e1s1
!>0>Z>
?&?B?
4)4F4}4
4+5T5^5q5
6^7o7
>]?t?
171A1
1+2+353H3h3
4 494J4g4
5)5~5
=1=C=
0'0n0
0@2W2a2w2
3%3D3N3a3
424<4O4o4y4
4,5R5\5o5
686=6
6'7>7
9'9B9L9\9|9
:J;O;
=.=8=J=j=
=4><>$?L?
0J0^0u0
3?3w3
585Y5
9P:u:
;,;a;
;"<?<Z<d<w<
?9?D?
 0+060A0
0L1Q1
2"2K2V2
2E3h3
5P7U7
:4:h:}:
;4;>;P;p;
<1=6=J=
2#3-3?3_374
5P5Z5l5
657?7R7
818@8
8A9K9e9
9+:5:O:&;
=?=Q=s=
1Y2|2
2C3O3u3z3
4A4|4
4=5a5
6Y6\7f7x7
73888
80959
:$<Q<
<4=_=
?*?4?C?c?
0,060E0e0
161@1O1o1
2!2@2J2Y2y2
3/3N3X3h3
5#5H5R5d5
9Z9k9
:=:d:
1"171L1i1
212S2q2F3K3
4=4Y4n4
7y8 9
627N7
8=9\9
9$:I:V:Z:^:b:f:j:n:r:v:y<
8_9y9
;,<_<
=2>f>
869I9
:a:V;#<6<
=R=d=
>a?p?
3-4?4
6j7|798K8
0*1Q1
4c4q485E5
5(6:6>7L7q8
98;E;
<$>g>
0#0G0h0
111C1c1
2#2H2Z2z2
333f3
4?4f4
5 5{5
6#6.6E6R6f6z6
7.7?7P7a7
8 8-8A8U8i8}8
9(9P9i9u9
;6;B;
;/<L<a<w<
>'>F>
?4?g?~?
<B=T=g=|=
=F>Y>l>
405w5
50676e6y6
747Z7e7r7
8"898^8{8
919H9m9
:);6;F;K;P;
<)<U<u<
=!>D>e>
0%1O1q1
263@3]3
4;5_5
677A7
8*979G9V9i9
:(;C;
;'<B<
7*7A7Z7q7
818J8a8z8
9!9:9Q9j9
=)=@=
='>D>p>w>
? ?C?
0L0S0f0
1,1B1l1
4,4`4g4z4
5>5v5}5
6)6L6V6]6
607M7T7f7
9X9n9
=E>O>a>
?U?k?
0#1J152
4-4T4
465Z5d5
5j6!7
0 0$0
1\1c1
273P3T3X3\3p3t3
3'464
6,656>6G6P6
7%717=7I7U7a7m7y7
8,8084888<8@8D8H8L8P8T8X8\8`8d8h8l8p8t8x8|8
6(7c7}7
<D=`=
5K5r5
878G8 9
2+353Y3f3j3n3
5y8P9
94=F={=
>>>O>m>
?+?<?
0S0\0l0u0
0'1f1
2"2)2@2Y2g2}2T5
7K7Q7
7(8:8`8s8
9C9X9t9
;7;?;K;X;h;r;
>+>L>X>u>
>;?G?k?
0(0^0
0;1S1]1{1
2?2]2r2o3v3
4&4B4L4
6&7Q7a7
9;9D9S9
96:Q:s:
;+;:;Z;y;
< <f<
<%=9=X=j=
>(>:>n>
>8?G?
,0@0S0`0
181G1a1t1
2I2f3s3
354B4
5#5*5
8/8=8g8
9;9A9T9
8g8y8
9W9i9z9
5q5r6
7^8O9#:]:}:
:<;H;
;.<U<r<
='===N=c>
?l?}?
1m132P2{2
4=5c5y5
6-6Y6_6f6s6z6
8j:H;V;~;
;(<9<|<
=9=F=
*0H0Y0h0
2/2J2
4X4j4
485t5
777Y7x7
8X8j8
:[>n>s>
?3?T?
6<6_6k6
687B7T7
9C9d:
; ;:;a>
>!?@?~?
2%262x4
4$5B5[5|5
5(6<6f6~6
747j7q7y7
818C8w8
809n9
:I:b:
;3;@;];{;
<&<B<[<w<
=4=9=u=
>*?5?=?E?M?U?
151^1v1
5W6u6
9o<a=
6H7c7
8[9z9
: ;3;
;4<n>
2c2v2
4c5~5
>!>%>)>->1>5>R>
>A?Q?
171U1{1
2L2|2
6 6B6
;9;R;t;
>H>e>
?'?\?
U0_0o0
0&1a1w1
2.304
6%657l7
?S?n?
0>0e0
0$1f1
79?e?
0-1g1
9;:m:
;M;`;0<r<
=9>{>
0B1`1
3U4G5
=[=v=
=1>x?
5-5=5j5
6#636`6w6
7)7u8
8,9t9
;C;w;
4D4x4
8)9S9$:t:
;-;Z;~;
=G=t=
0!1i1
7%8O8
:*;g;
=#>`>
0<1y1
3;4~4
6-7}7
;A;u;
<1=^=
=9>v>
031v1
3T3~3%4u4
7)8V8
;*<T<$=t=
>O>v>
>0?f?
+0U0T2
233p3
4F4p4
5,6i6
8B8i8
:B:i:
<N<u<
=M=t=
>*?m?
060l0
4(5q5
6G6}6
6+8{8
929V9
;A;w;
<#=`=
>L>s>
0K0o0
2A2w2
3-4v4
5E5{5
7G7r7
9W9{9
;D;k;
=;=e=
0G0q0'1w1
2E2o2@3
3"4k4
5@5r5
9H9s9
9&;v;
<&=c=
121d2
393d3
6D6o6
708W8
9):r:
;1<X<
>%?n?
0$1a1
2G2i2
6K6r6
8>9{9
9#:m:
=:=g=
041q1
3&4c4
6 7i7
839y9
:,;o;
1/2l2
354x4
6'7q7
9@9j9
;1<n<
>!?E?o?
102m2
596v6
8S8z8
95:x:
;-;'<w<
=*=`=
>8>b>
061`1b2
223Y3
5>5e5
737]7
8*9Q9
;#<J<q<
===T>
>6?y?
0)1r1
2"3_3
3&5v5
6'7]7
92:{:
;1<t<
===s=
0,1|1
5$6N6
9'9]9
9&:P:
;#<f<
</=e=
0-0W0
3G3n3
474a4x5
626h6
829{9
: ;F<
<"=_=
>.>U>|>
041[1
2.2Y2
5A5k5Q6
6-7j7
9!:j:
?+?V?}?
3>3p3
7>7h7
8*9t9
:#;`;
=)>T>~>&?p?
0%1O1
1@2}2
3,4V4
757`7
728|8
;B;l;
<.=x=
0S0~0
2(2<2S2j2
3"393P3g3~3
464M4d4{4
535J5a5x5
606G6e6y6
7%7<7S7j7
8"898P8g8~8
969M9d9{9
:3:J:a:x:
;0;G;^;u;
<-<D<[<r<
=*=A=X=o=
>'>>>U>l>
?$?;?R?i?
0 0$0(0,0004080<0@0D0H0L0P0T0X0\0`0d0h0l0p0t0x0|0
1 1$1(1,1014181<1@1D1H1L1P1T1X1\1`1d1h1l1p1t1x1|1
2 2$2(2,2024282<2@2
2%3\3r3
3|4x5
5/6F6q6
6Q7k7
:o:y:}:
:8;q;v;
<#<'<+</<3<7<;<?<C<G<K<O<S<e<
=O>\>
>,?9?
1?2L2
3)3X3g3
7!788J8
9(9[9
<8<{<
=$=u=
=#>M>
0(0@0O0V0
4H4Y4
5(575
8N8m8
979p90:
>E>X>
4Y6s6
6*7E7
<B=_=
8 8t8
9*9?9H9O9
9!:<:B:O:V:]:o:
;";4;s;
<%<,<3<E<Z<c<j<q<x<
?3?E?X?
0&0-0
0.1Q1_1
1H2R2d2
8X8g8
9%:D:h:y:
:(;9;
<X=j=
>(>:>
?>?~?
1+1j1
4H4W4
646X6i6
9H9V9
93:@:a:
;9;h;y;
<7<h<y<
<.=7=j=w=
=H>V>y>
>f?s?
0K0i0v0
1(272<3I3
5$5U5t5
6(6X6i6
818X8j8
<5<><
= =.=Q=j=
3@3Z3
5W5}5
596r6
859`9s9
:=;h;{;
<:=e=x=
>7?b?u?
0G1p1
2+3X3~3
464v4
5#5*515d5
9v:z:~:
:D;H;L;P;T;X;\;`;
;*<q<
3"3&3*3.32363:3G3
97:M:
>F>k>
?/?B?
1B2l2x2
7j8{8
;-=G=
>#?A?\?
040g0z0
0-1k1
4L4j4
6=6F6
536X6
809R9
1 2Y2
3%3Y3u3
9#;|;
;G<c=
383[3
4/4v4
5T5v5
707=7O7_7
7H8W8
909C9P9|9
070n0{0(121D1c1u1
282c2u2
2*3H3f3F4#5
839X9
8S9w9
<s=#>
2c334
9c:6;f<C=
2A3r3
4+4B4
5:5C5[5g5
586T6k6
999F9X9c9
:);?<
0<1f1
3[4I5
5E6`6
;h;;<
3%3d3
344j4
7e7~7
061N1l2
575h5w5
5%626A6~6
7-747G7
8Q8l8
:S;R=
>*?a?
0$0A0
626S6
;)>9>p?{?
2B3d4A5
7!8(8}8
9,:G:
?3?]?
2o3<4Y4r4
:.:;:Y:
<1=T=e=
2)2p2}2
2=3J3}3
4P4_4
435E5X5
506v6
7$7^7e7}7
8#8N8p8
9(9:9
:/:P:i:
</=<=}>
?&?>?
K0\0{0
3?4L4
5R5Y5q5
5.6^6q6
7N7j7
828C8_8v8Q9X9
:*:B:Q:]:z:
=?=}=
><>_?
0g0{0
0#1F1W1s1
1c2r2
2"333O3k3
656X6i6
8+8B8h9
;:;];n;
<;=V=y=
?%?H?Y?u?
021r1
122r2
223r3
324r4
425r5
526r6
627r7
728r8
829r9
92:r:
:2;r;
;2<r<
<2=r=
=0>=>l>s>
0E1z1
2#2T2c2
3p3}3
4*4[4j4
5$616b6q6
7+888i8x8
9)9p9}9
9=:J:{:
:(;7;
;D<Q<
</=>=
=K>X>
?6?E?
1=1L1
1Y2f2
3D3S3
4`4m4
5K5Z5
6g6t6
7!7R7a7
8n8{8
9(9Y9h9
:";/;`;o;
<)=6=g=v=
>0?=?n?}?
071D1u1
1"212
2>3K3)414V4|4
575E5c5q5
506v6
829x9
;4<z<
>6?|?
0 1f1
182~2
5#5;5f5
6.6h6o6
7-7X7z7
7 8Z8a8y8
9J9l9
:L:S:k:
;<;^;
<><E<]<
=.=P=
=0>7>O>z>
> ?B?|?
"0)0A0l0
141n1u1
232^2
3&3`3g3
4%4P4r4
5R5Y5q5
6B6d6
7D7K7c7
848V8
869=9U9
9&:H:
:(;/;G;r;
<:<t<{<
=!=9=d=
>,>f>m>
?+?V?x?
0X0_0w0
1H1j1
2J2Q2i2
3:3\3
4<4C4[4
5,5P5n5
7!888S8v8
:Q:i:
>>>g>x>
252Q2h2
505L5c5z7
8+8G8^8u:
;7;N;
=#=:=
1`2~2
2L4j4
486V6o6
6$8B8[8~8
:.:G:j:{:
<3<V<g<
>B>S>o>
0.0?0[0r0
2(2D2[2v3
5*5M5^5z5
7,7G8`8
:/:K:b:};
=-=P=a=}=
B0[0~0
2*2F2]2x3
5,5O5`5|5
7.7I8b8
9 :1:M:d:
=3=V=g=
2'282T2k2
5Q6n6
;R;Y;q;G=_=
3M3j3p3
4=4L4
4Y5f5
</<Q<
=D=J=P=h=
2?3k3
6@7M7x7~7
8K8Z8
9g9t9
:!:R:a:
;n;{;
<(<Y<h<
=">/>`>o>
0-0G0h0
0-1s1
3/4Q4
5>5D5J5b5
6<6^6
7K7Q7W7o7
8I8k8
9X9^9d9|9
:+:V:x:
;';e;k;q;
< <8<q<
3A4N4
4,5;5
5H6U6
737B7
8)9o9
:/;5;;;S;~;
</<Q<
=B=H=N=f=
>B>d>
1!1J1d1
292K2U2l2|3
9B9Z9i9
:#;0;a;p;
<*=7=h=w=
>1?>?o?~?
081E1v1
1#222
2?3L3}3
3*494
4F5S5
8%9k9
<J<l<
=Y=_=e=}=
>,>W>y>
?(?f?l?r?
0!090d0
151s1y1
1"2(2.2F2q2
2 3B3
3/454;4S4~4
505R5
6?6E6K6c6
7=7_7
8L8R8X8p8
:,:5:S:
=(=7=
>8>r>y>
3+3G3^3
3/5m5
5-6H6W6
7d7q7
7)8K8
8+929J9z9
?(?7?
080r0y0
1A1e1o1
2#2Q2
3g8}8
;';B;Y;
;1<r<
<2=r=
>o>|>
?)?Z?i?
0#101b1
8E8K8Q8i8
9C9e9
:R:X:^:v:
;%;a;
<1<(=
=]>j>
?8?r?y?
080s0
1$2-2<2l2
4%4N4]4
405v5
566]6
:G;T;
;"<)<A<o<
= >->=>X>2?
<0P0d0}0
1=1Z1`1
1-2<2
2I3V3
5%525?5L5Y5f5|5
7P7V7\7t7
8#8N8p8
9.9?9[9r9
:-;H;Y;
<m<z<
<&=H=
=">f>m>
1R1Y1q1
1.2_2x2I3_3q3|3
7:7U7x7
<%<Q<
?@?L?]?
40W0O3s3
585>5^5m5
6'747 8f8
9N9U9m9
:>:`:
0=0m0
0'141e1t1
2.3;3
3"4h4
9I9O9U9m9
:G:i:
;V;\;b;z;
;!<k<
=(=9=U=l=*>1>X>
080G0
0&1H1
222I2
393u3
4-5H5W5
566X6
8M8h8w8
9V9x9
9R:w:
=$=C=
?P?]?
+080i0x0
1M2w2
4.4Y4{4
5*5h5n5t5
6)6@6
8-9H9Y9
:m:z:
:&;H;
<7<N<
>M>h>w>
?V?x?
141M1
1H2q2z2
4(474
585r5y5
6M9}9
;1;c;t;X?
0"1)1A1q1
122C2t2
:2;P;W;
=(=f=l=r=
0E0-1m1
202v2
4=4F4Z4
6R6Y6q6
<,<X<~<
=<=J=T=
>8>G>
>&?H?
1=1X1g1
2F2h2
2B3V3r3
6-7J7P7p7
:V:x:
=`=m=
=->:>
?B?I?a?
374D4p4
515\5
5H6m6
8(8b8i8
8&979S9j9S:
; ;@;O;
; <f<
<&=L=]=|=
0g0t0
1B1I1a1
6-7:7
8>9U9p9
:1:q:
;1;:;W;
=8=G=
=&>H>
3)3p3}3
3=4J4
6W6^6v6
6M<h<w<
=V=x=
=M>a>}>
0=1J1
3 383v3
4#4?4V4
8%8N8]8
809v9
94:X:
<G=T=
=">)>A>
0*090Z0
0e1r1
1)2Z2{2
373h3
4$5T5m5
7F7p7
819S9
:j:v:
>4>B>
3<3J3\3k3
7*7F7R7n7z7
718D8`8l8
829J9_9l9
9#:Y:6={=
>%?f?
5&5E5b5v5
6.6L6e6
6-767E7i7
8$8o8z8
2D2^2
98:<:@:D:
:<;^;
;k<3>
 0)0-0105090=0A0E0I0V0
0"1+1M1Z1}1
6 6P6f6
:$;8;X;g;
<,<D<U<f<p<*=g=
>(>9>C>
3`4q4
4F5W5h5r5,6=6N6X6
7#747>7
:1:t:
;4<a<
=!>d>
>&?g?
0.1o1
1)2O3x3
354)5R5
8|9J:h:
;i;w;
<D=X=
>)>Z>n>
>Q?e?
1A1U1h1t1
182D2
3$3x3
485E5
5H6U6
6X7j7
8(8:8g8
9'9Q9
>*>_>s>
?!?%?)?-?1?A?
0(171
2,2H2W2
3'3W3k3
3-4A4q4
4(575
939c9w9}9
:E;Y;
<W<k<
<.=B=h=|=
>1>E>h>z>
?8?J?
7;7O7
=@=z=
=#>`?o?
414?4Z4f4
4#575g5{5
5!676K6q6
7,7@7X7g7
838F8R8q8
9c:r:
>'>B>
?,?5?a?
)080R0^0c0u0
1\1d1~1
242<2V2^2
3.363k3
434A4X4f4}4
565D5T5d5
9=:}:
=H>\>|>
>P?d?
/0C0s0
001D1t1
2/2C2h2z2
3(3:3i3}3
505U5x5
6+6A6
6%7H7Z7
8S8g8
;J;};
<(<:<
1!2=2U2o2
4H5W5
8H8W8
=$>X>i>
0,1j1-2
4T5r5
7 8'8<8V8
9I:f:
;?;a;
?0?L?V?k?
(070h0
1R1[1
262J2h2w2
3?3S3
4/545q5
5Y6j6~6
:#:7:K:[:k:{:
:J< =
1-222
; ;5;J;v<'=
=P>h>
081G1X1v1
182G2X2v2
283G3X3v3
4X5g5x5
6X6g6x6
7L7\7
8&8W8
>%?E?
0]0s0
202U2e2
313A3U3i3}3
8 9m9
;6<w<
1l1(2H2R2w2N3
4(4?4[4
5%5I5W5n5
6!686T6x6
7B7P7g7
818M8q8
9;9I9`9|9
;"<A<s<
<#=I=h=
0O0T0
0&2l3q3
7 757J7[7N8
8R9\9%:/:T:n:
<4=m=r=
1A2J2
2F3c3
4+4@6N6o6
628G9
4J5e5
7.8{8
<0<7<
=<>T>[>
?)?:?
6V6f6
<$=?=
>+?5?H?f?p?
2+252H2k2y2~2
2(4-4Q4V4
4Z5d5y5
7z7;8D8
949N9i9
:Z:a:z:
;!;2;O;d<n<
>->B>a>w>
?!?5?I?
0J1V1s1
2#292
5-5>5R5
=#=1=c=s=z=
3U3Z3
3<4u4z4
6#6o6
797>7~7
8?8,939B9_9
93:::O:d:
:$;j;
;'<P<
1-1#2-2@2[2
2&3U3
3\4f4y4
9$979U9
:2:M:
;7;A;T;r;
<0=:=I=f=
=#>-><>Y>
?7?j?
2-2J2
3^3h3z3
3%4N4X4l4
4 5V5
9):3:K:a:
>C>M>e>{>=?G?_?u?
1?2Z2
4"4:4P4~5
7)7V7`7x7
868L8
:4;f;
<(<@<V<
>K>U>m>
1 2Y2c2{2
2?3I3a3w3
404E4f4p4
5m5w5
6*646L6b6
727p7
9)9[9
;*;L;V;n;
<n=x=
2+2 4*4B4[4
555N5
556j6
7,7k7
9=:W:
; ;8;Q;
=[=e=}=
2$2V2`2x2
5 585Q5
5+6o6
6R7\7t7
:(:::T:^:
;k;u;
;.=8=J=g=
>(>Q>[>m>
>/?9?K?k?
0G0Q0c0
1s2}2
3?3I3[3x3
676E6O6^6x6
7R7\7
;);E;
=2><>T>m>
1P2Z2r2
3!3T3^3v3
6d6n6
7%7;7|7
808R8\8t8
8a9k9
9%:c:
<V<`<x<
=D?T?h?|?
0E012;2S2l2w4v5
9&9`9j9|9
=0>5>d>n>
1&202C2^2
2(323E3`3
6.6z6
7H8R8e8
</<i<s<
<1=w=
=V>`>x>
>\?f?~?
0W1a1y1
3A4[4
8$8:8Q9[9s9
9M;W;o;
<(<><
:J=g=
1R4"7
787g9
;!;:;K;
;&=1=
1_2d2W3d3
3!4;4I4
787q7
7&8u8
9.:@:e:
1W2r2
3\4m5
8F9V9
<G=W=N>i>
>??f?
0A0V0>1}1
3'3R3
<%<a<o<;=
?V?]?
6a6r6
9$9B9
:o;v;
<=<D<W<u<
<,=W=
0]1a2
6N7t7
7Q8}8
:$;2;H;
424R4Y4r4
5>5E5^5
8E9O9~9
='=.=J=Q=j={=
>=>_?
1v2z3
3>4a4}4
4&5~5
8@8G8Y8u8
939{9
92:d:
<1<G<k<
0"080F0P0h0~0
0"1j1
4'5X5
8B9]9
>Q?[?s?
0&0<0v0
1R2\2t2
303X3
575A5Y5
= >O>
>A?s?
3(424E4`4w4
585B5U5p5
6N6X6k6
9(9C9Y9c9v9
;+<5<H<c<}<
="=5=P=
? ?3?N?
3a4f4y7
9*:K:
<#=5=D=[=
9 9:9F9e9q9
0C0_0i0
;:<O<
2`2q2
2)3X3}3
:,:x:
;W<p<
=1?T?{?
222F2
:i;p;
;%=,=E={>4?
0%0>0O0
1&243a3
8@9c9
000D0X0p1
1[2`2
:(;\;
(0f0s0
2V3[3
697v7
8/9P9
= >e>j?o?
/060K0`0
1/1F1
1 202t2
5\5p5
:):J:\:c;q;
:3:I;v;
>D?_?
0c1~1
0(1F1 2b2
3/3N3y3
7f8Y9
94;a;T<a<}<
=?>D>
0r0L1Y1e1
3-4i4=5
<J<f<s<
=/>f>
2(3<3R3
1\1t1
1'2D2l2
2i3p3
7<8H8t8\9r9
<W<\<
= =+=
0)1;1m2
4B5J5]5d5
6}8%9
;$;/;5;;;J;S;Y;_;h;q;{;
>M>b>g>
>$?;?G?z?
-0;0M0S0X0v0~0
5T5]5
6p7u7;8v8
;N<a<
0B1U1
4B4m4
5^6s7
8J9h9
:?:n:
:3;m;
=&=X=
>[>b>w>
>!?*?
0F0n0O1w1
5\6f6
697O7
7G8T8g8q8
929I9R9\9r9v9
;';:;K;
?6?v?
#0G0N0`0}0
7%8*8i8
9k9p9
9+:0:o:
;e;l;
=>>T>
1O1z1
2O2{2
424W4
4F5W5
5=6]6n6
8B8e8
>v>^?
2#3B3
494k4s4
53696Y6|6
7L7i7
788}8
9H9c9}9
9H:P:]:
;7;T;o;
?O?q?
1-2V2
3{4#5^5
8&939Z9g9
:4:A:
;);L;Y;};
=J=u=
000`0}0
5R6q6
6+787
7^8k8
9#9F9
9S:q:
:/;Y;t;
<e=r=
0N0_0
0U1b1
2C2_2;3Z3}3
647D7U7q7
:#;?;
;(<B<t<
>'>c>
?h?{?
0<0X0m0
0F1V1d1
1-2}2
3!4S4v4
647Y7C8
91:s:K;
<<<i<
<M=s=
3.4|4
5,6G6
9+9o9
:<:n:
?.?S?
1&1V1
1o2{2
434H4f4
5W6a6z6
7v8\9
=&>i>
>J?d?
0M0,2:2Y2m2w2
:6;?<
<.=3=w=
3h4,515y5
5h6x6
7K7~7
9]9r9
:*:8:~:
;R;W;
;V<c<r<z<
>i?p?
1+1w1
2(2G3
444Q4
5!565w586B6W6l6
9,969K9`9z9
7%717Y7
7/898E8Q8p8
:p;z;
;[<e<z<
<B=L=a=v=
4Q5j5
6S6r6y6
6.8c9{9
:C:[:e:q:
:8;?;X;
<=<H<v<}<
<+=2=[=f=
>)>4>
>/?@?J?Q?u?
196C6X6m6
=#=*=?=T=n=
>)>5>]>
?!?S?]?i?u?
0!0S0]0i0u0
6%666
;*<A<
<)=A=H=]=r=
=3>p>|>
0P0\0h0v0
0T1o1v1
1M3T3m3w3
434l4q4
5S6q6
637s7
8S8q8
839Q9e9y9
:N;X;q;
<2<C<c<
='=5=q=
>b>l>
3.3g3
7k7u7
8a9}9
;e;&>+>]>b>
1n1s1
2:4?4
6?7M7[7
;F<X<
3)3Q3
3!4s4
425W6^6s6
8'8F8g8
9%:*:
;1<8<
6@6N6
9B;T;.<@<
=<>P>\>h>t>
?$?C?
0#1I1T1s1
1C2g2
2#3G3o3{3
4i4n4
7%7F7S7
;=;T;l;s;
<'<;<c<o<}<
<#=A=U=a=m=y=
>3>W>k>
?9?D?\?
0(1-1
4S6q6
6#7c7
738Q8e8q8}8
;+<X<c<
=?=_=
0-0S0
1-1S1
152I2U2a2r2
4*5r5
7_;'=
T3r3y3
314}4
5N5f5m5
7V8h8
9_:l:
9{:`<
\0%3~3b4E6
<+<E<
=2=<=Q=f=
4K5U5n5
7(7h738=8V8
92:<:Q:f:
>%>M>
?)?8?X?
0S0q0}0
1!1-191H1h1
313=3I3U3t3
4$4s4
435Y5d5
5C6i6t6
6S7y7
:1:=:I:X:x:
;(;s;
;#<A<M<Y<e<
=3>Q>]>i>u>
?$?C?a?m?y?
0)040S0q0}0
191D1c1
1#2I2T2s2
2#3A3M3Y3h3
435Q5]5i5u5
6%6D6
6C7a7m7y7
8)858]8
9!9-999E9m9
:1:=:I:U:}:
;#;A;M;Y;e;
<3<Q<]<i<x<
=(=H=
=C>a>m>y>
?)?5?T?
0S0q0}0
1!1-191E1d1
=1===I=U=t=
>$>s>
>#?A?M?Y?e?
0%0D0
0C1a1m1y1
2)252T2
8S8q8}8
9!9-999E9d9
;1;=;I;U;};
<1<=<I<U<t<
>$>s>
>3?s?
0#1A1M1Y1e1
435o5y5
696D6c6
9%919P9
<$<s<
<#=A=M=Y=e=
=#>A>M>Y>e>
>#?A?M?Y?e?
0%0D0
0S1y1
536Q6]6i6u6
7$7C7
7#8I8T8s8
93:Q:]:i:u:
;$;C;
;#<I<T<s<
=3>Q>e>q>}>
?)?4?S?q?
#0I0T0s0
0C1a1u1
212E2Q2]2i2
333Q3e3q3}3
4)444S4q4
515E5Q5]5i5
6)6H6
7S7q7
7#8I8T8s8
8C9i9t9
:3;Q;e;q;};
<)<4<S<q<
<#=I=T=s=
=3>Q>e>q>}>
?%?1?=?I?h?
1S1y1
1!2s2
4!4-494a4
5!555A5M5Y5
6#6A6U6a6m6y6
7%717=7I7q7
93:Q:e:q:}:
;%;1;=;I;h;
<(<s<
<3=Q=e=q=}=
>%>1>=>I>h>
?!?5?A?M?Y?x?
010E0Q0]0i0
1#1A1U1a1m1y1
232Q2e2q2}2
3$3C3a3u3
4)444S4q4
595D5c5
5#6I6T6s6
637Y7d7
7C8i8t8
8C9a9m9y9
93:Q:]:i:u:
:#;A;M;Y;e;
<1<=<I<U<}<
=#=A=M=Y=e=
>)>H>
?!?-?9?X?
0%010=0I0h0
1!151A1M1Y1x1
4*444M4T4p4z4
5$5)5f5[6z6
7`7m7t7y7\8q8
;#;+;3;i;1<
1&101I1
2*272S2]2v2
696D6c6
797D7c7
898D8c8
999D9c9
:9:D:c:
;9;D;c;
<9<D<c<
=9=D=c=
>9>D>c>
?9?D?c?
090D0c0
191D1l1
6$6n6
7'7<7Q7n7
9%9S9q9}9
:9:D:c:
<3<=<R<g<
1"151Q1[1p1
434b4
566s7
1<1F1Y1i1
2!2.2{2
4L4V4i4y4
5!515>5
657l7v7
8&8A8Q8^8
9+:2:D:Y:v:
;N;|;
<!<6<
<;=B=T=i=
?1?F?
0K0R0d0y0
1$1n1
1?2Q2f2
3)3k3r3
3;4D4
4O5a5v5
6)696{6
6K7T7
7?8Q8f8
9)9k9r9
9;:D:
:O;a;v;
<)<9<{<
<K=T=
=?>Q>f>
?)?k?r?
0O1a1v1
2)292{2
2K3T3
4!464
4;5B5T5i5
717F7
8K8R8d8y8
9$9n9
9/:A:V:
;[;b;t;
;+<4<~<
=+>2>D>Y>v>
161Q1a1n1
3<3F3Y3i3
4!4.4{4
4?5Q5f5
6)6k6r6
6;7D7
7/8A8V8
9[9b9t9
9+:4:~:
:?;Q;f;
<)<k<r<
<;=D=
>!>6>
>;?B?T?i?
2)2F2a2q2~2
5$595V5q5
5.6\6
8"848I8f8
8>9o9
98:B:O:]:o:
;);3;G;`;m;
;3<A<O<z<
<L=a=s={=
818C8K8S8<:Q:c:k:s:,<A<S<[<c<l>
7#7+737<9Q9c9k9s9
=5>T>^>w>~>
191N1U1\1o1
1<2Q2c2k2s2
3\5q5
9L:a:s:{:
<#<|=
=\?q?
1<2Q2c2k2s2
414C4K4S4
9!939;9C9
:<<Q<c<k<s<
>1>C>K>S>
<1Q1c1k1s1
505z5
616F6N6V6e7
8)8O8Y8r8
9>9O9
:<:Q:f:n:v:
=8=I={=
>Y>c>
?&?.?6?
2!2:2K2
6a798
<2=d=
2+252<2C2N2U2_2r2
2 303=3D3Q3X3e3u3
3K4]4
9#9m;{;
0'0/0
6C7w8
:D;R;_;g;o;
0,090A0I0
171Q2_2l2t2|2\3
7g9u9
?%?-?T?\?d?n?v?~?
0)01090F0N0V0s0{0
1$1,141y1
2!2l2
3+313D3a3{3
4!444Q4k4q4
5$5A5[5a5t5
616K6Q6d6
7!7;7A7T7q7
8+818D8a8{8
9!949Q9k9q9
:$:A:[:a:t:
;><E<L<T<
=F=M=T=\=
>D>K>R>Z>
?L?S?Z?b?
181?1F1Y1u1
:J:q:
;1;L;
617r8
223{3
2<3_5
8(8F8V8^8n8
1494c4
435s5
323F3Z3n3
7\7c7|7
7E8]8d8}8
96:Q:b:
;!;1;E;U;i;y;
<7<A<Q<b<
<#=0>
2>2p2~2
3'303:3D3
5S5`5
6C6P6
878F8g8w8
9"9@9H9I:k:
<7=Z=q=
>d?p?
0:0}0
3#3-3
4T4r4y4
5'5[5
647H7T7`7l7
738t8
<U>_>
0~0d1
3'454Y4
6$7A7H7a7
7(8j8
;(;4;@;L;k;
;O<Y<~<
L0Q0@6
7J7o7$8x8
9'9<9Q9n9
3#5W5
?@?J?c?
0#040Q0
101E1b1
2!3H3u3
324N4
636i6p6
6,737H7]7j7
8V8>9Z9a9v9
9/:G:N:c:x:
>I?q?
0!1(1=1R1s1
4K4R4
5*5M5
?B?N?
1#1s1
3&3L3
4(464\4
4 5,585I5g5
7#8`8l8x8
9*979c9
93:_:j:w:
;#;s;
=&=L=
>(>6>S>
?#?s?
C0o0z0
102<2H2V2|2
3@3L3X3f3
4P4\4h4v4
4#5`5l5x5
506<6H6V6s6
7&7C7
9`9j9
:::M:Y:
=2=B=
>E>N>
?!?6?
;0B0T0i0
0E1|1
232L2W2f2
2?3Q3f3
4)4k4r4
5E6|6
8,969I9Y9
;K;R;d;y;
?.?I?\?h?
051l1v1
253l3v3
4E5|5
636L6W6f6
6/7A7V7
8[8b8t8
:$;+;=;R;o;
;8<_<2=f=
>2>p>w>
>4?=?
2"2?2Z2m2y2
3/3A3_3q3
5.5^5~5
6"6]6
6,717i7
8)868C8T8i8s8}8
9 9*949>9X9t9
:?:j:o:
<I<l<
=3=V=
=">O>z>
?#?j?
1,1`1e1
3 4%4]4
6!646b6
7+717D7r7
818K8Q8d8
919K9Q9d9
:!:;:A:T:q:
;+;1;D;r;
=%=8=^=e=x=
=9>+?
S0Z0{0
829L;m;u;
<*<b<
>/>K>
4!4-494a4
5A5Q5]5i5
5#6A6q6
7S7q7
7C8a8
839Q9
9#:A:q:
:.;I;^;d;
5\5;<D<
5Q5k5
4b6A7m8
2)2B293C3\3m3
4p4z4
5J6T6m6
959,:6:O:`:c<n<|<
?P?\?h?v?
#0`0l0x0
001<1H1V1s1
334p4|4
5@5L5X5f5
6(666S6
7#8A8M8Y8e8
9S9z9
6g7l7
>3?Q?]?i?u?
30Q0]0i0u0
1)151T1
2!3}3
555?5U5
6M7{7
9 969N9g9
:9:H:a:v:
;2;k;r;y;
=,=F=U=n=
>+>E?
0*141I1^1z1
2#2@2
3#4\4s4
4#5\5s5
5#6\6s6
637Q7]7i7u7
8)858]8
:1:=:I:U:t:
<-<V<a<x<
=C>r>
0#1A1M1Y1e1
2C3r3
5#6A6M6Y6e6
7C8r8
:#;A;M;Y;e;
<C=r=
#0A0M0Y0e0
1C2r2
2S3q3}3
4!4-494E4m4
515=5I5U5}5
6#6A6M6Y6e6
737Q7]7i7u7
7"8C8a8m8y8
9)989X9
:S:q:}:
;!;-;9;H;h;
=1===I=X=x=
>(>s>
?(?s?
0$0s0
0#1A1M1Y1e1
233Q3]3i3u3
4%4M4
5)555]5
6!6-696E6m6
738Q8]8i8u8
9%9M9
:):5:]:
;#<R<s<
<3=b=
=C>r>
0S0q0}0
1!1-191H1h1
313=3I3X3x3
4(4s4
4#5A5M5Y5h5
6#6A6M6Y6h6
7#7A7M7Y7e7
839Q9]9i9u9
:%:D:
:C;a;m;y;
<2<S<q<}<
=B=c=
=#>R>s>
>3?Q?]?i?u?
0)050]0
212=2I2U2t2
5%5M5
6)656]6
7!7-797E7m7
8C9a9m9y9
:!:-:9:E:d:
<1<=<I<U<t<
?%?M?
0)050]0
132Q2]2i2u2
3%3D3
3C4a4m4y4
525S5q5}5
6B6c6
6#7A7M7Y7e7
8%8M8
9)959T9
:S:q:}:
;!;-;9;E;m;
<1<=<I<U<}<
=#=A=M=Y=e=
>%>M>
111=1I1U1t1
2d2n2
3!323O3Y3u3
4.4C4V4r4|4
5,5A5T5p5z5
6!6+6@6U6h6
6v7P=l=
3 353H3d3n3
3c4R6
868P8{8
819}9
:':^:
7 9d9i9n9
5[6c7
0V0[0`0
3O4:5~5
6-72777
8"9'9,9
< >d>i>n>
3K3P3U3
74898>8
9[::;~;
="='=x>
0a0f0k0
4B5G5L5
6:8~8
;M;R;W;
0J0O0T0
172<2A2
7^7c7h7
9K9P9U9
<2>v>{>
0_0d0i0
:\<e<r=
7w8(<
2A3a3
4A5a5
7a8|8
93:#;
3C435
5#7o7
899s:
5S6C7
:);C<
1#3o3
495S6
051Y1
172[2
3!3j3
9!:A:
0A1a1
7A8a8
9!:A:
>1?Q?
0$0q0
1#2D2
3C4d4
5Z5q5
616z6
6:7Q7
8Z8q8
919z9
9::Q:
;Z;q;
<1<z<
<:=Q=
>Z>q>
?1?z?
1Z1q1
212z2
2:3Q3
181y1
1=2^2
3"3c3
3'4H4
6C7Z78<
2M2e2l2
2D3\3c3
:7;i;
>:>f>
?7?A?Z?
0&1>1H1a1
1Y2q2{2
2L3g3q3
31=L=V=o=
='>B>L>e>
R8m8w8
8H9c9m9
8b:x:
:(;4;@;L;k;
<!<5<A<M<\<|<
1L1k1u1
172[2i3
=!=z=
=3>Q>]>i>u>
?$?C?a?m?y?
0)040S0q0}0
191D1c1
1#2I2T2s2
233Y3d3
334Q4]4i4u4
5%5D5
5C6a6m6y6
6d8|8
9#949
9T;l;s;
<$<F<
=8>T>[>t>
02090R0
2"3>3E3^3o3
414E4Q4]4i4
535Q5e5q5}5
6)646S6q6
717E7Q7]7i7
:%:1:Y:
;?;I;U;a;
=R=w>
4b4t4
9!9G9
:+:7:C:Q:n:
:A;Y;`;u;
;,<w<
>P?n?x?
0G1^1i1t1
3W3i3a4n4
4m;z;
;C<l>
:+;J;Q;f;{;
>'>=>
2#2'2+2/23272;2?2C2G2K2O2S2W2[2_2c2g2k2o2s2w2{2
6#6K6
6$7[7g7s7
748k8w8
=q>v>,?P?
6I6{6
8W9Y:
0]0u0|0
1!1-191a1
2A2Q2]2i2
<b<l<
2W3a3
8$9.9S9p9
>_?i?
4Y5k5W6i6q7
=C>h>
>H?U?
0_1x1
3 3,383D3l3
8#:A:K:N<X<
='=\=v=
0*0@0Z0v0
2'2C2g2
2#3c3
3C4g4
616*7
?3?M?
040@0L0t0
1O1t1
2O2t2
5f5U6\6
7#9>9H9]9r9
<2<O<I=N=
2Q4l4v4
7*747I7^7
<u=.>r>
>U?m?t?
0.3@3R3
9+9S9
93:U:q:
;$;D;
;/<_<i<u<
1<2F2o2z2
3B3M3
57687=7I8N8
718;8P8e8
0V0u0|0
032Q2]2i2u2
233Q3]3i3u3
4)454T4
4%565
7"7/:<:k:|:
3<4B5^5h5}5
9!929
93;=;R;g;
?b?x?
6&6<6
6d8n8
8K:U:j:
:I<h<~<
<:=R=Y=n=
?1?F?c?
1'1D1
3&3p3
4)4F4
5j7t7
7!8q8
9!9q9
:Q:q:
:1;Q;
=1=E=Y=e=q=
=3>Q>e>y>
?S?q?
233Q3e3y3
4!454I4U4a4m4
536I6X6h6
=T>r>y>
6 656J6Z6e6
6&707E7Z7
818F8V8a8
8"9,9A9V9s9
9+:<:v:
:"<S=q=}=
>S>q>}>
?-?@?
0E0l0
4N495>5
81989M9b9
:.;I;Z;`;
<*<4<
<B=I=b=s=
>2>C>
?<?R?c?i?
1,1S1
1#2c2
233p3|3
4@4L4X4f4
6!626R6Y6r6
7)757T7
8%8D8
:C;a;m;y;
<!<5<A<M<Y<x<
=!=5=A=M=Y=x=
>!>5>A>M>Y>
?)?Q?
0/0@0F0
111B1H1
1+2S2
4@6\6c6x6
: :G:
=!=-=9=a=
#0I0T0s0
1N2X2
<<=C=\=f=
>l>s>
>>?q?
132Q2]2i2u2
233Q3]3i3u3
334Q4]4i4u4
5)555]5
637Q7]7i7u7
8%8D8
=3>@>
5$5=5
<+=^=
0272`2k2
3&414i4p4
5B5L5u5
6/696b6m6
6"7-7H7O7x7
7C:a:m:y:
;C;a;m;y;
<)<4<S<q<}<
=1===I=U=}=
> >H>
?(?4?@?h?
010=0I0U0t0
1$1s1
132\2h2t2
:r;w;
1$151S1
2^2h2t2
3^3h3t3
4U5\5q5
7P8o8v8
9:9A9W9h9
<9<@<V<g<
>G>f>m>
?4?;?T?e?
5F6P6h6
7(7e7m7
7@8X8b8w8
839v9
:&;J;V;d;
6E7l8
3,353k3
4(4/4H4
5+555D5
6=6D6]6
3)353]3
4!4-494E4d4
6$6s6
637Y7d7
738Q8]8i8u8
9$9C9a9m9y9
:):5:]:
;!;-;9;E;d;
=$=s=
=3>Y>d>
>3?Q?]?i?u?
0$0C0a0m0y0
1)151]1
2!2-292E2d2
4$4s4
435Y5d5
536Q6]6i6u6
7$7C7a7m7y7
5B9@:v>
K0w4u5
6.;,<6=
D3B4V5+7W;U<f==?
o3m4w5a7
7&8A8
9!9q9
:Q:q:
;!;F;a;
<!<F<a<
=!=F=a=
>!>F>a>
233s3
4?5I5_5i5
8(8>8H8^8h8o9y9
; ;*;@;J;
;!?|?
0!0K0z0
1J1a1z1
3#3?3[3b3w3
6N7U7n7
9+9H9O9h9
;3;P;K<R<g<|<
>'>L>
0(1>1X1
1%2,2S2^2
4C5a5m5y5
6C6a6m6y6
7)757T7
:1:=:I:U:t:
<3=Q=]=i=u=
>%>D>
?5?=?N?w?
3C6w6
:&;a;
<P<Z<
2&2M2X2
>">K>V>
4:5}5:6n6
9,:o:,;`;
001J1
9):A:
;9<Q<
=I>a>
212E2Y2e2q2}2
233Q3e3y3
4!454I4U4a4m4
4#5A5U5i5u5
6%696E6Q6]6
717E7Y7e7q7}7
8)858A8M8u8
9!959I9U9a9m9
:%:1:=:e:
;%;9;E;Q;];|;
<!<-<U<
=)=5=A=M=l=
?%?1?=?\?
1!1-1L1
1%2s2
5,5s5
:-:3:
;*;C;I;
;*<@<Y<_<
<@=V=o=u=
>V>l>
>C?H?
2@3E3f5
;C<H<
=0B0f2
8S9X9
<V=[=v?
3Q4#5
9Y:g:
;a;k;
=k=u=
>->u>
?s?}?
1!1-191a1
212E2Q2]2i2
3&3C3a3u3
4.4C4c4
4#5N5c5
5C6m6
7(7s7
8!8-898X8
919E9Q9]9i9
2"3-383
92;D>
0*4p8a:
:$<6<b=
=,?Z?
0*1s2
4C4q4
5!6,7Z7
9/9h9s:
:H<v<
1/1_1
3=3H4v4
4!6f6
7F7a7
8!8f8
8&9A9
:F:a:
;!;f;
;&<A<
=F=a=
>!>f>
1{375
8k:'<
=3=K=T=
=/>9>E>Q>p>
4,565K5`5}5
6"6x6
=C>i>t>
06041
4Z4t4
:S:q:
;9;D;
;%<5<A<M<l<
=c=j=
?!?-?9?E?d?
0;1W1a1
212A2M2Y2x2
3q3v3c4
8)8H8
;%;1;=;I;h;
=#=X=t=~=
>A>Q>]>i>
>/?E?[?f?
1p3{3
334=4b4
4)535X5u5
8%8>8
<%</<H<Y<v<
>D>\>f>
2N2f2p2
2W4\4
4X8]8
1)1B1
5)535L5]5
7#8A8M8Y8e8
969O:n:u:
:(;f;
:6<@<U<j<
0l1q1x3}3
4M8c8y8Y9m9
9G:Y:-;
>*?5?@?K?
2Z2_2
2F3.434
4^6c6|7
?$?p?
3'3.3G3Y3q3x3
334Q4e4q4}4
5C5a5u5
6A6w6~6
7?7F7x7
9N9_9.:5:a:r:
<S=q=}=
>)>4>V>
0%0:0O0r0
001u1#2f2
3F4D:h>G?
l013Q4
515F5c5}5
6.6|6
:/:}:
<C=b=l=
1)171w1
2(3b5g5
:,<1<
>0?K?U?j?
181J3`3p3
5#5_5
<S=|>
><?N?
3!3j3
3S4q4}4
5)545V5v6}6
9Q:s:
:#;I;T;s;
;#<I<T<s<
<#=I=T=
041;1T1
1#2_2i2u2
3F3,4
70878L8a8t8x8|8
:5:O:
<,=1=9=A=
>2?=?H?
1$2g2l2q2y263q3
3b4m4x4
7*7l7
:P:U:Z:b:
;@<K<V<
1"2'2,242
4&4y4
7V7[7`7h7
8D9O9Z9
132=2R2g2
3)3}3
3"4,4A4V4
5$595V5
5O6Y6n6
7(7E7
8.989M9b9
:$:x:
;';<;Q;n;
<$=o=
?1?E?Q?]?i?
030Q0e0q0}0
0!1;1P1c1~1
102J2
<<>W>
364S4
5R6j6q6
658k8
9Q9]:s:
<9<@<Y<
=%=A=H=a=r=
>'>8>[>z>
>U?t?{?
0-0Z0^1
4(444S4
5H5\5p5|5
:$;.;C;X;v;
<c=h=
i1n1P2
435Q5e5y5
616F6f687Z7a7z7
8^:c:
1'1.1G1X1
7"8-8
;%<*<
>,>=>
0(040@0_0
282?2X2
3(3/3H3a3
4;6@6
:8:I:
<!=H=
>#>A>M>Y>e>
?h?o?
212G2d2
5,565T5^5w5
7N7W9
?$?A?R?
3:4W4
8<9a9
=:?R?d?
r0\1f1{1
212=2I2U2}2
2#3A3M3Y3e3
334Q4]4i4u4
4C5a5m5y5
6)686X6
7O798w8
=$=K=}>|?
1^2A3l3.4
1"1g1
1&2>2H2a2
383I3f3
42475O5Y5r5
7%7/7H7e7o7
7G8_8i8
;];u;
=i?p?u?
5!5]5
<-<><[<
<?=D=
>1>8>=>
014;4T4e4
0T0|0
:2;7;
;3<7=
1+2a2
2&3A3
4F4a4
6#7A7M7Y7e7
8#8A8M8Y8e8
9#9A9M9Y9e9
:#:A:M:Y:e:
;#;A;M;Y;e;
<#<A<M<Y<e<
=r>C?n?u?
C0n0u0
1>1E1P1\1
2 2,2f2m2t2
263=3D3
6$8k8x:
<j=(?o?
6L6`6l6x6
738M8h8
8/9J9T9
93:N:X:
=N>#?-?F?P?k?u?
!0<0F0_0i082B2[2
3&3?3I3d3n3
7s:y;
1#1=1
2J3e3
7I8W8!9?9[9q9
9$:+:D:r:y:
;(;4;B;_;
7 8?8
;#;);l;
;/<9<E<Q<p<
<#=L=
686=6
9?9J9
:?:N:
;(;F;M;f;w;
<*<1<J<
<a=j=
=s>|>
>b?k?
0R1[1
1F2O2
3/3@3R3V3Z3^3b3f3j3n3r3v3z3~3
4;5M5
8i8s8
:V;u;
<]=b=
=p>}>
?%?3?
e0j0 1Q1\1
5@7J7_7t7
8&;+;`;e;
<,=X=_=t=
=J>4?F?
313=3I3U3t3
7)909I9m9t9
:7:K:^:
:%;,;E;l;s;
<2<9<R<~<
>$>s>
?q?x?
0'080U0k0|0
141p1w1
<!<-<9<E<d<
<]=b=
40;0P0e0
3-3K3
5;6B6[6u6|6
6%7>7]7v7
7!8(8A8O8
8D9K9d9
9":8:c:}:
:2;9;R;`;
;7<><W<
>.>M>f>
?4?B?
70A0Z0
1,1@1
3#3_3i3u3
7)757T7
8!8S8]8i8u8
8C9p9
9!:C:m:
;9<><
<"=C=k=
=*>K>
?.?X?
4E5J5
<n>y>
00;0N0Y0
0e1q1
1 2(242W2l2z2
3$323|3
3 4A4U4
40575P5Z5
5j6q6
:!;1;=;I;h;
<5<a<q<}<
=C=u=
!010=0I0h0
151a1q1}1
2C2u2
5@6G6`6
6[7b7{7
9!969e9w9
9;:B:[:
:U;\;u;
=X=j=v=
=7>>>W>
>D?K?d?u?
1j2q2
5%5:5i5{5
5>6E6^6
6V7]7v7
9Y:`:y:
;v;};
=%=8=
>&?-?F?
0-1=1]1
5)6<6
6:7M7
7A8T8
9L;Y;#<d<p<|<
=B=X=h=v=
>5><>Q>f>
>(?^?
758<8Q8f8
<0<E<Z<{<
='=/=l=
1.2w2
273t3
1>1E1^1
1"2)2B2n2u2
6.686r6|6
7+757
9W9a9
:,:=:_:`;{;
;m<t<
=+=W=^=w=
>F>M>f>
4"5,5
9F9T9b9
;T=u>
6g7K<l=
5f6V;
=J>T>i>~>
1$1R1
1b2l2
3'3<3Q3n3
4%4:4O4l4
5*5G5
6&6w6
7.7K7
=>>u>
919v9
9A:a:
;!;f;
;&<A<
=V=q=
>1>v>
0!0f0
0&1A1f1
2!2F2a2
3!3f3
314Q4v4
465Q5
7!7f7
7&8A8
8Q9q9
:V:q:
;1;Q;v;
<*=5=1>
>/?Q?
1?2a2
3/4Q4
4O5q5
7 717s7
9 9,9K9
:H:T:`:l:
:3;Q;
;#<h<
>K>_>
191M1]4
535C5Z5
556~6
9?9{9
>.?w?
0/1r1
1+2i2
7'8^8r8
9":b:
;L;`;m>
304J4O6l8u8
=`=z=
3#4*4C4o4v4
5'5^5e5~5
;:<D<
><>C>\>
> ?'?@?w?~?
2$3.3
6S6]6
>I?S?h?}?
1-1J1
4"474L4i4
8"878V8
9g:q:
<==G=\=q=
> >5>
7;7H8
989E:|:
:5;B<y<
<J=N?
0&2C2a2
3)3@3
3&4A4
4Q5q5
616v6
667Q7
8!8F8a8
9!9f9
:&:A:f:
:&;A;f;
<&<A<
232Q2]2i2u2
2%3=3X3]3
3"464K4`4
4'5.5G5k5r5
5>6E6^6o6
98:?:T:i:z:
:T;7<
=*>5>E>g>q>
839D9q9
9*:|:
<n=u=
>'>4>e>l>
>a?{?
1f1o1
1Q3,4I4S4x4-575I5^5x5
576e6
7.7E7
9*9A9l9x9
:1:L:z:
<&<s<
>)>F>
0>0E0^0o0
1*2o2
2:3P3`3n3
364U4q4
5 5F5
6T7j7z7
7P8o8
9)9:9f9
=)=0=I=Z=
112Q2
738s8
:$:I:
<4<;<T<e<
>.>C>h>
;0S0Z0s0
212O2d2
3\4t4{4
446Q6o6
9>:T:`:v:
; ;3;E;
0[0Y1
4A4I5p6
;V;'<d<M=y=
233Q3]3i3u3
3%4;4K4b4
5.656N6_6
9=:q:
;U;k;
<-<t<
717F7c7
8$909<9H9g9
: :0:B:
<$<E<]<d<
>)>E>
0"2=2P2d2
4r6y6
6'7.7S7y7
3f3m3
4C5J5_5t5
7p7w7
9w9~9
:Y;`;u;
<R=Y=n=
?F?M?r?
080U0
172^2e2
223{3
3F4M4f4p4
4#5*5C5
6C6}6
:.:::P:e:
;9;C;h;
<j<q<
=.=F=M=f=
0E1;2I2g2
4h5r5
7#787M7j7
:V<`<u<
1S1y1
2S2y2
3S3y3
4S4y4
5S5y5
9S;Z;a;
0&1d1
1m3t3{3
658G8R8]8
9T9x9
<$<1=
=%>/>H>Y>v>
?3?`?j?
0[1k1
2&2@2c2}2
5Q6d6z6
6C7d7
8 8(888@8P8X8d8p8
8$9`9
:/:7:G:O:[:g:
;#;.;>;E;U;\;l;s;
<<<F<k<
=S=Z=
=C>J>
?$?/???F?V?]?m?t?
0=0G0l0
1Y1c1
2I2S2
3 3.393I3P3`3g3w3~3
4$4G4Q4v4
5\5f5
6L6V6
7-787H7O7_7f7v7}7
8#8F8P8u8
9&9|9
9<:F:
;\;f;
<'<2<8<><M<V<\<b<k<t<~<
=8=_=
415F5K5c5r5
6H6]6b6z6
7!7-7`7
8D8}8
9"9@9H9I:
;X<a<g<
=_>k>v>
:3:H:
;#;4;Q;
>&>->F>p>w>
279U9_9
4.4C4P4n4x4
4&7-7v7
7(8/8H8Y8v8
1"1L1S1l1
<1<;<
6a6h6}6
7,7J7T7m7~7
:R:p:w:
;$;5;O;x;
3E4J4
6,9O9r9~9
:0:>=
>4>`>o>
<&>v>
?1W1^1s1
797C7X7m7
<*<4<><
<k=G?Q?
0#0-0H0R0k0r0
0=1$2
4V4q4{4
5$5+5C5M5f5w5
5t6P8Z8
9,969Q9[9t9{9
=R=m=w=
> >'>?>I>b>s>
4%464P4
7>8C8W9-=2=a=
4=5B5
5L6Q6
; ;=;_;~;
<M>d?
5;9v:
<2=2>
4)5D5w6}7
4s6/?
3J4$7`879
5P6`8
939Q9]9i9u9
9#:A:M:Y:e:
;$;5;;;};
<"=~=
1'1-1o1
7/8q8
:3;s;
;#<c<
=R=h=y=
>1>U>\>u>
5:5P5a5g5
6=6D6]6n6
86;];s;
<#<<<`<g<
<1>6>
3#343:3z3
404A4f4
<?=a=
?1?=?I?U?t?
1$1s1<2R2c2i2
3&373=3}3
434D4d4
6&6C6
7(767S7
7 8,888F8l8
::;B;v;!<.<B<
=D=M=
=a>n>
?)?<?H?
1R1r1y1
334v4
4R5]5h5
: :,:K:
:=;j;
;V<a<l<w<
<==I=T=_=
>)>5>T>
?R?W?
4p4w4
5%5I5P5i5z5
9-9>9c9
:2;9;R;v;};
;r<y<
0%010=0I0h0
132Y2d2
233Y3d3
6#7U7Z7e8j8
:r:z:8;D;P;
2"2_2|2
3M3^3
3S5 6
4H627
30Q0e0q0}0
021S1d1l1
2,2?2
0(1a1
2)252T2
313=3I3U3t3
8T8^8
=!=5=>=y=
=%>Y>i>u>
Q0p0z0
041>1S1h1
2%262<2w2
3;3G3S3_3~3
3&4k4u4
535>5
7(838Q8
=,=7=
:0D0s0~0
071>1g1r1
4&444u4
4r5y5
838>8L8
<(<W<b<p<
'010`0k0y0
1*444c4n4|4
5J5K6c6~6
0"0-0v0}0
153:3S5q5
8#9e9
;!;K;c;j;
=!=:=s=
>.?5?
748#9
7E7U7[7
7t8{8
:e;l;
<T<h<t<
="><>
1+1d1
4&7+7f7w7
:/:@:
;-;>;
3E4b4
515=5I5U5t5
8&8C8
8#9I9T9x9
;1;=;I;U;t;
</<5<w<
=&=k=p=
405q5
6D7K7`7u7
8*9x9
;);R;
>*>H>b>x>
0#0=0B0T0
3p57638
:-;m<
;F<d<n<
=*=C=a=m=y=
=1>L>^>d>}>
?M?^?o?
0w0|0
3:3O3u3
5B6L6e6v6
1(1O1]1
3)3F3
;&;?;w;
<6<r<
=/=?=^=h=
0+171C1O1n1
2(282E3a3h3}3
7B7w7
9::?:
=/>Q>
060G0
3$6+6@6U6s6
7i8U9
3&4s4
6(666A6a6s6~7
7{9]:b:,>J>Q>f>{>
>c?h?
2M3R3
434Q4
4#5U5s5
7S8@9g9
93:Q:
=_=i=
9S9y9
:S:y:
;S;y;
<S<y<
1!11191A1|1
2<3Q3a3i3q3
3L4_5
626K6p6w6
7,737L7e7
7J9l9s9
9H:m:t:
;);0;I;b;
?5???T?i?
0.0q0
1)1\1
1*2S2q2}2
3C3a3m3y3
334Q4]4i4u4
4"5C5a5m5y5
526L6j6p6
'0G0]0
4!4:4Q4o4
515P5z5
616J6c6
6#7A7U7a7m7y7
8!8-898X8
9_9u9
<!<:<S<u<
`2e2$3
<Y=x=
> >s>,?
1I2a2
2'2.2G2Q2q2x2
4,5)606I6
848I8w8~8
;1<j<
=*=?=D=
=6>v>
1)1?1a1h1
1E3L3q3
4$4=4N4s4
5X5q5
6(7P7
3c4~4`5
5B6v6
4&5P5i5
7.8p8
;:;p;
<7<><W<l<
>*>1>J>[>x>
? ?E?L?e?~?
1/1g1
2*2D2
5=5m5
:A;K;p;
5/5L5s596k9
1h8R?
5$6+:
;$;x;
;\<v=
>.>k>N?
203L3f3m3
384H4a4
6[6o6
7H8Z8
9h9t9
9":6:\:p:X;j;
<_<c<g<k<o<s<w<{<
>X>m>
?<?T?^?
060J0`0
181J1
2^2r2
2 343Z3n3
5_6r6
7=7Q7w7
7(888^8=9
:G:x:
;!;G;[;
<,<A<^<
<T=l=s=
>!>>>
071>1S1
2"24263H3S4`4
5K5s5
6d7k7
8X8t8~8
9,9A9^9
:$:C:d:
;$;;;K;Q;Y;n;z;
<)<9<?<G<\<h<
=%=6=O=h=
>M>a>r>
?#?I?u?
&030U0
1f1p1
3)3B3
545>5W5h5
6F6Z6
7"737Q7u7
8*8Q8~8
:7:e:o:
;1;_;i;
<6<@<\<f<{<
===M=S=[=p=|=
>(>.>2><>c>s>y>
?)?5?N?T?X?b?
0*00080M0Y0r0x0|0
1@1P1V1^1s1
2+21252?2f2v2|2
3,383Q3W3[3e3
4/454=4R4^4w4}4
5E5U5[5c5x5
60666:6D6k6{6
717=7V7\7`7j7
8"82888@8U8a8z8
9!9H9X9^9f9{9
:3:9:=:G:n:~:
;4;@;Y;_;c;m;
<'<7<=<E<Z<f<
=:=J=P=X=m=y=
>$>4>:>B>W>c>}>
?$?,?A?M?g?
0+070Q0m0}0
1!1;1W1g1m1u1
2%2A2Q2W2_2t2
3+3;3A3I3^3j3
4%4+434H4T4n4
525>5X5t5
6(6B6^6n6t6|6
7,7H7X7^7f7{7
828B8H8P8e8q8
9,929:9O9[9u9
:$:9:E:_:{:
;#;/;I;e;u;{;
<3<O<_<e<m<
=9=I=O=W=l=x=
>#>3>9>A>V>b>|>
?#?+?@?L?f?
0*060P0l0|0
1 1:1V1f1l1t1
2$2@2P2V2^2s2
3*3:3@3H3]3i3
4$4*424G4S4m4
9<9A9K9R9Y9`9g9n9u9|9
:<;J;N;R;V;Z;^;b;f;j;n;r;v;z;~;
;:<H<L<P<T<X<\<`<d<h<l<p<t<x<|<
=%=5=<=I=T=Y=_=h=n=r=
>(>/><>D>J>T>^>h>l>
?"?)?6?>?D?N?X?b?f?
0%020:0@0J0T0^0b0
1,141:1D1N1X1\1}1
2&2.242>2H2R2V2w2
3 3(3.383B3L3P3q3y3
4(4B4^4n4t4|4
5,5H5X5^5f5{5
6(686>6F6[6g6
7&7;7G7a7}7
8/8;8U8q8}8
9!9t9
9*:3:
:1;A;Q;a;q;
<*<<<^<h<|<
=>=H=\=b=f=z=
>(><>B>F>Z>l>
?"?&?:?L?n?x?
0,0N0X0l0r0v0
1.181L1R1V1j1|1
2#2'2;2M2`2j2~2
3"3,3@3F3J3^3p3
4 424E4O4c4i4m4
5%5+5/5C5U5h5r5
6,6?6I6]6c6g6{6
7!7'7+7?7Q7
818?8I8]8c8g8{8
9%9)9=9O9b9l9
:$:.:B:H:L:`:r:
;";4;G;Q;e;k;o;
<'<-<1<E<W<j<t<
=,=6=J=P=T=h=z=
>#>q>
?A?O?Y?o?u?y?
0"0A0S0
1!1/191O1U1Y1m1
2%2)2=2R2q2
3!333
6!6*60666?6H6R6\6
7"7G7R7o7
=8>{>
?#?q?
1Y1c1x1
8O9}9
9/:O:j:t:
;7;R;\;
;d<n<
<<=F=o=z=~>
?J?T?m?~?
30=0R0g0
171B1
2p4u4
4)535b5s5
6)7M7r7|7
:-:B:_:
;/;@;];
<2<=<H<
<)=3=\=g=
?'?K?V?
6f6p6
919c9
:-:f:p:
;;;F;
<#<L<W<
<(=3=
>$>J>U>
>(?-?
2>2H2w2
7%7]7g7
8`8j8
9N9X9
:,:6:d:u:a;f;U>q>v>H?d?i?
0[0e0
373_3m3
484_4d5v5k6}6r7
7Y8k8
<5<r<
<:=I=
? ?o?v?
040I0c0
4L5S5m5
5Y6k6
7,7o7
8.9I9W9
9.:k:
;*;8;t;
;'<=<
=C=l=
=W>a>v>
>+?5?J?_?y?
1t1~1
1]2z2
293V3d3
6&6C6Z6a6
6?7Q7
9;:W:^:w:
;8<?<T<i<
151U1
:f:p:
7*7C7x8
9/9;9G9`9y9
:W;{;
132I2
4 454R4
6-6s6
6X7s7
= >y?
0H1e1
2&2@2
7"8,8V8a8
: :,:
<(<3<
=#>C>
>-?M?
031S1
1;2[2
2C3c3
3I4i4
4U5u5
8'9L9b9
;.;`;
<;=a=
2-2[2
2O3T3
4o4t4
4/545
5O6T6
7f7k7
7A8[8_8c8g8k8o8s8w8{8
9):.:n:
;";&;*;.;2;6;:;G;
0"1,1Z1k1
3G3Q3
4K4U4
4E7O7h7z7
7c8m8
9 :o:t:6;A;L;
;K<P<
02171
2,212F2K2
;:;G;n;
<@<Q<j<n<r<v<z<~<
6L6q6
627@7N7
1*2Q2
313z3
3]4d4
8[9|9
0 0*0O0l0
1#202k2v4
5/577
8H8O8
253m3
4:5F5R5^5
5%8$9)9|9
:&:?:P:m:
=">+>4>=>
0K0\0m0
2(292f2p2
4-4;4
7V;n;s;
<5<:<
6#8J9q:
;H<W<x<
=%={=
?5?:?
0C1|1
282k2
2$3V3
5"565J5Z5j5z5
81;};
;@<`<i<
=/=D=^=
80=0p0
0S1]1r1
1-272L2a2
4+505
6 6:6
6/72878
8>9j9t9
9I:a:f:
1!2-2Z2~2
7\8c8r8
:.:S:
=.=C=]=y?
23282t2
787O7m7
8&:+:v:{:
;,;1;x;
<.<?<
>Q?n?z?
2#282p2
5Y7^7
<#<f<k<
=?=J=U=
>+?0?]?g?
1A1M1X1c1o1z1
5>5C5
6P6U6
657:7
7f9p9
<_=i=
181X1b1{1
2!4`4
6'7@7
:I;W;e;s;
;,<G<
2[3v3
416t8
3Z3q3
3/4Q4
5!7P7z7
9x:o=
0/0m0
2,272
;'<f<
='>t>{>
;0V0i0
1C1c1j1
2p3q4z5
8Z9s9x9v:
;#;(;
3;3H4M4h4m4
6l7t9
:$:5:
<5=:=d=n=
=o>z>
0=1U1_1t1
4P4^4
9C9M9f9w9
>/>H>a>z>
?2?K?^?v?
1'1A1H1a1r1
1 2U2
3"373L3i3
4"4f4
5(5F5`5
7<7\7}7
8F9m9
;-;7;I;^;{;
;,<N<g<q<
=#>>>W>
0*0G0
1(1=1R1s1
192Q2t2
4/4;4
4"5B5
5_6i6
8l8v8
8!9+9D9K9j9t9
3%4,4G4a4
7$7)7
899s9z9
:3;K;R;g;|;
<(=2=K=x=
>(>9>s>
2c3l3
4Y5d5
679d9n9
;,;I;j;t;
<+=J=
?5?J?c?y?
0E0O0d0y0
1&1g1q1
5R5W5
798^8h8
9q:v:
;2;C;
;\<f<|<
=5=:=
0*0I0[0
8g9q9
=b>>?z?
0&0X0
0/1Q1z1
212:2G2Q2]2i2~2
3#3<3
4'414P4W4p4
676O6V6
7l8s8
9W9^9w9
:7;O;V;
;#<;<B<d<
=R=h=
0'1D1I1
253N3
6:6Q6x6
:*:5:
=0?V?
343K3
474A4f4~5
5G6l6
7'8~8
9,9H9P9a9
?5?F?R?_?e?k?q?
0"0J0
2B3P3
9C:M:_:}:
0Q0u0
172}2
:(;n;
<3<H<h<
=$=9=N=l=
0c0m0
121x1
2]2g2|2
2*343I3^3{3
7D7^7
>2?t?y?
0.0D0
1+1H1
3)3>3[3
4,4A4^4$7.7C7X7
7H8/9
:W;o;
<B>`>j>
>p?z?
4P4U4
6^6|7
7G9S:
>:?Q?
8-8J8
9~:U;Z;
;f<~<
=!=?=V=
212:2K2
3"343R3
4"414=4J4P4V4\4
4;5E5b5v5
5.637
9?9D9
9+:5:O:d:
:G;L;u;z;
0#1x1
1!2(2M2
2b3}3
596d6
8+8U8
9(:2:K:\:m:
: ;3;N;
<'<q<B=k=
=!>]>w>
2>3Y3
<*<n<
0J0~0
3h4o4
6)7i7
778X8l8
909T9
:W:a:q:
=!=y=
020X0
1$1n1
1Y2^2
2p3U4n4
4(5v5
5@7r7}7
<t<y<
1J1[1
1*2;2
4+464
6k6u6
6'717[7f7q7
8<9F9o9z9D:T:
7I7W7;:
<H<z<
=/>9>V>t>~>
>.?k?
0!0@0Y0v0
2%2B2N2Z2f2r2
2"3@3J3
5'585
7g;q;
<)<3<L<S<o<y<
<9>I>
0O2n2x2
3:3D3i3
5'5D5`5j5
76:M:
>'>2>
0#0c0
0>1z1
1S3i3
355?5
5Q6V6
7S8p8
9F9!:V<`<
>Q?b?
3*464K4
6.7r7w7
>F?P?c?
24393{3
3D4I4
5l6q6
6-7Q7V7
9/999N9c9
93:=:
:@;(<G<Q<f<{<
<X=k>)?
5B5G5
8!9~9N;N<
>Z>P?Z?o?
1I1Y1w1
414;4P4e4
4B7L7_7
<&<F<
3v4I5S5h5}5
587B7W7l7
;+;L;V;o;
J1x1a2
4F5b5l5
757`7w7
8!8J8T8i8
9s9x9
9C:e:v:}<n=x=
==>Q>w>
?4???
2P2Z2
5B5M5
9'9E9O9{9
91:6:w:
;(;N;Y;
= >O>v>
2 252{2~3
= >*>G>d>
>9?C?_?
0A0X0
0J1i1w1
5 6h6r6
<9=T=
3)4D4
4L6w7
;!;Z;q;
248W8
9::?:m:
<3<I<_<{<
>/>E>^>j>
?)???X?d?
$0;0S0
9V:i:
;:;F;O;T;a;y;
=T>g>
363F3Y3v3
6%6/6D6Y6
6J7a7
8Z8q8
9F:M:`:~:
;#;H;e;
2A3I5g5n5
7_8v8
9,:Z:_:
; ;e;
=:>/?
5b6l6
61767b8p8~8
8#919M9
:%;G;x;
?*?_?q?
1Q1]1
212R2
3P3\3
484g4
696S6k6
8Y9s9
:0:=:Q:e:v:
; ;7;H;Y;j;{;
<2<O<g<~<
=#===C=S=X=g=l={=
>%>0>N>a>z>
?"?'?6?;?J?O?^?c?r?w?
0$03080G0L0[0`0t0
1)1.1B1O1f1~1
2)2.2=2B2Q2V2j2z2
2C3I3b3g3
4,424G4^4e4|4
5#52575K5[5t5
6 676=6S6Y6k6v6|6
7(7/7?7X7h7s7
8,828D8O8U8Z8c8v8{8
9$9)989=9L9Q9`9e9y9
: :*:G:^:d:v:
;3;C;\;l;w;
</<n<z<
="=5=@=a=t=
>,><>A>O>T>b>g>u>z>
???Y?
#0:0X0t0
1'101r1
4i4}4
5 5)5A5U5
8.989r9
;:;Q;z;
<J<a<
>A?{?
2]2t2
4&434G4[4o4
5.5V5o5{5
7G7b7|7
809J9S9
;+;4;+<
0/0P0s0
4U4q4
:0:O:
>0>O>t>
!0V0t0
0c1~1
2;3R3m3
4#444E4J4a4l4
5!585P5f5m5
6 636C6\6l6w6
73797O7V7n7
8&8,8?8J8P8U8f8q8w8|8
989K9d9z9
:,:B:I:^:d:v:
;(;;;T;j;w;
<"<(<><D<\<t<
=.=:=M=]=v=
>4>I>P>h>
?0?C?M?R?c?n?t?
0!040:0M0_0e0z0
1!1'1?1W1o1
2%2+2B2V2\2o2y2~2
3 3A3T3m3
4&4?4O4Z4{4
5(5@5W5k5r5
6"6-6N6a6z6
7#7)7>7N7g7w7
818H8`8x8
969a9}9
:(:-:=:
;#;4;E;V;g;l;};
<,<=<N<_<p<
=+=<=M=^=o=
>*>;>L>]>n>
?"?7?<?R?W?i?t?z?
080K0n0{0
1J1c1s1~1
21262H2S2Y2c2~2
3%3B3M3e3j3x3
454H4m4z4
4;5T5d5o5
6-626I6N6e6
7)7<7^7g7l7
8c8|8
9/9:9@9J9a9f9
:*:G:R:h:t:y:
;$;=;M;X;};
<&<+<8<=<L<Q<`<e<
<!=-=2=H=M=^=h=s=
>1>D>q>
?%?/?:?R?W?c?|?
0(0-0B0H0M0c0h0y0
1"151`1q1
2)292D2e2x2
3)333>3S3X3k3}3
4"4'474<4J4V4[4q4v4
505C5p5
606B6[6k6v6
7$7:7?7P7Z7e7z7
878E8Q8V8k8s8x8
8"9,9H9M9Y9r9
:):.:D:I:Z:d:o:
;/;J;V;[;q;v;
< <+<P<c<|<
=*=6=;=P=U=Z=q=v=
>+>>>W>p>
?$?/?G?L?X?q?
0%0*0=0B0]0b0w0|0
1%181Q1j1x1
2(282C2d2w2
3%3=3B3N3g3w3
4!4&454:4F4[4`4z4
5)535>5S5X5k5}5
6&626G6L6c6h6u6z6
7 7%7<7A7X7]7t7y7
8@8S8l8
9"9,969@9G9c9h9{9
:-:2:W:p:
;";';>;C;Z;
<-<T<y<
=7=J=c=~=
>)>.>>>C>O>t>
?&?+?0?E?J?O?e?j?o?
070P0`0k0
1#181=1W1\1v1{1
2%2J2]2v2
30353F3P3[3f3w3|3
404Q4V4[4p4x4}4
5(565@5J5[5`5q5~5
6@6S6l6
7.737A7K7d7i7|7
8&898R8m8y8~8
94999[9h9
:!:&:6:;:O:T:Y:n:v:{:
;);.;:;G;`;p;{;
<"<7<<<A<X<]<t<y<
='=L=_=x=
>h>u>
?*?/?A?M?R?h?m?
050H0a0z0
1#1(1-1D1I1Z1d1o1
2&2O2b2|2
3$3:3?3V3[3r3w3
4$4=4B4{4
595>5W5\5q5}5
6,616?6[6`6
7!7/7;7@7V7[7o7
8#8D8W8p8
929D9]9m9x9
:!:+:6:K:P:s:
;+;0;E;J;O;e;j;o;
<'<1<8<I<N<q<
='=,=A=G=L=a=f=k=
>.>3>V>o>
?&?+?@?F?K?a?f?}?
0@0S0l0
10181=1Q1V1[1q1v1
2!2=2B2e2~2
3#3=3B3Z3j3{3
404I4Y4d4
575<5O5[5`5v5{5
6"6E6t6
7<8Z8t8
;N<k<
=@>s?
I0`0p0
1O1c1l1
30383T3r3
3H4o5
767X7r7
738G8P8U8
8<9T9
;%;Z;f;r;
<E<O<[<|<
=9=C=O=
>'>4>]>
?)?2?7?d?
0K0c0{0
0"1V1j1r1&2W2q2
2G3Q3
4+4E4_4i4
5&505E5Z5w5
718;8X8
:f:p:
;#;@;`;
<^<h<
<)=B=
>#>@>]>z>
?2?O?l?
0'090~0
1N1h1
2L2f2
3 3:3T3n3
3(4B4\4v4
4>5X5r5
6R6l6
7*7A7j7
8:8Q8z8
9!9J9a9
:1:Z:q:
;1;z;
<:<Q<z<
>!>:>Q>
>*?A?z?
010j0
3p4w4
535G5[5q5
5#6;6E6Z6o6
9z:9;q;
1@2J2
3'3A3
525<5Q5f5
618;8U8i8
;S;o;y;
0A0O0]0k0
3-4A5
8J8O8
:I:w;
;B<L<t<
1S1Z1j1
1k2u2
2F3P3x3
4!4J4U4
6F6P6q6x6
7=7D7Z7f7
819;9a9l9
:B:M:
;j<t<
=/=:=
=%>/>W>b>
?;?F?`?d?h?l?p?t?x?|?
2#2H2f2p2
777S7
818W8s8
8#9a9v9
=4=J=
=>>_>
1v3N4m4B7
=r>4?
0 050R0
191S1l1~1
3;3E3j3
4b5l5
0c1=2M2
4|5g7
;L<V<o<
=%=6=Z=
? ?Z?
5F6M6
9.979V9x9
:$:*:>:D:Z:m:s:
;9;O;
;><b<|<
=$=5=F=W=h=y=
>/>B>[>o>t>
?#?.?4?9?M?]?r?
0.040J0P0e0k0}0
111D1]1v1|1
2&2;2N2Z2y2
31373F3M3\3a3p3|3
4"4;4A4\4b4x4~4
5-5=5R5e5q5
616G6M6c6i6
7'7?7R7^7}7
8)8H8[8t8
9&9?9X9^9y9
:&:+:;:@:T:d:y:
;!;';<;B;X;^;t;z;
<,<8<W<j<
=,=2=D=O=U=Z=n=u=
>0>5>L>Q>^>c>r>w>
?!?'?<?B?W?]?r?x?
0*0@0S0_0l0y0~0
1.1A1Z1p1}1
2$2)2:2E2K2P2g2w2
343;3S3i3p3
4"4'4A4F4d4i4w4
5+565<5F5X5h5}5
5*6{6
707e7
8D8Z8
84:N:o:
=d={=
1*1A1
2*2A2e2
2Z3q3
4&4D4[4y4
5;5R5
6'6.656<6C6W6l6
717J7c7~7
778^8
:H:c:z:
;,;\;
<i<6=q=
?/?T?
080F0b0
1<2M2U2d2u2
3&3,3D3Z3a3w3}3
4"4(4-4>4I4O4T4e4p4v4{4
5)5.5F5K5c5s5
686J6U6[6h6z6
797I7T7u7
82888P8g8y8
919I9R9l9q9
:+:2:J:b:z:
;%;=;w;
<*=@=I=n=
?X?b?
0!0z0
1"1'1<1A1U1Z1f1x1}1
2&2>2V2n2
3+3@3T3
;.;8;B;L;V;
<)<3<=<G<Q<[<
=!=i=v=,>4>O>V>
192x2
3*343>3H3R3\3c3s3z3
4#4q4x4
5,5>5F5R5^5j5v5
1=2N2
6%616?6
:+:2:?:I:j:w:
;T;^;h;x;
<!<0<<<D<T<\<l<t<
1;1`1
4A5O5
6C73:%<
395\5_6
:B>K>b>n>|>
1 1X2
858j8|8
9':.:
O2 4l6
879L9b9
?.?>?
:N>@?
2#598
9"9(9.949:9@9F9L9R9X9_9c9g9k9o9s9w9{9
9 :':<:Q:n:
;$;c;{;
=4=A=S=z=
>(>P>
>&?J?|?
060=0b0
1=1E1K1r1
2'2B2R2
585N5
7-7D7J7`7e7
7<8J8P8c8h8
:5;?;g;r;
<3<8<
<Q=_=e={=
0#1C1
2D3I3
5U5c5i5
7&8Y8r8x8
8l9v9{9
<"<-<8<
<C=]=l=r=
>$>P>}>
?e?k?s?
(040V0u0
051h2
4=4G4l4
:Z;g;
?'?6?Z?a?i?|?
$0<0C0`0j0s0}0
1(1M1T1[1a1
7@7K7
8L9Q9w:
;.;H;
3A3'4j4
5H5_5u5
656<6Q6f6
7T8p8z8
9-9:9^9
:w:/;Q;[;p;
1$1S1
252^2z2
343R3
454@4K4d4
5\6f6{6
6(7u7
929<9Q9f9
?%?2?Q?
0,1D1K1
142d2
7B8g8
9'989j9
=J=O=
0^071]1
4C5g5
7Z7q7
:#:H:e:
:#;u;
<0<E<b<
<@=p=w=
=F>N>
?e?l?
2(242T2`2o2
4R5Q7
9B:U:c:
:3;g;9<W<]<a<i<z<~<
= =,=3=<=B=M=S=Z=r=
>Y>d>n>w>
?(?-?Y?d?m?v?
3Z4c4l4
6$7Q8+9z9
9V:]:
1=2\2
333T3Y3
4A4F4
7 8%8
=&=L=W=
>N>X>|>
0B0M0
1,171
2&212r2'313U3a3l3
3!4B4L4p4{4
6D6O6
898A8Q8X8e8m8
;L<f<
4 4$4(4,4044484<4@4D4H4L4P4
9!9;9
<P=m=
2H3w5
<W<u<
=(=.=6=K=W=k=
2b2G3]3
4I4n4
6 7E7]7
8D8i8
9.9h9
;P;u;
<><c<{<
1%282 3
9(989>9F9[9g9{9
;.;`;
;B<c<
<O=W=
>L>k>
?/?I?R?
0'020?0K0
5N5i5
7F7W7_7l7w7
8%9Z9
9.:c:
;7;m;{;
=*=8=y=
>*>/>9>E>S>
?&?7?=?B?N?X?j?o?
0L0\0u0
1G1M1b1h1
2@2E2Y2f2r2
333D3J3W3a3k3u3
4"4c4
5#545u5
656=6K6P6i6}6
7!7'7?7P7V7[7b7}7
8%878<8e8j8
9$9@9M9}9
:0:G:[:j:
;?;s;
;=<X<t<
=6=I=N=^=c=v={=
>+>r>
0/0}0
2(2/2U2[2`2l2v2
3"3a3
4&424C4
5*5;5@5M5R5q5v5
6,636Y6_6d6p6z6
777=7R7X7j7q7
8"82888I8Z8j8o8
9$9p9
:+:1:F:M:s:y:~:
;<;A;M;_;d;x;};
;"<(<=<C<U<\<
= =%=5=:=J=O=^=c=r=w=
>#>,><>A>U>b>n>
?;?A?F?R?\?l?q?
0#0(070<0K0P0`0e0u0z0
2 2,2=2y2
2%3r3
3H4m4
4Q5z5
6*6A6j6
717V7j7|7
8 8K8Q8f8l8
9+909H9U9a9o9
;H;Q;
=,=C=Z=
>&>=>Y>w>
?"?6?J?^?r?
0 0(0@0L0_0i0}0
111E1Y1m1
2!252I2]2n2
4#464<4U4f4l4q4{4
5)535=5G5Q5[5e5o5y5
5"6Q6W6j6p6
7(727M7^7d7n7x7
808q8
:);3;M;T;j;p;
<P<w<
<2=k=
1)1E1t1~1
1;2J2O2a2v2
2#3`3t3|3
7(7s7
8L8Q8c8j8
9#9\9
<5<K<
=1=L=
>6>W>
>!?7?a?
1.1b1
172q2
4#4;4S4k4
5-5:5F5W5
646g6
6>7`7s7|7`8
869m:
>+>?>_>
161Q1
1<2P2X2x2j3
5(5.5F5]5x5
7"7j7w7
8-8?8D8[8b8v8|8
9%9*9>9K9W9h9
:0:6:M:h:
;";f;
<7=J=b=y=
=>>d>
?^?o?u?z?
0)030K0Q0g0
6%6.6
6#7d7x7
8&8|8
9.9v9
9%;<;];
<"<<<
=5>L>m>
0J0S0i0
1*2D2q2
273D3P3a3
4,414E4W4\4l4~4
5)5C5
5Q6z6
7(7a7
7/848F8M8c8i8
9%969z9
9(:I:[:s:
;$;;;V;
<@<M<Y<j<
?-?Y?_?u?{?
0*0k0
1!12181=1T1c1h1z1
202=2T2k2
3!3e3
4$4=4\4
5K5X5d5u5
5J6z6
7d7q7}7
8+8I8d8~8
9 9'9>9U9p9
:B:W:k:
;?;U;
<0<J<z<
<;=H=T=e=
>">.>
>)?w?
0 0&0>0T0n0
0!1V1j1r1
3+3b3g3
3!4.4:4K4
5S7x7
7,8Q8[8{8
9"9\9
:M:r:|:
:-;R;
<D<i<s<
=T=y=
>B>g>q>
?B?g?q?
090C0}0
1M1r1|1A2j2
3*3A3j3
4Z4q4
5!5Z5q5
6*6A6j6
7*7A7j7
818Z8q8
9*9A9j9
:*:A:j:
;3;j;
<:<Q<z<
=!=z=
=J>b>z>
0&1F1a1n1
6X7b7
7&9F9
9,:F:
;6<Q<
161<1S1Y1j1o1
2)2:2v2
3=3C3[3a3p3|3
414@4E4W4^4k4w4
5+505@5E5Z5_5k5p5~5
5$6+686D6U6
7=7U7u7
8.8h8
969@9`9
93:X:b:
;U;z;
=G>w>|>
?/?A?F?X?e?|?
0+080D0U0
363N3e3~3
5%5*5=5C5]5
718R8n8
8(979<9N9[9g9u9
:$:l:
:<;U;
<%<Y<o<x<
=V={=
=&>K>U>
?'?a?
030q0
1.1?1g1
2+2?2S2g2{2
3"373V3r3
5g5~5
506U6
7,7Y7
768P8|8
929N9[9g9x9
:/:F:a:
;;;W;d;p;
<"<h<
=%=7=>=K=W=h=
>V>]>j>v>
?,?=?C?P?Z?l?s?
0&0,0D0U0[0`0l0v0
0&1,1B1H1`1q1w1|1
2E2K2]2d2}2
2/3^3d3v3}3
4/4p4
51585E5Q5b5
6.6?6E6R6d6k6x6
757:7K7R7i7
7*8h8m8|8
;#;9;
;%<v<
='>D>d>
?+?O?\?m?
1)1/1<1C1H1_1n1s1
2B2H2Z2a2z2
2,3[3a3s3z3
4E4t4z4
686I6O6T6`6j6t6~6
637b7h7z7
8$808A8
9)909=9I9Z9
:.:G:X:^:k:}:
;$;6;=;V;g;m;z;
<4<P<v<
='=k=
=&>->N>g>
?&?G?`?
0!0@0Q0W0\0h0r0|0
141E1K1P1\1f1p1|1
2!282O2j2
3K3g3t3
444X4
6L6l6
7,7@7o7
7'8l8
:7;k;
=4=P=]=i=z=
50R0u0
1 1O1
3<4l4
6:6S6l6
8#848z8
9!9.9:9K9
:,:3:@:L:]:
;!;9;J;P;U;a;k;u;
<#<7<D<P<a<
>:>V>
1+2?2Z2v2
3#3-373A3K3]3j3v3
464K4Q4l4
575<5V5c5o5
8;8j8p8
9J9y9
;';h;
<%<6<w<
=(=4=E=
>#>*>7>C>T>
?^?k?w?
0*030b0x0
151d1j1|1
2D2s2y2
4!4b4
505q5
6"6.6?6~6
6-747A7M7^7
8I8P8]8i8z8
9$9:9@9X9n9t9
: :%:<:O:T:h:u:
;@;U;
;)<E<s<y<
=2=E=J=[=n=s=
?)?0?F?L?c?~?
0*0s0
1'131D1
2"2/2;2L2
2,4I4h4
5"6m6
6"7W7}7
8%9v9
;X;l;};
;K<a<z<
<"=s=
?&?O?
0!0E0
1D2q2
3#4n4
626[6
7-7{7
839I9_9
:0;Y;
;C<f<
=0=n=
?&?I?
0Y0p0
041]1
1(2y2
4(4<4b4
585{5
5"6s6
677W7
8!878j8
8*9{9
9&:w:
<$<S<
=)=a=
=$>:>
0D0X0
1C1M1d1
1)2T2|2
748U8
9+9j9
;7;|;
;F<i<
>2>8>O>U>k>q>
?,?3?8?L?S?h?u?
0"0*0C0W0_0
2 2L2
7"797Z7w7
7<8I8U8f8
9+:Q:
;A;Q;e;
>S>j>
>.?W?
1A1N1q1
232C2W2}2
3O3`3m3
3)4T4Y4f4
475D5u5
576Y6
8!868<8Q8W8k8r8
909=9I9Z9
:>:q:!;6;=;Y;
<,<6<~<
<#=H=R=r=
>E>j>t>
3!3J3a3
4!4J4a4z4
5!5Z5q5
616V6z6
7!7<7j7
8!8J8a8
879N9k9
;.;3;B;G;V;[;j;o;~;
<)<g<w<
< =@=c=
>,>L>i>c?
0 1R1o1
1C2a2
3'3,3C3T3Z3_3k3u3
3<4L4f4
6<6X6
7 7,767@7J7T7^7h7r7
8*868G8
869P9l9
:0:7:P:a:g:t:
;/;L;Q;`;e;
<$<5<v<
=!=5=;=U=
>H>X>r>
>5?:?Y?^?n?s?
0,0=0C0H0\0i0u0
1#1(1D1I1e1k1~1
202o2
2+3P3
434b4h4{4
5#505<5M5
7 73797Q7b7h7m7t7
8)8>8K8W8h8
959G9h9
:1:|:
<*<[<p<
>_>I?r?
0D1.2W2G3
4?4_4
5@6e6o6
;+;5;q;
<*<A<j<
=.=z=
>*>A>j>{>
90M0V0
0-1^1
1!252>203n3
8 8:8Y8q8
9.9^9
:5:M:R:n:
;!;&;:;F;W;
<2<9<R<a<l<x<
<5=K=T=
=*>[>
?3?`?
1%1A1p1u1
2,22272C2X2e2z2
3"3.3>3C3b3g3
4H4\4h4y4
515E5N5
71767G7N7d7x7
8 8,8=8~8
9(9-9<9A9P9U9a9q9v9
:3:?:P:
;4;J;P;h;~;
<3<F<K<b<q<v<
=-=:=F=W=
>">U>
?H?N?d?k?
0/0B0G0S0e0j0
1*1n1
232m2
4V4j4s4h5
6;6R6v6
9B;Z;q;
< </<4<C<H<W<\<k<p<|<
=:=N=Z=k=
?%?M?y?
0.0:0K0
101X1
1V2m2
3-3C3J3b3w3}3
4&43484O4`4f4k4w4
5a5u5
6*6M6
8"8(8:8@8S8Z8s8
9=9G9`9y9
:):j:
;6;G;M;R;\;a;x;
<C<}<
=L=g=
>.>v>}>
?/?J?
081W1
2!2:2K2Q2^2h2z2
343;3T3e3k3p3|3
4-424A4F4V4[4o4|4
546H6c6}6
767I7k7p7
8W8h8|8
9 9<9e9o9
:%:1:B:
;S;x;
<!<J<a<
=*=A=j=
>:>Q>j>
?1?Z?q?
0!0Z0q0
111j1
1*2F2u2z2
3 3*343>3H3c3t3z3
4$4c4
4.5;5G5X5
989]9j9
:+:d:z:
;!;D;f;
<N<]<b<t<
=3=l=u=
>->3>M>
>2?]?j?
0+0<0
0G1T1`1q1
2_2d2|2
333]3q3z3
4C4u4
4_5y5
6,6k6
7,797Q7X7p7
8-83888L8Y8e8v8
9,9S9
031`1e1w1~1
2!2]2
3!4g4
4"5,5D5J5b5y5
6B6g6t6
6#707<7M7
8 878R8
8@9v9
:P:a:g:l:
:);3;G;T;i;o;
<(<4<E<
=!=;=
='>T>Y>k>r>
041N1{1
2*2;2w2
2<3P3j3
3H4o4x4
5^5r5{5
6+6X6]6i6v6
7T7{7
7;8H8T8e8
929}9
:F:V:[:~:
;(;/;B;H;\;b;w;};
<1<><J<[<
=!=i=p=}=
>!>;>
?#?8?>?V?g?m?r?~?
"0G0T0k0q0
091i1s1
1(252A2R2
3/3I3
4(414D4J4\4b4z4
5#5)5.5:5D5N5X5b5l5v5
5'6,6<6B6U6[6s6
7"7'737=7G7Q7[7e7o7y7
80858I8P8g8
969;9M9T9k9
90:T:
;Q<h<
=.=4=A=O=T=d=i=u=
>!>&>F>K>Y>f>|>
>)?S?X?h?m?
0A0l0r0
1%1*1<1I1U1f1
2(252K2c2t2z2
3:3C3P3b3g3}3
4$4h4
4c5~5
5I6c6
7O7\7h7y7
8,828G8a8
9<9I9]9b9r9w9
:-:i:
;J<S<v<
<3=G=P=
>->I>t>
?d?u?{?
0_0t0
8C8M8a8n8
9.94999M9Z9f9w9
;);=;J;_;e;|;
<*<;<
<F=S=_=p=
>$>=>]>q>z>
0f0s0
1=1Q1i1
2\2p2y2
2!3l3
5*565G5
5-6`6
8:8?8K8X8m8s8
9*9q9
9%:*:B:G:[:h:t:
;2;7;E;R;h;
;$<Y<m<v<
=E=Y=b=
=1>L>f>
>/?<?H?Y?
0-1K1e1
2+2<2u2
3-3:3Q3h3
5*5A5j5
6*6A6j6
7!7:7Q7
8*8A8j8
9*9A9Z9q9
:1:J:a:z:
;*;A;Z;q;
<1<J<a<
=!=:=Q=j=
>*>A>j>
?$?*?=?C?U?[?o?u?
0+000B0O0[0i0
1]1b1
282=2\2a2
3#343u3
4G4^4~4
5)5O5
7@8v8
849H9\9
:.:W:
:#;n;
<!<+<H<Y<_<d<p<
>I>b>{>
>2?u?
181Y1m1v1h2
617L7c7
8,878=8G8Y8~8
9$9)9@9G9^9p9w9
:(:l:
:2;s;
<6<J<V<g<
= =9=H=S=_=p=
>">9>>>P>W>p>
>/?T?Y?p?u?
'0V0[0i0n0~0
1(171J1_1l1x1
2,212E2J2i2n2|2
3F3i3n3
4/494A4G4\4
4"5H5R5g5{5
6:6?6U6Z6q6
6<7y7
9V9j9s9n:
>O>g>o>
121D1O1U1_1o1t1
2#2:2b2
333=3W3^3u3
5#5*5A5Y5q5
6"63696F6P6\6m6
7 7;7@7i7n7
8$8*8?8V8]8t8
:7:>:V:m:
; ;*;/;L;\;g;s;x;
<1<6<V<[<m<t<
=(=_=
>7>H>N>[>e>q>
? ?:?A?X?n?
/0]0g0
1(191z1
2.24292E2Z2i2|2
2V3\3x3}3
4+4C4Y4_4s4y4
5+5<5}5
6"6(6@6M6Y6j6
8.8~8
9x9~9
:(:@:V:\:p:v:
;(;9;z;
;/<}<
=1=Z=q=
>1>J>a>
:0Q0j0
3&4:4a4
7:8[8u8
9)9/9A9H9]9c9u9{9
;';3;D;
<!<-<C<X<k<~<
='=,=L=Q=g=s=
>*>q>
>F?a?n?
1>2V2
3_4h4
>(>->=>B>Y>h>
>F?a?
1$2`2
43494N4U4l4}4
5#555<5I5U5f5
6 676H6M6f6
7 717z7
8!8/848B8G8^8m8r8
:F:`:
=)=t=
? ?[?
0J0l0u0A1]1
1A2P2U2g2n2{2
3)3/3@3E3U3Z3s3
4V4q4
4"5c5
576O6
7(818e9
:2:K:r:
:7;\;f;
<#<m<
<&=K=U=
>%>+>H>M>{>
>5?d?j?}?
0"040A0M0[0
1C1H1]1l1q1{1
1/2P2
5)5/5C5M5i5
6V6p6
7K8u9
:[:m:y:
;";.;?;
;8<r<
=1===N=
=?>y>
>&?8?D?U?
0 1%1?1E1[1`1w1~1
2N2b2k2
2Q3r3%4C4]4
4*5>5J5[5
6!6Z6q6
7#8O8c8o8}8
9 :*:?:T:i:
;!;/;s;
<=<Q<]<n<
=P=}=
>H>\>d>i>
?9?M?Y?j?
0O0{0
0 1*1>1K1b1y1
2C2n2t2
3)3:3y3
3&4-4:4F4W4
5!535:5G5S5d5
6 6%616C6J6W6c6t6
7*70757A7S7Z7g7s7
8)8:8@8E8Q8c8j8w8
9!999J9P9U9a9s9z9
:,:3:L:]:c:p:z:
:);/;D;J;b;s;y;~;
<9<?<T<Z<r<
=I=O=d=j=
=.>]>c>u>{>
?$?0?:?D?N?X?b?l?v?
(0.0A0G0Z0`0x0
1$1.181P1]1i1z1
20262;2G2Y2`2m2y2
373K3
4-4U4
4)555F5
5"6.6?6
7&7o7
8W8|8
839:9G9S9d9
:#:;:L:R:W:c:m:w:
;/;<;H;Y;
<+<4<l<
?c?w?
0.0?0E0R0\0f0p0z0
1'1?1E1\1v1
1$2I2V2o2
3>3c3p3
5$555;5@5J5O5a5k5
586n6
<0<n<
<"=y=
0)0q0
778M8
8#9s9
<5<|<
273M3
5A6x6
6%7e7
8 8q8
<#<p<
=1>{>
5H5X5{5
5 6j6
6B7|7
8R8h8
<-<n<
=8>X>
060l0|0
2+2{2
:F;\;
1-1w1
384a4/5Z5`5v5|5
6 6,6A6Q6X6]6q6x6
7,797>7U7Z7j7p7
818B8H8]8d8y8
9-949J9\9g9m9w9
:&;:;F;W;
<%<:<L<W<]<g<
=+=o=v=
=?>k>
? ?,?=?
"0N0b0n0
1 1d1
212E2Q2b2
393>3V3[3p3
3@4l4v4
515>5J5e5
6,6=6C6H6T6^6j6{6
747E7K7P7\7f7r7
8 8%8<8M8S8X8d8n8z8
9(9-9D9U9[9`9l9v9
: :0:5:L:]:c:h:t:~:
;";(;8;=;T;e;k;p;|;
<*<0<@<E<\<m<s<x<
=$=>=E=\=m=r=
>C>M>t>y>
? ?d?
0$0.0:0K0
1 1,161B1S1
2#2(242>2J2[2
3%3+303<3F3R3c3
464G4M4R4^4h4t4
5'5.5F5W5]5j5|5
6#666<6O6V6n6
7A7G7\7b7r7w7
8L8V8}8
9)9j9
:!:2:s:
;*;;;|;
<'<3<D<
=&=0=<=M=
>.>?>E>R>\>h>y>
?!?4?:?R?c?i?n?z?
0)0.0:0F0W0
1,1=1C1H1T1j1o1
2*262G2
3[3o3x3
3<4`4
5$5I5t5z5
6$6D6I6p6u6
7C7I7_7e7y7
8;8@8g8l8
9;9A9V9\9t9
9 :L:R:g:m:
:1;];c;x;~;
<B<n<t<
=S=~=
>$>5>v>
212L2h2
3#323E3J3k3p3
4L5j5
6/656:6F6[6j6
7N7S7o7t7
8"8c8
91989E9Q9b9
:#:v:
;/;s;
;!<Z<q<
=!=J=a=z=
>1>Z>q>
?!?:?Q?j?
0J0a0z0
1!1J1a1
2:2Q2j2
3'4S4m4
5E5_5r5w5
676Q6d6i6
6/7P7i7
7+8]8~8
:':1:Q:V:`:l:}:
;,;;;@;R;Y;f;r;
<'<-<E<V<\<a<m<w<
=;=\=u=
>S>~>
?C?W?k?
0 040`0
1.1G1}1
1?2X2
3!3J3a3
4 4/444C4H4W4\4j4v4
5@5g5p5
6>6N6g6
7/7w7
8/8v8
9Q9^9j9{9
;6;[;e;
<:<Q<z<
<(=P=
>2>D>N>j>
?C?W?`?
(1B1n1
2(242B2
2(3T3l3x3
4(4l4
5=5U5a5r5
7,7D7P7a7
8%808C8I8^8
9'9k9
:,:4:A:Y:_:n:y:
;=;];
=D=[=g=x=
>*?A?M?^?
0@0n0
1'131D1
1&2T2k2w2
3*3j3
4<4S4_4p4
5/555J5
6E6e6
718]8q8}8
9,929A9L9_9e9z9
:-:9:J:
:1;j;
<1<j<
=1=j=
>">j>w>
>+?n?
1H1d1
1'2V2c2x2
3+3J3u3
3;4T4
4+5H5c5
5H6p6
7B7O7g7m7
8>8O8U8Z8n8{8
9"9+9D9_9{9
:L:]:c:h:|:
;';>;D;^;
;B<m<z<
=(=.=3=L=e=
>V>n>
>Z?{?
3#3B3]3w3
525N5V5j5~5
6)6Z6o6x6
728W8a8
8$9I9S9
:3:|:
;6;@;`;
<:<Q<u<
=,=G=j=
>*>A>j>{>
?J?k?
2(222<2\2a2k2w2
323<3N3S3a3l3
4!444:4O4y4
5%5n5
9'9p9
:0:y:
;(;9;
<(<4<E<
=%=*=4=@=Q=
>:>Q>z>
?%?E?J?h?m?
0/050:0F0[0j0}0
1$191K1R1j1
1A2e2
3(3/3E3K3`3f3
4E4t4z4
5&525@5
6"61696G6\6a6w6|6
7'767@7`7
8f8u8}8
8V9j9
;P<`<m<v<
="=5=;=N=T=g=m=
='>r>
? ?U?
0*0?0R0^0o0
131H1]1r1
2%2-2
6.636B6G6V6[6n6s6
7*747I7P7r7
7)8<8X8d8y8
:#:6:J:V:g:
;,;1;K;T;j;
<Q<z<
=(=o=
=I>f>
?5?e?
0"1U3o3
71:K:p:z:
<1<;<Y<y<[>X?y?
141Y1
3,4l4
4@5l5
767@7P7l7
8@8J8j8
8-9R9
:2:<:
<.<b<
=+>B>X>n>
1!1N1
122~2
2&3T3d3h3l3p3t3x3
495V5i7
0Q0l0
1.1>1q1
2"2a2
7 7N748
9@9\9t9
9':L:V:f:v:
;Y;~;
<Q<v<
=I=n=
>L>q>
?J?o?
0V0{0
1D1i1
1S2x2
344Y4q4
5B5g5
666p6
7,767F7V7f7
8<8T8
9*9B9|9
:0:j:
<L<q<
=?=d=|=
>#>3>m>
?$?`?
#0H0`0
3!3(353=3
4"4F4x4
7,8a8
959i9
:8:l:
2?3Q3
:,:?:
<">&>*>.>2>6>:>T>`>
:,:7:C:U:t:
171G1g1
2V2{2
303H3X3v3
4+4;4u4
4*5O5g5w5
6,6J6Z6
7I7n7
7#8;8K8i8y8
9.9>9z9
:>:^:
1)1F1
2B2Z2
3h3o3v3}3
7<7]:j:
;7<D<W<
0A1&4E4l4.5E5
8`8{8
8,9P9,:
1?2Y2
4!5.6S6
6*7l7
859Y9
:2:y:
;C;h;r;
;%<J<T<t<
<E=j=t=
=#>->u>
6!6:6Q6
;*<H<
<*=D=
>C>d>
?8?r?
040@0
80888]8h8t8
:L:k:w:
4 4$4(4,4044484<4@4D4H4L4P4T4X4
:$;m;
<0<\<~<
3.3?3|3
4/4;4[4g4
<9<_<
0m1z1u2
3&4D4
5@5L5p5
1)1?1U1k1
2-2x2)3Q3p3
7&7Y7~7
: ;I;n;
=$=2=E=K=
1?4L4
7%7;7Q7g7}7
>A>R>u>
>D?R?e?k?
6 6$6(6,6064686<6@6D6H6L6P6c6
<2=^=
?F?d?
0$0*0
6A8N8
9D9y9Y:r:
<>=t=
0D1t1
2o2}2
2 4-4
595R5g5
899R9g9
99:W:e:y:
<?<c<
3)4v4
5!5y5
8)9N9e9
9Z:w:
333b3
4)525p5
5D6P6e6p6
6C7]7
7,898
9)9j9
:(:e:q:
;2;?;t;
;5=V=]>~>
?W?x?
011R1
>#>M>
?7?B?J?f?n?v?
010@0
0"1`1d1h1l1p1t1x1|1
6,6c6
637O7t7
7*8y8
>&>?>K>
?*?6?
0/0;0m0
1&1{1
1[2u2
2^3m4
7 7*777k7
8*8Z8q8
?"?&?*?.?2?6?X?
162E2\2
?$?A?N?
4M5n5
:D;}<
1<2B2P6
1 1$1(1,1014181<1@1D1H1L1P1T1X1\1`1d1h1u1
4_5h5
=!=9=O=
2;4e4
9%9<9~9I:
'0>0/1F1
2e3y3
3&4}5
7 :7:
X425v6j8
8J;;=?>V>
0+1B1~1
193@3T3n3v3
6?7V7
8)8b8
:~;Y=p=H>_>
2=4T4X5o5!6>8U8
9&<=<
1'3>3
617H7?8U8
8#9@9V9}9
k0D3[3
7P7g7y:
414>8/;
"0&0*0.02060:0>0S0
1$24383<3@3D3H3L3P3
;'<;<
1%1/1
2H2L2P2T2X2\2
:6;T;[;
<0=4=8=<=@=D=
111G1
2"3g3
3 4G4v4
4)5C5]5p5t5x5|5
566Z6a6
7b7y7
8)848?8K8k8p8
9]9p9
:2:D:
:!;w;
=L=Z=q=
?O?i?
4)4S4X4
8)8S8X8
9*969B9N9Z9f9r9~9
:&:2:>:J:V:b:n:z:
;";.;:;F;R;^;j;v;
<*<6<B<N<Z<f<r<~<
=&=2=>=J=V=b=n=z=
>">.>:>F>R>^>j>v>
?*?6?B?N?
060;0H0
3'3-32383C3I3T3[3`3i3w3|3
4+434?4X4_4
5$5*515
6!6'6-63696?6F6M6T6[6b6i6p6x6
7'7?7]7b7r7
869q9
:0:::
<&<U<p<<=\=f=
>J?S?[?
0 0)0K0R0g0
1e1}1
9*979Y9E:X:
<.<5<><g<u<{<
=!=8=>=C=P=W=p=|=
<E<Q<n=u=
>$?7?U?c?
1H1O1T1X1\1`1
6%636
8$8(8,808
8<;H;T;`;
<'=`=
3d8o8|8
9 9&959<9E9K9Q9Z9a9
9G:P:Y:d:j:p:y:
;=;N;[;};
;)<S<^<t<
=!=6=
>&><>t>
>b?r?
4 4+4<4G4X4`4u4
;/<:<
=(>^>r>
1#1Q1
1#1'1+1/13171;1?1C1G1K1O1S1W1[1_1c1g1k1o1s1w1{1
2F2s2
3*3N3~3
3!4D4g4
6M6p6
7K7{7
8)9x9
9 :P:s:
;C;f;
;,<{<
=?=w=
>-?]?
0!0E0u0
2>2x2
=P>s>
455w5
5"6O6|6
737m7
859o9
:;:^:
:$;G;
>8>[>
?V?y?
0J0z0
1%2t2
5$6f6
7:7t7
7.8^8
<<<_<
</=_=
?J?z?
(0K0x0
5D5y5
6D667p7
<F<y<
=">R>
?,?<?`?
2$3G3j3
4>4|4
6@6p6
6)7Y7
7)8c8s8
0-1o1
2L3|3
7n9*;
>??w?
>=?{?
1?1l1
242~2
354j4
5*6Z6
8H8x8
:4:d:
:$;T;w;
;"<r<
<7=G=
= >0>
7&95:
4M4Y4
5"6d6
7?7o7
7*8x8
9`:!;
3)4s4
6>6a6
6!7Q7t7
7"8E8u8
8N9~9
98:[:~:
=1>k>
041n1
3.4p4
5>6a6
8H8X8b8l8v8
:*;u;
>6?t?
031c1
5B6j6
6.7N7
7-8o8
8%9s9
:B:]:
:5;M;
<!<o<
<@=p=
=$>D>
0%1f1
5.6r6
6,7v7
4^4 5j5B6
9c9u:
0#2m2
5,6b7
:2;|;
=@=c=
>3>n>
?3?V?
-0Z0}0
0*1}1l2
253v3
5+6q6
747W7
8=8w8
8$9G9j9
:#:P:}:
;K;x;
464v4
4(5c5
7$7\7
8:8t8
8<9o9
:*;M;
;<<~<
>(?j?
5.6p6
=.>o>
738m8
93:m:
<=<w<
<%=_=
>/?i?
091s1
1!2[2
8>8x8
8&9`9
<@<z<
>.?h?
081r1
829l9
:::t:
:8;h;
5B6|6
9':a:
;9<s<
0?1y1
5%6_6
9N:~:
:$;^;
<!=c=
=2>t>
>+?m?
334C4{4
787e7
8F8i8
:&;};
<0<S<
>>>a>
090y0
0"1E1r1
1&2I2l2
3D3g3
434m4
6 7b7
;3<c<
1>1n1
162f2
2"3Z3
4"5R5
9l9|9
:P:v:
;6<v<
2?3x4
5,657q7
;?<9=
1N2~2
2%3Y3
1}1C2i2
2B4h4]5
6#7c7
8_9^<5=
2)2T2"3
7@7p7
8@:x:
;;<^<
=%>}>
414k4
4)5U5k5
999q9
:!;Q;
0%1{1
1)2|2
2:3{3
=*>w>
3-4s4
8$9T9
:::m:
:';i;
=->s>
1 2P2
798n8
3-4w4
2F4v4
5;5k5
5#6c6
899{9
4X4Z5
8&9V9
:h;c<
0&1p1
3<4~4
657z7
8*9s9
0<1v1
2:3|3
405u5
>$?T?
:_;y<
<;=s=
2&4f4
>'?m?
112k2
9:9j9
<@<p<
>*?r?
0+1{1
2>2n223b3
4;4k4
5-6g6
:*;X<
91:i:
1$2\2
7!8_8
:k;b<
080p0
0<1l1
4f5C6#7
96:n:
<.<Q<
:+:Y:
=U>0?{?
031c1
?O?e?
5y5F7:8t8
?<?|?
1\2Y3
3"4\4
495z5
0W1>2-3
6C7~7
7!8[8
9N9~95:
2S2d3
<,=l=
6I6l6
6#7`7
7$8T8
9A9v9
;F;i;
?&?J?Z?~?
121B1N1p1
2*2:2F2h2x2
3"323>3`3p3|3
4*464X4h4t4
5"5.5P5`5l5
6&6H6X6d6
7@7P7\7~7
8*8N8^8
9.9R9b9
9":2:V:f:
;&;6;Z;j;
<*<:<^<n<
=.=>=b=r=
>2>B>f>v>
?6?F?j?z?
0:0J0n0~0
1>1N1r1
2W2g2
3'3h3x3
3#434W4
555m5
0$1m1
2/3y3
5;5s5
6=7u7
1D2t2
2,3d3
3)4Y4
>"?R?
0<1v1
2>3x3
8<9l9
;:<x<
>+?m?
4:5t5
;u;S<X=N>
7W8u8
8<9_9
5-5]5
7%858
=+>k>
>/?z?
0(1j1
<8<z<
===m=
=5>e>
>-?n?
081H1T1x1
8(9`9
:,;<;
</<N<m<
484W4v4
505j5
6D6c6
7&7E7d7
8=8l8
9"9A9`9
:!:@:_:~:
:);H;r;
<3<R<q<
?<?t?
2 3P3
415k5
5%6_6
8/9_9
;B=|==>
9&9E9d9
:.:M:l:
<><n<
=%=g=
=)>Y>
1$2n2
=$>4>M>z>
0J0z0
7(8h8
>(?8?\?l?
0)1Y1
3>4x4
8%9e9
:-;=;
;;=v=
>">5>t>
1*2d2
6#7q7
7)8z8
1`1p1
3*4r4
9=:V:l:
:';=;
;,<B<
<0=I=
>Y>o>
>5?Y?o?
'0=0|0
273q3
3b4{4
4d5}5
5M6q6
6O7e7
7%8;8
8'9@9S9
:):~:
:_;x;
>b>{>
>Y?o?
1N2d2
273P3
394O4
6[6q6
697R7e7
7F8\8
<$<7<
<.=G=Z=
>">l>
172w2
7"8l8
:&;n;
?5?u?
5(6r6
9U9k9
91:G:
:};D<
>3?g?
0Q0t0
4(5r5
9%989
9&:?:R:
:0;F;
;,<B<
=#=6=
>&???
0#060
3.3x3
6c7|7
8 8u8
8S9l9
:]:s:
?d?z?
5!6\6
777}7
3<4y435
7D7~7
061w1
2V3y3
3+4N4q4
5>5a5
8,8O8
:$;J;z;
<F<v<
< =P=
>'?z?
262l2
3%4U4
4 5P5
6:7p7
7D8j8
8K9q9
9(:i:
?!?*?A?J?a?q?
010A0Q0a0q0
1!111A1Q1a1q1
2!212A2Q2a2q2
3!313A3Q3a3q3
4!414A4Q4a4q4
5!515A5Q5a5q5
7!717A7Q7a7q7
7"8a8q8
9!919A9Q9a9q9
91;A;Q;a;q;
5 5$5(5,5054585<5@5D5H5L5P5T5X5\5`5d5h5l5p5t5x5|5
6 6$6(6,6064686<6@6D6H6L6P6T6X6\6`6d6h6l6p6t6x6|6
7 7$7(7,7074787<7@7D7X7d7h7
>P>T>X>\>`>d>h>l>p>t>x>|>
? ?$?(?,?0?4?8?<?@?D?H?L?P?T?X?\?`?d?h?l?p?t?x?|?
0 0$0(0,0004080<0@0D0H0L0P0T0X0\0`0d0h0l0p0t0x0|0
1 1$1(1,1014181<1@1D1H1L1P1T1X1\1`1d1h1l1p1t1x1|1
2 2$2(2,2024282<2@2D2H2L2P2T2X2\2`2d2h2l2p2t2x2|2
3 3$3(3,3034383<3@3D3H3L3P3T3X3\3`3d3h3l3p3t3x3|3
4 4$4(4,4044484<4@4D4H4L4P4T4X4\4`4d4h4l4p4t4x4|4
5 5$5(5,5054585<5@5D5H5L5P5T5X5\5`5d5h5l5p5t5x5|5
6 6$6(6,6064686<6@6D6H6L6P6T6X6\6`6d6h6l6p6t6x6|6
7 7$7(7,7074787<7@7D7H7L7P7T7X7\7`7d7h7l7p7t7x7|7
8 8$8(8,8084888<8@8D8H8L8P8T8X8\8`8d8h8l8p8t8x8|8
9 9$9(9,9094989<9@9D9H9L9P9T9X9\9`9d9h9l9p9t9x9|9
: :$:(:,:0:4:8:<:@:D:H:L:P:T:X:\:`:d:h:l:p:t:x:|:
; ;$;(;,;0;4;8;<;@;D;H;L;P;T;X;\;`;d;h;l;p;t;x;|;
< <$<(<,<0<4<8<<<@<D<H<L<P<T<X<\<`<d<h<l<p<t<x<|<
= =$=(=,=0=4=8=<=@=D=H=L=P=T=X=\=`=d=h=l=p=t=x=|=
> >$>(>,>0>4>8><>@>D>H>L>P>T>X>\>`>d>h>l>p>t>x>|>
? ?$?(?,?0?4?8?<?@?D?H?L?P?T?X?\?`?d?h?l?p?t?x?|?
0 0$0(0,0004080<0@0D0H0L0P0T0X0\0`0d0h0l0p0t0x0|0
1 1$1(1,1014181<1@1D1H1L1P1T1X1\1`1d1h1l1p1t1x1|1
2 2$2(2,2024282<2@2D2H2L2P2T2X2\2`2d2h2l2p2t2x2|2
3 3$3(3,3034383<3@3D3H3L3P3T3X3\3`3d3h3l3p3t3x3|3
4 4$4(4,4044484<4@4D4H4L4P4T4X4\4`4d4h4l4p4t4x4|4
5 5$5(5,5054585<5@5D5H5L5P5T5X5\5`5d5h5l5p5t5x5|5
6 6$6(6,6064686<6@6D6H6L6P6T6X6\6`6d6h6l6p6t6x6|6
7 7$7(7,7074787<7@7D7H7L7P7T7X7\7`7d7h7l7p7t7x7|7
8 8$8(8,8084888<8@8D8H8L8P8T8X8\8`8d8h8l8p8t8x8|8
9 9$9(9,9094989<9@9D9H9L9P9T9X9\9`9d9h9l9p9t9x9|9
: :$:(:,:0:4:8:<:@:D:H:L:P:T:X:\:`:d:h:l:p:t:x:|:
; ;$;(;,;0;4;8;<;@;D;H;L;P;T;X;\;`;d;h;l;p;t;x;|;
< <$<(<,<0<4<8<<<@<D<H<L<P<T<X<\<`<d<h<l<p<t<x<|<
= =$=(=,=0=4=8=<=@=D=H=L=P=T=X=\=`=d=h=l=p=t=x=|=
> >$>(>,>0>4>8><>@>D>H>L>P>T>X>\>`>d>h>l>p>t>x>|>
? ?$?(?,?0?4?8?<?@?D?H?L?P?T?X?\?`?d?h?l?p?t?x?|?
0 0$0(0,0004080<0@0D0H0L0P0T0X0\0`0d0h0l0p0t0x0|0
1 1$1(1,1014181<1@1D1H1L1P1T1X1\1`1d1h1l1p1t1x1|1
2 2$2(2,2024282<2@2D2H2L2P2T2X2\2`2d2h2l2p2t2x2|2
3 3$3L3T3X3\3d3h3l3p3
40484<4@4H4L4P4T4|4
5 5$5,5054585`5h5l5p5x5|5
6@6D6L6P6T6\6`6d6h6
7(7074787@7D7H7L7t7|7
8$8(8,808X8`8d8h8p8t8x8|8
9<9D9H9L9T9X9\9`9
9 :(:,:0:8:<:@:D:l:t:x:|:
; ;$;(;P;X;\;`;h;l;p;t;
<4<<<@<D<L<P<T<X<
= =$=(=0=4=8=<=d=l=p=t=|=
> >H>P>T>X>`>d>h>l>
?(?,?4?8?<?D?H?L?P?t?x?
0 0(0,00040\0d0h0l0t0x0|0
1@1H1L1P1X1\1`1d1
1$2,20242<2@2D2H2p2x2|2
3 3$3(3,3T3\3`3d3l3p3t3x3
484@4D4H4P4T4X4\4
5$5(5,54585<5@5h5p5t5x5
6 6$6L6T6X6\6d6h6l6p6
70787<7@7H7L7P7T7|7
8 8$8,8084888`8h8l8p8x8|8
9D9L9P9T9\9`9d9h9
:(:0:4:8:@:D:H:L:t:|:
;$;(;,;0;X;`;d;h;p;t;x;|;
<<<D<H<L<T<X<\<`<
< =(=,=0=8=<=@=D=l=t=x=|=
> >$>(>P>X>\>`>h>l>p>t>
?4?<?@?D?L?P?T?X?
0 0$0(0004080<0d0l0p0t0|0
1 1H1P1T1X1`1d1h1l1
2,24282<2D2H2L2P2x2
3 3(3,30343\3d3h3l3t3x3|3
4@4H4L4P4X4\4`4d4
4$5,50545<5@5D5H5p5x5|5
6 6$6(6,6T6\6`6d6l6p6t6x6
787@7D7H7P7T7X7\7
8$8(8,84888<8@8h8p8t8x8
9 9$9L9T9X9\9d9h9l9p9
:0:8:<:@:H:L:P:T:|:
; ;$;,;0;4;8;`;h;l;p;x;|;
<D<L<P<T<\<`<d<h<
=(=0=4=8=@=D=H=L=t=|=
>$>(>,>0>X>`>d>h>p>t>x>|>
?<?D?H?L?T?X?\?`?
 0(0,00080<0@0D0h0l0t0x0|0
1 1$1(1P1X1\1`1h1l1p1t1
242<2@2D2L2P2T2X2
3 3$3(3034383<3d3l3p3t3|3
4 4H4P4T4X4`4d4h4l4
5,54585<5D5H5L5P5x5
6 6(6,60646\6d6h6l6t6x6|6
7@7H7L7P7X7\7`7d7
7$8,80848<8@8D8H8p8x8|8
9 9$9(9,9T9\9`9d9l9p9t9x9
:8:@:D:H:P:T:X:\:
;$;(;,;4;8;<;@;h;p;t;x;
< <$<L<T<X<\<d<h<l<p<
=0=8=<=@=H=L=P=T=|=
> >$>,>0>4>8>`>h>l>p>x>|>
?D?L?P?T?\?`?d?h?
0(0004080@0D0H0L0t0|0
1$1(1,101X1`1d1h1p1t1x1|1
2<2D2H2L2T2X2\2`2
2 3(3,30383<3@3D3l3t3x3|3
4 4$4(4P4X4\4`4h4l4p4t4
545<5@5D5L5P5T5X5
6 6$6(6064686<6d6l6p6t6|6
7 7H7P7T7X7`7d7h7l7
8,84888<8D8H8L8P8x8
9 9(9,90949\9d9h9l9t9x9|9
:@:H:L:P:X:\:`:d:
:$;,;0;4;<;@;D;H;p;x;|;
< <$<(<,<T<\<`<d<l<p<t<x<
=8=@=D=H=P=T=X=\=
>$>(>,>4>8><>@>h>p>t>x>
? ?$?L?T?X?\?d?h?l?p?
00080<0@0H0L0P0T0|0
1 1$1,1014181`1h1l1p1x1|1
2D2L2P2T2\2`2d2h2
3(3034383@3D3H3L3t3|3
4$4(4,404X4`4d4h4p4t4x4|4
5<5D5H5L5T5X5\5`5
5 6(6,60686<6@6D6l6t6x6|6
7 7$7(7P7T7X7\7`7d7h7l7p7t7x7|7
8 8$8(8,8084888<8@8D8H8L8P8T8X8\8`8d8h8l8p8t8x8|8
9 9$9(9,9094989<9@9D9H9L9P9T9X9\9`9d9h9l9p9t9x9|9
: :$:(:,:0:4:8:<:@:D:H:L:P:T:X:\:`:d:h:l:p:t:x:|:
; ;$;(;,;0;4;8;<;@;D;H;L;P;T;X;\;`;d;h;p;t;x;|;
< <$<(<,<0<4<8<<<@<D<H<L<P<T<X<\<`<d<h<l<p<t<x<|<
= =$=(=,=0=4=8=<=@=D=H=L=P=T=X=\=`=d=h=l=p=t=x=|=
> >$>(>,>0>4>8><>@>D>H>L>P>T>X>\>`>d>h>l>p>t>x>|>
? ?$?(?,?0?4?8?<?@?D?H?L?P?T?X?\?`?d?h?l?p?t?x?|?
0 0$0(0,0004080<0@0D0H0L0P0T0X0\0`0d0h0l0p0t0x0|0
1 1$1(1,1014181<1@1D1H1L1P1T1X1\1`1d1h1l1p1t1x1|1
2 2$2(2,2024282<2@2D2H2L2P2T2X2\2`2d2h2l2p2t2x2|2
3 3$3(3,3034383<3@3H3L3T3X3`3d3l3p3x3|3
4 4$4,40484<4@4D4H4L4P4T4X4\4`4d4h4l4p4t4x4|4
5 5$5(5,5054585<5@5D5H5L5P5T5X5\5`5d5h5l5p5t5x5|5
6 6$6(6,6064686<6@6D6H6L6P6T6X6\6`6d6h6l6p6
7 7$7(7,7074787<7@7D7H7L7P7T7X7\7`7d7h7l7p7t7x7|7
8 8$8(8,8084888<8@8D8H8L8P8T8X8\8`8d8h8l8p8t8x8|8
9 9$9(9,9094989<9@9D9H9L9P9T9X9\9`9d9h9l9p9t9x9|9
: :$:(:,:0:4:8:<:@:D:H:L:P:T:X:\:`:d:h:l:p:t:x:|:
; ;$;(;,;0;4;8;<;@;D;H;L;P;T;X;\;`;d;h;l;p;t;x;|;
< <$<(<,<0<4<8<<<@<D<H<L<P<T<X<\<`<d<h<l<p<t<x<|<
= =$=(=,=0=4=8=<=@=D=H=L=P=T=X=\=`=d=h=l=p=t=x=|=
> >$>(>,>0>4>8><>@>D>H>L>P>T>X>\>`>d>h>l>p>t>x>|>
? ?$?(?,?0?4?8?<?@?D?H?L?P?T?X?\?`?d?h?l?p?t?x?|?
0 0$0(0,0004080<0@0D0H0L0P0T0X0\0`0d0h0l0p0t0x0|0
1 1$1(1,1014181<1@1D1H1L1P1T1X1\1`1d1h1l1p1t1x1|1
2 2$2(2,2024282<2@2D2H2L2P2T2X2\2`2d2h2l2p2t2x2|2
3 3$3(3,3034383<3@3D3H3L3P3T3X3\3`3d3h3l3p3t3x3|3
4 4$4(4,4044484<4@4D4H4L4P4T4X4\4`4d4h4l4p4t4x4|4
5 5$5(5,5054585<5@5D5H5L5P5T5X5\5`5d5h5l5p5t5x5|5
6 6$6(6,6064686<6@6D6H6L6P6T6X6\6`6d6h6l6p6t6x6|6
7 7$7(7,7074787<7@7D7H7L7P7T7X7\7`7d7h7l7p7t7x7|7
8 8$8(8,8084888<8@8D8H8L8P8T8X8\8`8d8h8l8p8t8x8|8
9 9$9(9,9094989<9@9D9H9L9P9T9X9\9`9d9h9l9p9t9x9|9
: :$:(:,:0:4:8:<:@:D:H:L:P:T:X:\:`:d:h:l:p:t:x:|:
; ;$;(;,;0;4;8;<;@;D;H;L;P;T;X;\;`;d;h;l;p;t;x;|;
< <$<(<,<0<4<8<<<@<D<H<L<P<T<X<\<`<d<h<l<p<t<x<|<
= =$=(=,=0=4=8=<=@=D=H=L=P=T=X=\=`=d=h=l=p=t=x=|=
> >$>(>,>0>4>8><>@>D>H>L>P>T>X>\>`>d>h>l>p>t>x>|>
? ?$?(?,?0?4?8?<?@?D?H?L?P?T?X?\?`?d?h?l?p?t?x?|?
0 0$0(0,0004080<0@0D0H0L0P0T0X0\0`0d0h0l0p0t0x0|0
1 1$1(1,1014181<1@1D1H1L1P1T1X1\1`1d1h1l1p1t1x1|1
2 2$2(2,2024282<2@2D2H2L2P2T2X2\2`2d2h2l2p2t2x2|2
3 3$3(3,3034383<3@3D3H3L3P3T3X3\3`3d3h3l3p3t3x3|3
4 4$4(4,4044484<4@4D4H4L4P4T4X4\4`4d4h4l4p4t4x4|4
5 5$5(5,5054585<5@5D5H5L5P5T5X5\5`5d5h5l5p5t5x5|5
6 6$6(6,6064686<6@6D6H6L6P6T6X6\6`6d6h6l6p6t6x6|6
7 7$7(7,7074787<7@7D7H7L7P7T7X7\7`7d7h7l7p7t7x7|7
8 8$8(8,8084888<8@8D8H8L8P8T8X8\8`8d8h8l8p8t8x8|8
9 9$9(9,9094989<9@9D9H9L9P9T9X9\9`9d9h9l9p9t9x9|9
: :$:(:,:0:4:8:<:@:D:H:L:P:T:X:\:`:d:h:l:p:t:x:|:
; ;$;(;,;0;4;8;<;@;D;H;L;P;T;X;\;`;d;h;l;p;t;x;|;
< <$<(<,<0<4<8<<<@<D<H<L<P<T<X<\<`<d<h<l<p<t<x<|<
= =$=(=,=0=4=8=<=@=D=H=L=P=T=X=\=`=d=h=l=p=t=x=|=
> >$>(>,>0>4>8><>@>D>H>L>P>T>X>\>`>d>h>l>p>t>x>|>
? ?$?(?,?0?4?8?<?@?D?H?L?P?T?X?\?`?d?h?l?p?t?x?|?
0 0$0(0,0004080<0@0D0H0L0P0T0X0\0`0d0h0l0p0t0x0|0
1 1$1(1,1014181<1@1D1H1L1P1T1X1\1`1d1h1l1p1t1x1|1
2 2$2(2,2024282<2@2D2H2L2P2T2X2\2`2d2h2l2p2t2x2|2
3 3$3(3,3034383<3@3D3H3L3P3T3X3\3`3d3h3l3p3t3x3|3
4 4$4(4,4044484<4@4D4H4L4P4T4X4\4`4d4h4l4p4t4x4|4
5 5$5(5,5054585<5@5D5H5L5P5T5X5\5`5d5h5l5p5t5x5|5
6 6$6(6,6064686<6@6D6H6L6P6T6X6\6`6d6h6l6p6t6x6|6
7 7$7(7,7074787<7@7D7H7L7P7T7X7\7`7d7h7l7p7t7x7|7
8 8$8(8,8084888<8@8D8H8L8P8T8X8\8`8d8h8l8p8t8x8|8
9 9$9(9,9094989<9@9D9H9L9P9T9X9\9`9d9h9l9p9t9x9|9
: :$:(:,:0:4:8:<:@:D:H:L:P:T:X:\:`:d:h:l:p:t:x:|:
; ;$;(;,;0;4;8;<;@;D;H;L;P;T;X;\;`;d;h;l;p;t;x;|;
< <$<(<,<0<4<8<<<@<D<H<L<P<T<X<\<`<d<h<l<p<t<x<|<
= =$=(=,=0=4=8=<=@=D=H=L=P=T=X=\=`=d=h=l=p=t=x=|=
> >$>(>,>0>4>8><>@>D>H>L>P>T>X>\>`>d>h>l>p>t>x>|>
? ?$?(?,?0?4?8?<?@?D?H?L?P?T?X?\?`?d?h?l?p?t?x?|?
0 0$0(0,0004080<0@0D0H0L0P0T0X0\0`0d0h0l0p0t0x0|0
1 1$1(1,1014181<1@1D1H1L1P1T1X1\1`1d1h1l1p1t1x1|1
2 2$2(2,2024282<2@2D2H2L2P2T2X2\2`2d2h2l2p2t2x2|2
3 3$3(3,3034383<3@3D3H3L3P3T3X3\3`3d3h3l3p3t3x3|3
4 4$4(4,4044484<4@4D4H4L4P4T4X4\4`4d4h4l4p4t4x4|4
5 5$5(5,50545@5D5H5L5P5T5X5\5`5d5h5l5p5t5x5|5
6 6$6(6,6064686<6@6D6H6L6P6T6X6\6`6d6h6l6p6t6x6|6
7 7$7(7,7074787<7@7D7H7L7P7T7X7\7`7d7h7l7p7t7x7|7
8 8$8(8,8084888<8@8D8H8L8P8T8X8\8`8d8h8l8p8t8x8|8
9 9$9(9,9094989<9@9D9H9L9P9T9X9\9`9d9h9l9p9t9x9|9
: :$:(:,:0:4:8:<:@:D:H:L:P:T:X:\:`:d:h:l:p:t:x:|:
; ;$;(;,;0;4;8;<;@;D;H;L;P;T;X;\;`;d;h;l;p;t;x;|;
< <$<(<,<0<4<8<<<@<D<H<L<P<T<X<\<`<d<h<l<p<t<x<|<
= =$=(=,=0=4=8=<=@=D=H=L=P=T=X=\=`=d=h=l=p=t=x=|=
> >$>(>,>0>4>8><>@>D>H>L>P>T>X>\>`>d>h>l>p>t>x>|>
? ?$?(?,?0?4?8?<?@?D?H?L?P?T?X?\?`?d?h?l?p?t?x?|?
0 0$0(0,0004080<0@0D0H0L0P0T0X0\0`0d0h0l0p0t0x0|0
1 1$1(1,1014181<1@1D1H1L1P1T1X1\1`1d1h1l1p1t1x1|1
2 2$2(2,2024282<2@2D2H2L2P2T2X2\2`2d2h2l2p2t2x2|2
3 3$3(3,3034383<3@3D3H3L3P3T3X3\3`3d3h3l3p3t3x3|3
4 4$4(4,4044484<4@4D4H4L4P4T4X4\4`4d4h4l4p4t4x4|4
5 5$5(5,5054585<5@5D5H5L5P5T5X5\5`5d5h5l5p5t5x5|5
6 6$6(6,6064686<6@6D6H6L6P6T6X6\6`6d6h6l6p6t6x6|6
7 7$7(7,7074787<7@7D7H7L7P7T7X7\7`7d7h7l7p7t7x7|7
8 8$8(8,8084888<8@8D8H8L8P8T8X8\8`8d8h8l8p8t8x8|8
9 9$9(9,9094989<9@9D9H9L9P9T9X9\9`9d9h9l9p9t9x9|9
: :$:(:,:0:4:8:<:@:D:H:L:P:T:X:\:`:d:h:l:p:t:x:|:
; ;$;(;,;0;4;8;<;@;D;H;L;P;T;X;\;`;d;h;l;p;t;x;|;
< <$<(<,<0<4<8<<<@<D<H<L<P<T<X<\<`<d<h<l<p<t<x<|<
= =$=(=,=0=4=8=<=@=D=H=L=P=T=X=\=`=d=h=l=p=t=x=|=
> >$>(>,>0>4>8><>@>D>H>L>P>T>X>\>`>d>h>l>p>t>x>|>
? ?$?(?,?0?4?8?<?@?D?H?L?P?T?X?\?`?d?h?l?p?t?x?|?
0 0$0(0,0004080<0@0D0H0L0P0T0X0\0`0d0h0l0p0t0x0|0
1 1$1(1,1014181<1@1D1H1L1P1T1X1\1`1d1h1l1p1t1x1|1
2 2$2(2,2024282<2@2D2H2L2P2T2X2\2`2d2h2l2p2t2x2|2
3 3$3(3,3034383<3@3D3H3L3P3T3X3\3`3d3h3l3p3t3x3|3
4 4$4(4,4044484<4@4D4H4L4P4T4X4\4`4d4h4l4p4t4x4|4
5 5$5(5,5054585<5@5D5H5L5P5T5X5\5`5d5h5l5p5t5x5|5
6 6$6(6,6064686<6@6D6H6L6P6T6X6\6`6d6h6l6p6t6x6|6
7 7$7(7,7074787<7@7D7H7L7P7T7X7\7`7d7h7l7p7t7x7|7
8 8$8(8,8084888<8@8D8H8L8P8T8X8\8`8d8h8l8p8t8x8|8
9 9$9(9,9094989<9@9D9H9L9P9T9X9\9`9d9h9l9p9t9x9|9
: :$:(:,:0:4:8:<:@:D:H:L:P:T:X:\:`:d:h:l:p:t:x:|:
; ;$;(;,;0;4;8;<;@;D;H;L;P;T;X;\;`;d;h;l;p;t;x;|;
< <$<(<,<0<4<8<<<@<D<H<L<P<T<X<\<`<d<h<l<p<t<x<|<
= =$=(=,=0=4=8=<=@=D=H=L=P=T=X=\=`=d=h=l=p=t=x=|=
> >$>(>,>0>4>8><>@>D>H>L>P>T>X>\>`>d>h>l>p>t>x>|>
? ?$?(?,?0?4?8?<?@?D?H?L?P?T?X?\?`?d?h?l?p?t?x?|?
0 0$0(0,0004080<0@0D0H0L0P0T0X0\0`0d0h0l0p0t0x0|0
1 1$1(1,1014181<1@1D1H1L1P1T1X1\1`1d1h1l1p1t1x1|1
2 2$2(2,2024282<2@2D2H2L2P2T2X2\2`2d2h2l2p2t2x2|2
3 3$3(3,3034383<3@3D3H3L3P3T3X3\3`3d3h3l3p3t3x3|3
4 4$4(4,4044484<4@4D4H4L4P4T4X4\4`4d4h4l4p4t4x4|4
5 5$5(5,5054585<5@5D5H5L5P5T5X5\5`5d5h5l5p5t5x5|5
6 6$6(6,6064686<6@6D6H6L6P6T6X6\6`6d6h6l6p6t6x6|6
7 7$7(7,7074787<7@7D7H7L7P7T7X7\7`7d7h7l7p7t7x7|7
8 8$8(8,8084888<8@8D8H8L8P8T8X8\8`8d8h8l8p8t8x8|8
9 9$9(9,9094989<9@9D9H9L9P9T9X9\9`9d9h9l9p9t9x9|9
: :$:(:,:0:4:8:<:@:D:H:L:P:T:X:\:`:d:h:l:p:t:x:|:
; ;$;(;,;0;4;8;<;@;D;H;L;P;T;X;\;`;d;h;l;p;t;x;|;
< <$<(<,<0<4<8<<<@<D<H<L<P<T<X<\<`<d<h<l<p<t<x<|<
= =$=(=,=0=4=8=<=@=D=H=L=P=T=X=\=`=d=h=l=p=t=x=|=
> >$>(>,>0>4>8><>@>D>H>L>P>T>X>\>`>d>h>l>p>t>x>|>
? ?$?(?,?0?4?8?<?@?D?H?L?P?T?X?\?`?d?h?l?p?t?x?|?
0 0$0(0,0004080<0@0D0H0L0P0T0X0\0`0d0h0l0p0t0x0|0
1 1$1(1,1014181<1@1D1H1L1P1T1X1\1`1d1h1l1p1t1x1|1
2 2$2(2,2024282<2@2D2H2L2P2T2X2\2`2d2h2l2p2t2x2|2
3 3$3(3,3034383<3@3D3H3L3P3T3X3\3`3d3h3l3p3t3x3|3
4 4$4(4,4044484<4@4D4H4L4P4T4X4\4`4d4h4l4p4t4x4|4
5 5$5(5,5054585<5@5D5H5L5P5T5X5\5`5d5h5l5p5t5x5|5
6 6$6(6,6064686<6@6D6H6L6P6T6X6\6`6d6h6l6p6t6x6|6
7 7$7(7,7074787<7@7D7H7L7P7T7X7\7`7d7h7l7p7t7x7|7
8 8$8(8,8084888<8@8D8H8L8P8T8X8\8`8d8h8l8p8t8x8|8
9 9$9(9,9094989<9@9D9H9L9P9T9X9\9`9d9h9l9p9t9x9|9
: :$:(:,:0:4:8:<:@:D:H:L:P:T:X:\:`:d:h:l:p:t:x:|:
; ;$;(;,;0;4;8;<;@;D;H;L;P;T;X;\;`;d;h;l;p;t;x;|;
< <$<(<,<0<4<8<<<@<D<H<L<P<T<X<\<`<d<h<l<p<t<x<|<
= =$=(=,=0=4=8=<=@=D=H=L=P=T=X=\=`=d=h=l=p=t=x=|=
> >$>(>,>0>4>8><>@>D>H>L>P>T>X>\>`>d>h>l>p>t>x>|>
? ?$?(?,?0?4?8?<?@?D?H?L?P?T?X?\?`?d?h?l?p?t?x?|?
0 0$0(0,0004080<0@0D0H0L0P0T0X0\0`0d0h0l0p0t0x0|0
1 1$1(1,1$7(7,707
4$4,444<4D4L4T4\4d4l4t4|4
5$5,545<5D5L5T5\5d5l5t5|5
6$6,646<6D6L6T6\6d6l6t6|6
?(?8?H?X?h?x?
0 000@0P0`0p0
1,101@1L1\1l1t1
2(282H2X2h2x2
3(3,3<3H3X3h3x3
4 404@4P4`4h4x4|4
5 505<5L5\5l5|5
6$646D6T6h6x6
7 7$747@7P7`7p7
8 808@8P8`8p8
9 909@9H9X9\9l9p9
:,:<:D:T:`:p:
; ;0;@;P;`;p;x;
< <0<4<D<H<X<h<x<
= =0=@=P=`=p=
> >0>@>H>X>\>l>
?(?8?@?P?T?d?h?x?
0(080H0X0h0x0
1 10141D1P1`1p1
2 20282H2X2h2x2
3 30343D3H3X3h3x3
4 4$444@4P4`4h4x4
5(585H5X5h5x5
6 606@6P6`6p6
7 707@7P7`7p7
8 8$84888H8X8h8x8
9(989H9X9h9x9
:$:4:8:H:T:d:t:
; ;0;@;P;`;p;
<(<8<H<P<`<l<|<
=(=,=<=H=X=h=x=
>(>0>@>D>T>X>h>l>|>
? ?0?@?P?`?p?x?
0(080H0X0h0x0
1(181H1X1h1x1
2(282H2X2h2x2
3(383H3X3h3x3
4(484H4X4h4x4
5(585H5X5h5p5
6(6,6<6@6P6\6l6|6
7$7(787<7L7P7`7p7
8(888@8P8`8p8
9(989H9X9h9x9
:(:8:H:X:h:x:
; ;0;@;P;`;p;
<(<,<<<H<X<h<p<
= =0=@=P=X=h=t=
> >,><>L>T>d>x>
?(?8?H?P?`?d?t?
0 0,0<0P0`0p0
1(181H1X1h1x1
2 202@2P2`2p2
3$343D3L3\3h3x3
4 4$444@4P4X4h4x4
5(585H5P5`5p5
6 6(686D6T6l6|6
7(707@7L7\7d7t7x7
8$848D8X8h8x8
9(989H9X9h9x9
:,:<:L:\:l:t:
; ;$;4;
30444H4L4X4\4`4l4p4
50545H5L5P5
l1t1|1
D3H3L3
L2P2T2X2
4686<6@6D6H6L6P6T6X6\6`6d6h6l6`<h<x<
= =(=8=@=P=X=h=p=
>(>0>@>H>X>`>p>x>
? ?0?8?H?P?`?h?x?
2 2(282@2H2P2`2h2p2
3(30383@3H3X3h3p3
4 4(40484@4H4P4`4h4x4
5 50585H5P5`5h5x5
6(686@6H6X6`6h6p6x6
7 7(707@7H7X7`7h7x7
8 80888H8P8X8`8h8p8
9 9(90989@9H9P9X9`9p9x9
: :0:@:H:X:`:p:x:
; ;0;8;@;H;P;`;h;p;x;
< <(<0<@<P<X<`<p<x<
=(=0=@=H=X=`=p=x=
> >0>8>H>P>`>h>p>
? ?(?0?@?P?X?`?p?x?
0(000@0H0P0X0`0h0p0x0
1 1(181@1P1X1h1p1
2(202@2H2X2`2p2
3 303@3H3P3`3h3x3
,>0>4>8><>@>D>H>L>P>T>X>\>`>
@>D>H>L>d>h>x>|>
?$?<?L?P?`?d?t?x?|?
0(0,0@0D0X0\0`0h0
1 1$1<1@1X1\1`1d1x1|1
2$242D2H2X2h2l2p2t2x2
3 383<3@3T3d3h3
4$44484<4@4D4H4P4X4`4d4h4|4
5 5$5(5,50545<5@5D5H5L5T5l5|5
6$6(6,6D6H6`6d6|6
7 787<7@7D7H7L7T7X7\7`7d7h7|7
8 8$8(8,848L8\8l8p8
9 9(9,90949<9@9D9H9L9T9l9|9
: :0:4:8:<:@:T:d:t:
;0;4;L;\;`;d;|;
< <$<(<,<4<8<<<@<T<X<h<l<p<t<x<|<
= =(=@=P=T=d=h=
> >$><>@>X>h>l>p>t>x>
?$?(?,?0?4?8?@?X?h?l?p?t?x?|?
0 00040L0P0T0X0\0`0h0l0p0t0x0
1 1$1(1,10181P1`1d1t1x1|1
2 2$282<2L2P2`2p2t2
3 30343D3H3L3d3h3
4 4$4(4<4@4P4T4X4p4t4
5 5$5(5,545L5P5h5l5
6(6,6D6H6`6d6h6l6t6
,;L;T;\;h;
<,<4<@<`<l<
<,=`=
><>H>h>t>
? ?,?4?L?p?
0L0\0h0p0
1$1,181X1d1
2<2H2
2@3`3l3
4(4L4\4h4p4
5$505P5X5`5h5t5
6$60686P6t6
7\7|7
848@8x8
9(9H9T9t9
: :(:0:<:\:d:p:
:L;T;|;
<0<8<D<d<l<t<|<
=<=H=h=t=
>$>0>T>\>d>l>t>|>
>L?T?
0d0l0
101P1\1|1
2$2D2L2X2x2
3$3,343<3D3P3p3x3
484@4H4P4X4d4
5$5,545<5D5L5T5\5d5l5t5|5
6 6(60686D6d6l6t6|6
747<7D7L7T7\7d7l7t7|7
80888@8H8T8t8
9 9,9L9T9\9h9
:4:<:D:L:T:\:h:
;,;8;X;`;h;t;
<$<0<8<X<t<|<
=8=T=\=d=l=t=
=$>,>4><>D>P>p>x>
?0?8?@?H?P?\?d?
0 0(00080@0L0l0t0|0
1$1,141<1D1L1T1\1d1t1|1
2,242<2D2P2X2|2
3(303P3X3`3|3
4$40484X4`4h4p4
5$5,545<5H5P5
6 6@6\6d6l6t6|6
6,747<7D7L7X7`7
7(8L8T8\8d8l8t8|8
9(9L9T9\9d9l9t9|9
:@:T:\:d:l:t:|:
; ;(;8;@;d;t;|;
<<<D<`<h<p<x<
=,=4=<=X=`=h=p=x=
>,>4><>D>`>h>p>x>
?<?D?L?h?p?x?
040<0X0`0h0p0x0
0(10181@1H1T1\1
2$2,2`2h2p2x2
20383@3H3P3\3d3
4 4(404<4\4h4p4
5$5,545<5D5P5p5x5
646P6X6`6h6p6|6
70787@7H7P7\7d7
8 8(80888D8L8
9P9X9`9h9p9|9
9 :(:0:8:@:L:T:
;<;H;P;
<D<L<T<\<d<p<
<,=4=<=D=L=X=`=
>,><>D>L>T>\>d>l>|>
?L?T?\?d?l?x?
0$0,040<0H0P0p0x0
1$10181X1
1(2<2D2L2T2\2d2l2x2
343<3`3p3|3
404T4d4p4
5,545<5H5h5p5x5
6$606T6\6d6l6t6|6
7$7,747<7D7L7T7\7h7
8 8,8L8T8\8d8l8t8|8
989@9L9l9t9|9
9$:4:@:H:|:
;$;,;4;<;D;L;X;x;
<0<8<@<H<P<X<`<l<
=<=D=L=T=\=d=p=
>$>X>h>t>
? ?@?H?T?t?
0<0H0h0p0|0
1(1L1\1h1p1
2(202P2l2t2|2
3,343<3D3L3T3\3d3l3t3|3
4$4,444<4D4L4X4x4
5$505T5\5d5l5t5|5
6$6,646<6D6L6T6\6d6l6t6|6
7,747@7H7|7
8$8,848<8D8L8T8\8d8l8t8|8
9(949T9\9h9
:$:,:8:X:d:
;<;L;X;|;
<$<,<8<X<`<l<
=0=8=D=d=p=
> >@>H>P>\>|>
?$?D?`?
000<0\0h0
1$1,181t1
2$2,242<2D2P2p2x2
383T3\3d3l3t3
3,444<4D4L4X4`4
5$50585X5t5|5
6 6,6L6X6x6
7(707X7`7h7p7
888@8L8l8x8
9$9D9P9p9x9
: :(:0:8:@:L:T:t:
;$;,;4;<;D;L;T;`;
<P<`<l<
=$=D=L=X=x=
><>D>L>T>\>d>l>x>
? ?(?8?\?d?l?t?|?
0(0H0T0t0|0
1 1(10181D1d1l1t1|1
2(2L2T2\2d2l2t2|2
3$3,343@3`3p3
4$4,444<4D4L4T4\4d4l4x4
5$5,545<5D5L5T5\5d5l5t5|5
6,646<6H6h6p6x6
7$7,747<7D7L7T7\7d7l7t7|7
8$8,848<8D8L8T8\8d8l8t8|8
9$9,949<9D9L9T9\9d9l9t9|9
:$:4:@:d:l:t:|:
;$;0;P;X;`;h;x;
<,<4<<<D<L<T<d<l<t<|<
=0=8=@=H=P=\=|=
>$>,>4><>D>L>T>\>d>l>t>|>
? ?D?L?T?\?d?l?t?|?
0$0,040@0`0h0t0
1$10181\1d1t1|1
2$2,242<2D2L2T2\2d2l2t2|2
3$303P3\3|3
4<4D4L4T4\4d4l4t4|4
5 5D5L5T5\5l5t5|5
6 6@6H6P6\6|6
7$7,747<7D7P7p7x7
8<8D8L8T8\8d8p8
9$949@9`9h9t9
:0:8:D:d:l:x:
;4;@;`;h;t;
< <(<L<\<h<p<
=$=,=4=@=`=l=
>(>4>T>\>d>p>
?(?4?T?`?
0 0@0H0P0\0|0
141D1P1X1p1
282@2H2T2t2|2
3$3,3D3T3p3
4 4@4L4l4x4
545<5L5X5x5
6$6,646<6D6L6T6\6d6l6t6|6
7$7,747<7D7L7X7`7
8$8,848<8D8L8T8\8d8l8t8|8
9$9,949<9D9L9T9\9d9l9t9|9
:$:,:4:<:D:L:T:\:d:l:t:|:
;(;H;P;X;`;h;x;
<$<,<4<<<D<L<T<\<d<l<t<|<
=$=,=4=<=D=L=T=\=d=l=t=|=
>4><>D>L>T>\>d>l>t>|>
?$?0?T?\?d?l?t?|?
0$0,040<0D0L0T0\0d0l0t0|0
1$1,141<1D1L1T1\1d1l1t1|1
2 2D2L2T2\2d2l2t2|2
3$3,343<3D3L3T3\3d3l3t3|3
4$4,444<4D4L4T4\4d4l4t4|4
5$5,545<5D5L5T5\5d5l5t5|5
6(6L6T6\6d6l6t6|6
787@7L7l7t7
8$8,848<8L8T8d8p8
9 9(949T9\9h9
90:@:L:l:t:
;,;8;@;d;x;
<8<@<L<l<t<|<
= =(=0=<=\=d=l=t=
><>D>L>T>\>d>l>t>|>
?$?,?8?X?`?h?p?x?
0$0,040@0`0l0t0
181@1H1P1X1h1
2 2(202@2d2l2t2|2
343<3D3L3T3\3d3l3
4$4,444<4D4L4T4\4d4p4
5$5,545<5D5L5T5\5d5l5t5|5
6<6D6L6X6x6
7(7H7T7t7|7
8 8,848h8x8
989@9H9T9t9|9
:,:4:@:H:|:
; ;(;8;D;L;T;\;d;l;|;
<$<D<L<T<`<
=$=,=8=@=`=|=
>8>L>T>t>
?8?@?P?t?|?
0 0(00080@0P0t0|0
1$1,1D1h1x1
2(282D2d2p2
3$3,343<3D3L3T3`3
484D4d4l4t4|4
5 505<5D5x5
6(606d6t6
7,787@7`7h7p7x7
7(888H8P8t8
949D9P9X9
:<:D:L:T:\:d:l:t:
;$;D;P;p;x;
<$<0<P<X<`<p<
=$=,=8=X=`=h=p=|=
> >@>H>T>t>|>
?(?4?T?`?
0 0D0L0T0\0d0
141<1D1L1T1\1d1l1t1|1
2 2,2L2T2\2h2
343H3\3
4<4H4h4t4
585@5H5T5t5|5
6<6D6L6T6\6d6l6t6|6
707<7\7d7l7t7|7
8(80888D8d8l8t8|8
9 9@9H9P9X9`9h9p9|9
:$:,:4:<:D:L:T:\:h:
;$;,;4;@;`;h;p;|;
<(<H<P<X<`<l<
=$=,=4=<=D=P=p=x=
>4><>D>L>T>\>d>p>
?$?D?P?p?|?
0(080\0d0l0t0|0
1$1,141<1D1P1t1|1
2$2,242<2D2L2T2\2d2l2t2|2
3$3,343<3D3L3X3|3
4 4D4L4T4\4d4l4t4|4
5$5,585\5d5l5t5|5
6$6,646<6D6L6T6\6d6l6t6|6
7$7,747<7D7L7T7\7d7l7t7|7
8$8,848<8D8L8T8\8d8l8t8|8
9$9,949<9D9L9T9\9d9l9t9|9
:$:,:4:<:D:L:T:\:d:l:t:|:
;$;,;4;<;D;L;T;\;d;p;
<$<,<4<<<D<L<T<\<d<l<t<|<
=$=,=4=<=D=L=T=\=d=l=t=|=
>$>,>4><>D>L>T>\>d>l>t>|>
? ?D?L?T?\?d?l?t?|?
0$0,040<0D0L0T0\0d0l0t0|0
1$1,141<1D1L1T1\1d1p1
242<2D2L2T2\2d2l2t2|2
3<3D3L3T3\3d3l3t3|3
4$4,484\4d4l4t4|4
5$5,545<5D5L5T5\5d5l5t5|5
646<6D6L6T6\6d6l6t6|6
7$7,747<7D7L7T7\7d7l7t7|7
8(8L8T8\8d8l8t8|8
9$9,949<9D9L9T9\9d9l9t9|9
:$:,:4:<:D:L:T:\:d:l:t:|:
;$;,;4;<;D;L;T;\;d;l;t;|;
<$<,<4<<<D<L<T<\<d<p<
=4=<=D=L=T=\=d=l=t=|=
>$>,>4><>D>L>T>\>d>l>t>|>
?$?,?4?<?D?L?T?\?d?l?t?|?
0$0,040<0D0L0T0\0d0l0t0|0
1<1D1L1T1\1d1l1t1|1
2$2,242<2D2L2T2\2d2l2t2|2
3$3,343<3D3L3T3\3d3l3t3|3
4$4,444<4D4L4T4\4d4l4t4|4
5$5,545<5D5L5T5\5h5
6<6D6L6T6\6d6l6t6|6
7$7,747<7D7L7T7\7d7l7t7|7
8$8,848<8D8L8T8\8d8p8
9$9,949<9D9L9T9\9d9p9
:$:,:8:\:d:l:t:|:
;$;,;4;<;D;L;T;\;d;l;t;|;
< <D<L<T<\<d<l<t<|<
=$=,=8=\=d=l=t=|=
> >D>L>T>\>d>l>t>|>
?$?,?4?<?D?L?T?\?d?l?t?|?
0$0,040<0D0L0T0\0d0l0t0|0
1$1,141<1D1L1X1|1
2$2,242<2D2L2T2\2d2l2t2|2
3$3,343<3D3L3T3\3d3l3t3|3
4$4,444<4D4L4T4\4d4l4t4|4
5$5,545<5D5L5T5\5d5p5
6$6,646<6D6L6T6\6d6l6t6|6
7$7,747<7D7L7T7\7d7l7t7|7
8$8,848<8D8L8T8\8d8l8t8|8
9$9,949<9D9L9T9\9d9l9t9|9
:$:,:4:<:D:L:T:\:d:l:t:|:
;8;D;L;
<8<H<T<\<
=,=H=X=d=l=
>L>\>h>p>
?P?`?l?t?
040<0D0L0T0\0d0l0t0
1,181@1`1h1p1x1
2$202P2X2h2
3(3H3X3|3
4$404P4\4|4
4 505<5D5d5
6$6,646<6D6L6T6\6d6l6t6|6
70787@7H7T7\7
8@8P8\8d8
9H9X9d9l9
:`:p:|:
;(;4;<;p;
; <0<<<D<d<
<0=@=L=T=
>H>X>d>l>
?$?,?`?p?|?
0 0,040h0x0
1(141T1\1h1
2<2D2L2T2`2
343@3`3h3p3x3
4$404P4X4d4
545<5D5L5T5\5d5l5t5|5
6 60686\6l6t6|6
7$747<7D7L7T7\7d7l7t7|7
8(8H8P8X8d8
9(949<9p9
:$:,:8:X:d:
;(;4;T;`;
<8<H<T<t<|<
= =@=H=T=t=|=
>$>D>P>X>x>
?(?0?8?D?d?l?t?
0H0X0d0l0
1$1,181X1`1h1t1
282@2L2l2t2|2
3 3@3H3P3\3|3
4(40484D4d4l4t4
50585@5L5l5t5|5
60686D6d6l6t6|6
7 7(70787@7H7T7t7|7
8(80888@8H8T8t8|8
9$9,949@9`9h9t9
:$:,:4:@:`:h:p:x:
; ;@;L;l;x;
<(<L<T<\<d<l<t<|<
=$=,=4=<=H=h=t=
>$>,>4><>D>L>X>|>
? ?@?L?l?x?
0$0,040@0d0l0t0|0
141<1D1L1T1\1d1l1x1
2,282@2`2|2
3 3(30383H3l3t3|3
4$404T4\4d4l4t4|4
5 5(50585@5H5T5t5|5
60686@6H6P6X6`6p6
7 7(70787@7L7l7t7|7
888T8d8p8x8
9$90989X9t9
:4:D:P:X:x:
;8;T;d;p;x;
<$<0<8<X<t<
=4=D=P=X=x=
>8>T>d>p>x>
?$?0?8?X?t?
040D0P0X0x0
181T1d1p1x1
2$20282X2t2
343D3P3X3x3
484T4d4p4x4
5$50585X5t5
646D6P6X6x6
787T7d7p7x7
8$80888X8t8
949D9P9X9x9
:8:T:d:p:x:
;$;0;8;X;t;
<(<0<8<@<H<P<X<h<
=$=,=4=<=D=L=T=\=h=
> >@>H>P>X>`>h>p>|>
?$?,?4?<?D?L?X?`?
0 0@0\0l0x0
181@1L1T1t1
242P2`2l2t2
3 3,343T3p3
404@4L4T4t4
545P5`5l5t5
6 6,6L6T6\6d6l6t6
747P7`7l7t7
8 8,848T8p8
9$9,949<9D9L9X9`9
: :@:\:l:x:
;,;8;@;`;|;
< <<<L<X<`<
= =@=\=l=x=
>,>8>@>`>|>
? ?<?L?X?`?
0 0@0\0l0x0
1(181\1d1l1t1|1
2$2,242@2H2h2
3$3,343<3D3L3T3\3d3l3t3
444<4D4L4T4\4d4l4t4|4
5$5@5P5\5d5
6$6D6`6p6|6
7$7,747<7D7L7X7`7
8$8,848<8D8P8X8x8
9 9@9\9l9x9
:(:D:T:`:h:
; ;@;H;P;\;|;
< <<<L<X<|<
= =@=L=l=t=
>$>,>4><>H>l>t>|>
?$?,?4?<?D?P?X?x?
080T0d0p0x0
1$1,141<1D1L1T1\1h1p1
2(202P2l2|2
3$3,343<3D3P3X3x3
4$4,484@4`4|4
5(585H5l5t5|5
6 6(6H6d6t6
7$747@7d7l7t7|7
888H8T8\8|8
9(909P9l9|9
:(:8:H:l:t:|:
;(;L;T;\;d;l;t;|;
<(<L<T<\<d<l<t<|<
=8=@=H=P=X=d=l=
>$>0>8>X>t>
?0?@?P?t?|?
0(0H0P0X0`0h0p0x0
1$10181X1t1
2 2@2\2l2x2
3$3,3L3h3x3
4(404P4l4|4
5 505@5d5l5t5|5
686@6H6P6X6d6l6
7$7,747<7D7L7T7`7h7
8(808P8l8|8
9 9(949<9\9x9
:$:,:4:<:D:L:T:\:d:l:t:|:
;$;,;4;<;D;L;T;\;d;p;x;
<$<,<4<<<D<L<T<\<d<l<x<
=$=,=4=<=D=P=X=x=
>$>,>4><>D>L>T>`>
? ?(?0?8?D?d?l?t?
0 0(040T0\0h0
1$181\1d1l1t1|1
2(2H2P2\2|2
3 3,3L3T3`3
4$404P4X4`4h4p4x4
5$5,545@5H5h5
6 6@6H6T6t6
747<7D7L7T7\7d7l7t7|7
8 8(848<8\8x8
9<9D9`9h9p9x9
:$:X:`:h:p:x:
;,;D;\;d;|;
<(<H<P<X<t<|<
= =(=0=8=D=L=
><>D>L>T>\>h>
?$?X?`?h?p?x?
(00080@0H0T0\0
1,1D1L1d1|1
2,242<2D2P2p2|2
303<3\3h3
4$4,4`4h4p4x4
585@5H5P5X5h5p5
6,6<6L6T6\6d6l6t6
7$7,747<7H7P7p7
8$80888l8t8|8
9<9D9L9T9\9h9p9
:4:<:D:L:T:`:
;$;,;`;h;p;x;
<P<X<`<h<p<|<
=,=4=<=D=L=T=d=l=t=|=
>H>X>d>l>
?P?`?l?t?
0$0X0h0t0|0
141D1P1p1|1
202L2T2\2d2l2x2
3$3,343<3H3h3x3
4<4D4L4T4\4d4l4t4
5$5,545<5D5L5T5\5d5l5x5
6$6,646<6D6L6T6\6d6l6t6|6
7$7,747<7D7P7t7|7
8$8,848<8D8L8T8\8d8l8t8|8
9$9,949<9D9L9T9\9d9l9t9|9
:$:,:4:<:H:h:p:x:
;$;,;4;@;`;l;
<$<,<4<@<`<h<t<
= =@=P=t=|=
>$>,>4><>D>L>T>\>d>l>t>|>
?<?H?h?t?
0(000<0\0h0
1$1,141<1D1\1d1l1
2$2,242L2T2\2t2|2
3$3,343@3`3l3
40484@4P4t4
5$5,545<5D5L5T5\5h5
6$6D6L6T6\6h6
7$7,747<7D7L7T7\7d7l7t7|7
8$8,848<8D8L8T8\8h8
9 9,9L9X9x9
:(:L:T:\:d:l:t:|:
;$;,;4;<;D;L;T;\;d;l;t;|;
<,<4<<<D<L<T<\<d<l<t<|<
=,=4=<=D=L=T=\=d=l=t=|=
=D>L>T>\>d>l>
?$?,?4?<?D?L?T?\?d?l?t?|?
0$0,040<0D0L0T0\0d0l0t0|0
1$1,141<1D1L1T1\1d1l1t1|1
2,242<2D2L2T2\2d2l2t2|2
3$3,343<3D3L3T3\3d3l3t3|3
4(4L4T4\4d4l4t4|4
5$5,545<5D5L5T5\5d5l5t5|5
6$6,646<6D6L6T6\6d6l6t6|6
7$7,747<7D7L7T7\7d7l7t7|7
8$8,848<8D8L8T8\8d8l8t8|8
9<9D9L9T9\9d9l9t9|9
:$:,:4:<:D:L:T:\:d:l:t:
;4;<;D;L;T;\;d;l;t;|;
<$<,<4<<<H<h<x<
=$=,=4=<=D=L=T=\=d=l=t=|=
>$>,>4><>D>L>T>\>d>l>t>|>
?$?,?4?<?D?L?T?\?d?l?t?|?
0(0H0P0X0`0l0
1 1D1L1T1\1d1l1t1|1
2$2,242<2D2L2T2\2d2l2t2|2
3$3,343<3D3L3X3|3
4$4,444<4D4L4T4\4d4l4t4|4
5$5,545<5D5L5T5\5d5l5t5|5
606<6\6d6l6x6
7$7,747<7
8,848<8D8L8T8\8d8p8
9$9,949<9D9L9T9\9d9l9t9|9
:$:0:T:\:d:l:t:|:
;$;,;4;<;D;L;T;\;d;l;x;
<,<4<<<D<L<T<\<d<l<x<
=$=D=L=X=x=
>$>,>4><>H>l>
?$?,?4?<?D?L?T?\?d?l?t?|?
0$0,040<0D0L0X0|0
1$1,141<1D1L1T1\1d1l1t1
2,242<2D2L2T2\2d2p2
3$3,343<3D3`3
4$4,444<4D4L4T4\4d4l4t4|4
5$5,545<5D5L5T5\5d5l5t5|5
646<6D6L6T6\6d6l6t6|6
7$7,747<7D7L7T7\7d7l7t7|7
7H8h8p8
9<9\9
:$:,:4:<:D:L:T:\:d:l:x:
;$;,;4;<;D;L;T;\;d;l;t;|;
<$<,<4<<<D<L<T<\<d<l<t<|<
=$=,=4=<=D=L=T=\=d=l=t=|=
>$>,>4><>D>L>T>\>d>l>t>|>
?$?,?4?<?D?L?T?\?d?l?t?|?
0$0,040<0D0L0T0\0h0
10181@1H1X1|1
2$2,242<2D2|2
3$3L3T3
4$4,4D4L4d4l4x4
5$5L5T5\5d5|5
6$6,686\6|6
707P7\7|7
80888D8d8p8
8l9t9|9
:$:,:4:<:D:L:T:\:d:l:t:|:
;$;,;4;@;d;l;t;|;
<(<0<8<D<d<p<
=$=,=4=<=D=L=X=|=
>$>0>P>\>|>
?$?,?4?<?d?l?t?|?
0,040<0D0l0t0|0
1$1,141<1D1L1T1\1d1l1t1|1
242T2\2d2l2t2|2
3$3,343<3D3L3T3\3d3l3x3
5(5H5h5p5|5
6$6,646<6D6L6T6\6d6l6t6|6
7 7@7H7T7t7
848T8\8d8l8t8|8
:$:,:4:@:`:h:t:
;$;0;T;t;|;
<\<d<l<t<|<
=<=\=d=l=t=|=
>$>,>4>l>t>|>
?(?4?T?`?
0$000P0\0|0
1 1(141T1\1d1p1
2$2,242<2D2L2T2`2
3 3(343T3\3d3l3x3
444<4H4h4p4|4
585@5H5T5t5|5
6<6H6h6p6|6
70787D7d7l7t7
8 8(80888@8H8T8t8|8
9$909P9X9d9
: :D:L:T:\:d:l:t:|:
;$;D;P;p;x;
<8<D<d<l<t<
=$=,=4=<=D=L=T=\=d=l=t=|=
>8>@>H>P>X>`>l>
?$?D?L?T?`?
0,040<0D0L0T0\0h0
1 1(101<1\1d1l1t1|1
2(242T2\2d2l2x2
3(3L3T3\3d3l3t3|3
4$4,444<4D4L4T4\4d4l4x4
545<5D5P5t5|5
6,646<6H6h6p6x6
7H7P7X7d7
808P8l8
9(9H9T9t9
:0:<:\:d:l:x:
;$;,;4;<;D;L;T;\;d;l;t;|;
<$<,<4<<<H<l<t<|<
= =(=4=T=\=h=
><>H>h>p>|>
? ?,?L?T?`?
0$0,080X0d0
1$1,141<1D1L1X1x1
2$2,242<2H2h2p2x2
3 3(343T3\3p3
4(40484@4L4l4x4
5 5(585\5d5l5t5|5
6(60686H6l6t6|6
7$707P7X7d7
888@8L8l8x8
989D9d9l9t9|9
:(:H:P:X:`:h:x:
;$;,;4;<;D;L;T;\;d;p;
<$<,<8<X<`<h<x<
=,=4=<=D=L=T=\=d=l=x=
> >(>4>T>\>d>l>t>
?$?,?4?<?H?l?t?|?
0$0,080X0`0p0
1 1,1L1T1`1
2$2,242@2d2l2t2|2
3(3H3P3X3`3h3t3
4 4D4L4T4\4d4l4t4|4
5$5,545<5D5L5T5\5d5l5t5
6$6,646<6D6L6T6\6d6l6t6
7 7(70787D7d7l7t7|7
888@8H8P8X8`8p8
9$9,949<9H9h9p9x9
:4:<:D:P:p:x:
;4;<;D;L;T;`;
<(<L<T<\<d<l<t<|<
=$=,=4=<=D=P=p=x=
> >@>H>T>t>
?$?,?4?<?D?L?T?\?d?l?t?|?
0$0,040<0D0L0T0\0d0l0t0|0
1$1,141<1D1L1T1\1d1l1t1|1
2$2D2L2X2x2
3$3,343<3D3L3T3\3d3l3t3|3
40484@4L4l4x4
5 5(545<5p5
6$6,646<6D6L6T6\6d6l6t6|6
7 7,7L7T7\7d7l7x7
8 8(888\8d8l8t8|8
9$909T9\9d9l9t9|9
:$:,:4:<:D:L:T:\:d:l:t:|:
;,;4;<;D;L;T;\;d;p;
< <,<L<T<\<d<l<t<|<
=8=@=H=P=X=d=
>,>4><>D>L>X>x>
?$?,?4?<?D?L?T?\?d?l?t?|?
0 0@0H0P0\0|0
1<1D1L1T1\1d1l1t1|1
2(2H2P2\2|2
3 3(383\3d3l3t3|3
4$4,444<4D4L4T4\4d4l4x4
5(5L5T5\5d5l5t5|5
6$6,646<6D6L6T6`6
7 707T7\7d7l7t7|7
80888H8l8t8|8
9,949<9H9h9p9|9
:$:,:4:<:D:L:X:|:
;$;,;4;<;D;L;T;`;
< <,<L<X<x<
=<=D=L=T=\=h=
>$>D>L>X>x>
?(?H?P?\?|?
0 0(040T0`0
141<1H1h1t1
2$2,242<2D2L2T2\2d2p2
3$303P3X3h3
444<4D4L4T4`4
5$5,585X5`5h5p5x5
6$6,646@6`6h6p6x6
7 7(747T7\7h7
8$8,888X8`8h8p8x8
9$9,989X9d9
:$:L:T:\:d:
;<;D;L;T;
<$<,<8<\<d<
=$=D=P=p=|=
>(>0><>\>h>
?$?,?8?X?`?h?p?x?
0$0,040@0`0h0p0x0
1,141@1`1h1p1x1
2 2(20282D2d2l2t2|2
383@3L3l3t3
444<4D4L4T4\4d4l4t4|4
5$5,585X5`5h5p5x5
6$6,646<6D6L6T6\6d6l6t6|6
747<7D7P7p7x7
8(80888D8d8l8t8|8
9 9(949T9\9d9l9x9
: :(:4:T:\:d:l:x:
:<;D;L;X;x;
<<<D<L<T<\<d<l<t<
=$=,=4=<=D=L=T=\=d=l=x=
>(>H>P>\>|>
?<?D?L?X?|?
0$0,040<0D0L0T0\0d0l0t0|0
1 1(141T1\1h1
2 2(20282H2l2t2|2
3$3,343<3D3L3T3\3d3l3t3
4 4,4L4T4\4d4p4
50585@5H5P5X5d5
6,646<6D6L6T6`6
70787@7H7P7X7d7
8$8,888X8`8h8p8x8
9 9(949T9`9
:$:,:4:<:D:L:T:\:d:l:t:|:
;$;,;4;<;D;L;T;\;d;l;t;|;
< <(<0<<<\<d<l<x<
=$=,=4=<=D=L=T=\=d=p=
>$>,>4><>D>P>p>x>
?8?@?H?P?X?`?l?
0(0H0P0\0|0
1$1,141@1`1h1p1x1
2,242<2D2L2T2\2d2l2t2|2
3$3,343<3D3L3X3x3
4$404P4\4|4
5$5,545<5D5L5T5\5d5l5x5
6<6D6L6T6\6d6l6t6|6
7$707P7\7|7
8,848<8H8h8p8x8
9(9H9P9\9|9
: :,:L:T:\:d:l:x:
;0;8;@;H;P;X;d;
< <@<P<|<
=(=0=<=\=d=l=t=|=
>$>,>4>@>d>l>t>|>
?4?<?D?L?T?\?d?l?t?|?
0(0L0T0\0d0l0t0|0
1(10181@1H1P1X1h1
242<2D2L2T2\2d2l2t2|2
3$3,343<3D3L3T3\3d3l3x3
404<4\4h4
5(5H5P5X5`5h5x5
6$6,686X6`6h6p6
7 7,7L7T7\7d7p7
8$8,848<8D8P8p8x8
9$9,949<9D9L9T9\9h9
:$:,:8:@:t:
;4;<;D;L;X;x;
<$<,<4<@<`<h<p<x<
=(=L=T=\=d=l=t=|=
> >@>H>X>|>
? ?(?8?\?d?l?t?|?
0$0,080X0d0
1$1,141<1D1L1T1\1d1l1t1|1
2$2,242<2D2L2T2\2d2l2t2|2
3$303P3X3`3l3
444<4D4L4T4\4d4l4t4|4
505@5d5l5t5|5
6$6,646<6D6L6T6\6d6l6t6|6
787@7H7P7X7d7
8 8(808<8\8d8l8t8
9$9D9P9p9|9
:$:,:4:<:D:L:T:\:d:l:x:
; ;,;L;T;\;h;
<0<8<@<H<T<t<|<
= =(=4=T=\=h=
>,>4><>H>h>p>|>
?$?,?4?<?D?L?T?\?d?l?t?|?
0$0,040<0D0L0T0\0d0l0t0|0
1$1,141<1D1L1T1\1h1
2$202P2X2d2
3$303P3X3h3
444<4H4h4p4x4
4 5D5L5T5\5d5l5t5|5
646<6D6L6l6t6|6
7$7,7L7T7\7d7
8,848<8`8
9(989\9d9l9t9|9
:8:@:H:T:t:|:
;$;,;4;<;D;L;T;`;
<$<0<T<\<d<l<t<|<
=$=,=4=<=D=L=T=\=d=l=x=
>$>,>4><>D>L>T>\>d>l>x>
?$?,?4?<?D?L?T?\?d?l?t?|?
0 0D0L0T0\0d0l0t0|0
1<1D1L1T1`1
2$202P2X2h2x2
3(3H3P3X3`3l3
4$4,444<4D4L4T4\4d4l4t4|4
5$5,545@5d5l5t5|5
6$6,646<6D6L6T6\6d6l6t6|6
7$7,747<7D7L7X7|7
848<8D8L8T8\8d8l8t8|8
9,949@9`9h9t9
:$:,:4:<:D:L:T:\:h:
;<;L;X;`;
<$<,<4<<<D<L<T<\<d<l<t<|<
=$=,=4=<=D=L=T=\=h=
>$>,>4><>D>L>T>\>d>l>t>
?$?,?8?X?`?h?x?
0(0H0X0|0
1(141T1`1
2$2,242<2D2L2T2\2d2l2t2|2
3$3,343<3D3L3T3\3d3p3
4(4H4X4|4
5(50585D5d5l5t5|5
6$6,646<6D6L6T6\6d6l6t6
7$7D7L7X7x7
8$8,848<8D8L8T8\8d8l8t8|8
9$9,949<9D9L9T9\9d9l9x9
: :(:4:T:\:d:l:t:|:
;$;,;4;<;D;L;T;\;d;p;
<$<,<4<<<D<L<T<\<d<l<t<|<
=(=0=@=d=l=t=|=
> >,>L>X>x>
?$?,?4?<?D?L?T?\?d?p?
0(00080H0l0t0|0
1 1D1L1T1\1d1l1t1|1
2$2,282@2h2p2x2
3(303X3`3h3p3
4 4H4P4X4`4t4|4
5,545<5D5L5T5\5d5l5t5|5
6$6,646<6D6L6T6\6d6l6t6|6
7$7,747<7D7L7X7x7
848<8D8L8X8x8
9 9D9L9T9\9d9l9t9|9
:$:,:4:<:D:P:t:|:
;,;4;<;H;h;p;x;
<$<,<8<X<d<
=,=<=H=h=t=
>$>,>4><>D>L>T>\>h>
? ?(?0?8?@?L?l?t?|?
0$0,040<0D0L0T0\0d0l0t0|0
1$1,141<1D1L1T1\1d1l1t1|1
2$202P2X2`2h2p2
3$3,343<3D3L3T3\3d3p3
4$4,444<4D4L4X4`4
5 5D5L5T5\5d5l5t5|5
6$6,646<6H6l6t6|6
7(747T7`7
8,848<8D8L8T8`8
9(90989@9H9T9t9
:<:D:L:T:\:d:l:t:|:
;4;<;D;L;T;\;h;p;
<$<,<4<<<D<P<p<x<
=<=D=L=T=\=d=p=
>(>H>P>\>|>
?<?H?h?p?|?
0<0D0L0T0\0d0l0t0
1$1,181X1`1h1x1
2 2(20282@2L2l2t2|2
3,343<3D3L3T3\3d3l3t3|3
4 4D4L4T4\4d4l4t4|4
5$5,545<5H5l5
646<6D6L6X6x6
787@7L7l7t7
8(8L8T8\8d8l8t8|8
989@9H9P9X9`9h9t9
:0:8:@:H:X:|:
;4;<;D;L;T;\;d;l;t;|;
<$<,<4<@<d<l<t<|<
= =D=L=T=\=d=l=t=|=
>$>0>P>X>`>h>x>
?4?<?D?L?T?\?d?l?t?|?
0$0,040<0D0L0T0\0d0l0t0|0
1$1,141<1D1L1T1`1
2$2,242<2D2L2T2\2d2l2t2|2
3$303P3X3`3h3p3|3
4$4,444<4D4L4T4\4d4l4t4|4
5$5,545<5D5P5t5|5
6 6D6L6T6\6d6l6t6|6
787@7H7P7X7`7h7t7
8$8,848<8D8L8T8\8d8l8t8|8
9(9H9P9X9`9h9p9x9
:(:H:P:\:|:
;$;,;4;<;D;L;T;\;d;l;x;
<$<,<4<<<D<L<T<\<d<l<x<
=$=,=4=<=H=h=t=
>(>H>P>\>|>
?$?,?4?<?D?L?X?|?
0<0D0L0T0\0d0l0t0
1 1(101@1d1l1t1|1
2,242<2D2L2T2\2d2l2t2|2
3$3,383X3d3
4,444<4D4L4T4\4d4l4t4|4
5$5,545<5D5L5T5\5h5
6$6,646<6D6L6X6|6
7 7(707<7\7d7p7
8$808P8X8`8h8p8|8
9(9H9X9|9
: :@:L:l:t:|:
;<;D;L;T;\;d;l;t;|;
< <(<0<8<@<P<t<|<
=$=,=8=X=`=h=p=x=
>(>L>T>\>d>l>t>|>
?,?4?<?D?L?T?\?d?l?x?
040<0H0h0t0
1$1D1P1p1|1
2 2,2L2T2`2
3L3T3\3t3
444P4X4`4h4p4|4
4 5(50585@5L5T5
6 6<6D6L6T6\6h6p6
7$7,787@7t7|7
8D8L8T8\8d8p8
9 9(9\9d9l9t9|9
:<:D:L:T:\:h:
; ;(;0;8;D;d;l;x;
< <(<0<8<D<d<p<
=(=H=P=X=`=h=p=
>$>D>L>X>x>
?(?H?P?\?|?
0 0,0L0T0`0
1$101P1X1d1
2 2(242T2\2h2
3$3,383X3`3l3
4$4D4P4p4|4
5 5,5L5X5x5
6(646T6`6
707<7\7h7
888D8d8p8
9 9@9L9l9x9
:(:H:T:t:
;$;0;P;\;|;
<,<8<X<d<
=<=H=h=t=
> >,>L>X>x>
? ?(?@?H?l?|?
0 0,0L0T0`0
10181@1H1P1X1d1
2<2D2L2T2\2d2l2t2|2
3$3,343<3D3L3T3\3d3l3t3|3
4 4@4H4T4t4|4
5$505T5\5d5l5t5|5
6$6,646<6H6l6t6|6
7$7,747<7D7L7T7\7d7l7t7|7
8$8,848<8D8L8T8\8d8l8t8|8
9 9(949T9\9h9
:<:H:h:p:|:
;$;,;8;X;`;h;p;|;
< <@<H<T<t<|<
=$=D=L=T=\=d=l=x=
>8>@>H>T>t>|>
?$?,?4?<?D?L?T?\?d?l?t?|?
0$0,040<0D0L0T0\0d0l0t0|0
101\1|1
2$2,242<2D2L2T2\2d2l2t2|2
3$3,343<3D3L3T3\3d3l3t3|3
484@4H4P4`4
5$5,545<5D5L5T5\5d5l5t5|5
646<6H6l6t6|6
7$7,747@7`7h7p7x7
8(8H8P8X8d8
9(90989@9L9l9x9
:$:,:4:<:D:L:T:\:d:l:t:|:
;$;,;4;<;D;L;T;\;d;l;t;|;
<$<,<4<<<H<l<t<|<
=$=0=P=\=|=
>8>@>H>P>X>h>
? ?(?0?<?\?d?p?
0 0D0L0T0\0d0l0t0|0
1$1,141<1D1L1X1x1
20282@2L2l2t2|2
383@3H3T3t3|3
4$4,444<4D4L4T4\4d4l4t4|4
5(50585D5d5p5
6$6,646<6D6L6T6\6h6
7 7(787\7d7l7t7|7
8 8@8H8P8X8`8l8
9,949<9D9L9T9\9d9l9t9|9
:8:@:L:l:x:
;4;<;H;h;p;|;
<8<@<L<l<x<
=4=<=H=h=p=x=
>$>,>8>X>d>
?8?@?L?l?t?
0 0(000@0d0l0t0|0
1$1,141<1D1L1T1\1d1l1t1|1
2$202P2X2`2l2
3$3,343<3D3L3T3\3d3l3t3|3
4$4,444@4`4h4p4|4
5<5D5L5T5\5d5l5t5|5
646T6\6d6l6t6|6
7$7D7L7T7\7d7l7x7
888D8d8l8t8|8
9$9,989@9X9h9p9x9
:$:0:P:X:`:h:p:
;$;,;4;<;D;L;T;\;d;l;t;|;
<$<,<4<<<D<L<T<\<d<l<t<|<
=,=4=<=D=L=T=`=
> >D>L>T>\>d>l>t>|>
?4?<?D?P?p?x?
0$0,040@0`0h0p0|0
1$1,141<1H1l1t1|1
2,242<2H2l2t2|2
3(3H3P3X3d3
4$4,444<4D4L4T4\4d4l4x4
5$5<5D5L5T5\5d5l5t5|5
6,686@6t6
7$7,747L7T7\7d7l7t7|7
8$8,848<8D8L8T8\8d8l8x8
9$9,949<9D9L9T9`9
: :,:L:T:\:d:p:
;,;4;@;d;l;t;|;
<$<,<4<<<D<L<T<\<d<l<t<|<
= =@=H=P=X=h=
>,>8>X>d>
?$?,?4?<?D?P?p?x?
0<0H0h0t0
1$1,181\1d1l1t1|1
2$2,242<2D2P2p2x2
3$3,343<3d3l3t3|3
4<4D4T4\4l4t4|4
5 5@5L5l5x5
6<6D6L6T6\6d6l6t6|6
7(70787D7d7l7t7|7
8,888X8`8h8p8|8
9$9,949<9D9L9T9\9d9l9t9|9
:,:4:<:D:L:T:\:d:l:t:|:
; ;@;H;P;X;`;h;t;
<$<,<4<@<`<h<p<x<
=$=,=4=<=D=L=T=`=
>$>,>4><>D>P>p>x>
?0?8?@?H?P?\?|?
0 0(00080@0H0P0`0h0
1$1,141<1D1L1T1\1d1l1t1|1
2$2,242<2D2P2t2|2
3$3,343L3T3\3d3l3t3|3
4$4,444<4H4h4p4|4
5$5,545<5D5L5T5\5d5l5t5
6 6<6\6d6p6
7 7(747T7\7h7
8$8,848<8D8L8p8
949<9D9L9T9\9d9l9t9|9
:$:0:P:X:`:h:x:
;(;L;T;\;d;l;t;|;
<$<D<L<T<\<h<
=$=,=4=<=D=L=T=\=d=p=
>$>,>4><>D>L>T>\>d>l>t>|>
?4?<?H?h?p?|?
1(1H1P1\1|1
2$2<2`2p2|2
3$3,343<3D3L3T3\3d3l3t3|3
4<4D4P4p4x4
5$5,545<5D5L5T5\5d5l5t5|5
6 6D6L6T6\6d6l6t6|6
7$7,747<7D7L7T7\7d7l7t7|7
80888@8L8l8t8
9(90989D9d9p9
:0:<:D:\:d:
;4;<;D;L;T;\;d;|;
< <@<L<l<t<
=$=,=8=\=d=l=t=|=
>$>,>4><>D>L>X>|>
?L?d?l?t?|?
0$000P0X0d0
141<1D1L1T1\1d1l1t1|1
2(2H2P2X2`2p2
4<4d4
5$5,545<5
787\7
8$8,848<8D8L8T8\8d8l8t8|8
9 9(90989H9l9t9|9
:$:,:4:<:D:L:T:\:d:l:t:|:
;$;,;4;<;D;L;T;\;d;l;t;|;
<$<,<4<<<D<L<T<\<d<l<t<|<
=$=,=4=<=D=L=T=\=d=l=t=|=
>$>,>4><>D>L>T>\>d>l>t>|>
?$?,?4?<?D?L?T?\?d?l?x?
0X1x1
202`2h2
2 3D3L3T3\3d3l3t3|3
4(40484H4l4
5$5,545<5H5l5t5|5
6(606<6\6h6
7$7,747@7`7l7
8 8,8L8X8x8
949<9H9h9p9x9
:0:8:D:d:p:
;$;,;4;<;D;L;T;`;
<$<,<4<<<D<L<T<\<d<l<x<
=$=,=4=<=D=L=T=\=d=l=t=|=
>$>,>4><>D>L>T>\>h>
? ?0?T?\?d?l?
@0d0l0t0|0
1$1<1D1L1T1\1d1l1t1|1
2$2,242@2
3$3,343<3D3P3p3x3
4$4,444<4D4L4T4\4h4
5<5D5L5T5\5d5l5t5|5
6,6D6L6T6\6d6l6t6|6
7$707P7\7|7
8$8,8h8
9$9,949<9D9L9T9\9d9l9t9|9
:$:,:4:<:D:P:p:x:
:H;x;
<P<p<
= =@=H=P=X=d=
>8>@>L>l>x>
? ?,?L?X?x?
040X0`0h0t0
1$1,141<1D1L1T1\1d1l1t1
2$2,242<2D2L2T2\2d2l2t2|2
3$3,343<3D3P3t3|3
4(4H4T4t4|4
5(5L5T5\5d5l5t5|5
6 6D6L6T6\6d6l6t6|6
7,747<7D7L7T7\7d7p7
8$8D8P8X8
8\9l9|9
:8:@:H:T:t:|:
;4;<;D;L;l;x;
< <,<L<T<\<d<l<t<
=<=D=L=X=x=
>$>0>P>\>|>
?$?,?8?X?`?h?p?x?
00080D0d0l0t0
10181@1H1P1\1|1
2D2P2t2|2
3$303P3X3`3h3p3|3
4$4,444<4D4P4p4x4
585@5H5T5t5
646<6T6\6d6l6t6|6
7 7(707<7\7d7l7t7
8$8D8L8T8\8h8
9 9,9L9T9`9
:<:H:h:p:
;(;4;T;`;
< <@<H<T<\<
=,=8=X=`=h=t=
> >,>L>T>\>d>l>t>|>
?$?,?4?<?D?L?T?\?d?l?t?|?
0,080X0h0
1(1H1P1\1|1
2$2D2L2X2|2
3$3,3D3L3T3\3h3
4(4H4P4\4|4
50585@5L5l5t5|5
6$6,646@6d6l6t6|6
7$7,747<7D7L7T7`7
8$8,848<8D8L8T8\8d8p8
8<9L9
:4:<:H:p:x:
;<;D;L;T;
<D<L<T<\<d<
=$=,=4=<=D=\=d=l=
? ?@?H?P?X?d?
0X0x0
0,1H1h1p1x1
242<2H2h2p2x2
3$3D3L3T3`3
4D4L4T4\4
545<5D5P5p5x5
5$6P6p6x6
7(7H7P7X7`7h7t7
8$8,848<8D8L8d8l8t8|8
9$9,989\9d9l9t9|9
:$:,:4:<:D:L:T:\:d:p:
;<;D;L;T;\;d;p;
<(<L<T<\<d<l<t<|<
=H=h=p=x=
>(>H>T>t>|>
?$?,?4?<?D?L?T?\?d?l?t?|?
0(0H0T0t0
1 1(10181D1d1l1x1
2$2,242<2D2L2T2\2d2l2t2|2
3$3,343<3D3L3T3\3d3l3t3|3
4$4,444<4D4L4T4\4d4l4t4|4
5,545<5D5L5T5\5d5l5t5|5
6$6,646<6D6L6T6\6d6l6t6|6
7$7,747<7D7L7T7\7d7l7t7|7
8$8,848<8D8L8T8\8d8l8t8|8
9$9,9D9\9h9
;0;P;X;`;l;
<,<4<P<t<|<
=$=,=4=<=D=L=T=\=d=l=t=|=
>$>,>4><>D>L>T>\>d>l>t>|>
?$?,?4?<?D?L?T?\?d?l?t?|?
0$0,040<0D0L0T0\0d0l0t0|0
1$1,141<1D1L1T1\1d1l1t1|1
2$2,242<2D2L2T2\2d2l2t2|2
3$3,343<3D3L3T3\3d3l3t3|3
4$4,444<4D4L4T4\4d4l4t4|4
5$5,545<5D5L5T5\5d5l5t5|5
6$6,646<6D6L6T6\6d6l6t6|6
7$7,747<7D7L7T7\7d7l7t7|7
8$8,848<8D8L8T8\8d8l8t8|8
9$9,989\9d9
:$:,:4:<:D:L:T:\:h:
;$;,;4;<;D;L;T;\;d;l;t;|;
<$<,<4<<<D<L<T<\<d<l<t<|<
=$=,=4=<=D=L=T=\=d=l=t=|=
> >(>0><>\>h>
?$?,?4?<?D?L?T?\?t?
0 0@0L0l0t0|0
1$1,141<1T1\1d1p1
2$2,242<2D2L2T2\2d2l2t2|2
3$3,343<3D3L3T3\3d3l3t3|3
4$4,444<4D4P4p4|4
5$505P5X5`5l5
6D6L6T6\6
7$7,747<7D7L7T7\7h7
8<8D8L8T8\8d8l8t8|8
9 9(989\9d9l9t9|9
:<:D:L:T:\:d:
:8;X;
<4<<<D<L<X<x<
=(=0=<=\=d=p=
=4><>l>x>
?$?@?d?l?t?|?
040d0
2$2,242<2D2
3$3@3d3
4$404T4\4d4
5<5D5L5T5\5d5l5t5|5
6$6,646<6D6L6T6\6d6l6t6|6
7$7,747<7D7L7T7\7d7l7t7|7
8$8,848<8D8L8T8\8d8l8t8|8
9<9D9L9T9
:(:L:T:\:d:l:t:
;X;x;
<$<,<4<<<H<l<t<|<
=$=,=4=<=D=L=T=\=d=l=t=|=
>$>,>4><>D>L>t>|>
?4?<?D?L?T?\?l?t?
0<0D0L0T0\0d0l0t0|0
181@1H1T1t1|1
2(20282X2|2
3$3,343<3D3L3T3\3d3l3t3|3
4$4,444<4D4L4T4\4d4l4t4|4
5$5,545<5D5L5T5\5d5l5t5|5
6$6,646<6D6L6T6\6d6l6t6|6
7$7,747<7D7L7T7\7d7l7t7|7
8$8,848<8D8L8T8\8d8l8t8|8
9$9,949<9D9L9T9\9d9l9t9|9
:$:,:4:<:D:L:T:\:d:l:t:|:
;$;,;4;<;D;L;T;\;d;l;t;|;
<$<,<4<<<D<L<T<\<d<l<t<|<
=$=,=4=<=D=L=T=\=d=l=t=|=
>$>,>4><>D>L>T>\>d>l>t>|>
?$?,?4?<?D?L?T?\?d?l?t?|?
040<0H0l0t0|0
0H1h1p1x1
2$2D2L2T2\2h2
3$3,343<3D3L3T3\3d3l3t3|3
4 4@4H4T4t4|4
5$5,5H5h5p5
6<6D6P6p6x6
7,747<7D7L7d7p7
888\8d8l8t8|8
9(9L9T9\9d9l9t9|9
:<:D:L:T:\:d:l:t:|:
;$;D;P;p;|;
<$<<<D<P<t<|<
=,=4=@=d=l=t=|=
> >D>L>T>d>t>
?,?8?X?`?h?x?
0,0<0L0T0d0l0|0
1$1,141<1H1
242@2l2t2
3$3,343<3D3L3T3\3d3l3t3|3
4$40484\4d4l4t4|4
5(50585H5l5t5|5
6$606T6\6d6l6t6|6
7$7,747<7D7L7T7\7d7l7t7
8,888X8d8
9 9(949T9\9h9
:(:H:P:X:d:
;$;,;4;<;H;h;p;x;
<$<,<4<<<D<L<T<\<d<p<
=$=,=4=<=D=L=T=\=t=|=
=,>4>T>\>d>l>t>|>
?,?4?\?d?l?t?|?
040<0T0l0
1 1,1L1T1`1
2H2P2\2|2
383@3H3P3X3`3h3t3
4$444D4L4T4\4d4l4x4
50585H5T5t5|5
6<6D6t6|6
8 8H8T8t8|8
989D9d9l9t9
:$:,:4:@:`:l:
;$;,;4;<;D;L;T;\;d;l;t;
<,<8<X<`<l<
= =,=L=X=`=x=
> >@>L>l>t>|>
?0?<?\?d?p?
0,040@0`0h0p0x0
1(1H1T1t1|1
20282D2d2l2x2
3 3,3L3T3\3d3l3t3
4<4H4h4p4x4
5<5H5h5t5
6 6,6L6T6\6d6l6t6|6
7<7D7P7p7|7
9<9H9h9
:<:D:P:p:x:
;4;<;H;h;p;x;
<$<,<4<<<D<L<T<`<
=$=0=
>4>8>X>`>d>
? ?(?0?8?<?D?X?t?x?
00080L0T0h0p0x0
1$1,1@1H1P1X1l1t1|1
2$282@2H2P2X2`2h2p2x2
3$3,343<3P3X3`3h3|3
4,444<4D4L4T4h4p4x4
50585L5T5h5p5
6,646H6P6X6`6h6p6x6
7 7(70787@7T7\7d7l7t7|7
8 8$8(8084888@8D8L8P8T8\8`8h8l8p8x8|8
9 9(9,94989@9D9L9P9X9\9d9h9p9t9|9
D0H0L0T0d0p0t0
1(181x1
2(202P2X2x2
3 3@3H3h3p3
40484
5 5(5H5P5p5x5
686@6`6h6
7(707P7X7x7
8(888H8X8h8x8
:$:0:<:H:T:`:l:x:
=(=8=D=
>(>P>t>
> ?@?`?
080h0
141l1
102x2
3<3t3
445l5
686p6
787p7
8D8|8
9H9t9
94:l:
<D<|<
<$=\=
><>t>
?P?|?
0<0h0
1<1h1
2,3d3
4H4t4
4(5`5
6D6x6
6(7`7
8H8t8
909\9
:8:d:
<@<l<
<0=h=
>(?`?
0@0x0
0 1X1
282p2
304h4
5(6`6
7@7x7
7 8X8
989p9
:0;h;
<(=`=
>@>x>
> ?X?
080p0
102h2
3(4`4
5@5x5
5 6X6
7,7d7
8D8|8
8$9\9
:<:t:
;4<l<
041`1
1$2P2|2
3,3X3
4<4t4
546l6
7,8d8
9,:d:
;D;d;
<D<|<
<$=\=
><>t>
1,2d2
3D3|3
3$4\4
5<5t5
647l7
8,9d9
:D:|:
:$;\;
<<<t<
=4>l>
1D1|1
1$2\2
3<3t3
445l5
607h7
8D8|8
8,9h9
:(;`;
<@<x<
< =X=
>@?x?
0(1`1
2@2x2
2 3X3
4@4|4
4$5\5
6<6t6
748l8
90:X:
:0;h;
<(=`=
>@>x>
> ?X?
080p0
102h2
4(5`5
6@6x6
6 7X7
888p8
90:h:
;<;t;
<4=l=
>,?d?
0D0|0
0$1\1
2<2t2
344l4
5,6d6
7D7|7
7$8\8
9<9t9
:4;l;
;4<l<
=,>d>
?D?|?
1<1t1
243l3
4,5d5
6D6|6
6$7\7
8<8t8
94:l:
;,<d<
=D=|=
=$>\>
?<?t?
041l1
2,3d3
4D4|4
4$5\5
6<6t6
748l8
9,:d:
;D;|;
;$<\<
=<=t=
>4?l?
0,1d1
2 2D2p2
383\3|3
4 5x5
6(6P6
6<7\7
9,:P:
=(=H=h=
=$>@>l>
0 0$0,040
7eME~
 r1~k`ay
%[n:|
>`lBPe
fv_z~$
9$`\B+
{Ks_!
ed6{/
"0o`SE
sJF_d
YHRc\
gDz>uR
PZR_G
<juy7
&kMtB
dT)iU
Xogy[K
`dNn\
rZy8T
 [*!_/
9M>]GM
.@$d%tr
hrg,r
+3J >
|]dHH
O"*!U{
W2%x#
>V=IXb
mdk07
"wa co
DyScB
K|- 6
w.&$p
ZeY4u[l
!ZsN}
xxUG6
y#LPA
*>{69
d/P{;
J/deE
|49.V
I(lA!%
pq27L
x.?79
>0m]Z
&l9IBi
"4>\HFN3W
GG2> 
{.BMi
Xv}dw.
US3,2
ZsQ\|(
@^l}#=
Y(HmX
aT<,(?
n@/)@
'Ea<L
\i80)W
Q!m"3
f1.3R
G-r+o
,+_1K
}.wMSN
n>019
B!LJ7
o'p<5
w/uj6A
qh9%jhx
?.%>s
]=1C:
rP/KEi4
*wmUrK "
9&Zmm
gi^t8
S{6SL
]S9SH
4x+L](
1hi#7
_m\p?S
DA{0d]
RHJz}
aYe9m
e@M6=
x;'bC
I?Qdd*
u#mj?
EM\:Y2
aX2is
dxUwP
`r*wigWk
M~D(]k
?`a?i,
?gSv0t
L@I#s
"nO@-g
@h"OZ
zK.mD!
Sz9Jw
2"pb'
zo"8=c
 ;VyT
\*V%<
2<SH3
@Wi!_
> [R8
W2A$s
"&*USMa
PuY,Rg>
O~x ~
}m7lu
WpCh*
W,|g,]&
aN42J
Kp`LE
b[|=x
-Ti)%
XBRQqS
3_Gn=
z/S!i>
=<Jr?]s
w.k!N
m?I)y
ew96W
f}O\w5
; :vk
R-$y2
E<wv:
wds2eJ
@>t!zl
}4&6&
-z%Ji9
LO_M$
1Ta`-,
fD>0&
nJ)[/&6
xQ5cam
[k/.pt
;YY7|
*TB_Xd
&BpaGneS
W6!2L
g "i(
(. xN
S-"OzjU
caDyE
D!gVM
7*X*r5
|u$`a
IijP7
:Z\@~
9Fev[
s5Q~)Ot
Ijw3mG
x^vmE
r~FG{P
T#I2{
{e/j;vL
OYV4Z
\M1N"uc
Washington1
Redmond1
Microsoft Corporation1.0,
%Microsoft Windows Production PCA 20110
220310192419Z
230308192419Z0p1
Washington1
Redmond1
Microsoft Corporation1
Microsoft Windows0
sGJ]Wp
I0G1-0+
$Microsoft Ireland Operations Limited1
232770+4695750
M0K0I
Chttp://www.microsoft.com/pkiops/crl/MicWinProPCA2011_2011-10-19.crl0a
U0S0Q
Ehttp://www.microsoft.com/pkiops/certs/MicWinProPCA2011_2011-10-19.crt0
e%6E)
Washington1
Redmond1
Microsoft Corporation1200
)Microsoft Root Certificate Authority 20100
111019184142Z
261019185142Z0
Washington1
Redmond1
Microsoft Corporation1.0,
%Microsoft Windows Production PCA 20110
O0M0K
Ehttp://crl.microsoft.com/pki/crl/products/MicRooCerAut_2010-06-23.crl0Z
N0L0J
>http://www.microsoft.com/pki/certs/MicRooCerAut_2010-06-23.crt0
TlP0X
R!s4Z
Washington1
Redmond1
Microsoft Corporation1.0,
%Microsoft Windows Production PCA 2011
,ASApw7gBYXBcDWocFzTlsOwO1K9CneYG12fQ0xzcO3c=0Z
"Microsoft Window
 http://www.microsoft.com/windows0
20220506220735.059Z0
Washington1
Redmond1
Microsoft Corporation1)0'
 Microsoft Operations Puerto Rico1&0$
Thales TSS ESN:60BC-E383-26351%0#
Microsoft Time-Stamp Service
Washington1
Redmond1
Microsoft Corporation1&0$
Microsoft Time-Stamp PCA 20100
220302185121Z
230511185121Z0
Washington1
Redmond1
Microsoft Corporation1)0'
 Microsoft Operations Puerto Rico1&0$
Thales TSS ESN:60BC-E383-26351%0#
Microsoft Time-Stamp Service0
frPy=?=
``PJn
X0V0T
Nhttp://www.microsoft.com/pkiops/crl/Microsoft%20Time-Stamp%20PCA%202010(1).crl0l
`0^0\
Phttp://www.microsoft.com/pkiops/certs/Microsoft%20Time-Stamp%20PCA%202010(1).crt0
|jyd&1
 MR^o
"XkU-
Washington1
Redmond1
Microsoft Corporation1200
)Microsoft Root Certificate Authority 20100
210930182225Z
300930183225Z0|1
Washington1
Redmond1
Microsoft Corporation1&0$
Microsoft Time-Stamp PCA 20100
q\Q17
&S|9a
!]_0t
U0S0Q
3http://www.microsoft.com/pkiops/Docs/Repository.htm0
O0M0K
Ehttp://crl.microsoft.com/pki/crl/products/MicRooCerAut_2010-06-23.crl0Z
N0L0J
>http://www.microsoft.com/pki/certs/MicRooCerAut_2010-06-23.crt0
>NGdx
fg:SM
xSu$W
as.,k{n?,
J>f;O
!TkjE
Washington1
Redmond1
Microsoft Corporation1)0'
 Microsoft Operations Puerto Rico1&0$
Thales TSS ESN:60BC-E383-26351%0#
Microsoft Time-Stamp Service
Washington1
Redmond1
Microsoft Corporation1&0$
Microsoft Time-Stamp PCA 20100
20220506181423Z
20220507181423Z0w0=
1/0-0
1(0&0
Washington1
Redmond1
Microsoft Corporation1&0$
Microsoft Time-Stamp PCA 2010
9_m#a
Washington1
Redmond1
Microsoft Corporation1&0$
Microsoft Time-Stamp PCA 2010
qdA3)
&x]c[
