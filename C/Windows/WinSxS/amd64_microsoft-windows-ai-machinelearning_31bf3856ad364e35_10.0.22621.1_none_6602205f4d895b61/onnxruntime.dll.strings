!This program cannot be run in DOS mode.
&Rich
.text
`.rdata
@.data
.pdata
@.didat
_RDATA
@.rsrc
@.reloc
L$HH3
UAVAWH
A_A^]
` UAVAWH
A_A^]
relu@
tanh@
softsignH
softplusH
T$xH;
T$8H;
T$XH;
L$HH3
L$HH3
D$0ATen@
L$PH3
t$ UAVAWH
A_A^]
|$ UH
t39]'u.H
|$ UH
9]'u,H
x ATAVAWH
 A_A^A\
WAVAWH
 A_A^_
WATAUAVAWH
 A_A^A]A\_
WATAUAVAWH
 A_A^A]A\_
L$XH3
@SUVWAVAWH
A+>Hc
(A_A^_^][
t$ WH
tTH;>v
HcL$ H
HcT$ H
wjH9Q
WAVAWH
 A_A^_
\$ UVWH
VWAVH
 A^_^
q#Xr0
@USWH
)t$`H
(t$`H
L$XH3
|$ @8y
L$`H3
[ UVWH
\$ 8Y
L$`H3
\$ 8Y
L$`H3
L$XH3
|$ @8y
L$`H3
@SUVWATAVAWH
\$ 8Y
H!\$@f
L$`H3
pA_A^A\_^][
VWAVH
8\$0u
pA^_^
VWAVH
pA^_^
x AVH
VWAVH
@SVWH
u<!T$@
HcC0HcK,H+
UWAUH
H9Y u`
;w,~d
L$`H3
HcQ0H
|$ 9y0tZ
L$`H3
u%HcK(
K HcC,H
C(+C,
UWATH
G(9G,td
;w,~d
HcA,H
@SVWH
;y(|:
L$(;|$ ~HLcD$ 
HcD$ H
+|$ H
HcC,H
H SVWH
L$ SUVWH
8_^][
L$pH3
I9Jhs
I9Jhs
WAVAWH
 A_A^_
x AVH
l$8H+
x AVH
l$8H+
Ap$"<
H;YhH
BYhI+
l$ VWAVH
 A^_^
@SUVWAVH
t*HcA
tbHcC
0A^_^][
@SUVWAVAWH
(A_A^_^][
@SUVWAVAWH
(A_A^_^][
tdHcL$0
\$ WH
L$ E3
WATAUAVAWH
 A_A^A]A\_
VWAVH
t$HLc
 A^_^
UVWAVAWH
tPD+v
L$HH3
PA_A^_^]
D$(Hc
UVWAVAWH
tPD+v
L$HH3
PA_A^_^]
D$(Hc
@SUVWATAVAWH
0A_A^A\_^][
UVWAVAWH
tPD+v
L$HH3
PA_A^_^]
D$(Hc
@USVWAVH
pA^_^[]
x ATAVAWH
 A_A^A\
@SUVWAUAVAWH
 A_A^A]_^][
@SUVWAUAVAWH
 A_A^A]_^][
VWAVH
 A^_^
VWAVH
 A^_^
VWAVH
 A^_^
WAVAWH
 A_A^_
UVWAVAWH
pA_A^_^]
t$ WH
VWAWH
H93t{H
@A__^
p AWH
H9T$PwfH
H+D$PL;
L+t$PL
x ATAVAWH
 A_A^A\
WAVAWH
 A_A^_
q;TXi
f#L$@f
p;TXi
qcVx:
p;TXi
L$(H3
l$ VWAVH
L$(H3
0A^_^
I9Khs
I9Khs
qP[\=
v:fD;
x ATAVAWH
 A_A^A\
WAVAWH
\$@H+
 A_A^_
WAVAWH
\$@H+
 A_A^_
pP[\=
qP[\=
\$0HcH
fA94Qu
x AVH
Ap$"<
H;QhH
BQhI+
x ATAVAWH
 A_A^A\
@SVWH
u<!T$@
WAVAWH
8\$(t6I
0A_A^_
WAVAWH
 A_A^_
WATAUAVAWH
t$pE3
t$`fD
 A_A^A]A\_
x ATAVAWH
 A_A^A\
VATAUAVAWH
@A_A^A]A\^
H WATAUAVAWH
 A_A^A]A\_
L$ SUVWH
8_^][
SVWAVAWH
(D$Pf
A_A^_^[
@SVWH
USVWAUAVAWH
A_A^A]_^[]
USVWAUAVAWH
A_A^A]_^[]
@SVWH
D$P%H
@SVWH
D$P%H
@SVWH
D$P%H
@SVWH
D$P%H
@USVWATAUAVAWH
A_A^A]A\_^[]
UVWATAUAVAWH
<}w\I
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
@SVWH
u<!T$@
t$ WATAUAVAWH
l$xM+
 A_A^A]A\_
@SVWH
u<!T$@
WAVAWH
 A_A^_
ATAVAWH
A_A^A\
l$ VWAVH
T$$A;
0A^_^
t'HcS
UVWAVAWH
L$`H3
pA_A^_^]
UVWAVAWH
L$`H3
pA_A^_^]
t$ WH
u>Hc/H
r;Ic8I
s(HcD$
x AVH
D9L$`}kH
)Hct$`
UWAVH
H;5g'p
l$ VWAVH
@A^_^
T$`A9r
fA99}
x AVH
WAVAWH
(t$ H
0A_A^_
WAVAWH
(t$ H
0A_A^_
UVWAVAWH
@A_A^_^]
D$(L;
@USVWATAVAWH
pA_A^A\_^[]
UVWAVAWH
H9D$8t%
A_A^_^]
UVWAVAWH
H9D$8t0
A_A^_^]
@USVWATAVAWH
pA_A^A\_^[]
@USVWATAVAWH
pA_A^A\_^[]
qRP|I
@USVWATAVAWH
pA_A^A\_^[]
@USVWATAVAWH
pA_A^A\_^[]
UVWATAUAVAWH
pA_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
D$0E3
T$0D9m
:D$1uVH
s@D;m
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
D$0E3
D$1L!m
D9l$8
:D$1uVH
A_A^A]A\_^]
@SVWH
u<!T$@
@SVWH
u<!T$@
@USVWATAUAVAWH
A_A^A]A\_^[]
@UAVAWH
A_A^]
VWATAVAWH
 A_A^A\_^
p WAVAWH
 A_A^_
SUVWATAUAVAWH
!l$ H
l$ A*
H+L$(x<H
;D$ t
8A_A^A]A\_^][
H9n(H
H9n8H
|$ UH
f9,Yu
UWATAVAWH
d$ E3
A_A^A\_]
\$ UVWAVAWH
pA_A^_^]
UATAUAVAWH
A_A^A]A\]
{ AVH
UVWATAUAVAWH
p#Xr0
<:w/H
p#Xr0
<:w/H
pA_A^A]A\_^]
WAVAWH
p#Xr0
<:u0H
p#Xr0
<:u,H
A_A^_
q!\PI
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
D$QE3
A_A^A]A\_^]
@USVWATAVAWH
8-uCH
A_A^A\_^[]
@USVWATAUAVAWH
A_A^A]A\_^[]
UVWATAUAVAWH
PA_A^A]A\_^]
WAVAWH
@A_A^_
t$ UWATAVAWH
p#Xr0
p#Xr0
p#Xr0
A_A^A\_]
WATAUAVAWH
<%ujL
p#Xr0
A_A^A]A\_
@8sDt
UVWATAUAVAWH
p!\PI
$< t6<$t,<+t"<vt
)D$`M
(D$`H
t$0E3
D,$< 
<xuL@
)D$`M
(D$`H
t$0E3
)D$`H
(D$`fA
)D$`H
)D$`H+
(D$`L
)D$`H
)D$`H
(D$`H
|$8fA
(D$`fA
)D$`H
(D$`fA
)D$`M
(D$`H
A_A^A]A\_^]
UVWATAUAVAWH
D$pE3
D8}ptrI
<xuyH
|$4H9]
d$`t&H
|$@;|$P
p!\PI
p!\PI
p!\PI
p!\PI
A_A^A]A\_^]
@SVWH
u<!T$@
@SVWH
u<!T$@
VWATAVAWH
l$pfD
0A_A^A\_^
x AVH
H;L$pt\H
l$ VWAUAVAWH
D$ I;
L$0H3
A_A^A]_^
UVWAVAWH
L$(H3
0A_A^_^]
WAVAWH
t$`H+
0A_A^_
UVWATAUAVAWH
d$ E3
A_A^A]A\_^]
UVWATAUAVAWH
d$ E3
A_A^A]A\_^]
\$ UVWATAUAVAWH
A_A^A]A\_^]
UATAUAVAWH
A_A^A]A\]
{ AVH
UVWATAUAVAWH
p;TXi
p;TXi
<:wLH
p;TXi
p;TXi
p;TXi
<:wLH
p;TXi
p;TXi
pA_A^A]A\_^]
WAVAWH
<:u0H
<:u,H
A_A^_
VWAVH
q!\PI
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
D$RE3
A_A^A]A\_^]
|$PfF
D$`fD
@USVWATAVAWH
8-uGH
A_A^A\_^[]
@USVWATAUAVAWH
A_A^A]A\_^[]
L;L$(t
r5w,H
L;L$(A
x UAVAWH
A_A^]
qX\pG
@SVWH
USVWAUAVAWH
A_A^A]_^[]
USVWAUAVAWH
A_A^A]_^[]
@SVWH
D$P%H
@SVWH
D$P%H
@SVWH
D$P%H
@SVWH
D$P%H
@USVWATAUAVAWH
D$(fD
A_A^A]A\_^[]
@USVWATAVAWH
pA_A^A\_^[]
UVWAVAWH
H9D$8t%
A_A^_^]
UVWAVAWH
H9D$8t0
A_A^_^]
@USVWATAVAWH
pA_A^A\_^[]
@USVWATAVAWH
pA_A^A\_^[]
qRP|I
@USVWATAVAWH
pA_A^A\_^[]
@USVWATAVAWH
pA_A^A\_^[]
UVWATAUAVAWH
pA_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
UATAUAVAWH
A_A^A]A\]
{ AVH
UVWATAUAVAWH
p;TXi
p;TXi
<:wLH
p;TXi
p;TXi
p;TXi
<:wLH
p;TXi
p;TXi
pA_A^A]A\_^]
WAVAWH
<:u0H
<:u,H
A_A^_
q!\PI
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
D$RE3
A_A^A]A\_^]
|$PfF
D$`fD
@USVWATAVAWH
8-uGH
A_A^A\_^[]
@USVWATAUAVAWH
A_A^A]A\_^[]
UVWATAUAVAWH
u(!D$(H
@A_A^A]A\_^]
x AVH
UWAVH
qX\pG
@SVWH
USVWAUAVAWH
A_A^A]_^[]
USVWAUAVAWH
A_A^A]_^[]
@SVWH
D$P%H
@SVWH
D$P%H
@SVWH
D$P%H
@SVWH
D$P%H
@USVWATAUAVAWH
D$(fD
A_A^A]A\_^[]
@USVWATAVAWH
pA_A^A\_^[]
UVWAVAWH
H9D$8t%
A_A^_^]
UVWAVAWH
H9D$8t0
A_A^_^]
@USVWATAVAWH
pA_A^A\_^[]
@USVWATAVAWH
pA_A^A\_^[]
qRP|I
@USVWATAVAWH
pA_A^A\_^[]
@USVWATAVAWH
pA_A^A\_^[]
UVWATAUAVAWH
pA_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
t$ UWATAVAWH
p;TXi
A_A^A\_]
WATAUAVAWH
<%ujL
p;TXi
A_A^A]A\_
USVWAVH
D8wDt
G@D8uPt
`A^_^[]
UVWATAUAVAWH
l$0M9o
8MwtFI
p!\PI
0< t6<$t,<+t"<vt
)D$`H
(D$`L
|$8E3
D,0< 
)D$`H
(D$`L
|$8E3
)D$`H
(D$`fA
)D$`H
)D$`H+
(D$`L
)D$`H
)D$`H
(D$`L
|$8fA
(D$`fA
)D$`H
(D$`fA
)D$`H
(D$`H
A_A^A]A\_^]
UVWATAUAVAWH
t$pE3
D8eptvI
D4h< t <$
p;TXi
d$8H9]
t$`t%H
D8,8~
D8l$4uSH
|$@;|$D
d$8H9]
p!\PI
p!\PI
p!\PI
d$8E3
p!\PI
d$8L9u
A_A^A]A\_^]
UVWATAUAVAWH
<}w^I
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
pP[\=
UVWATAUAVAWH
T$89T$@
D;t$@
d$48\$<tDH
H;\$Ht'
H;\$Hu
d$4D8t$1u
A_A^A]A\_^]
UVWATAUAVAWH
0tmfE
8\$1u
A_A^A]A\_^]
WATAUAVAWH
<%ujL
p;TXi
A_A^A]A\_
@8sDt
UVWATAUAVAWH
l$0M9o
8MwtFI
p!\PI
0< t6<$t,<+t"<vt
)D$`H
(D$`L
|$8E3
D,0< 
)D$`H
(D$`L
|$8E3
)D$`H
(D$`fA
)D$`H
)D$`H+
(D$`L
)D$`H
)D$`H
(D$`L
|$8fA
(D$`fA
)D$`H
(D$`fA
)D$`H
(D$`H
A_A^A]A\_^]
UVWATAUAVAWH
t$pE3
D8eptvI
D4h< t <$
p;TXi
d$8H9]
t$`t%H
D8,8~
D8l$4uSH
|$@;|$D
d$8H9]
p!\PI
p!\PI
p!\PI
d$8E3
p!\PI
d$8L9u
A_A^A]A\_^]
UVWATAUAVAWH
<}w^I
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
T$89T$@
D;t$@
d$48\$<tDH
H;\$Ht'
H;\$Hu
d$4D8t$1u
A_A^A]A\_^]
UVWATAUAVAWH
0tmfE
8\$1u
A_A^A]A\_^]
UVWATAUAVAWH
D$`E3
D9l$8
d$4E3
d$4D8l$1u
A_A^A]A\_^]
UVWATAUAVAWH
D$`E3
D9l$8
d$4E3
d$4D8l$1u
A_A^A]A\_^]
@SVWH
u<!T$@
@SVWH
u<!T$@
@SVWH
u<!T$@
@SVWH
u<!T$@
@SVWH
u<!T$@
@SVWH
u<!T$@
@SVWH
u<!T$@
@SVWH
u<!T$@
UVWATAUAVAWH
pA_A^A]A\_^]
pP[\=
pP[\=
@USVWATAUAVAWH
fE9!t A
A_A^A]A\_^[]
fC9D}
@USVWATAUAVAWH
fE9!t A
A_A^A]A\_^[]
fC9D}
WATAUAVAWH
 A_A^A]A\_
WATAUAVAWH
 A_A^A]A\_
WATAUAVAWH
 A_A^A]A\_
WATAUAVAWH
 A_A^A]A\_
WAVAWH
 A_A^_
x ATAVAWH
 A_A^A\
WATAUAVAWH
 A_A^A]A\_
H WATAUAVAWH
 A_A^A]A\_
SUVWATAUAVAWH
H+L$ xFH
8A_A^A]A\_^][
@USVWAUAVAWH
e0A_A^A]_^[]
WAVAWH
D8<:u
D8<.u
 A_A^_
t$ UWAVH
@USVWAVAWH
GPH9GHtPH
A_A^_^[]
VWAVH
 A^_^
L$@H3
L$XH3
L$XH3
L$XH3
L$XH3
UWAVH
UWATAVAWH
H98|XH
L+D$PI
L$0E3
H9\$0
A_A^A\_]
@SVWATAUAVAWH
L$XH3
`A_A^A]A\_^[
@SVWAVAWH
L$HH3
PA_A^_^[
VWAVH
H;D$hu
(D$Pf
u18Ptu,H
s WAVAWH
A_A^_
s WATAUAVAWH
A_A^A]A\_
VWAVH
VWATAVAWH
H;D$hu
A_A^A\_^
VWAVH
qCZpO
VWAVH
L$8H3
@A^_^
P(H;P0t
@SVWATAUAVAWH
d$ E3
t$0L9t$pt
M94$u
t$0L9t$pt
L$xH3
A_A^A]A\_^[
@USVWATAUAVAWH
D$8L;
D$8L;
D$8L;
A_A^A]A\_^[]
UVWAVAWH
UUUUUUU
@A_A^_^]
UVWATAUAVAWH
pA_A^A]A\_^]
SVWAVH
L$XH3
hA^_^[
VWAVH
L$XH3
`A^_^
@SVWATAUAVAWH
A_A^A]A\_^[
!T$TI!S
T$Xf!T$xI
L$HH3
L$XH3
[ VWAVH
t$8L9t$ht
L$pH3
@SVWATAUAVAWH
G@H9G8u
W@H+W8H
D$0H;
T$0H;
T$0H;
D$0H;
T$0H;
T$0H;
G@H+G8H
A_A^A]A\_^[
@SVWATAUAVAWH
H9FPu
VXH+VPH
D$0H;
T$0H;
T$0H;
A_A^A]A\_^[
L$PH3
L$PH3
SVWATAUAVAWH
D$pH;
0A_A^A]A\_^[
|$ AVH
 H;D$8u
L$@H3
L$0H+
L$@H3
@SVATAVAWH
\$0H+
H;\$8tgH
 H;D$8u
IH;\$8t9H
H;D$8u
L$@H3
PA_A^A\^[
@SVWH
D$8H+D$0H
L$@H3
L$@H3
L$@H3
@SVWAVH
H9\$ t
L$0H3
HA^_^[
\$ WH
L$XH3
|$ AVH
\$ WH
L$XH3
\$ WH
L$XH3
\$ WH
L$XH3
\$ WH
L$XH3
\$ WH
L$XH3
@SVWAVH
L$0H;L$hu
L$xH+
A^_^[
@SVWATAUAVAWH
L$@H3
PA_A^A]A\_^[
t$ WH
D$HH+
D$HH+
L$PH3
x AVH
@SUVWAVH
L$HH3
PA^_^][
@SVWH
T$0H+
L$8H3
UVWAVAWH
A_A^_^]
\$ UVWATAUAVAWH
A_A^A]A\_^]
\$ UVWAVAWH
9\$ u H
T$8H+
L$@H3
PA_A^_^]
x AVH
SVWAVAWH
A_A^_^[
SVWAVH
pSV~0
A^_^[
VWATAVAWH
@A_A^A\_^
WAVAWH
D$HI+
L$PH3
A_A^_
WATAUAVAWH
\$XE3
A_A^A]A\_
SVWAVAWH
L$XH3
`A_A^_^[
VWAVH
L$XH3
`A^_^
Y2}>D
t$ WH
fffffff
UVWAVAWH
 A_A^_^]
t$ WH
S H+S
t$ WH
SVWAVAWH
(D$Pf
A_A^_^[
UVWAVAWH
 A_A^_^]
WAVAWH
 A_A^_
t$ WH
H;{ H
SUVWATAUAVAWH
L9|$x
L9|$xu
(A_A^A]A\_^][
SVATAVAWH
B84:u
pA_A^A\^[
UVWATAUAVAWH
 A_A^A]A\_^]
UVWATAUAVAWH
 A_A^A]A\_^]
@SVWH
L$0H3
@SVWH
L$0H3
@USVWATAVAWH
A_A^A\_^[]
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
D$@H;
D$@H;
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
@SVWAVAWH
L$0H3
@A_A^_^[
@SVWAVAWH
L$0H3
@A_A^_^[
\$ UH
\$ UH
@SUVWATAVAWH
 A_A^A\_^][
L$8H3
L$8H3
l$ VWAVH
0A^_^
VWAVH
 A^_^
t$ WATAUAVAWH
t$@I;
L$HH3
A_A^A]A\_
?H+1H
L$8H3
WAVAWH
 A_A^_
@SVWH
|$ AVH
L$@H3
|$ AVH
L$@H3
|$ AVH
L$@H3
|$ AVH
L$@H3
|$ AVH
L$@H3
|$ AVH
L$@H3
\$ UVWATAUAVAWH
G L+G
G L+G
A_A^A]A\_^]
\$ UVWATAUAVAWH
G L+G
G L+G
A_A^A]A\_^]
\$ UVWATAUAVAWH
G L+G
G L+G
A_A^A]A\_^]
\$ UVWATAUAVAWH
G L+G
G L+G
A_A^A]A\_^]
\$ UVWATAUAVAWH
G L+G
G L+G
D$0H9x }
H;y |
|$xE3
A_A^A]A\_^]
\$ UVWATAUAVAWH
G L+G
G L+G
D$0H9P }
H;Q }iH
UUUUUUU
A_A^A]A\_^]
\$ UVWATAUAVAWH
G L+G
G L+G
L$0H9Q }
H;P }iH
UUUUUUU
A_A^A]A\_^]
\$ UVWATAUAVAWH
G L+G
G L+G
D$0H9P }
H;Q }iH
UUUUUUU
A_A^A]A\_^]
UVWATAUAVAWH
 A_A^A]A\_^]
SVWATAUAVAWH
0A_A^A]A\_^[
@SVWATAUAVAWH
D$0I;
|$`I;
L$hH3
pA_A^A]A\_^[
L$8H3
L$8H3
@SUVWATAVAWH
@A_A^A\_^][
@SUVWATAVAWH
3333333
PA_A^A\_^][
@SVWH
\$ UVWH
l$ VWATAVAWH
 A_A^A\_^
l$ VWATAVAWH
 A_A^A\_^
l$ VWATAVAWH
 A_A^A\_^
UVWATAUAVAWH
0A_A^A]A\_^]
t$ WH
H;{ H
UVWATAUAVAWH
t$0I#
H9uPu9K
@A_A^A]A\_^]
\$ UVWH
\$ UVWH
VWATAVAWH
@A_A^A\_^
p WATAUAVAWH
A_A^A]A\_
fA94Qu
fA94Qu
qIR|[
q"Zzv6
H9\$ t
L$(H3
` AUAVAWH
UUUUUUU
 A_A^A]
\$(H;
L$8H3
L$@H3
D$=H!S
L$@H3
@SUVWATAVAWH
L$@H3
PA_A^A\_^][
UVWATAUAVAWH
l$ H!|$(
`A_A^A]A\_^]
[ UVWH
L$@H3
\$ UVWH
\$ UVWH
l$ VWAVH
L$pH3
@USVWATAVAWH
I9>rkH
pA_A^A\_^[]
@USVWAVH
pA^_^[]
qSPV&
@USVWAVH
pA^_^[]
L$PH3
\$ UVWATAUAVAWH
pP^P[
S H;S(t
pP^P[
|$PH9
|$PH9
A_A^A]A\_^]
UVWAVAWH
|$ H!\$(
PA_A^_^]
VWAVH
 A^_^
UVWAVAWH
 A_A^_^]
VWAVH
 A^_^
|$ AVH
@SUVWATAVAWH
UUUUUUU
(D$ f
0A_A^A\_^][
@SVWATAUAVAWH
\$HMi
D$0I;
t$`M;
L$hH3
pA_A^A]A\_^[
t$ WAVAWH
@A_A^_
|$ AVH
WAVAWH
 A_A^_
L$8H3
VWAVH
 A^_^
l$ VWAVH
L$@H3
p AWH
t$ WH
H;{ H
SUVWATAUAVAWH
L#q0M
(A_A^A]A\_^][
L#q0L
VWAUH
\$HE3
 A]_^
\$ UVWH
\$ UVWH
s WAVAWH
L$HH3
A_A^_
l$ VWATAVAWH
 A_A^A\_^
t$ WH
qRT8Y
\$ UVWATAUAVAWH
PA_A^A]A\_^]
p WATAUAVAWH
A_A^A]A\_
\$ UVWH
\$ UVWH
VWAVH
 A^_^
UVWATAUAVAWH
0A_A^A]A\_^]
t$ WH
H;{ H
SUVWATAUAVAWH
L9|$x
L9|$xu
(A_A^A]A\_^][
t$ WH
S H+S
@SUVWATAVAWH
(D$ f
0A_A^A\_^][
VWAVH
 A^_^
H;{ H
UVWATAUAVAWH
M#u0M
H;\$xt/H;
H9l$pu
 A_A^A]A\_^]
H9l$pu
M#u0H
H;\$pt
L$@H3
WAVAWH
D8<:u
D8<.u
 A_A^_
\$ UVWH
L$@H3
\X,>Y
qCZTG?)
\X,>Y
qCZTG?)
q!P~'
qpU2D
D;B(|
WAVAWH
0A_A^_
q3]x!7d
q)U^#
q2\Zi
q X>Y
(D$ f
qsVZD
q8^pi
q+Rr`
q9R43
qq]t5
L$(H3
qb]rn
qqWp)
|$ AVH
qyQ|%
q#Sz}
qZTXT
q*]p-
qyT~}
q!_X;
qJXX%
qaPp|
q+Q<^
pCZTG?)
pCZTG?)
pCZTG?)
pCZTG?)
qbVZd
qY]4r
t$ WH
t$ WH
t$ WH
L$@H3
UVWATAUAVAWH
A_A^A]A\_^]
q!UVt
qCPTn
L$0H3
L$8H3
S6=7p
qQZXU&8
x UAVAWH
A_A^]
UVWATAUAVAWH
UXI;U`t/H
A_A^A]A\_^]
UATAWH
A_A\]
x UAVAWH
L$0I;
\$ E3
A_A^]
L$@H3
L$@H3
qBTXM
pqXrv
qqV|!
L$@H3
L$@H3
x AVH
UVWAVAWH
|$0H!\$8
@A_A^_^]
t$ WH
H;{ H
l$ VWATAVAWH
 A_A^A\_^
SUVWATAUAVAWH
L9|$x
L9|$xu
(A_A^A]A\_^][
@SUVWAVH
@A^_^][
@SUVWAVH
@A^_^][
WATAUAVAWH
wTH;J
 A_A^A]A\_
p WAVAWH
D$(Hc
D$0H9H s
H;K r
UUUUUUU
A_A^_
t$ WH
@SUVWATAVAWH
 A_A^A\_^][
\$ UVWATAUAVAWH
D$0I;
|$ H+
@A_A^A]A\_^]
|$ UATAUAVAWH
A_A^A]A\]
WAVAWH
 A_A^_
@SUVWATAVAWH
@A_A^A\_^][
@SUVWH
@SUVWATAVAWH
PA_A^A\_^][
UVWATAUAVAWH
0A_A^A]A\_^]
t$ WH
H;{ H
SUVWATAUAVAWH
L9|$x
L9|$xu
(A_A^A]A\_^][
\$ WH
L$HH3
H9\$ t$H
L$(H3
H9\$ t$H
L$(H3
@USVWAVH
A^_^[]
\$ UVWATAUAVAWH
H9>tW
|$ E3
|$ E3
I9>t]H
A_A^A]A\_^]
VWAVH
 A^_^
@USVWAVAWH
A_A^_^[]
UVWATAUAVAWH
H;GxtBH
`A_A^A]A\_^]
@SUVWAVH
G@H;GHt
OhH;Wxt
0A^_^][
@SUVWATAVAWH
PA_A^A\_^][
UVWATAUAVAWH
 A_A^A]A\_^]
x ATAVAWH
gfffffffI
fffffff
 A_A^A\
\$ UVWH
SVWATAUAVAWH
\$pL;
0A_A^A]A\_^[
ATAVAWH
 A_A^A\
L$@H3
t$ WH
H9:t+
t$ WH
t$ WH
t$ WH
@USVWATAUAVAWH
L$xH+
F H;F(t
A_A^A]A\_^[]
@USVWATAVAWH
L$0E3
A_A^A\_^[]
WATAVH
L$0E3
@A^A\_
WATAUAVAWH
0A_A^A]A\_
VWAVH
 A^_^
L$8H+
pcP>w
x AVH
H+A8H;
y(+y0A
q8+q(M
|$ AVH
C@f99H
p WATAUAVAWH
L9aHs
Q(+Q0
H+C8I;
s(+s0
H+C8H;
D8SQtUL
s D+s0D
A_A^A]A\_
Q +Q0
H+C8H;
H+C8H;
C +C0
D8Q(uRH
E8A(u.M
t$ WH
s)f9j
s&f9j
f;)s*f9Y
VWATAVAWH
fD;!s
s+fD9z
s+fD9z
s+fD9y
D9>v%
 A_A^A\_^
VWAVH
s%f9i
fD;1s
 A^_^
t$ WAVAWH
fD;9s
s*f9i
fD;1s
fD;1s
 A_A^_
VWATAVAWH
fD;!s
f;2s+fD9z
s+fD9z
s+fD9z
D9>v%
D9;v!
 A_A^A\_^
s*f9r
s*f9r
s)f9r
s!f9r
s"f9r
s"f9r
VWATAVAWH
fD;!s
fD;!s
s#fD9z
s#fD9z
 A_A^A\_^
x ATAVAWH
fD;!s
fD;!s
D9>v%
D9>v'
s+fD9y
fD;!s
fD;!s
 A_A^A\
t$ WAVAWH
fD;9s*f9i
fD;1s
s*f9i
fD;1s
fD;1s
 A_A^_
x AVH
fD;1s
t$ WH
s*f9Y
)D$0f
UWAVH
@USVWATAUAVAWH
G8H;G@t
A_A^A]A\_^[]
\$ UVWATAUAVAWH
L$PE3
A_A^A]A\_^]
CT$0L
\$ UVWATAUAVAWH
A_A^A]A\_^]
T$HH;
UVWATAUAVAWH
9L$@u 9
A_A^A]A\_^]
UATAUAVAWH
A_A^A]A\]
@USVWATAUAVAWH
A_A^A]A\_^[]
UATAUAVAWH
A_A^A]A\]
@USVWAUAVAWH
A_A^A]_^[]
@USWH
@USVWAUAVAWH
`A_A^A]_^[]
@USVWAVH
A^_^[]
@USVWATAVAWH
L9|$8t>
L9|$@t6
|$p+}
T$p+U
L$p+M
T$p+U
L$p+M
D$p+E
ORTMH
T$p+U
L$p+M
D$p+E
D$xLc
A_A^A\_^[]
SVWATAUAVAWH
|$`@8
A_A^A]A\_^[
\$ UVWATAUAVAWH
T$ E3
A_A^A]A\_^]
\$ UVWATAUAVAWH
A_A^A]A\_^]
@USVWAVH
A^_^[]
@USVWATAUAVAWH
H9\$ht8
D$hH;
D$hH;
L9t$htX
L9|$ht8
L9|$h
L9|$ht8
L9|$ht8
\$xI;
L9|$ht]
A_A^A]A\_^[]
t$ UWAUAVAWH
A_A^A]_]
\$ UVWATAUAVAWH
fD;1s
L$PI;
L;D$`u
8T$@I
L$pI;
A_A^A]A\_^]
UVWAVAWH
A_A^_^]
UVWATAUAVAWH
D$hE3
A_A^A]A\_^]
@USVWH
H_^[]
|$ AVH
L9P0H
EH0D9Q(t+H
L9Q0H
EQ0D9R(t+H
s WATAUAVAWH
H;Gxu
@8t$pt^H
D$xI9
D$qL;l$x
@8t$pt9H
@8t$pt9H
p[TVd?
@8t$pt9H
@8t$pt9H
@8t$pt9H
@8t$pt9H
@8t$pt9H
@8t$pt9H
@8t$q
@8p8t$H
H9p0H
A_A^A]A\_
x UAVAWH
A_A^]
\$ UH
\$ UH
x UATAUAVAWH
A_A^A]A\]
UVWATAUAVAWH
L$XE3
L$PH;
t$XI;
A_A^A]A\_^]
@USVWAVH
L$@I;
A^_^[]
@USVWATAUAVAWH
p8H;]
pcP>w
A_A^A]A\_^[]
@USVWAVAWH
A_A^_^[]
@SVWATAUAVAWH
|$xH9
l$XI9}
l$PH;
l$PH9
D$X@8x)t
D$XH9
L$pH+
D$hH+
A_A^A]A\_^[
t$ UWATAVAWH
D$ L;
A_A^A\_]
@USVWATAUAVAWH
H9]pt53
D$pHcH
\$)fD
A_A^A]A\_^[]
UWAVH
\$ UVWATAUAVAWH
)t$PI
H;W8v,H
(t$PH
`A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
\$ UVWH
q[TVd?
\$ UVWATAUAVAWH
t)D8}`t#I
A_A^A]A\_^]
SVWAVAWH
(D$Pf
pA_A^_^[
t$ WH
t$ WH
H;{ H
t$ WH
VWAVH
 A^_^
t$ WH
@SUVWATAVAWH
@A_A^A\_^][
t$ WH
S H+S
t$ WH
H;{ H
@SVWH
D$ tl
L$(H3
@SUVWAVH
L90u"H
L$(H3
0A^_^][
x AVH
\$ UVWAVAWH
 A_A^_^]
U?H;UGs H
E'H9E
E/ueH
CM/L+
teH9Khu
t$ UWAVH
D8K|t+L9
VWAVH
`A^_^
SVWAVAWH
(D$Pf
A_A^_^[
SVWAVAWH
(D$Pf
A_A^_^[
|$ UATAUAVAWH
C@H;U
tDH;U
C@H;U
A_A^A]A\]
fffffff
SUVWATAUAVAWH
L9|$x
L9|$xu
(A_A^A]A\_^][
UVWAVAWH
 A_A^_^]
SUVWATAUAVAWH
L9|$x
L9|$xu
(A_A^A]A\_^][
L$XH3
D$8H+
L$8H3
VWAVH
 A^_^
UVWATAUAVAWH
H;|$ 
0A_A^A]A\_^]
|$ UATAUAVAWH
A_A^A]A\]
t$ WH
WAVAWH
 A_A^_
Q(+Q0
H+C8H
C(+C0
VWATAVAWH
fD;9s
fD;!s
 A_A^A\_^
Q(+Q0
W(+W0
VWAUAVAWH
fD;)s
fD;)s
D9;v 
 A_A^A]_^
l$ VWATAUAVH
fD;!s
fD;)s
 A^A]A\_^
WATAUAVAWH
fD;9s+fD9q
fD;!s
fD;)s
 A_A^A]A\_
WATAUAVAWH
fD;!s
fD;)s
fD;9s
 A_A^A]A\_
t$ WAVAWH
fD;9s
s$fD9q
 A_A^_
l$ VWATAUAVH
fD;!s
fD;)s
 A^A]A\_^
VWATAVAWH
fD;9s
 A_A^A\_^
SVWATAUAVAWH
fB94zu
@8t$(u
@@H90t
@X90~
@@H90t
@X90~
0A_A^A]A\_^[
|$ UH
CT$8L
\$ UVWATAWH
A_A\_^]
@USVWAVAWH
A_A^_^[]
t$ UWATAVAWH
A_A^A\_]
UVWATAUAVAWH
 A_A^A]A\_^]
@SUVWATAVAWH
@A_A^A\_^][
@SVWH
@SUVWATAVAWH
@A_A^A\_^][
\$ UVWATAUAVAWH
 A_A^A]A\_^]
@SUVWATAVAWH
PA_A^A\_^][
@SUVWATAVAWH
UUUUUUU
@A_A^A\_^][
l$ VWATAVAWH
 A_A^A\_^
@SUVWATAVAWH
 A_A^A\_^][
@SUVWATAVAWH
 A_A^A\_^][
\$ UVATAVAWH
 A_A^A\^]
\$ UVWH
UWAUAVAWH
A_A^A]_]
|$ UATAUAVAWH
A_A^A]A\]
VWAVH
`A^_^
SVAVH
pA^^[
(D$Pf
(L$`f
\$ UVWH
UVWATAUAVAWH
0A_A^A]A\_^]
UVWATAUAVAWH
0A_A^A]A\_^]
t$ WH
H;{ H
t$ WH
H;{ H
SUVWATAUAVAWH
L9|$x
L9|$xu
(A_A^A]A\_^][
SUVWATAUAVAWH
L9|$x
L9|$xu
(A_A^A]A\_^][
WATAUAVAWH
|$$E2
A_A^A]A\_
@SVWH
@SUVWAVH
A^_^][
@SVWAVH
A^_^[
@SVWH
SVWATAUAVAWH
8\$(u
@A_A^A]A\_^[
t$ WH
@SVWAVH
A^_^[
t$ UWAVH
@USVWATAVAWH
A_A^A\_^[]
@SUVWH
l$ VWAVH
 A^_^
\$ UVWH
@SUVWAVH
 A^_^][
\$ UVWH
@SUVWAVH
 A^_^][
@SUVWAVH
 A^_^][
@SVWH
L$8H3
D$(L#
@USVWAVH
D$PfD
A^_^[]
qHZR4
x AWH
UUUUUUU
u>A9P(t}H
uKA9P(t:H
@USVWAVH
A^_^[]
@USWH
@USWH
UVWAVAWH
9K(t;H
9K(t;H
D9{(t8H
A_A^_^]
D$<E3
VWATAVAWH
V H+V
A_A^A\_^
x UATAUAVAWH
A_A^A]A\]
x UATAUAVAWH
A_A^A]A\]
x UATAUAVAWH
A_A^A]A\]
UATAUAVAWH
A_A^A]A\]
UATAUAVAWH
A_A^A]A\]
x UATAUAVAWH
A_A^A]A\]
x UATAUAVAWH
A_A^A]A\]
` UAVAWH
A_A^]
T$ H;
T$ H;
T$ H;
UATAUAVAWH
A_A^A]A\]
@SUVWATAVAWH
C0H!C(H!C0H
D$0L;
L$@H3
PA_A^A\_^][
@USVWAVH
L;A(tCD
M;A(u
`A^_^[]
UVWAVAWH
T$0E3
)D$`f
A_A^_^]
SVWATAUAVAWH
A_A^A]A\_^[
UWAVH
T$0H;
UAVAWH
A_A^]
\$0HcH
|$ AVH
D$09P }
@USVWAWH
`A__^[]
L$@H3
@SVWH
\$ UVWAVAWH
D$09P }
PA_A^_^]
SVWATAUAVAWH
C8<!u
tKL+t$ L+
@A_A^A]A\_^[
@SUVWATAVAWH
PA_A^A\_^][
\$ UVWATAUAVAWH
PA_A^A]A\_^]
UVWATAUAVAWH
0A_A^A]A\_^]
UVWATAUAVAWH
0A_A^A]A\_^]
t$ WH
H;{ H
t$ WH
H;{ H
SUVWATAUAVAWH
L9|$x
L9|$xu
(A_A^A]A\_^][
SUVWATAUAVAWH
L9|$x
L9|$xu
(A_A^A]A\_^][
SVWATAUAVAWH
D$@H+
tLH+\$8H+
PA_A^A]A\_^[
t$ WATAUAVAWH
A_A^A]A\_
qHZR4
qHZR4
L$ SUVWH
8_^][
@USVWATAUAVH
D$*E3
A^A]A\_^[]
@USVWATH
A\_^[]
@USVWAWH
A__^[]
@USVWAWH
A__^[]
@USVWAVH
A^_^[]
@USVWAVH
A^_^[]
@USVWAVH
A^_^[]
\$ UVWAVAWH
A_A^_^]
@USVWAVH
A^_^[]
|$ AUAVAWH
Ic@8H
A_A^A]
@USVWATAUAVH
A^A]A\_^[]
S H;S(t\H
(D$@f
t$ UWATAVAWH
D8e@t.
A_A^A\_]
SUVWAVAWH
hA_A^_^][
UATAUAVAWH
E8,9u
D8,8u
A_A^A]A\]
UVWATAUAVAWH
D$(L;
L;|$(
L$HH3
PA_A^A]A\_^]
|$ UH
trueH
nullH
falsH
t$ WATAUAVAWH
D$$H;WHt
t$(H;
D9 uHH
L$8H3
A_A^A]A\_
UVWATAUAVAWH
EgH;SHt
A_A^A]A\_^]
 !!"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
D$0H;SHt
x AVH
D$0H;
L$0H3
@SUVWAVAWH
)D$ H
L$(fH
8A_A^_^][
t$ WH
L$`H3
\$PM;
t$ WH
UVWAVAWH
A_A^_^]
t$ WATAUAVAWH
tNxQH
 A_A^A]A\_
p UWATAVAWH
L$hE3
A_A^A\_]
UVWATAUAVAWH
n(@8~)
n(@8~)
n(@8~)
T$hI;
n(@8~)
T$hI;
n(@8~)
L$PE3
n(@8~)
n(@8~)
T$hI;
n(@8~)
n(@8~)u~H
A_A^A]A\_^]
l$ VWATAVAWH
 A_A^A\_^
x UAUAVH
A^A]]
D$pE3
L$xM9K
(D$pf
(L$pf
VWAVH
3333333
UVWAVAWH
L$XH3
`A_A^_^]
t$ WH
L$HH3
l$ VWAVH
 A^_^
UVWAVAWH
 A_A^_^]
t$ UWAWH
@USVWATAVAWH
A_A^A\_^[]
t$ WH
L$8H3
\$ UVWAVAWH
pA_A^_^]
UVWAVAWH
pA_A^_^]
UVWAVAWH
pA_A^_^]
UVWAVAWH
pA_A^_^]
UVWAVAWH
pA_A^_^]
UVWAVAWH
pA_A^_^]
UVWATAUAVAWH
 A_A^A]A\_^]
UVWATAUAVAWH
 A_A^A]A\_^]
L$ L;L$ u
H;L$(tC
VATAUAVAWH
\$ M;
PA_A^A]A\^
\$ VATAUAVAWH
\$ M;
PA_A^A]A\^
UVATAVAWH
 A_A^A\^]
UVATAVAWH
 A_A^A\^]
UVATAVAWH
 A_A^A\^]
UVATAVAWH
0A_A^A\^]
UVATAVAWH
 A_A^A\^]
l$ VWAVH
 A^_^
u)A8p
u%A8p
t$ WH
p WATAUAVAWH
A_A^A]A\_
WAVAWH
0A_A^_
L$ SUVWAVAWH
8A_A^_^][
L$ SWH
L$XH+
\$ UVWAVAWH
t,D8=
A_A^_^]
x AVH
\$ UVWH
D$0H;
L$ SUVWAVAWH
H!\$(L
HA_A^_^][
t$ WATAUAVAWH
f9<Au
f9<Au
fD94Ou
t;9)u
|$ H;
|$(H;
|$0H;
|$@H;
|$xH;
|$hH;
t$PE3
t4E88t/H
E8<0u
 A_A^A]A\_
VWAVH
L95gJg
PA^_^
WATAUAVAWH
tr@8=
t"@8=
@8=WGg
A_A^A]A\_
x AVH
L$8E3
D$ E3
L$(H3
x UATAUAVAWH
D$0H+
HcL$ H
HcD$$H
A_A^A]A\]
WATAUAVAWH
L9O@t
H9oHsI
 A_A^A]A\_
VWAVH
 A^_^
l$ VWAVH
 A^_^
T$8H!|$8
x AVH
UVWAVAWH
0A_A^_^]
\$ UVWAVAWH
`A_A^_^]
|$ UH
L$@H3
t$ UWAVH
t$ WH
UVWATAUAVAWH
D$@H+
A_A^A]A\_^]
t:fA9(t4H
fA9,@u
L$8H+
@USVWH
h_^[]
\$ UVWATAUAVAWH
D$xH;
A_A^A]A\_^]
UVWATAUAVAWH
D$@H;
D$@H;
EPH;U0t
EPH;U0t
D$@H;
D$@H;
EPH;U0t
D$@M;
A_A^A]A\_^]
VWATAVAWH
@8|$(
@X98~
AX98~
@X98~
pA_A^A\_^
@SVWATAUAVAWH
|$PLi
D$ I;
t$hI;
>HiL$@
L$pH3
A_A^A]A\_^[
@SVWATAUAVAWH
|$PLi
D$ I;
t$hI;
>HiL$@x
L$pH3
A_A^A]A\_^[
l$ VWATAVAWH
 A_A^A\_^
l$ VWATAVAWH
 A_A^A\_^
l$ VWATAVAWH
 A_A^A\_^
L$@H3
L$@H3
l$ VWAVH
0A^_^
t$ WH
S H+S
@SUVWATAVAWH
fD9$Ou
|$@E3
L$XH3
`A_A^A\_^][
L$(H3
l$ VWAVH
0A^_^
@USVWH
@USVWAVH
A^_^[]
L$0H3
VWAVH
L$(H3
0A^_^
qq^^|
UVWATAUAVAWH
A_A^A]A\_^]
q2W<R.
|$ AVH
pz^:7'
L$HH3
@SVWAUAVAWH
D$`H+
A_A^A]_^[
@SVWATAUAVAWH
L$pH!
|$pI;
D$xH+
D$xH+
H+t$PH
(D$Pf
(L$@f
A_A^A]A\_^[
@USVWATAVAWH
A_A^A\_^[]
\$ WH
(D$Pf
(L$@f
L$pH3
WAVAWH
D$ E3
A_A^_
L$HH3
@SUVWAVAWH
A_A^_^][
\$ UVWATAUAVAWH
L$PH;
\$hI;
E88MHH
D$pM;
D$pL;
L$pH;
d$pM+
l$PI+
(D$pf
(D$Pf
A_A^A]A\_^]
d$@E3
L$(H3
UVWATAUAVAWH
pA_A^A]A\_^]
t$ WH
t$ WH
u8D9J
u'D9J
VWAVH
 A^_^
VWAVH
 A^_^
L$HH3
@SVWAVH
L$XH3
hA^_^[
WAVAWH
@A_A^_
l$ VWATAVAWH
 A_A^A\_^
UVWATAUAVAWH
 A_A^A]A\_^]
UVWATAUAVAWH
 A_A^A]A\_^]
l$ VWATAVAWH
 A_A^A\_^
l$ VWATAVAWH
 A_A^A\_^
l$ VWATAVAWH
 A_A^A\_^
@SUVWATAUAVAWH
HA_A^A]A\_^][
UATAUAVAWH
A_A^A]A\]
l$ VWAVH
 A^_^
@SVWH
L$HH3
L$8H3
@SUVWH
T$HH+
L$PH3
h_^][
)t$PH
L$HH3
(t$PH
L$(H3
@USVWATAUAVAWH
~(H;~0tGA
)D$ f
H;~0u
D8nDt
Eo9F@
A_A^A]A\_^[]
p UWATAVAWH
A_A^A\_]
@SVWH
\$0H+\$(H
T$8H+
L$@H3
@SUVWAVH
 A^_^][
t$ WH
?H+1H
L$HH3
)D$0f
L;D$(uYH
)L$0f
O 9K t
L$PH3
H!E'M
D$ H;U't
UVWAVAWH
A_A^_^]
\$ UVWATAUAVAWH
(D$@f
u\8D$<t
A_A^A]A\_^]
t$ WAVAWH
0A_A^_
@USVWATAUAVAWH
T$`I;
A_A^A]A\_^[]
@USVWATAUAVAWH
@8|$Q
A_A^A]A\_^[]
\$ UVWATAUAVAWH
A_A^A]A\_^]
\$ UVWATAUAVAWH
EHH!E(H
|$HH+
A H+A
A_A^A]A\_^]
t$ WH
S H+S
x AVH
t$ WH
t$ WH
\$ UVWATAUAVAWH
@A_A^A]A\_^]
@SUVWATAVAWH
UUUUUUU
(D$ f
@A_A^A\_^][
\$ UVWATAUAVAWH
 A_A^A]A\_^]
@SUVWATAVAWH
@A_A^A\_^][
UVWATAUAVAWH
 A_A^A]A\_^]
@SUVWATAVAWH
(D$ f
0A_A^A\_^][
@SUVWATAVAWH
@A_A^A\_^][
l$ VWATAVAWH
 A_A^A\_^
l$ VWATAVAWH
 A_A^A\_^
WATAUAVAWH
 A_A^A]A\_
|$ AVH
|$ AVH
UVWATAUAVAWH
0A_A^A]A\_^]
VWAVH
H;~ H
 A^_^
t$ WH
H;{ H
SUVWATAUAVAWH
L9|$x
L9|$xu
(A_A^A]A\_^][
SUVWATAUAVAWH
M#x0M
H;|$ t$L;
L#z0M
8A_A^A]A\_^][
UVWATAUAVAWH
D$8H+
PA_A^A]A\_^]
\$ UVWH
)t$`I
(t$`H
\$ UVWATAUAVAWH
(D$ f
0A_A^A]A\_^]
WATAUAVAWH
\$ M;
D$`I+
T$@I;
t$HI;
H+\$XH
L$pH3
A_A^A]A\_
L$ I;
\$ UVWATAUAVAWH
@A_A^A]A\_^]
t$ WH
l$ VWAVH
L9r0H
EB0D9p(
C D80
C(D80t
C0D80u
C8D80
C8D80uwM
A0D8p:t
0A^_^
UVWATAUAVAWH
0A_A^A]A\_^]
t$ WH
H;{ H
SUVWATAUAVAWH
L9|$x
L9|$xu
(A_A^A]A\_^][
x ATAVAWH
@A_A^A\
L$8H3
\$ UVWATAUAVAWH
PA_A^A]A\_^]
l$ VWAVH
L$@H3
l$ VWAVH
0A^_^
\$ UVWH
@USVWATAVAWH
A_A^A\_^[]
@SVWH
L$HH3
L$(H3
t$ WH
l$ VWAVH
D$ H;S
0A^_^
l$ VWAVH
 A^_^
t$ WH
D$ H;S
L$(H3
VWAVH
0A^_^
x AVH
D9VLvYA+
@USVWATAUAVAWH
A_A^A]A\_^[]
\$ UVWAVAWH
|$0H;S
|$0H;W
@A_A^_^]
\$ UH
UVWATAVH
A^A\_^]
@SUVWATAVAWH
L$(H;
L$(H;
L$@H3
PA_A^A\_^][
\$ UVWATAUAVAWH
A_A^A]A\_^]
l$ VWAVH
 A^_^
|$ UH
C H+C
t$ UWAWH
M/H+M'H
E/L+E'I
t$ UWATAVAWH
A_A^A\_]
|$ UH
\$ UVWATAVH
A^A\_^]
x UATAUAVAWH
O(H+O H
W@H;WHt
u%A:^
fffffff
W(H;W0t
A_A^A]A\]
W(H;W0t
G(H+G H
W@H;WHt
UATAUAVAWH
L+~ L
A_A^A]A\]
\$ UVWATAUAVAWH
V@H+V8H
V(H+V H
A_A^A]A\_^]
\$ UVWATAUAVAWH
\$@H;U
(D$ f
(L$@f
D$@H;U
A_A^A]A\_^]
@USWH
t$ UWAWH
UVWAVAWH
A_A^_^]
|$ UH
|$ UH
|$ UH
H!E'9
@USVWAWH
A__^[]
(D$ f
T$ H+
(D$ f
@USVWAWH
A__^[]
\$ UVWAVAWH
A_A^_^]
|$ UH
@USVWAVH
A^_^[]
(D$0f
(L$Pf
D$xH+
VWATAVAWH
NPH9NHt
H+NHH
NHH;NPu
D$PH;V(t
D$PH;V(t
 A_A^A\_^
@SUVWATAVAWH
)|$0H
D$ H;U
L$(H3
(|$0H
PA_A^A\_^][
VWAVH
L$8H3
@A^_^
l$ VWAVH
L$@H3
t$ WH
t$ WH
t$ WH
VWAVH
0A^_^
t$ WH
@SVWH
L$XH3
@SUVWAVH
0A^_^][
l$ VWAVH
D$08P
UVWATAUAVAWH
 A_A^A]A\_^]
@UVWATAUAVAWH
PA_A^A]A\_^]
l$ VWATAVAWH
?H+)M
 A_A^A\_^
UVWATAUAVAWH
T$ M+
PA_A^A]A\_^]
@SVWH
l$ VWAVH
 A^_^
l$ VWAVH
 A^_^
@SUVWH
H+D$(H
8_^][
\$ VWAVI
D8RPtv
P8E8Q8t)I
\$ UVWATAVH
A^A\_^]
w A8p t.I
@8u't
@SUVWATAVAWH
L$(E3
PA_A^A\_^][
X UVWH
X UVWH
X UVWH
UWAVH
UWAVH
UWAVH
UVWAVAWH
t$XD8p
A_A^_^]
UVWAVAWH
t$`D8p t4H
A_A^_^]
UWAWH
UWAWH
UWAWH
t$X@8p
t$X@8p
t$X@8p
UWAVH
t$X@8p
t$X@8p
UWAVH
t$XD8p
UWAVH
t$XD8p
UWAVH
UWAVH
UWAVH
t$`@8p t4H
UWAVH
UWAVH
UVWAVAWH
t$`D8p t4H
A_A^_^]
UWAVH
UWAVH
t$x@8p
@8t$Xt
UWAVH
t$X@8p
UVWAVAWH
t$x@8p
@8t$Xt
A_A^_^]
UWAVH
t$`@8p t4H
UWAWH
UWAVH
UWAVH
UWAVH
t$X@8p
t$X@8p
UWAVH
UWAVH
t$`@8p t4H
UWAVH
t$XD8p
UWAVH
UWAVH
t$X@8p
UWAVH
t$X@8p
t$X@8p
UWAVH
t$X@8p
UWAVH
t$XD8p
t$X@8p
UVWAVAWH
A_A^_^]
UVWAVAWH
A_A^_^]
UVWAVAWH
A_A^_^]
UWAVH
UWAVH
UVWAVAWH
A_A^_^]
UWAVH
t$`@8p t4H
t$ WH
L$8H3
VWAVH
l$(H+l$ H
@A^_^
AHH+A@H
A0H+A(H
A`H+AXH
AHH+A@H
A0H+A(H
A`H+AXH
A0H+A(H
A`H+AXH
L$PH3
|$ ATAVAWH
 A_A^A\
q0S|I&
p0S|I&
{ AVH
L$xE3
|$ ATAVAWH
 A_A^A\
q0S|I&
p0S|I&
VAVAWH
L9s8t?I
L$0H;
`A_A^^
t$ WH
VWAVH
0A^_^
WAVAWH
_@tEI
0A_A^_
A H+A
WAVAWH
P H+P
L$8D;
 A_A^_
t$ UWATAVAWH
H+V0H
NPH+NHH
L+F0I
N8H+N0H
FPH+FHH
NPH+NHH
A_A^A\_]
VWATAVAWH
H9Gxt?H
0A_A^A\_^
@SVWAVAWH
G8H+G0H
\$HH;
L$PH3
`A_A^_^[
CPI+CHH
@SVWATAUAVAWH
GPH+GHH
\$PH;
D$hH+
A_A^A]A\_^[
@SVWAVAWH
D$0H;
L$PH3
`A_A^_^[
\$ UVWAVAWH
QPH+QHH
L$PH3
`A_A^_^]
\$ UVWATAUAVAWH
H9t$h
(D$pf
A_A^A]A\_^]
\$ UVWATAUAVAWH
D$xL9~ 
D$PL;
D$hH+
L;l$P
A_A^A]A\_^]
F@H+F8H
D$PL;
FXH+FPH
f0M+f(I;
L;|$P
@USVWATAUAVAWH
A_A^A]A\_^[]
WAVAWH
P H+P
0A_A^_
UWATAVAWH
MGHcG
H9G uM
A_A^A\_]
UWAVH
VWAVH
l$(H+l$ H
@A^_^
WATAUAVAWH
9x(t7H
9x(t7H
0A_A^A]A\_
L$PH3
@SVWAVH
8A^_^[
VWAVH
 A^_^
VWAVH
 A^_^
t$ UWATAVAWH
Lcr8J
M(L9 
Lcr8K
M(L9 
Lcr8K
M(L9 
Hcr8H
M(L9 
Hcr8H
M(L9 
Lcr(J
M(L9 
LcrhJ
M(L9 
LcrhJ
M(L9 
M(L9 
M(L9 
Lcr8K
M(L9 
hHcr8H
M(L9 u8H
A_A^A\_]
89:uH
p WAVAWH
0A_A^_
8Q tXH
8QPt8H
l$ VWAVH
vb'vb'v
L$PH3
l$ VWAVH
 A^_^
Q D8R t/H
VWATAUAWH
D8<>u
L$@H;
L$HH3
PA_A]A\_^
@SVWATAUAVAWH
D$PH;
L$XH3
`A_A^A]A\_^[
L$(I;
L$(L;
@SVWH
L$8tBH
L$8tBH
|$ ATAVAWH
H9X0H
 A_A^A\
H9X0H
EH09Y(t
H9X0H
EH09Y(t
H;AHu
T$HH;AHtfH
L$0H3
VWATAUAWH
d$@E3
D8<>u
L$@H;
L$HH3
PA_A]A\_^
t$ WATAUAVAWH
D$PH;
L$XH3
A_A^A]A\_
L$(I;
L$(L;
@SVWH
|$ ATAVAWH
 A_A^A\
H;APu
T$HH;APtfH
L$0H3
L$8tBH
|$ ATAVAWH
H9X0H
 A_A^A\
H9X0H
EH09Y(t
H9X0H
EH09Y(t
H;AHu
T$HH;AHtfH
L$0H3
VWATAUAWH
D8<>u
L$@H;
L$HH3
PA_A]A\_^
@SVWATAUAVAWH
D$PH;
L$XH3
`A_A^A]A\_^[
L$(I;
L$(L;
q"]Th
L$8tBH
qYT6+
L$8tBH
H9X0H
EH09Y(t
p"]Th
H9X0H
EH09Y(t
VWAVH
L$XH3
`A^_^
VWAVH
L$XH3
`A^_^
[ UVWAVAWH
(D$pf
A_A^_^]
[ UVWAVAWH
(D$pf
A_A^_^]
VWAVH
L$XH3
`A^_^
L$HH3
UVWATAUAVAWH
 A_A^A]A\_^]
UVWATAUAVAWH
 A_A^A]A\_^]
l$ VWATAVAWH
 A_A^A\_^
l$ VATAUAVAWH
 A_A^A]A\^
WAVAWH
 A_A^_
\$ UVWATAUAVAWH
D8,>u
D;s@s-H
PA_A^A]A\_^]
\$ UVWATAUAVAWH
D8,>u
D;s@s-H
PA_A^A]A\_^]
\$ UVWATAUAVAWH
D8,>u
D;s@s-H
PA_A^A]A\_^]
@USVWATAVAWH
PA_A^A\_^[]
@USVWATAVAWH
PA_A^A\_^[]
UVWAVAWH
`A_A^_^]
UVWAVAWH
`A_A^_^]
@USVWATAVAWH
A_A^A\_^[]
@USVWATAVAWH
A_A^A\_^[]
UVWATAUAVAWH
l$0L9
A_A^A]A\_^]
UVWATAUAVAWH
@Hc@(L;
E9i(~#I
E;A(|
l$0L9
A_A^A]A\_^]
@USVWATAVAWH
PA_A^A\_^[]
@USVWATAVAWH
PA_A^A\_^[]
UVWAVAWH
`A_A^_^]
UVWAVAWH
`A_A^_^]
UVWATAUAVAWH
T$ M+
PA_A^A]A\_^]
UVWATAUAVAWH
 A_A^A]A\_^]
l$ VWAVH
 A^_^
UVWAVAWH
 A_A^_^]
WATAUAVAWH
L$(I+
t$0M;
L$8H3
@A_A^A]A\_
Y A8X t/I
@USVWAVH
`A^_^[]
\$ UVWH
t$ WH
UWAVH
0A^_]
l$ VWAVH
3333333
L$PH3
l$ VWAVH
L$@H3
UVWAVAWH
vb'vb'v
L$@H3
PA_A^_^]
l$ VWAVH
0A^_^
x AVH
wouKL
UVWATAUAVAWH
A84>u
E8<6u
MXD;J
A84>u
D;kHs
MXH9u
A_A^A]A\_^]
\$ UVWATAUAVAWH
E9l$(
E;t$(
E9t$8
D849u
t$XE3
E;l$8
A_A^A]A\_^]
SVWATAUAVAWH
D9x |
D;y |
t$0H9
A_A^A]A\_^[
UVWATAUAVAWH
D$ D9k0
E8,>u
\$x;C0s
A_A^A]A\_^]
t$ WATAUAVAWH
D$0H;
A_A^A]A\_
t$ WATAUAVAWH
uRD8m
L$ H3
0A_A^A]A\_
L$09Q |
t$ WATAUAVAWH
t$@I;
L$HH3
A_A^A]A\_
l$ VWATAVAWH
 A_A^A\_^
@SUVWAVH
0A^_^][
@SUVWAVH
0A^_^][
~!LcA
@SVWATAUAVAWH
\$HMi
D$0I;
t$`M;
L$hH3
pA_A^A]A\_^[
l$ VWAVH
(D$`f
@SUVWATAVAWH
fffffff
(D$ f
0A_A^A\_^][
l$ VWATAVAWH
 A_A^A\_^
t$ WH
|$ AVH
T$ H;
L$`H3
t$ WH
H;{ H
SUVWATAUAVAWH
L#q0M
(A_A^A]A\_^][
L#q0L
\$ UVWH
\$ UVWH
(D$Pf
VWATAVAWH
L$8H3
A_A^A\_^
WATAUAVAWH
 A_A^A]A\_
VWATAVAWH
 A_A^A\_^
x UAVAWH
A_A^]
t$ WH
@USVWAVH
A^_^[]
t$ WH
VWATAVAWH
A_A^A\_^
l$ VWAVH
0A^_^
UVWATAUAVAWH
A_A^A]A\_^]
\$ UVWH
L$hH3
WAVAWH
@A_A^_
l$ VATAUAVAWH
0A_A^A]A\^
t$ WATAUAVAWH
 A_A^A]A\_
UVWATAUAVAWH
H;Q tfL9
rLH9r
 I;V 
I;V(t
A_A^A]A\_^]
H;C rWH
\$ UVWATAUAVAWH
UUUUUUU
A_A^A]A\_^]
@SVWATAUAVAWH
|$XM;
t$`L;
L$hH3
pA_A^A]A\_^[
VWAVH
L$@H;
 A^_^
|$ UATAUAVAWH
A_A^A]A\]
L$ L;
L$@H3
L$HH3
@USVWAWH
pA__^[]
\$ WH
L$(H3
x UATAUAVAWH
UvZH+
N,E8n4I
A_A^A]A\]
@SVWH
L$XH3
M7H!EGE3
x UAVAWH
A_A^]
x UAVAWH
A_A^]
l$ VWATAVAWH
 A_A^A\_^
l$ VWATAVAWH
?H+)M
 A_A^A\_^
l$ VWAVH
L$@H3
@USVWATAVAWH
A_A^A\_^[]
@USVWATAVAWH
A_A^A\_^[]
VWATAVAWH
 A_A^A\_^
\$ UVWH
(D$ f
D$PH+
L$`H3
UAVAWH
(D$ f
D$XH+
D$@H+
(D$ f
T$@H+
A_A^]
(D$ f
D$@H+
|$ UH
@USVWAVH
pA^_^[]
\$ UVWATAUAVAWH
A_A^A]A\_^]
@USVWAVH
`A^_^[]
|$ UH
\$ UVWATAVH
A^A\_^]
\$ UVWATAUAVAWH
D$`H+
D$`H+
D$`H+
D$xH+
(D$Pf
|$pL;
D$8H+D$xH
D$pI;
T$0Hc
A_A^A]A\_^]
t$0H+
L$8H3
t$ WH
WAVAWH
L$(I+
t$0M;
L$8H3
@A_A^_
UVWAVAWH
 A_A^_^]
L$@H;
L$@H;
\$ UVWH
L$PE3
L$XH3
@SUVWATAUAVAWH
L$PE3
L$XH3
hA_A^A]A\_^][
\$ UVWATAUAVAWH
A_A^A]A\_^]
VWAVH
L$0H3
@A^_^
I90w>I
@SUVWATAUAVAWH
L$hH3
xA_A^A]A\_^][
@USVWATAUAVAWH
MhH!ExH
p2S\S
(D$pf
A_A^A]A\_^[]
\$ UVWATAUAVAWH
T$0E3
L9aht<H
pq^^|
L$0E3
D$hH+
(D$0f
(L$@f
D$pH+
A_A^A]A\_^]
@USVWATAUAVAWH
D$8H;
L$pH!E
|$0H+
(D$@f
(L$0f
A_A^A]A\_^[]
\$ UVWATAUAVAWH
pq^^|
A_A^A]A\_^]
x UATAUAVAWH
p2S\S
p2W<R.
A_A^A]A\]
UVWATAUAVAWH
p2S\S
L$0H;
A_A^A]A\_^]
t$ WH
l$ VWAVH
 A^_^
l$ VWAVH
 A^_^
l$ VWATAVAW
D$@E3
A_A^A\_^
x UATAUAVAWH
ua@83
|$@I;
u]@83
D$8I;
L$PH;L$pt,
L$HH;L$Xt)
D$HH;]
A8q8t8D
H;\$ht$
A_A^A]A\]
\$ WH
L$0H;
UVWAVAWH
 A_A^_^]
\$ UVWATAUAVAWH
@A_A^A]A\_^]
UVWAVAWH
L$PH3
`A_A^_^]
2wDt8
@SUVWATAVAWH
@A_A^A\_^][
UVWATAUAVAWH
 A_A^A]A\_^]
UVWATAUAVAWH
 A_A^A]A\_^]
UVWATAUAVAWH
 A_A^A]A\_^]
@SUVATAUAVAWH
 A_A^A]A\^][
UVWATAUAVAWH
0A_A^A]A\_^]
t$HH#
\$ UVWATAUAVAWH
W@8o`
@8oXt)
@A_A^A]A\_^]
@8o tvH
@8o tNH
@8w`ui@8o t*H
UVWATAUAVAWH
 A_A^A]A\_^]
t$ WH
L$0H3
UVWATAUAVAWH
B0I;@
Q0H;P
A_A^A]A\_^]
t$ WH
L$0H3
t$ WH
L$0H3
t$ WH
L$0H3
t$ WH
L$0H3
L$0H3
VWATAVAWH
 A_A^A\_^
@SVWH
L$HH3
t$ UWAWH
(D$ f
D$XH+
D$8H;
q0Y62
|$ UH
q0Y62
|$ UH
L$@H3
L$@H3
L$@H3
L$HH3
L$HH3
L$HH3
q0Y62
q0Y62
t$ UWAVH
q0Y62
L$@H3
t$ WH
t$ WH
L$HH3
q0Y62
p UWATH
q0Y62
L$@H3
L$HH3
q0Y62
L$(H3
L$(H3
L$(H3
UWAVH
@SUVWAVH
L$HH3
PA^_^][
UVWATAUAVAWH
F I+F
(D$@f
A_A^A]A\_^]
UVWATAUAVAWH
A H+A
G H+G
G H+G
A_A^A]A\_^]
q0Y62
UWAVH
UWAVH
UWAVH
UWAVH
UWAVH
UVWAUAVH
A^A]_^]
UVWATAVH
A^A\_^]
UWAVH
L$@H3
UVWATAUAVAWH
A H+A
A_A^A]A\_^]
G H+G
G H+G
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
@SUVATAUAVAWH
 A_A^A]A\^][
UVWATAUAVAWH
{ H+{
(D$@f
A_A^A]A\_^]
UWAVH
UVWATAUAVAWH
L$XE3
s H+s
D+L$PA
(L$pf
A_A^A]A\_^]
UVWATAUAVAWH
L$XE3
s H+s
D+L$PA
(L$pf
A_A^A]A\_^]
UVWATAUAVAWH
{ H+{
(D$@f
A_A^A]A\_^]
UVWATAUAVAWH
{ H+{
(D$@f
A_A^A]A\_^]
UVWATAUAVAWH
{ H+{
(D$@f
A_A^A]A\_^]
UWAVH
UWAVH
UWAVH
UWAVH
UWAVH
UWAVH
UWAVH
UWAVH
UWAVH
UWAVH
UWAVH
UWAVH
UWAVH
UWAVH
UWAVH
UWAVH
UWAVH
UWAVH
UWAVH
UWAVH
UWAVH
UWAVH
UWAVH
UWAVH
UWAVH
UWAVH
UWAVH
UWAVH
UWAVH
UWAVH
UWAVH
UWAVH
UWAVH
q0Y62
UVWATAUAVAWH
D$ L+
|$HL+
L$XH3
`A_A^A]A\_^]
q0Y62
q0Y62
E`!D$p
(D$Pf
(D$Pf
L$@H3
L$HH3
@SUVWAVH
 A^_^][
UVWAVAWH
+BL9JLH
PA_A^_^]
UVWATAUAVAWH
A_A^A]A\_^]
q0Y62
L$@H3
L$@H3
L$@H3
\$ UVWH
(D$ f
D$HH+
L$XH3
L$XH3
L$XH3
L$XH3
|$ UH
\$ UVWATAVH
A^A\_^]
q0Y62
X UVWATAUAVAWH
A_A^A]A\_^]
|$ UH
q0Y62
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
@USVWAVH
pA^_^[]
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
q0Y62
UVWATAUAVAWH
E0H+E(H
A_A^A]A\_^]
q0Y62
L$(H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
UAVAWH
(D$ f
D$XH+
D$@H+
(D$ f
T$@H+
A_A^]
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
q0Y62
X UVWAVAWH
|$ E3
|$ E3
A_A^_^]
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
p UWATAVAWH
C H+C
C8H+C0H
D$ E3
D$ E3
A_A^A\_]
L$@H3
L$HH3
t$ UWATAVAWH
A_A^A\_]
t$ UWAVH
D$@H;
L$@H3
L$@H3
L$HH3
L$HH3
p UWAUAVAWH
t$0E3
|$ E3
l$ E3
l$ E3
A_A^A]_]
L$@H3
L$HH3
UWATAVAWH
A_A^A\_]
UVWATAUAVAWH
U H+U
U H+U
(L$pf
(L$Pf
|$PH+
(D$Pf
(D$Pf
(D$Pf
(D$Pf
(D$Pf
(D$Pf
(D$Pf
(D$Pf
(D$Pf
D8d$tt1
A_A^A]A\_^]
q0Y62
UWATH
L9E/u
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
UVWATAUAVAWH
;D$ u
D$(H+
L$PL+
D$ ;D$ u
L$XH3
`A_A^A]A\_^]
\$ UVWATAUAVAWH
A_A^A]A\_^]
L$HH3
L$HH3
L$HH3
L$HH3
L$HH3
q0Y62
UVWAVAWH
(D$pf
D$`H+
A_A^_^]
q0Y62
L$@H3
L$HH3
q0Y62
t$ UWATAVAWH
A_A^A\_]
q0Y62
L$@H3
L$HH3
q0Y62
UVWATAUAVAWH
t$ E3
t$ E3
A_A^A]A\_^]
L$@H3
L$HH3
UVWATAUAVAWH
D$`H+
(D$pf
A_A^A]A\_^]
q0Y62
L$@H3
L$@H3
L$@H3
L$HH3
L$HH3
L$HH3
q0Y62
L$@H3
L$HH3
q0Y62
UWAVH
L$@H3
L$HH3
{ AVH
9\$`viD
;\$`r
L$hH3
xH;F8tU
t$ UH
@USVWATAUAVAWH
L!D$ E3
D$0L;
A_A^A]A\_^[]
q0Y62
|$ UH
Eh!D$`
EP!D$@
(D$0f
Ex!D$P
E`!D$0
(D$@f
UVWAUAVH
(D$0f
A^A]_^]
L$@H3
L$@H3
L$@H3
L$HH3
L$HH3
L$HH3
l$ VWAVH
 A^_^
UVAVH
 A^^]
q0Y62
UVWATAUAVAWH
(D$Pf
I+<$H
A_A^A]A\_^]
L$@H3
L$@H3
L$@H3
L$@H3
VWAVH
 A^_^
L$HH3
L$HH3
L$HH3
L$HH3
\$ UVWATAUAVAWH
D$`H+
D$`H+
D$`H+
D$xH+
(D$Pf
|$pL;
D$8H+D$xH
D$pI;
T$0Hc
A_A^A]A\_^]
p UWAVH
(D$@f
@8t$D
L$@H3
L$HH3
p UWATAVAWH
(D$Pf
(L$pf
A_A^A\_]
L$@H3
L$HH3
t$ UWAVH
L$@H3
L$HH3
t$ UWATAVAWH
E@H+E8H
E(H+E H
L$PH;
A_A^A\_]
|$ UH
q0Y62
L$@H3
L$@H3
L$@H3
L$HH3
L$HH3
L$HH3
q0Y62
t$ UWAVH
L$@H3
L$HH3
q0Y62
|$ UH
MhH!Ex
L$@H3
L$HH3
UVWATAVH
!D$PD
A^A\_^]
L$@H3
L$HH3
UVWATAUAVAWH
]@H;]H
}`L+}XI
D$@H;
D9|$`
A_A^A]A\_^]
L$@H3
L$HH3
UWATH
L$@H3
L$HH3
UVWATAUAVAWH
A_A^A]A\_^]
}xL+}pI
(D$@f
(D$@f
UxH+UpH
MxL+MpI
(D$@f
(D$@f
q0Y62
L$@H3
L$HH3
l$ VWAVH
 A^_^
q0Y62
UVWAVAWH
E@!D$@
A_A^_^]
L$@H3
L$HH3
|$ UH
L$@H3
L$HH3
p UWAVH
t$ E3
t$ E3
t$ E3
L$@H3
L$HH3
|$ UH
E(H+E H
(D$@f
L$@H3
L$HH3
|$ UH
L$@H3
L$HH3
UVWATAWH
L$PE3
D$pH+
(D$`f
(D$`f
A_A\_^]
q0Y62
L$@H3
@USVWATAVAWH
A_A^A\_^[]
L$HH3
q0Y62
|$ UH
L$@H3
L$HH3
q0Y62
p UWAWH
t$ E3
t$ E3
L$@H3
L$HH3
|$ UH
L$@H3
L$HH3
t$ UWAWH
(D$Pf
q0Y62
L$@H3
L$HH3
q0Y62
UWAWH
L$@H3
L$HH3
UVWATAUAVAWH
|$PH+
(D$Pf
(D$Pf
(D$Pf
(D$Pf
(L$pf
(D$Pf
(L$pf
M8H+M0H
A_A^A]A\_^]
q0Y62
L$@H3
@USVWATAVAWH
pA_A^A\_^[]
L$HH3
q0Y62
L$@H3
L$HH3
UVWATAUAVAWH
M H!E03
A_A^A]A\_^]
|$ UH
L$@H3
L$HH3
l$ VWATAVAWH
 A_A^A\_^
UVWATAUAVAWH
 A_A^A]A\_^]
q0Y62
UVWATAVH
t$ E3
t$ E3
A^A\_^]
L$@H3
L$HH3
t$ UWAVH
q0Y62
L$@H3
L$HH3
q0Y62
UVWAVAWH
(D$@f
D$pH+
(D$@f
(D$@f
A_A^_^]
q0Y62
L$@H3
L$HH3
q0Y62
MpH+MhH
L$@H3
L$HH3
@SUVWATAUAVAWH
H;t$8H
A_A^A]A\_^][
@USVWATAUAVAWH
p2S\S
E9<$t H
E9<$u
pz^:7'
A_A^A]A\_^[]
@USVWATAUAVAWH
p2S\S
|$0A9
pz^:7'
A_A^A]A\_^[]
UVWAVAWH
pz^:7'
A_A^_^]
@USVWATAVAWH
A_A^A\_^[]
pz^:7'
VWAVH
UWATAVAWH
A_A^A\_]
D$(H;Q`t4H
L$(H3
VWATAVAWH
 A_A^A\_^
l$ VWAVH
L$PH3
VWATAVAWH
 A_A^A\_^
@USVWAVAWH
p2W<R.
A_A^_^[]
@USVWAVH
p2W<R.
A^_^[]
@SUVWH
L$8H3
H_^][
UVWATAUAVAWH
(D$pf
A_A^A]A\_^]
x ATAVAWH
I+C0H
I+C0H
CPI+CHL
I+CHH
 A_A^A\
t$ WH
D$0H9P s
UUUUUUU
UVWAVAWH
0A_A^_^]
t$ UWATAVAWH
I;N t8H
xI;N u
N0I;N8t8H
xI;N8u
p2S\S
FPE8fxI
A_A^A\_]
@USVWATAUAVAWH
D$XtXI;
S H;S(t@
S H;S(tH
S8H;S@t@
S8H;S@tH
A_A^A]A\_^[]
|$ UH
D9HHt1
xH;A u
D9HHt
WAVAWH
L$HH3
A_A^_
WAVAWH
L$HH3
A_A^_
Q H+Q
Q8H+Q0H
\$ UVWAVAWH
A_A^_^]
@USVWAVH
p0S|I&
A^_^[]
@USVWAVH
p0S|I&
A^_^[]
t$ WH
l$ VWATAVAWH
"""""""
 A_A^A\_^
WAVAWH
 A_A^_
t$ WATAUAVAWH
L$ E2
u"L;!
L$8H3
A_A^A]A\_
KHL9C`u&
AXL9I`t
VWAVH
L$@H3
PA^_^
@SUWAVAWH
H;G0rP
L$HH3
PA_A^_][
@USVWATAUAVAWH
A_A^A]A\_^[]
@USWH
x ATAVAWH
 A_A^A\
WAVAWH
 A_A^_
\$(H;
L$8H3
providerH
WAVAWH
H!L$`H
A_A^_
D$HE3
L$HH3
l$ VWAVH
0A^_^
UAVAWH
A_A^]
UAVAWH
A_A^]
@USVWATAVAWH
L$ I;
A_A^A\_^[]
UAVAWH
A_A^]
UAVAWH
A_A^]
UAVAWH
A_A^]
UAVAWH
A_A^]
UAVAWH
A_A^]
UAVAWH
A_A^]
UAVAWH
A_A^]
UAVAWH
A_A^]
UAVAWH
A_A^]
@SUVWATAVAWH
3333333
@A_A^A\_^][
UVWAVAWH
PA_A^_^]
l$ VWATAVAWH
H9Y0t
9k ~GD8{
A_A^A\_^
Ic@8H
UVWAVAWH
Ic@PH
`A_A^_^]
UVWATAUAVAWH
IcE8H
HcBhH
A_A^A]A\_^]
t$ UWATAVAWH
D$8L;
A_A^A\_]
@SVWH
{ AVH
L$0H3
R|Z'(
L$8H3
\$ VWATAVAWH
L$8H3
@A_A^A\_^
VWAVH
L$8H3
@A^_^
qRV>.
qRV>.
qRV>.
qRV>.
q0SZ+6p
q0SZ+6p
|$ AVH
|$ ATAVAWH
L$8H3
@A_A^A\
UVWATAUAVAWH
|$ E3
A_A^A]A\_^]
\$ VWATAVAWH
PA_A^A\_^
@SVWATAUAVAWH
@A_A^A]A\_^[
\$ VWATAVAWH
PA_A^A\_^
t$ UWATAVAWH
E8<8u
A_A^A\_]
SVWATAUAVAWH
|$PH;
HcSP;
IcUX;
A_A^A]A\_^[
qp^pE
|$ ATAVAWH
Icw H
0A_A^A\
VWAVH
(D$0f
(D$0f
L$ E3
q _xY
SVWATAUAVAWH
;w }ELc
;w8}ELc
IcE L
IcE H
l$pHc
A_A^A]A\_^[
x AVH
uILc7M
t$ WH
UWATAVAWH
A_A^A\_]
\$ UVWATAUAVAWH
 A_A^A]A\_^]
UVWATAUAVAWH
 A_A^A]A\_^]
UVWATAUAVAWH
 A_A^A]A\_^]
x AVH
D9L$P}
)Hct$P
t$ UWATAVAWH
L$8E3
\$`H+
CL$hE3
A_A^A\_]
@SVWH
L$8H3
@SVWH
L$8H3
x AVH
x AVH
t$ UWAVH
|$ UAVAWH
A_A^]
L$0H3
UWATAVAWH
M94$t2I
A_A^A\_]
qcU>>
pcU>>
[ VATAUAVAWH
L$hH!D$x
A8F0thH
T$pH;T$xt
d$pM;
A_A^A]A\^
\$ VWAVH
C H+C
D$0H;Cxt
L$8H3
@A^_^
L$8H3
L$PH3
L$8H3
@USVWATAVAWH
H!D$0H!D$@H
A_A^A\_^[]
@SVWATAVAWH
L$pH3
A_A^A\_^[
L$PH;
t$ WATAUAVAWH
D$0H;
L$pL;
A_A^A]A\_
VWAVH
@A^_^
L$HH3
D$0H;
L$(H3
L$(H3
D$ H;
L$0H3
s WAVAWH
L$`H3
A_A^_
VWATAVAWH
UUUUUUU
L$`H3
pA_A^A\_^
L$(H3
L$(H3
t$ WH
VWATAVAWH
(D$ f
G0H9G(
PA_A^A\_^
A0H9A(t;H
@SVWH
L$0H3
x AVH
\$0H;
VWAVH
@A^_^
@SVWH
L$0H3
UVWAVAWH
@A_A^_^]
\$ VWAVH
 A^_^
|$ AVH
@SUVWAVH
 A^_^][
L$8H3
)D$0H
L$`H3
L$XH3
q:U>N
@SUVWATAVAWH
L$HH3
PA_A^A\_^][
H99t0H9y0H
\$ UVWATAUAVAWH
A_A^A]A\_^]
(D$ f
D$ H;
WAVAWH
CPUExecuH9
tionProvH9H
(D$ f
D$ H;
HcK,H
kHcK(H
0A_A^_
tensor(bI
ool)u
tensor(iI
nt16u
nt32u
nt64u
tensor(uI
int1u
6)tsH
int3u
2)tLH
int6u
4)t%H
int8u
float16)L9H
tensor(dL9
oublu
e)tMH
tensor(fL9
loatu
@USVWATAUAVAWH
|$PH#MPH
u=L9|$`u6L;d$hu/I
l$@M;
A_A^A]A\_^[]
@USVWATAUAVAWH
|$`E3
L$XH;
H9X s
u.H;Y r(H
EpH;C
L9epu:H
D$hH;D$X
D8#tZL
L9ept:
A_A^A]A\_^[]
D$0H9P s
UUUUUUU
D$0H9P s
UUUUUUU
l$ VWAVH
0A^_^
l$ VWATAVAWH
 A_A^A\_^
@SUVWATAVAWH
(D$ f
0A_A^A\_^][
D$0Conv
D$0Gemm
UVWATAUAVAWH
|$HE3
A_A^A]A\_^]
@USVWATAVH
A^A\_^[]
x AVH
@USVWATAUAVAWH
D$0E3
(D$@f
(D$@f
A_A^A]A\_^[]
@USVWAVH
(D$@f
A^_^[]
t$ WH
t$ WAVAWH
L$(H3
0A_A^_
\$ UVWAVAWH
 A_A^_^]
UWAVH
t$ WH
t$ WH
t$ WH
t$ WH
t$ WH
t$ WH
t$ WH
t$ WH
t$ WH
t$ WH
t$ WH
t$ WH
t$ WH
t$ WH
t$ WH
t$ WH
t$ WH
t$ WH
t$ WH
@SUVWATAVAWH
 A_A^A\_^][
WAVAWH
 A_A^_
@USVWAVH
PA^_^[]
@USVWATAUAVAWH
YhH;Yp
 I;^pu
A_A^A]A\_^[]
VWAVH
 A^_^
UVWATAUAVAWH
A_A^A]A\_^]
X UVWATAUAVAWH
@8x8t~H
H9X s
H;Y r
fffffff
A_A^A]A\_^]
\$ UVWATAUAVAWH
L9@ s
L;A r
A_A^A]A\_^]
X UVWATAUAVAWH
I9~0I
UUUUUUU
L9p s
ujL;s rdH;
L9p s
ujL;s rdH;
A_A^A]A\_^]
UVWATAUAVAWH
D$PH9p s
H;q sxH
UUUUUUU
L97uJH
A_A^A]A\_^]
t$ WAVAWH
M9H s
M;H I
H;H s
L$ H3
0A_A^_
t$ WH
D$0H9P s
UVWAVAWH
|$ H!\$(
PA_A^_^]
l$ VWAVH
fffffff
@SVWAVAWH
)t$`M
\$0@8s
fffffff
(t$`H
pA_A^_^[
UVWATAUAVAWH
fffffff
PA_A^A]A\_^]
t$ WH
\$ UVWAVAWH
 A_A^_^]
L9X s
L;Z r
L9@ s
L;A r
WAVAWH
0A_A^_
M9H s,I
L;I r_I
$L9@ s
M;K r
@USVWAVH
A^_^[]
UVWATAUAVAWH
`A_A^A]A\_^]
@USVWATAUAVAWH
MPI;MP
A_A^A]A\_^[]
@SUVWATAVAWH
UUUUUUU
(D$ f
0A_A^A\_^][
|$ AVH
L#V0H
t$ WH
ttH9k
H;{ H
\$ UVWATAUAVAWH
L$PD;G
HcG8H;
H;F@|
HcGhH;
H;F@|
HcG8H;
H;F@|
HcG8H;
H;F@|
HcG8H;
H;F@|
HcG(H;
VhH;Vpt
H;F@|
A_A^A]A\_^]
@USVWAUAVAWH
H H99
`A_A^A]_^[]
H;APu^
\$ VWAVH
0A^_^
l$ VWATAVAWH
 A_A^A\_^
UVWATAUAVAWH
 A_A^A]A\_^]
@USVWAVH
`A^_^[]
t$ WH
UVWATAUAVAWH
L$HE3
t$XD9P
D$0H;
|$PH+
D9p(t
A_A^A]A\_^]
@USVWATAUAVAWH
d$8E3
D$hH+
D$ E3
E;<$}
H;D$xH
d$1E3
A_A^A]A\_^[]
l$ VWAVH
0A^_^
@USVWAVH
A^_^[]
\$ UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
@USVWATAUAVAWH
D8D$8tzH
A_A^A]A\_^[]
x UATAUAVAWH
A_A^A]A\]
T$HI;
T$hI;
@USVWATAUAVAWH
A_A^A]A\_^[]
@USVWATAUAVAWH
D$0H;
H;\$0
D$XH+
A_A^A]A\_^[]
D$XH+
D$XH+
x UATAUAVAWH
A_A^A]A\]
l$ VWAVH
0A^_^
UVWATAUAVAWH
 A_A^A]A\_^]
@SUVWATAVAWH
@A_A^A\_^][
\$ UVWH
@USVWAVH
`A^_^[]
x ATAVAWH
@A_A^A\
t$ WH
tHH9Y
UVWAVAWH
E8y u
A0I9A(u
D$ E3
A_A^_^]
@USVWAVH
A8Y u
A0I9A(u
D$ E3
A^_^[]
@USVWAVH
A8Y u
A0I9A(u
D$ E3
A^_^[]
UVWAVAWH
E8y u
A0I9A(u
D$ E3
A_A^_^]
@USVWAVH
A8Y u
A0I9A(u
D$ E3
A^_^[]
@USVWAVH
A8Y u
A0I9A(u
D$ E3
A^_^[]
@USVWAVH
A8Y u
A0I9A(u
D$ E3
A^_^[]
@USVWAVH
A8Y u
A0I9A(u
D$ E3
A^_^[]
@USVWATAVAWH
A_A^A\_^[]
@USVWATAUAVAWH
Gemm@
u}D9g(
A_A^A]A\_^[]
UATAUAVAWH
A_A^A]A\]
\$ UVWATAUAVAWH
@ H9B 
vD9w(
A_A^A]A\_^]
\$ UVWATAUAVAWH
A_A^A]A\_^]
|$ UH
@USVWATAUAVAWH
uu9_(
u~9_(
F89_(
A_A^A]A\_^[]
UVWATAUAVAWH
F(9_(
A_A^A]A\_^]
UVWATAUAVAWH
u{9_(
uy9_(
ty9_(
ty9_(
ty9_(
uy9_(
uy9_(
ty9_(
D$`H9
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
@USVWATAUAVAWH
t$09s(
permD
A_A^A]A\_^[]
L$hH3
UVWATAUAVAWH
D$(permD
A_A^A]A\_^]
UVWATAUAVAWH
perm@
t}A9v(
t(H93
A_A^A]A\_^]
\$ UVWATAUAVAWH
MaskCastH
A_A^A]A\_^]
@SUVWATAUAVAWH
hA_A^A]A\_^][
@USVWATAUAVAWH
t$ E3
@8|$P
A_A^A]A\_^[]
@USVWATAUAVAWH
L$`H;
L$`H!D$pH
D$pH+
A_A^A]A\_^[]
L$`H!D$pH
D$pH+
@SUVWATAUAVAWH
hA_A^A]A\_^][
@USVWATAUAVAWH
L$pH;
\$4I;
D$hH;D$ptQH
A_A^A]A\_^[]
@USVWATAUAVAWH
A_A^A]A\_^[]
@USVWATAUAVAWH
A_A^A]A\_^[]
@USVWATAUAVAWH
A_A^A]A\_^[]
@USVWATAUAVAWH
uyD9o(
A_A^A]A\_^[]
VWAVH
 A^_^
l$ VWAVH
0A^_^
l$ VWAVH
0A^_^
H;APu^
H0HcP(L
@SUVWAUAVAWH
PA_A^A]_^][
WAVAWH
 A_A^_
H;APu^
WAVAWH
 A_A^_
UVWATAUAVAWH
 A_A^A]A\_^]
x AUAVAWH
 A_A^A]
UVWATAUAVAWH
L$(I;
0A_A^A]A\_^]
l$ VWATAVAWH
 A_A^A\_^
L$@H3
UVWATAUAVAWH
D$8H+
PA_A^A]A\_^]
UVWATAUAVAWH
 A_A^A]A\_^]
l$ VWATAVAWH
 A_A^A\_^
UVWATAUAVAWH
)t$pH
GPH9GHtnH
^8H9_@
H;G@|
H;G@|
^hH9_@
H;G@|
^8H9_@
H;G@|
^8H9_@
H;G@|
^8H9_@
H;G@|
^(H9_@~B
H;G@|
(t$pH
A_A^A]A\_^]
L$0H3
t$ WH
L$0H3
t$ WH
L$0H3
t$ WH
L$0H3
t$ WH
L$0H3
VWAVH
L$0H3
@A^_^
\$ UVWATAUAVAWH
~XL9g@
~*L9g@
~WL9g@
~'L9g@
~%L9g@
~*L9g@
L$0H3
@A_A^A]A\_^]
UVWATAUAVAWH
L9T$X
D$0L;
T$0L;
L$XH9
A_A^A]A\_^]
t$ WH
H;APu^
H;APu^
H;APu^
H;APu^
H;APu^
H;APu^
H;APu^
H;APu^
l$ VWAVH
 A^_^
@USVWATAUAVAWH
A_A^A]A\_^[]
H;APu^
x UAVAWH
D$PfD
D$PfD
(D$`f
(D$`f
D$PH+
A_A^]
x UATAVH
(D$`f
D$HH+
A^A\]
UWATAVAWH
D$HH+
A_A^A\_]
x UAVAWH
(D$`f
D$HH+
A_A^]
(D$@f
UAVAWH
(D$`f
A_A^]
t$ WH
@SUVWATAVAWH
PA_A^A\_^][
UVWATAUAVAWH
0A_A^A]A\_^]
t$ WH
H;{ H
SUVWATAUAVAWH
L9|$x
L9|$xu
(A_A^A]A\_^][
|$ UAVAWH
@A_A^]
@USVWAVH
A^_^[]
USVWATAUAVAWH
H9Y0H
D$hH+
t$PE3
A_A^A]A\_^[]
UVWATAUAVAWH
A_A^A]A\_^]
t$pE3
t$ WAVAWH
0A_A^_
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
H9p0H
EH09q(
D9A(u
D9A(u
D$XH+
uUI;Q
A_A^A]A\_^]
@SUVWAVH
D$ H;D$ u
0A^_^][
l$ VWATAVAWH
 A_A^A\_^
@SUVWAVH
 A^_^][
@SUVWAVH
D$ ;D$ u
0A^_^][
@SUVWAVH
 A^_^][
x AVH
@USVWATAUAVAWH
D$PL;
FastGeluH
L;|$P
A_A^A]A\_^[]
@USVWATAUAVAWH
L;|$X
A_A^A]A\_^[]
UVWATAUAVAWH
9z(uUH
A_A^A]A\_^]
WAVAWH
tWIcJ
@ I;A u
 A_A^_
@USVWATAUAVAWH
|$HE3
A_A^A]A\_^[]
UVWATAUAVAWH
A_A^A]A\_^]
@USVWATAUAVAWH
!D$`D
H!E(H
d$ E3
D$HE3
D;|$H|
A_A^A]A\_^[]
\$ UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
\$ E3
\$ E3
A_A^A]A\_^]
UWAVH
|$ UH
\$ UVWATAUAVAWH
A_A^A]A\_^]
@USVWATAVAWH
D$ E3
A_A^A\_^[]
UVWATAUAVAWH
L$0E2
A_A^A]A\_^]
UVWATAUAVAWH
L$0E2
A_A^A]A\_^]
@USVWATAUAVAWH
A_A^A]A\_^[]
\$ UVWATAUAVAWH
L9j0H
T$xI;
T$xI;
T$XI;
A_A^A]A\_^]
UVWATAUAVAWH
u|D9g(
u|D9g(
sD9g(
A_A^A]A\_^]
@USVWATAUAVAWH
tzA9v(
t$ E3
A_A^A]A\_^[]
@USVWATAUAVAWH
A_A^A]A\_^[]
D$0I;V
UVWATAUAVAWH
A_A^A]A\_^]
\$ UVWATAUAVAWH
D$`E3
D$HE3
l$GL;
A_A^A]A\_^]
@USVWATAUAVAWH
A9t$(
u{9W(
} D!m(H
M0D!m8H
H L9!
t{9W(
u|D9g(
} D!e(H
M0D!e8H
D$`I;
D$`I;
A_A^A]A\_^[]
@USVWATAUAVAWH
ut9{(
ut9{(
D9p(u
D9p(u
L$@H;
L$@H;
A_A^A]A\_^[]
@USVWATAUAVAWH
rlwjH
EPH;u
l$xH;
A_A^A]A\_^[]
L$(H3
@SUVWATAVAWH
T$ M;
0A_A^A\_^][
@SVWH
8Yielu
D9P(u
@USVWATAUAVAWH
B H9A
|$HE3
L;d$h
A_A^A]A\_^[]
UVWATAUAVAWH
L98uSH
A_A^A]A\_^]
@SUVWAVH
A^_^][
@SUVWAVH
0A^_^][
UVWAVAWH
H9|$`
A_A^_^]
l$ VWAVH
 A^_^
UVWATAUAVAWH
D$PH;
0H;t$Pt
A_A^A]A\_^]
@USVWATAUAVAWH
H9z0H
|$$9x
H;Cht
H;Cxt=@8|$ t
@8|$ 
D$PD;@
A_A^A]A\_^[]
@USVWATAUAVAWH
D$xE3
A_A^A]A\_^[]
t$ WH
UVWATAUAVAWH
Selu@
A_A^A]A\_^]
@USVWATAUAVAWH
L$xH;
D$XH+
L$@`L
L$HE3
D$XH+
H;D$x
A_A^A]A\_^[]
@SVWATAUAVAWH
H!D$PH
D$PGemm
H!D$P
D$PGemm
D8l$AA
D8l$@A
A_A^A]A\_^[
x UATAVH
A^A\]
\$ UVWH
t$ UWAVH
D$0perm
tdLcBXH
T$@H+
t(HcP
D$ H;
L$(H!D$8H
T$0H+
T$8H+
L$@H3
\$ UH
@USVWATAUAVAWH
D9a(u
A_A^A]A\_^[]
@USVWATAUAVAWH
M9g0I
A_A^A]A\_^[]
t$ WH
@SUVWATAVAWH
(D$ f
0A_A^A\_^][
@SUVWATAVAWH
(D$ f
0A_A^A\_^][
|$ AVH
H;{ H
UVWATAUAVAWH
M#u0M
H;\$xt/H;
H9l$pu
 A_A^A]A\_^]
H9l$pu
M#u0H
H;\$pt
l$ VWAVH
 A^_^
l$ VWATAVAWH
 A_A^A\_^
UVWATAUAVAWH
T$hI;
A_A^A]A\_^]
p AWH
 H;=[
@USVWATAUAVAWH
H!EpD
HcH(H
E8H;EH
A_A^A]A\_^[]
@USVWATAUAVAWH
HcH(H
D$PE3
|$PD8
EPH;E`
A_A^A]A\_^[]
|$PE3
x AVH
D9R(u%I
E9P(u
B I;@ u
B H;A t
@USVWATAUAVAWH
\$HE3
A_A^A]A\_^[]
I;C8t
DC8I;C8t
t$ WH
@USVWATAVAWH
A_A^A\_^[]
l$ VWATAVAWH
D$ L;
L$@H3
A_A^A\_^
t$ UWATAVAWH
H;Gxt
DGxH;Gx
|$pE3
A_A^A\_]
UVWATAUAVAWH
SAME_UPPH9
SAME_LOWH9
L)l$ I
A_A^A]A\_^]
UVWATAUAVAWH
$H90u
@8t$@tPI
A_A^A]A\_^]
UVWATAUAVAWH
L9j0H
EB0D9h(
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
I9\$ 
L((H9J
~6H;B 
D8D$0
D$8E3
t$8E3
A_A^A]A\_^]
D$8E3
UVWATAUAVAWH
A_A^A]A\_^]
|$ UATAUAVAWH
A_A^A]A\]
UVWATAUAVAWH
H L91
H L91
H L91
H L91
bn_scaleH
D$`bn_B
ConvD
A_A^A]A\_^]
UVWATAUAVAWH
D$`permD
@0L9 
channelsH
A_A^A]A\_^]
UVWATAUAVAWH
H H99
A_A^A]A\_^]
H H99u
|$ UAVAWH
A_A^]
UVWATAUAVAWH
D$LE3
channelsH
M9t$(t
A_A^A]A\_^]
@USVWATAUAVAWH
A_A^A]A\_^[]
\$ UVWATAUAVAWH
(D$ f
0A_A^A]A\_^]
UVWATAUAVAWH
 A_A^A]A\_^]
@SUVWATAVAWH
(D$ f
0A_A^A\_^][
|$ AVH
t$ WH
H;W H
UVWATAUAVAWH
I#u0H
H9D$ t-H;
0A_A^A]A\_^]
I#u0L
UWATAVAWH
A_A^A\_]
@USVWATAUAVAWH
A_A^A]A\_^[]
@USVWATAVAWH
|$pHc
D$PH+
A_A^A\_^[]
@USVWATAUAVAWH
A_A^A]A\_^[]
@USVWATAUAVAWH
A_A^A]A\_^[]
UVWATAUAVAWH
~:I;]
L$HH3
PA_A^A]A\_^]
zvutH
3HcF(;E(u
UWAVH
HcF@H
N0HcF(L
V HcF
0A^_]
p AWH
B`9A`
Bp9Ap
BhH9Ah
L;wHt>M+
L;wHu
t$ WH
HiD$ %y
L$ H;
@USVWATAUAVAWH
|$hE3
L$`H;
E8o8L
E8o8I
D8h8u
E8o8I
UUUUUUU
E8o8L
H;D$`
l$`M;
T$0E3
L;|$X
A_A^A]A\_^[]
t$ WH
x ATAVAWH
 A_A^A\
UVWATAUAVAWH
yxI;]
UUUUUUU
@A_A^A]A\_^]
@SUVWATAVAWH
(D$ f
0A_A^A\_^][
\$ UVWAVAWH
 A_A^_^]
UVWATAUAVAWH
 A_A^A]A\_^]
t$ WATAUAVAWH
t$0L#
uiK94
0A_A^A]A\_
VWAVH
0A^_^
thH9s
H;{ H
thH9s
H;{ H
A8C8u
A0L#D$(L
A8A8u
A8@8u
@SUVWATAVAWH
(D$ f
0A_A^A\_^][
@USVWAVH
PA^_^[]
UVWATAUAVAWH
A_A^A]A\_^]
@USVWATAUAVAWH
L$XH!D$hH
A_A^A]A\_^[]
@USVWATAUAVAWH
p8V||
D$HH+
D$`I+
D$hI+
L$8H+
D$@H+
D$pI+
D$HH+
A_A^A]A\_^[]
UVWATAUAVAWH
D9x(A
`A_A^A]A\_^]
@USVWATAUAVAWH
MXH!Eh
M@H!EPH
@ H;B t_A
UHH;UPt
U`H;Uht
U`H;Uht
H;UPt
H;UPt
A_A^A]A\_^[]
UVWAVAWH
zGuEH
A_A^_^]
@SUVWAVH
0A^_^][
JD9H(tKH
@USVWATAUAVAWH
M(H!E8H
L!t$pH
A9p(u;I9p u5H
D$PE3
UHH;UPt
U0H;U8t
D$HH;U8t
D$HH;U8t
\$HH;U8t
t$HH;U8t
A_A^A]A\_^[]
@USVWATAVAWH
SpH;Sxt
`A_A^A\_^[]
UVWATAUAVAWH
A_A^A]A\_^]
@USVWATAUAVAWH
D$PL;
l$@I;
L;t$P
A_A^A]A\_^[]
@SUVWATAVAWH
@A_A^A\_^][
UVWATAUAVAWH
0A_A^A]A\_^]
t$ WH
H;{ H
SUVWATAUAVAWH
L9|$x
L9|$xu
(A_A^A]A\_^][
p AWH
@USVWATAUAVAWH
A_A^A]A\_^[]
@USVWATAUAVAWH
L$pH;
MHH!EXH
@ H;B 
D$hH;D$p
A_A^A]A\_^[]
BiasGeluH
FastGeluH
WAVAWH
CPUExecuH9
tionProvH9P
ideruVH
 A_A^_
|$8@2
UATAUAVAWH
A_A^A]A\]
UATAUAVAWH
A_A^A]A\]
@USVWATAUAVAWH
A_A^A]A\_^[]
L!|$XH
L!|$hH
H9y0L
FastGeluH
UVWATAUAVAWH
0A_A^A]A\_^]
H9Y0H
EQ09Z(
J Hcy H
UVWATAUAVAWH
u(D9u,u
A_A^A]A\_^]
UVAUAVAWH
D$xH;E
A_A^A]^]
@SUVWAVH
L$HH3
PA^_^][
@USVWATAUAVAWH
@0I9@(
H9}Pt.
H9}Pt]
H9}PtS
(D$Pf
H9}Ptv
A_A^A]A\_^[]
l$ VWAVH
L$@H3
t$ UWAVH
X UVWH
X5Z<N
T=wHN
T5qGN
Y5a;N
X59;N
T=VGN
\$ UVWAVAWH
A_A^_^]
t$ WH
D$@H9
L$PH3
x AWH
l$ WH
UVWATAUAVAWH
D$8H+
PA_A^A]A\_^]
l$ VWAVH
VWATAVAWH
0A_A^A\_^
t$ WH
H;{ H
SUVWATAUAVAWH
L9|$x
L9|$xu
(A_A^A]A\_^][
t$ UWAVH
UVWATAUAVAWH
A_A^A]A\_^]
\$ UVWATAUAVAWH
x0L!e
A0H9A(u
A_A^A]A\_^]
\$ UVWATAUAVAWH
A_A^A]A\_^]
D$@H+
q"Rz-
@USVWATAVAWH
pA_A^A\_^[]
pcP>w
pj[\x
l$ VWAVH
T$8H;
@SUVWATAVAWH
(D$ f
0A_A^A\_^][
@SUVWATAVAWH
(D$ f
0A_A^A\_^][
@SUVWATAVAWH
(D$ f
0A_A^A\_^][
@SUVWATAVAWH
(D$ f
@A_A^A\_^][
UVWATAUAVAWH
PA_A^A]A\_^]
|$ AVH
|$ AVH
|$ AVH
|$ AVH
H;{ H
VWAVH
H;~ H
 A^_^
t$ WH
H;W H
UVWATAUAVAWH
I#u0H
H9D$ t-H;
0A_A^A]A\_^]
I#u0L
SUVWATAUAVAWH
M#x0M
H;|$ t$L;
L#z0M
8A_A^A]A\_^][
UVWATAUAVAWH
M#u0M
H;\$xt/H;
H9l$pu
 A_A^A]A\_^]
H9l$pu
M#u0H
H;\$pt
UVWATAUAVAWH
A_A^A]A\_^]
@USVWAVH
A^_^[]
@USVWAVAWH
D$8H+
A_A^_^[]
@SVWH
D$0H;
H9P0H
H9P0L
E9A(u
D9@(u
D9A(u
D;J t
D;X uDI
A E;X 
L9X0H
L9X0H
uQD9H uKH
L9X0L
t$ WH
UVWAVAWH
 A_A^_^]
UVWATAUAVAWH
PA_A^A]A\_^]
|$ UH
|$ UH
|$ UH
@SUVWAVAWH
S H+S
L$PH3
hA_A^_^][
\$ UVWATAUAVAWH
L$PH+
D$PE3
A_A^A]A\_^]
@USVWAVAWH
A_A^_^[]
@USVWATAUAVAWH
D$hE3
t$@H;
D$0H;
A_A^A]A\_^[]
@USVWATAUAVAWH
D$HH;
H;|$H
A_A^A]A\_^[]
@SUVWATAUAVAWH
HA_A^A]A\_^][
@SUVWATAVAWH
@A_A^A\_^][
UVWATAUAVAWH
0A_A^A]A\_^]
t$ WH
H;{ H
SUVWATAUAVAWH
L9|$x
L9|$xu
(A_A^A]A\_^][
t$ UWATAVAWH
A_A^A\_]
@SVWH
@SUVWH
L$8H3
H_^][
@USVWATAUAVAWH
A_A^A]A\_^[]
@USVWAVH
pA^_^[]
@USVWATAUAVAWH
A_A^A]A\_^[]
l$ VWATAVAWH
 A_A^A\_^
h VWAVH
(D$ f
0A^_^
|$ AVH
H;{ H
UVWATAUAVAWH
D$ 9E0uFH
A_A^A]A\_^]
T$8H+
L$@H3
k VWAVH
T$8H+
L$@H3
@SUVWAVAWH
T$@H+
L$HH3
XA_A^_^][
UVWATAUAVAWH
\$@H+
)D$ f
L;D$8uJH
T$PH+
L$XH3
`A_A^A]A\_^]
\$ UVWAVAWH
A_A^_^]
(D$0H
(D$@f
UVWAVAWH
A_A^_^]
T$8I+
L$HH3
T$8I+
L$HH3
@USVWH
x_^[]
WAVAWH
0A_A^_
\$ UVWATAUAVAWH
A_A^A]A\_^]
t$ UWAVH
WAVAWH
0A_A^_
t$ UWAVH
UVWATAUAVAWH
(D$pf
p@PRg7
M+<$I
(D$pf
[T&' 
(D$pf
t?D8}
(D$pf
p@PRg7
p0Qt 
(D$pf
[T&' 
(D$pf
A_A^A]A\_^]
USVWATAUAVAWH
T$`H+
(D$@f
D$xD8`
T$`H+
p@PRg7
(D$@f
(L$ f
[T&' 
T$`H+
D$xD8`
T$`H+
T$0H+
p@PRg7
D$xD8`
(|$ D
(D$@H
|$ fD
[T&' 
T$`H+
p@PRg7
A_A^A]A\_^[]
T$`H+
@USVWATAUAVAWH
A_A^A]A\_^[]
@SUVWAVH
T$@H+
L$HH3
PA^_^][
\$ UVWH
@USVWAVAWH
)t$pH
p@PRg7
(t$pH
A_A^_^[]
@SUVWAVH
T$@H+
T$hH+
L$pH3
A^_^][
H;L$@u
L$XH3
t$ UWAVH
[T&' 
@SUVWAVH
T$@H+
L$HH3
PA^_^][
\$ UVWH
E'H+E
(D$0H
(D$0f
D$0x`H;
(D$ L
L$@H3
(D$ f
(D$ f
(D$ f
(D$ H
x UATAUAVAWH
L$`E3
(D$@f
(D$@f
(D$@f
(D$@f
D$`H;
(D$@f
(D$@f
p@PRg7
\$`H+
T$pH+
A_A^A]A\]
USVWATAVAWH
T$xH+
L$hH+
(D$Pf
D$xH+
D$xH+
D$xH+
(D$@f
(L$Pf
T$xH+
p@PRg7
(D$@f
A_A^A\_^[]
x UATAUAVAWH
A_A^A]A\]
UWAUAVAWH
A_A^A]_]
UWATAUAVH
(D$ f
)D$0f
(D$0f
A^A]A\_]
UAVAWH
(D$0f
(D$0f
A_A^]
\$ UVWH
L$ E3
(D$0f
8\$XtBH
T$HH+T$@H
L$`H3
t$ UH
|$ UAVAWH
A_A^]
\$ UVWATAUAVAWH
A_A^A]A\_^]
UATAUAVAWH
D$0H!E
H+D$0H
d$ H;U
L$0E3
D$8H+D$0H
A_A^A]A\]
UWAWH
(L$@f
T$PH+
p@PRg7
x UATAUAVAWH
T$pH+
T$pH+
T$pH+
t$@D8a
)D$@f
(D$Pf
(L$@f
T$pH+
(D$@f
A_A^A]A\]
T$@H+
D$HH;
D$HH;
L$PH3
|$ UAVAWH
A_A^]
x UATAUAVAWH
p8V||
A_A^A]A\]
USVWATAUAVAWH
D$hE3
(D$0f
(L$ f
[T&' 
(D$0f
(L$ f
[T&' 
D$ H;
H;\$ 
A_A^A]A\_^[]
x UATAUAVAWH
[T&' 
\$PE3
(L$0f
[T&' 
D8l$p
A_A^A]A\]
p9U4X
p9U4X
p9U4X
@USVWAVAWH
A_A^_^[]
t$ WH
t$ WH
@SUVWATAVAWH
UUUUUUU
@A_A^A\_^][
l$ VWATAVAWH
 A_A^A\_^
@SUVWATAVAWH
)D$ 3
fffffff
@A_A^A\_^][
UVWATAUAVAWH
)D$ f
)D$0f
)L$ f
)D$0f
H9D$(u
tdK94
@A_A^A]A\_^]
H;{ H
UVWATAUAVAWH
)D$ H
0A_A^A]A\_^]
)D$ f
)D$0f
H9D$(u
T$ I+
D$ H;
L$(H3
[T&' 
)D$ f
p8V||
)D$ f
)D$ f
)D$ f
L$`H3
\$0@8{
L$8H3
q8V||
|$ UH
|$ UATAUAVAWH
A_A^A]A\]
UVWATAUAVAWH
A_A^A]A\_^]
q0Qt 
|$ UATAUAVAWH
L9 t.H
A_A^A]A\]
D$xL+D$pI
D$@H+
UVWAVAWH
(D$0f
A_A^_^]
\$ UVWH
@USVWAVH
HcF(H
pA^_^[]
@USWH
\$ UVWH
\$ UVWATAUAVAWH
E8l$8
A_A^A]A\_^]
q9U4X
\$ UVWH
HcA0H
t$ UWAVH
@USVWAVH
PA^_^[]
@USVWAVH
PA^_^[]
@USVWAVH
A^_^[]
\$ UVWATAUAVAWH
A_A^A]A\_^]
t$ UWAVH
@USVWAVH
`A^_^[]
\$ UVWATAUAVAWH
(D$0f
(D$0f
A_A^A]A\_^]
UVWATAUAVAWH
K HcC
A_A^A]A\_^]
@USVWATAUAVAWH
t$pE3
D$HH;]
D$HH;
A_A^A]A\_^[]
UVWATAUAVAWH
A_A^A]A\_^]
@USVWATAUAVAWH
A_A^A]A\_^[]
q@PRg7
\$ UVWATAUAVAWH
L$PH;
L9o0H
l$ E3
A_A^A]A\_^]
t$ WAVAWH
0A_A^_
M#B0M
UVWAVAWH
|$ H!\$(
PA_A^_^]
t$ WH
WAVAWH
 A_A^_
t$ WH
VWAVH
0A^_^
l$ VWATAVAWH
 A_A^A\_^
\$ UVWAVAWH
 A_A^_^]
@SUVWATAVAWH
(D$ f
0A_A^A\_^][
@SUVWATAVAWH
PA_A^A\_^][
|$ AVH
t$ WH
H;{ H
SUVWATAUAVAWH
L9|$x
L9|$xu
(A_A^A]A\_^][
@SUVWAVH
A^_^][
@SVWH
@SUVWAVH
D9X,tMH
D$@Mc@
D$0H;
L$HH3
PA^_^][
@USVWATAVAWH
pA_A^A\_^[]
USVWATAUAVAWH
L$0E2
T$8E3
A_A^A]A\_^[]
@USVWATAUAVAWH
|$hM;
t$0A9
\$xI;
t$`M;
A_A^A]A\_^[]
l$ VWATAVAWH
 A_A^A\_^
@SVWH
l$ VWATAVAWH
 A_A^A\_^
L$0H3
WAVAWH
@A_A^_
UAVAWH
A_A^]
UVWAVAWH
A_A^_^]
H!;H!{
UVWATAUAVAWH
L$XE3
D$8permD
L$0H;
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
x ATAVAWH
A_A^A\
x UATAUAVAWH
A_A^A]A\]
UWAVH
@USVWATAUAVAWH
(D$Pf
(D$Pf
(D$Pf
(D$Pf
A_A^A]A\_^[]
UVWATAUAVAWH
D$HE3
PA_A^A]A\_^]
UVWATAUAVAWH
D$HE3
PA_A^A]A\_^]
@USVWAVAWH
A_A^_^[]
D$ L;
\$ UVWATAUAVAWH
A_A^A]A\_^]
(D$@f
D$ L;
@SUVWH
@SUVWAVH
A^_^][
(|$@D
(D$0D
(L$ D
|$ UH
|$ UH
\$ UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
L9}xt5
MpL9}
}xL9}
H;D$P
MpH;M
A_A^A]A\_^]
t$ WH
l$ VWATAVAWH
 A_A^A\_^
UVWATAUAVAWH
 A_A^A]A\_^]
UVWAVAWH
 A_A^_^]
\$ VWI
z#u!I
@USVWATAUAVAWH
f M+f
E I+E
A_A^A]A\_^[]
\$ UVWATAUAVAWH
A_A^A]A\_^]
USVWAVAWH
T$@H;
A_A^_^[]
USVWH
T$PH;
|$ UH
|$ UH
|$ UH
UVWATAUAVAWH
V I+V
t&L9+u
\$0H+
(D$0f
A_A^A]A\_^]
\$ UH
t$ WH
t$ WH
L$0H3
|$ UH
|$ UH
|$ UH
|$ UH
x UATAUAVAWH
H9X0H
L$@D;o
A_A^A]A\]
UVWATAUAVAWH
pA_A^A]A\_^]
|$ UH
H!T$0H
D$0body
T$`H;
\$ UVWATAUAVAWH
I;D$xu
D$0H;
A_A^A]A\_^]
\$ UVWAUAVH
A^A]_^]
UVWATAUAVAWH
T$PE3
C L+C
H;Gxu
A_A^A]A\_^]
@USVWATAUAVAWH
A_A^A]A\_^[]
\$ UVWATAUAVAWH
T$`E3
fffffff
A_A^A]A\_^]
D$pI;
@USVWATAUAVAWH
udM9(t
A_A^A]A\_^[]
t$ WH
t$ UWAVH
t$ UWAVH
UVWATAUAVAWH
 A_A^A]A\_^]
l$ VWATAVAWH
 A_A^A\_^
@USVWATAVAWH
A_A^A\_^[]
@SVWH
@SVWAVAWH
A_A^_^[
t$ WATAUAVAWH
 A_A^A]A\_
|$ UH
|$ UH
|$ UH
|$ UH
UWAUAVAWH
D$0L;
D$@H+
A_A^A]_]
\$ UVWATAUAVAWH
D$0H;
IcP(H
L$@Mch(E3
L$@E3
HcP,H
Lc`,M
D$8H;
A_A^A]A\_^]
@USVWATAVAWH
A_A^A\_^[]
|$ UH
UVWATAUAVAWH
CHH+C8H
C`H+CPH
M9q0I
S(H;S0tFD
SXH;S`
EXL+EPI
S(H;S0tH
S(H;S0t
S@H;SHt
L;|$ 
A_A^A]A\_^]
\$ UVWATAUAVAWH
fffffff
HcQ,H
F D9$
A_A^A]A\_^]
\$ UVWATAUAVAWH
UUUUUUU
 A_A^A]A\_^]
@SUVWATAVAWH
(D$ f
@A_A^A\_^][
VWAVH
H;W I
 A^_^
UVWATAUAVAWH
I#u0H
H9D$ t.H;
0A_A^A]A\_^]
I#u0L
@USVWATAVAWH
A_A^A\_^[]
@SUVWH
x AVH
f9Ovu'
|$ UH
|$ UH
|$ UH
|$ UH
UVWATAUAVAWH
L9a0H
A_A^A]A\_^]
UVWATAUAVAWH
L9a0H
A_A^A]A\_^]
D$0type
|$ UH
|$ UH
|$ UH
\$ UVWAVAWH
L$@I;
A_A^_^]
@USVWAVH
A^_^[]
t$ WH
L$8H3
|$ UH
UVWAVAWH
@A_A^_^]
\$ UVWATAUAVAWH
X H+X
G H+G
@A_A^A]A\_^]
|$ UH
|$ UH
|$ UH
|$ UATAUAVAWH
A_A^A]A\]
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
X UVWATAUAVAWH
|$HI9_ thL
M9o u}L
M9o H
D|$xH
A_A^A]A\_^]
USVWAUAVAWH
momentumH
A_A^A]_^[]
X UVWATAUAVAWH
|$HI9_ thL
M9o uzL
M9o H
D|$xH
A_A^A]A\_^]
USVWAUAVAWH
momentumH
A_A^A]_^[]
@SVWH
@SUVWAVH
A^_^][
|$ UH
|$ UH
|$ UH
|$ UH
D$XH9Q
L$pH3
|$ UH
D$XH9Q
L$hH3
|$ UH
(|$0D
(D$ H
)t$@L
)|$0D
)D$ L
(|$0D
(D$ H
\$ UVWH
S H;S(t
S8H;S@t
@USVWAVH
H;C(t
S H;S(t
S8H;S@L
A^_^[]
\$ UVWATAUAVAWH
H9|$0u
H9|$Pu^A
H;t$P
H;D$@
H;|$@
V8I+V0H
A_A^A]A\_^]
@USVWAVH
A^_^[]
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
(D$ f
(D$ f
(D$ f
(D$ f
(D$ f
(D$ f
(D$ f
(D$ f
(D$ f
(D$ f
(D$ f
(D$ f
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
t$ UWAUAVAWH
A_A^A]_]
@SVWH
L$HH3
t$ UWAUAVAWH
A_A^A]_]
@SVWH
L$HH3
l$ VWAVH
T$(H+
L$@H3
L$@H3
L$@H3
L$`H3
L$`H3
t$ UWAUAVAWH
A_A^A]_]
\$ UVWH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
L$@H3
|$ UH
|$ UH
A H9A
A H9A
t$ UWAVH
D$PH+
\$ E3
|$ UH
|$ UH
|$ UH
)t$0M
(t$0H
)t$0M
(t$0H
t$ WH
L$0H3
\$ E3
X UVWAVAWH
A_A^_^]
t$ UWATAVAWH
A_A^A\_]
UVWATAUAVAWH
D;l$0uWH
t$ E3
l$8D;l$@
A_A^A]A\_^]
l$ VWAVH
0A^_^
l$ VWAVH
0A^_^
l$ VWAVH
0A^_^
t$0H+
t$0H+
t$0H+
t$0H+
t$0H+
t$0H+
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UH
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UH
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UH
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UH
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UH
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UH
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UH
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UH
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UH
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UH
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UH
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UH
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UH
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UH
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UH
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UH
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UH
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UH
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UH
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UH
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UH
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UH
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UH
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UH
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UH
t$ WH
t$ UWAVH
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
t$ UWAUAVAWH
A_A^A]_]
t$ UWAUAVAWH
A_A^A]_]
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
L$@H3
\$ UH
\$ UH
\$ UH
\$ UH
A H9A
A H9A
A H9A
)d$PH
(l$0f
L$`H3
)\$PH
(d$0f
L$`H3
(|$pI
@SUVWAVH
A^_^][
@SUVWH
@USVWATAUAVAWH
A_A^A]A\_^[]
)t$ M
(t$ H
)t$ I
(t$ H
)t$ M
(t$ H
)t$ I
(t$ H
)t$ M
(t$ H
)t$ I
(t$ H
)t$ M
(t$ H
)t$ I
(t$ H
)t$ M
(t$ H
)t$ I
(t$ H
)t$ M
(t$ H
)t$ I
(t$ H
)t$ M
(t$ H
)t$ I
(t$ H
)t$ M
(t$ H
)t$ I
(t$ H
)t$ M
(t$ H
)t$ I
(t$ H
)t$ M
(t$ H
)t$ I
(t$ H
)t$ M
(t$ H
)t$ I
(t$ H
)t$ M
(t$ H
)t$ I
(t$ H
)t$ M
(t$ H
)t$ I
(t$ H
)t$ M
(t$ H
)t$ I
(t$ H
)t$ M
(t$ H
)t$ I
.5AsH
(t$ H
)t$ M
(t$ H
)t$ I
(t$ H
L$0fD
L$0fD
@SUVWAVH
@A^_^][
M;A M
UVWAVAWH
 A_A^_^]
)t$ I
(t$ H
)t$ I
(t$ H
)t$ I
(t$ H
)t$ I
(t$ H
)t$ M
(t$ H
)t$ M
(t$ H
)t$ M
(t$ H
)t$ M
(t$ H
)t$ M
(t$ H
)t$ M
(t$ H
)t$ M
(t$ H
)t$ M
(t$ H
)t$ M
(t$ H
)t$ M
(t$ H
)t$ M
(t$ H
)t$ M
(t$ H
)t$ M
(t$ H
)t$ M
(t$ H
)t$ M
(t$ H
)t$ M
(t$ H
\$PfD
\$@fD
(D$0H
(D$0H
(|$0D
(D$ H
L$hH3
x AVH
(|$@A
|$ UH
|$ UH
X UVWATAUAVAWH
A_A^A]A\_^]
t$hH;
UVWATAUAVAWH
D$`H+
D$`H+
A_A^A]A\_^]
VWAVH
D$ ;D$ u
0A^_^
\$ UVWATAVH
A^A\_^]
UVWATAUAVAWH
L$HIc
A_A^A]A\_^]
UVWATAUAVAWH
|$hE3
|$pI;
 A_A^A]A\_^]
L$(I!C
T$8H+
L$@H3
t$ WH
D$0H!|$8
(t$ H
VWATAVAWH
0A_A^A\_^
t$ WH
H;{ H
SUVWATAUAVAWH
L9|$x
L9|$xu
(A_A^A]A\_^][
UVWAVAWH
PA_A^_^]
L$XH3
UVWAVAWH
A_A^_^]
UVWATAUAVAWH
D\$PH
\$PL9 u
T$HH;
D$hH+
T$HH;
D$hH+
T$pH;
L9d$P
tanhA
A_A^A]A\_^]
@USVWATAUAVAWH
A_A^A]A\_^[]
\$ UVWATAUAVAWH
E8/tpH
S H;S(H
PA_A^A]A\_^]
\$ UVWATAUAVAWH
A_A^A]A\_^]
@SVWH
L$0H90H
L$HH3
SVWATAUAVAWH
L+t$(I
@A_A^A]A\_^[
SVWATAUAVAWH
L+t$(I
@A_A^A]A\_^[
@SUVWATAVAWH
 A_A^A\_^][
ATAVAWH
 A_A^A\
t$ WH
UVWATAUAVAWH
L$pHc
\$pL9 u
T$HH;
D$`H+
T$HH;
D$`H+
T$hH;
tanhD
L9g(ufH
A_A^A]A\_^]
\$ UVWATAUAVAWH
A_A^A]A\_^]
L9m@u
H(D9o 
@USVWAVAWH
A_A^_^[]
UVWATAUAVAWH
@8k,H
@8k t*LcK
@A_A^A]A\_^]
X UVWATAUAVAWH
D9F(u
D8F,tBH
E0D8F t
pXQtf
pXQtf
t%LcF
E LcF
8^,t4HcF
A_A^A]A\_^]
D$0Lc
D$@Lc
D$PLc
D$`Lc
@8s tX
\$ UVWH
@USVWAVAWH
A_A^_^[]
WATAUAVAWH
 Lc\$pE3
Lcl$x
T$pHc
 A_A^A]A\_
UAVAWH
A_A^]
UAVAWH
A_A^]
\$ UVWH
UAVAWH
A_A^]
UVWAVAWH
(D$@f
A_A^_^]
|$ UH
\$ UVWH
|$ UH
|$ UH
|$ UH
|$ UH
@SUVWAVH
A^_^][
\$ UVWATAUAVAWH
N H+N
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
@USVWAVH
`A^_^[]
|$ UH
|$ UH
|$ UH
|$ UH
L$(H3
|$ UH
L$(H3
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
L$(H3
|$ UH
L$(H3
|$ UH
|$ UH
L$(H3
|$ UH
L$(H3
|$ UH
|$ UH
|$ UH
L$(H3
|$ UH
L$(H3
|$ UH
L$(H3
|$ UH
L$(H3
|$ UH
|$ UH
|$ UH
|$ UH
x UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UH
x UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UH
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
x UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UH
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UATAUAVAWH
T$pH;
A_A^A]A\]
x UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UH
x UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UH
x UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UH
\$ UVWAVAWH
`A_A^_^]
\$ UVWAVAWH
`A_A^_^]
UVWATAUAVAWH
pA_A^A]A\_^]
\$ UVWAVAWH
`A_A^_^]
\$ UVWAVAWH
`A_A^_^]
x ATAVAWH
(t$@L
(|$0I
A_A^A\
WAVAWH
A_A^_
WAVAWH
(t$@L
(|$0I
A_A^_
x ATAVAWH
A_A^A\
WAVAWH
 A_A^_
x AVH
T%#hF
(|$0D
(D$ H
@SUVWATAUAVAWH
\$PH+
H+D$8J
A_A^A]A\_^][
@USVWAVH
`A^_^[]
\$ UVWATAUAVAWH
\$0M+
A_A^A]A\_^]
I;@8u]I
B H+B
L$(H3
|$ UH
UVWATAUAVAWH
p L+p
D$ E3
H;t$P
H;t$P
A_A^A]A\_^]
|$ UH
UVWATAUAVAWH
p L+p
D$ E3
H;t$P
H;t$P
A_A^A]A\_^]
|$ UH
UVWATAUAVAWH
p L+p
D$ E3
H;t$P
H;t$P
A_A^A]A\_^]
|$ UH
UVWATAUAVAWH
p L+p
D$ E3
H;t$P
H;t$P
A_A^A]A\_^]
L$(H3
@USWH
(D$0f
(D$@f
t$ UWAVH
(D$0f
(D$@f
(D$@f
(D$0f
t$ UWAVH
(D$0f
(D$@f
(D$@f
(D$0f
(D$@f
(D$0f
@USWH
(D$0f
(D$@f
t$ UWAVH
(D$0f
(D$@f
(D$@f
(D$0f
t$ UWAVH
(D$0f
(D$@f
(D$@f
(D$0f
(D$@f
(D$0f
@USWH
(D$0f
(D$@f
t$ UWAVH
(D$0f
(D$@f
(D$@f
(D$0f
t$ UWAVH
(D$0f
(D$@f
(D$@f
(D$0f
(D$@f
(D$0f
@USWH
(D$0f
(D$@f
t$ UWAVH
(D$0f
(D$@f
(D$@f
(D$0f
t$ UWAVH
(D$0f
(D$@f
(D$@f
(D$0f
(D$@f
(D$0f
I;@(u]I
@USVWAVH
`A^_^[]
I;@(u]I
@USVWAVH
`A^_^[]
I;@(u]I
@USVWAVH
`A^_^[]
I;@(u]I
@USVWAVH
`A^_^[]
@USVWATAUAVAWH
A_A^A]A\_^[]
@USVWATAUAVAWH
A_A^A]A\_^[]
@USVWATAUAVAWH
A_A^A]A\_^[]
@USVWATAUAVAWH
A_A^A]A\_^[]
@USVWATAVAWH
A_A^A\_^[]
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
WATAUAVAWH
A_A^A]A\_
WATAUAVAWH
A_A^A]A\_
UVWATAUAVAWH
(D$@f
(D$`f
(D$@f
t$@I+
(D$@f
A_A^A]A\_^]
UVWATAUAVAWH
X I+X
(D$Pf
(D$pf
(D$Pf
t$PI+
(D$Pf
A_A^A]A\_^]
X UVWATAUAVAWH
T$pE3
T$pH;
(D$@f
(D$@f
\$@I+
(D$@f
D8t$0
A_A^A]A\_^]
|$ UH
|$ UH
|$ UH
|$ UH
UVWATAUAVAWH
D\$8H
~)@8wbt#L
A_A^A]A\_^]
q#UP:
q#UP:
q#UP:
q#UP:
q#UP:
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
L$HH3
|$ UH
(D$ f
|$ UH
(D$ f
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
(D$ f
|$ UH
(D$ f
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
(D$ f
|$ UH
(D$ f
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
(D$ f
|$ UH
(D$ f
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
(D$ f
|$ UH
|$ UH
|$ UH
(D$ f
|$ UH
(D$ f
|$ UH
(D$ f
|$ UH
(D$ f
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
(D$ f
|$ UH
(D$ f
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
(D$ f
|$ UH
(D$ f
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
(D$ f
|$ UH
|$ UH
|$ UH
(D$ f
|$ UH
(D$ f
|$ UH
(D$ f
|$ UH
(D$ f
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
(D$ f
|$ UH
(D$ f
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
(D$ f
|$ UH
(D$ f
|$ UH
(D$ f
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
(D$ f
|$ UH
(D$ f
|$ UH
(D$ f
|$ UH
(D$ f
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
(D$ f
|$ UH
(D$ f
|$ UH
(D$ f
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
(D$ f
|$ UH
(D$ f
|$ UH
|$ UH
|$ UH
(D$ f
|$ UH
|$ UH
|$ UH
|$ UH
(D$ f
|$ UH
(D$ f
|$ UH
|$ UH
|$ UH
(D$ f
|$ UH
|$ UH
|$ UH
A8H+A0H
\$ UVWATAUAVAWH
V`H;Vht
V`H;Vht
A_A^A]A\_^]
@USVWATAUAVAWH
T$HH+
T$HH+
L$8E3
L$8H9A }
H;B }bH
fffffff
L;d$P
L9` }
L;b }
T$(L;
D$HH+
@8}wt
d$0H;W
A_A^A]A\_^[]
A H+A
x UATAUAVAWH
L$@I;
DD$PH
(D$Pf
A_A^A]A\]
\$ UVWATAUAVAWH
L$XE3
Dt$0H
A_A^A]A\_^]
@USVWATAVAWH
D$pH+
D|$XH
D|$XH
A_A^A\_^[]
@USVWATAUAVAWH
D$pH+
EHL+E@I
(D$@f
(D$`f
A_A^A]A\_^[]
@USVWATAUAVAWH
D$pH+
EHL+E@I
(D$@f
(D$`f
A_A^A]A\_^[]
@USVWATAUAVAWH
D$pH+
EHL+E@I
(D$@f
(D$`f
A_A^A]A\_^[]
@USVWATAUAVAWH
D$pH+
EHL+E@I
(D$@f
(D$`f
A_A^A]A\_^[]
l$ VWAVH
D$0H9P }
fffffff
UVWAVAWH
|$` uWH
(D$pf
A_A^_^]
UVWAVAWH
|$` uKH
(D$pf
A_A^_^]
UVWAVAWH
|$` uWH
(D$pf
A_A^_^]
UVWAVAWH
|$` uMH
(D$pf
A_A^_^]
L$(I!C
T$8H+
L$@H3
@USVWATAUAVAWH
\$PI+
A_A^A]A\_^[]
UAVAWH
A_A^]
@USVWATAUAVAWH
A_A^A]A\_^[]
UAVAWH
A_A^]
@USVWATAUAVAWH
A_A^A]A\_^[]
UAVAWH
A_A^]
@USVWATAUAVAWH
A_A^A]A\_^[]
UAVAWH
A_A^]
VWAVH
D$ H;D$ u
0A^_^
t$ WAVAWH
0A_A^_
UVWAVAWH
D$ E3
|$` ubH
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` uSH
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` ucH
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` ubH
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` u_H
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` u]H
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
(D$pf
A_A^_^]
t$ UWATAVAWH
D$ E3
|$` usH
(D$pf
A_A^A\_]
UVWAVAWH
D$ E3
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` uKH
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` uKH
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` uMH
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` uMH
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` uKH
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` uKH
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` uWH
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` uKH
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` uWH
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` uKH
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` uKH
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` uMH
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` uMH
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` uKH
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` uKH
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` uKH
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` uKH
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` uMH
(D$pf
A_A^_^]
UVWAVAWH
A_A^_^]
@USVWATAUAVAWH
F8H+F0H
T$ H;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$ H;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$PH;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$ H;
A_A^A]A\_^[]
UVWAVAWH
D$ E3
|$` u[H
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` uNH
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` u[H
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` uHH
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` uHH
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` uHH
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` uHH
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` uHH
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` uHH
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` uHH
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` uHH
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` uHH
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` uHH
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` uHH
(D$pf
A_A^_^]
UVWAVAWH
D$ E3
|$` uHH
(D$pf
A_A^_^]
@USVWATAUAVAWH
F8H+F0H
T$0H;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$0H;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$0H;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$0H;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$ H;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$ H;
A_A^A]A\_^[]
USVWATAUAVAWH
F8H+F0H
T$0H;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$0H;
A_A^A]A\_^[]
USVWATAUAVAWH
F8H+F0H
T$0H;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$ H;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$ H;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$PH;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$ H;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$PH;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$PH;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$ H;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$ H;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$ H;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$ H;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$ H;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$PH;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$ H;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$PH;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$PH;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$ H;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$ H;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$PH;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$0H;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$0H;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$0H;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$PH;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$PH;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$PH;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$PH;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$PH;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$PH;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$PH;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$PH;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$PH;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$PH;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$PH;
A_A^A]A\_^[]
@USVWATAUAVAWH
F8H+F0H
T$PH;
A_A^A]A\_^[]
t$ WH
L$XH3
2H;q }hI+
H;w |
t$ WH
t$ WH
L$XH3
2H;q }hI+
H;w |
t$ WH
H;Q }CM+
I;Q |
t$ WH
t$ WH
L$XH3
2H;q }hI+
H;w |
VWATAVAWH
A_A^A\_^
UWATAVAWH
A_A^A\_]
UAVAWH
A_A^]
UWATAVAWH
A_A^A\_]
UAVAWH
A_A^]
UWATAVAWH
A_A^A\_]
UAVAWH
A_A^]
UWATAVAWH
A_A^A\_]
UAVAWH
A_A^]
UWATAVAWH
A_A^A\_]
UAVAWH
A_A^]
UWATAVAWH
A_A^A\_]
UAVAWH
A_A^]
UWATAVAWH
A_A^A\_]
UAVAWH
A_A^]
UWATAVAWH
A_A^A\_]
UAVAWH
A_A^]
UWATAVAWH
A_A^A\_]
UAVAWH
A_A^]
UWATAVAWH
A_A^A\_]
UAVAWH
A_A^]
UWATAVAWH
A_A^A\_]
UAVAWH
A_A^]
UWATAVAWH
A_A^A\_]
UAVAWH
A_A^]
t$ WH
t$ WH
t$ WH
t$ WH
l$ VWL
I;@8t/H
HPH;O
I;@8u
I;Pp|
l$0_^
l$ VWAVAWH
I;@8t.L
HPH;K
I;@8u
I;Pp|
l$@A_A^_^
l$ VWAVAWH
I;@8t-L
HPH;K
I;@8u
I;Pp|
l$@A_A^_^
l$ VWL
I;@8t/H
HPH;O
I;@8u
I;Pp|
l$0_^
H;Q }=I
I;Q |
H;Q }>I
I;Q |
H;Q }>I
I;Q |
H;Q }=I
I;Q |
L;A }<M
M;B |
L;A }<M
M;B |
H;Q }=I
I;Q |
H;Q }>I
I;Q |
H;Q }>I
I;Q |
H;Q }=I
I;Q |
L;A }<M
M;B |
L;A }<M
M;B |
l$ VWATAUAVAWH
I;@8u
M;Xp| I
l$PA_A^A]A\_^
l$ VWATAUAVAWH
I;@8u
M;Xp| I
l$PA_A^A]A\_^
UVWATAUAVAWH
I;@8tCH
I;@8u
M;Xp| I
A_A^A]A\_^]
UVWATAUAVAWH
I;@8tCH
I;@8u
M;Xp| I
A_A^A]A\_^]
l$ VWATAUAVAWH
I;@8u
M;Xp| I
l$PA_A^A]A\_^
l$ VWATAUAVAWH
I;@8u
M;Xp| I
l$PA_A^A]A\_^
l$ VWATAUAVAWH
I;@8u
M;Xp| I
l$PA_A^A]A\_^
l$ VWATAUAVAWH
I;@8u
M;Xp| I
l$PA_A^A]A\_^
UVWATAUAVAWH
I;@8tCH
I;@8u
M;Xp| I
A_A^A]A\_^]
UVWATAUAVAWH
I;@8tCH
I;@8u
M;Xp| I
A_A^A]A\_^]
l$ VWATAUAVAWH
I;@8u
M;Xp| I
l$PA_A^A]A\_^
l$ VWATAUAVAWH
I;@8u
M;Xp| I
l$PA_A^A]A\_^
l$ VWL
I;@8t7H
I;@8u
I;Pp|
l$0_^
l$ VWAUAVAWH
I;@8t3L
I;@8u
M;Xp| I
l$HA_A^A]_^
l$ VWL
I;@8t7H
I;@8u
I;Pp|
l$0_^
l$ VWAVAWH
I;@8t/L
HPH;K
I;@8u
I;Pp|
l$@A_A^_^
l$ VWAVAWH
I;@8t.L
HPH;K
I;@8u
I;Pp|
l$@A_A^_^
l$ VWL
I;@8t/H
HPH;O
I;@8u
I;Pp|
l$0_^
l$ VWAUAVAWH
hPH;n
I;@8u
I;Xp|
l$HA_A^A]_^
l$ VWAUAVAWH
hPH;n
I;@8u
I;Xp|
l$HA_A^A]_^
l$ VWAVL
I;X8t4H
HPH;N
I;@8u
I;Pp|
l$8A^_^
l$ VWAUAVAWH
PPH;V
I;@8u
I;Xp|
l$HA_A^A]_^
l$ VWAUAVAWH
PPH;V
I;@8u
I;Xp|
l$HA_A^A]_^
l$ VWAVL
I;X8t3H
HPH;N
I;@8u
I;Pp|
l$8A^_^
l$ VWL
I;@8t/H
HPH;K
I;@8u
I;Pp|
l$0_^
l$ VWAVAWH
I;H8t-M
PPI;S
I;H8u
I;pp|
l$@A_A^_^
l$ VWL
I;@8t/H
HPH;K
I;@8u
I;Pp|
l$0_^
l$ VWAUAVAWH
hPH;n
I;@8u
I;Xp|
l$HA_A^A]_^
l$ VWAUAVAWH
hPH;n
I;@8u
I;Xp|
l$HA_A^A]_^
l$ VWAVL
I;X8t5H
HPH;N
I;@8u
I;Pp|
l$8A^_^
l$ VWAUAVAWH
PPH;V
I;@8u
I;Xp|
l$HA_A^A]_^
l$ VWAUAVAWH
PPH;V
I;@8u
I;Xp|
l$HA_A^A]_^
l$ VWAVL
I;X8t4H
HPH;N
I;@8u
I;Pp|
l$8A^_^
UVWATAUAVAWH
wPI;u
H;_8u
H;_8u
H;_p|)H
d$8L;
(t$pL
(|$`I
A_A^A]A\_^]
UVWATAUAVAWH
VPI;Q
l$HfA
L;Fp|)H
l$@H;
|$0M;
PA_A^A]A\_^]
UVWATAUAVAWH
wPI;u
H;_8u
H;_8u
H;_p|)H
d$8L;
(t$pL
(|$`I
A_A^A]A\_^]
l$ VWATAVAWH
H;C8t4I
H;C8u
L;sp| H
 A_A^A\_^
l$ VWATAVAWH
H;C8t/M
KPI;O
H;C8u
L;sp| H
 A_A^A\_^
l$ VWATAVAWH
H;C8t:I
H;C8u
L;sp| H
 A_A^A\_^
l$ VWATAVAWH
H;C8t7M
H;C8u
L;sp| H
 A_A^A\_^
l$ VWATAUAVAWH
I;H8t5L
I;H8u
M;Xp| I
l$PA_A^A]A\_^
l$ VWL
I;@8tCH
I;@8u
I;Pp|
l$0_^
L$(H3
L$HH3
L$(H3
L$HH3
` UAVAWH
A_A^]
L$(H3
L$HH3
` UAVAWH
A_A^]
` UAVAWH
A_A^]
L$(H3
L$HH3
L$(H3
L$HH3
` UAVAWH
A_A^]
L$(H3
L$HH3
` UAVAWH
A_A^]
` UAVAWH
A_A^]
L$PH3
(D$ H
L$PH3
L$PH3
(D$ H
L$PH3
UVWATAUAVAWH
H!t$0I
A_A^A]A\_^]
Q0H+Q(H
UVWAVAWH
`A_A^_^]
UVWAVAWH
 A_A^_^]
|$ UH
(D$ f
(D$ f
D$PH+
D$hH+
(D$ f
(D$ f
D$PH+
D$hH+
VWAVH
 A^_^
t$ WAVAWH
 A_A^_
\$ UVWATAUAVAWH
x L+x
|$0H+
D$`H+
D$8H9CX
A_A^A]A\_^]
@USVWATAUAVAWH
L$0H;
L$0M;
G L+G
(D$`f
L9}pt?
D9?uMH
A_A^A]A\_^[]
(D$`f
(L$@f
(D$@f
(D$@f
@USVWATAUAVAWH
L$0H;
L$0M;
G L+G
(D$`f
L9}pt?
D9?uOH
A_A^A]A\_^[]
(D$`f
(L$@f
(D$@f
(D$@f
@USVWATAUAVAWH
L$`M;
L$`I;
C L+C
(D$@f
H9u`t;
93u<H
D$PH+
A_A^A]A\_^[]
(D$@f
\$@H+
D$`L+
(D$@f
(L$`f
(D$@f
(D$@f
D$pI+
H+ExH
H+ExH
Ep@8uhJ
H+ExH
Ep@8uhJ
D$PH+
|$ UH
|$ UH
|$ UH
\$ UVWATAUAVAWH
(D$@f
A_A^A]A\_^]
P@I+P8H
P@I+P8H
@USVWATAUAVAWH
D$(E3
T$`H;
A_A^A]A\_^[]
@USVWATAUAVAWH
D$(E3
T$`H;
A_A^A]A\_^[]
@SUVWAVH
 A^_^][
WAVAWH
I9h0~3H
 I;h0|
 A_A^_
x AVAWLc
|$0A_A^
x AVAWLc
|$0A_A^
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
UVWATAUAVAWH
2D8&t-L
EhH;Ept
A_A^A]A\_^]
UVWATAUAVAWH
2D8&t-L
EhH;Ept
A_A^A]A\_^]
UVWATAUAVAWH
2D9&t-L
EhH;Ept
A_A^A]A\_^]
UVWATAUAVAWH
2L9&t-L
EhH;Ept
A_A^A]A\_^]
UVWATAUAVAWH
EhH;Ept
A_A^A]A\_^]
UVWATAUAVAWH
D$8H+
PA_A^A]A\_^]
|$ UH
|$ UH
|$ UH
|$ UH
t$ WH
VWAVH
PA^_^
@USVWAVH
A^_^[]
x UAVAWH
A_A^]
UVWATAUAVAWH
H!D$0L
H!|$0L
d$8tCH
A_A^A]A\_^]
P@I+P8H
P@I+P8H
UWAVH
x AVH
UWAVH
UWAVH
UWAVH
\$ UVWATAUAVAWH
I9EXt}H
A_A^A]A\_^]
t$ UWAVH
t$ UWAVH
@USVWATAUAVAWH
A_A^A]A\_^[]
@USVWATAUAVAWH
A_A^A]A\_^[]
@USVWATAVAWH
pA_A^A\_^[]
UVWATAUAVAWH
H;UhM
pA_A^A]A\_^]
UVWATAUAVAWH
H;UhI
pA_A^A]A\_^]
UVWATAUAVAWH
H;UhI
pA_A^A]A\_^]
UVWATAUAVAWH
H;UhI
pA_A^A]A\_^]
UVWATAUAVAWH
L;MhI
`A_A^A]A\_^]
t$ UWAVH
@SUVATAUAVAWH
#wht^
 A_A^A]A\^][
@USWH
GenuH
ineIu
nteltd
Authu
entiu
cAMDt
AMDiuA
sbetu9
ter!u13
XL$0f
\$ UVWAVAWH
A_A^_^]
\$ UVWAVAWH
A_A^_^]
x UATAUAVAWH
A_A^A]A\]
x UATAUAVAWH
A_A^A]A\]
L$0H3
@USWH
@USWH
x UAVAWH
L9y8E
D9{0u<L
A_A^]
USVWATAUAVAWH
\$pA9pxu`M
t$ E9pxu-M
)d$Pf
)l$@H
D$PL+
d$PE2
A_A^A]A\_^[]
USVWATAUAVAWH
\$pA9pxu`M
t$ E9pxu-M
)d$Pf
)l$@H
D$PL+
d$PE2
A_A^A]A\_^[]
x UATAUAVAWH
T$ E3
L$(H91u
t$hHcD$0K
t$8E3
t$HMc
l$ M;
t$ fI
H;L$H~
\$8H;
HcD$0
l$@Hk
|$PL;
H;L$(~
|$8H;
l$ E3
A_A^A]A\]
x UATAUAVAWH
L$(H91u
t$hHcD$ H
\$8Mc
L$ L;
t$ fI
H;L$0~
T$(H;
l$0H;
H;L$0~
\$8H;
\$8E3
A_A^A]A\]
l$ VWATAVAWL
l$HA;
A_A^A\_^
x UATAUAVAWH
H;X0u
T$hH+
t$ I;
A_A^A]A\]
x UATAUAVAWH
H;X0u
T$hH+
t$ I;
A_A^A]A\]
t$ WH
}SHcN
|$(H;V t
L$0H3
l$ VWATAVAWH
 A_A^A\_^
D$(L+
L$0H3
|$ UH
\$ UVWATAUAVAWH
V I+V
CXI9FX
`A_A^A]A\_^]
\$ UVWATAUAVAWH
^ H+^
D$@I;
EPH!D$hL
A_A^A]A\_^]
t$ WH
L$0H3
\$ UVWATAUAVAWH
^ H+^
D$@I;
H!D$pL
(D$pf
A_A^A]A\_^]
\$ UVWATAUAVAWH
y H+y
D$@I;
E`H!D$PL
A_A^A]A\_^]
\$ UVWATAUAVAWH
y H+y
D$PI;
E`H!D$XL
A_A^A]A\_^]
t$ UWAVH
@USWH
t$ WH
t$ WH
t$ WH
(D$0f
D$PH+
(D$0f
D$PH+
\$ UVWATAUAVAWH
C H+C
@A_A^A]A\_^]
UVWATAUAVAWH
(D$Pf
A_A^A]A\_^]
V I+V
D$HH;
t$ UWAVH
@USVWATAVAWH
A_A^A\_^[]
UVWATAUAVAWH
(D$0f
A_A^A]A\_^]
|$ UH
|$ UH
|$ UH
t$ UWAVH
@SUVWH
H+D$(H
8_^][
\$ VWI
t$ UWAVH
D\$XH
(D$ f
(D$ f
(D$ f
(D$ f
(D$ f
(D$ f
(D$ f
(D$ f
(D$ f
@USVWATAUAVAWH
A_A^A]A\_^[]
t$ UWAVH
t$ UWATAVAWH
A_A^A\_]
@USVWATAUAVAWH
D$hE3
|$`E3
9\$Tu
D$PLc
HcD$TH
|$`H;|$x
A_A^A]A\_^[]
@USVWATAUAVAWH
D$hE3
|$`E3
9\$Tu
D$PLc
HcD$TH
|$`H;|$x
A_A^A]A\_^[]
@USVWATAUAVAWH
D$hE3
|$`E3
9\$Tu
D$PLc
HcD$TH
|$`H;|$x
A_A^A]A\_^[]
@USVWATAUAVAWH
D$hE3
|$`E3
9\$Pu
HcD$PH
l$pHc
|$`H;|$x
A_A^A]A\_^[]
@USVWATAUAVAWH
D$hE3
|$`E3
9\$Pu
HcD$PH
l$pHc
|$`H;|$x
A_A^A]A\_^[]
@USVWATAUAVAWH
D$`E3
(D$pf
A_A^A]A\_^[]
UVWATAUAVAWH
0A_A^A]A\_^]
VWAVH
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
H;w8r
H;w8r
|H9q8vvH
H;w8r
H9q8v H
L;W8r
'H9q8v!H
H;w8r
(D$0f
(D$@f
D$xH+
(D$0f
(D$@f
D$xH+
(D$0f
(D$@f
D$xH+
H WATAUAVAWH
A_A^A]A\_
@USVWATAUAVAWH
A_A^A]A\_^[]
\$ UVWATAUAVAWH
(D$Pf
(D$Pf
A_A^A]A\_^]
@USVWAVH
(D$0f
(D$0f
(L$Pf
(D$@f
(D$@f
(L$Pf
(D$0f
A^_^[]
@SUVWATAVAWH
(D$ f
0A_A^A\_^][
@USVWAVH
(D$0f
(D$0f
(L$Pf
(D$@f
(D$@f
(L$Pf
(D$0f
A^_^[]
@USVWAVH
(D$0f
(D$0f
(L$Pf
(D$@f
(D$@f
(L$Pf
(D$0f
A^_^[]
@USVWAVH
(D$0f
(D$0f
(L$Pf
(D$@f
(D$@f
(L$Pf
(D$0f
A^_^[]
@USVWAVH
(D$0f
(D$0f
(L$Pf
(D$@f
(D$@f
(L$Pf
(D$0f
A^_^[]
UWAVH
new_axisH
|$ UH
|$ UH
|$ UH
@USVWATAUAVAWH
E M+E
\$0H;
D$PH+
L$h8Y
A_A^A]A\_^[]
D$PH+
D$@M+
E@A8Z
I;L$0t
~$McL$0H
MhH;Mpt1H;
(D$@f
I;L$0v
D$ I;L$0w
HXI9IX
M;l$0v
I+D$0H
T$0I;
UVWATAUAVAWH
M0@8x
A_A^A]A\_^]
\$ UVWAVAWH
A_A^_^]
x UAVAWH
PXL9WX
A_A^]
UVWATAUAVAWH
 A_A^A]A\_^]
l$ VWATAVAWH
 A_A^A\_^
@USVWAVAWH
T$`H;
T$`H;
A_A^_^[]
x UATAUAVAWH
t$0E3
t$HI;
A_A^A]A\]
@USVWAVH
T$`H;
T$`H;
A^_^[]
x UATAUAVAWH
L$8E3
L;L$PurM
A_A^A]A\]
@USVWAVH
T$`H;
T$`H;
A^_^[]
x UATAUAVAWH
L$8E3
L;L$PurM
A_A^A]A\]
@USVWAVH
T$`H;
T$`H;
A^_^[]
x UATAUAVAWH
L$8E3
L;L$PurM
A_A^A]A\]
@USVWAVH
T$`H;
T$`H;
A^_^[]
x UATAUAVAWH
L$8E3
L;L$PurM
A_A^A]A\]
x AVH
UVWATAUAVAWH
A_A^A]A\_^]
@SUVWATAVAWH
A_A^A\_^][
@SUVWATAVAWH
A_A^A\_^][
@SUVWATAVAWH
A_A^A\_^][
@SUVWATAVAWH
A_A^A\_^][
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
@USVWAUAVAWH
A_A^A]_^[]
@USVWAUAVAWH
A_A^A]_^[]
@USVWAUAVAWH
A_A^A]_^[]
@USVWAUAVAWH
A_A^A]_^[]
t$ WH
L$@H3
(t$PI
|$ UH
|$ UH
|$ UH
p UWAUAVAWH
T$$Hc
A_A^A]_]
UWATH
H!\$0H!\$@L
L$PH3
L$PH3
(|$@D
(D$0D
(L$ H
D$@L;
L$PH3
X UVWATAUAVAWH
A_A^A]A\_^]
L$HHc
Hct$8H
d$HIc
L$pE3
Hc|$4H
D;l$PH
|$ UH
|$ UH
t$ UWATAUAVH
H!t$0L
H!t$0L
A^A]A\_]
(t$ H
UAVAWH
A_A^]
|$ UH
|$ UH
|$ UH
|$ UH
` UAVAWH
A_A^]
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
@SUVWAUAVAWH
`A_A^A]_^][
@SUVWAUAVAWH
`A_A^A]_^][
\$ UVWATAUAVAWH
A_A^A]A\_^]
X UVWATAUAVAWH
A_A^A]A\_^]
SUVWATAUAVAWH
XA_A^A]A\_^][
t$ WH
L$pH3
(D$0f
D$PH+
(D$0f
D$PH+
t$ WH
L$8H3
UVWATAUAVAWH
D$PL+
T$PH+
D$PL+
T$PH+
D$PL+
T$PH+
A_A^A]A\_^]
T$(H+
L$0H3
UWATAUAVH
H!\$0L
H!\$0L
H!\$0L
A^A]A\_]
UWAUAVAWH
H91tUH
tDH9q
t>L9)u
L9;|1L9{
~+L9{
A_A^A]_]
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
@USVWATAUAVAWH
xA_A^A]A\_^[]
SUVWAVH
S H;S(t
L$@H3
PA^_^][
X UVWATAUAVAWH
I9wpt
H9uhu
t:H9uh
A_A^A]A\_^]
p AWH
(t$`L
T$@H;
X UVWATAUAVAWH
H9}hu
t:H9}h
A_A^A]A\_^]
H;L$@|
p AWH
(t$`L
T$@H;
L$PH3
UVWATAUAVAWH
@A_A^A]A\_^]
@USVWATAUAVAWH
D$HH+
D$HH+
D$pM;
L$h@8
A_A^A]A\_^[]
|$ ATAVAWH
H+Q H
OhL;G
@A_A^A\
WATAUAVAWH
0A_A^A]A\_
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
SUVWAVH
S H;S(t
L$@H3
PA^_^][
\$ UVWATAUAVAWH
D$HE3
M9f`t
M9fpt
E I+E
d$@M9fxt
A_A^A]A\_^]
UVWATAUAVAWH
L9d$@t>
LcEPHcUHHcM@L
A_A^A]A\_^]
t$ UWATAVAWH
LcEPHcUHHcM@L
A_A^A\_]
t$ UWATAVAWH
LcEPHcUHHcM@L
A_A^A\_]
@SVWAVAWH
A_A^_^[
|$ UH
UWAVH
UWAVH
l$ VWAVH
@A^_^
UWAVH
UWATAVAWH
A_A^A\_]
@SUVWAVH
A^_^][
@SUVWAVH
A^_^][
@SUVWAVH
A^_^][
@SUVWAVH
A^_^][
H;\$ t*
L$(H3
H;\$ t*
L$(H3
H;\$ t*
L$(H3
@USVWAVH
A^_^[]
|$ UH
WAVAWH
0A_A^_
UWAVH
UWAVH
p UWATAVAWH
A_A^A\_]
UVWATAUAVAWH
`A_A^A]A\_^]
UVWATAUAVAWH
PA_A^A]A\_^]
UVWATAUAVAWH
|$@E3
d$0L+
A_A^A]A\_^]
UVWATAUAVAWH
PA_A^A]A\_^]
UVWATAUAVAWH
PA_A^A]A\_^]
UVWATAUAVAWH
fA9|U
PA_A^A]A\_^]
UVWATAUAVAWH
fA9|U
PA_A^A]A\_^]
UVWATAUAVAWH
PA_A^A]A\_^]
UVWATAUAVAWH
PA_A^A]A\_^]
UVWATAUAVAWH
PA_A^A]A\_^]
UVWATAUAVAWH
PA_A^A]A\_^]
UVWATAUAVAWH
PA_A^A]A\_^]
UVWATAUAVAWH
PA_A^A]A\_^]
UVWATAUAVAWH
@A_A^A]A\_^]
l$xuUM
l$xujM
VWAVH
t$xuoM
L$(H3
0A^_^
l$huEM
l$xu`M
l$xupM
x AVH
l$huEM
l$xu`M
l$xupM
x AVH
l$huDM
l$xuaM
l$xunM
x AVH
l$huDM
l$xuaM
l$xunM
x AVH
l$huDM
l$xu`M
l$xupM
x AVH
l$huDM
t$xu|M
x AVH
l$huFM
l$xu^M
UWAVH
l$huDM
l$xu]M
l$xumM
x AVH
l$huDM
l$xu]M
UWAVH
x AVH
l$huEM
l$huRM
l$huRM
x AVH
p UWATAVAWH
A_A^A\_]
VWAVH
(t$ H
0A^_^
VWAVH
(t$ H
0A^_^
VWAVH
(t$ H
0A^_^
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
UVWATAUAVAWH
|$0H+
(D$0f
A_A^A]A\_^]
UVWATAUAVAWH
 A_A^A]A\_^]
t$ WATAUAVAWH
 A_A^A]A\_
UVWATAUAVAWH
|$0H+
(D$0f
A_A^A]A\_^]
UVWATAUAVAWH
 A_A^A]A\_^]
t$ WATAUAVAWH
 A_A^A]A\_
UVWATAUAVAWH
|$0H+
(D$0f
A_A^A]A\_^]
UVWATAUAVAWH
 A_A^A]A\_^]
t$ WATAUAVAWH
 A_A^A]A\_
UVWATAUAVAWH
|$0H+
(D$0f
A_A^A]A\_^]
UVWATAUAVAWH
 A_A^A]A\_^]
t$ WATAUAVAWH
 A_A^A]A\_
UVWATAUAVAWH
|$0H+
(D$0f
A_A^A]A\_^]
UVWATAUAVAWH
|$0H+
(D$0f
A_A^A]A\_^]
UVWATAUAVAWH
|$0H+
(D$0f
A_A^A]A\_^]
UVWATAUAVAWH
|$0H+
(D$0f
A_A^A]A\_^]
UVWATAUAVAWH
|$0H+
(D$0f
A_A^A]A\_^]
UVWATAUAVAWH
|$0H+
(D$0f
A_A^A]A\_^]
UVWATAUAVAWH
|$0H+
(D$0f
A_A^A]A\_^]
UVWATAUAVAWH
|$0H+
(D$0f
A_A^A]A\_^]
UVWATAUAVAWH
D$ L+
|$HL+
L$XH3
`A_A^A]A\_^]
UVWATAUAVAWH
D$ E3
A_A^A]A\_^]
UVWATAVH
D$8H;
A^A\_^]
\$ UVWAVAWH
A_A^_^]
UWATAVAWH
T$0H;
T$0H;
T$0H;
T$0H;
A_A^A\_]
q P6!
VWAVH
t$ WH
UVWATAUAVAWH
L9d$`
CPH+CHH
ChH+C`H
A_A^A]A\_^]
@USVWATAVAWH
A_A^A\_^[]
SUVWATAVAWH
A_A^A\_^][
FHIcT
UVWATAVH
pA^A\_^]
@USVWATAUAVAWH
G@H+G8H
(D$`f
EHL+E@I
A_A^A]A\_^[]
\$ UVWH
|$ UH
|$ UH
|$ UH
|$ UH
(D$ f
(D$ f
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
\$ UVWATAUAVAWH
A8<$tkI
|$@H+U(H
M0H+M(H
A_A^A]A\_^]
@8~0t
D$x@8~0t
\$p@8~0t
T$@H;
T$@H;
T$@H;
UVWATAUAVAWH
)t$@D
)D$0L
t$ I;
C@H+1H
L9{8L
t$ E3
(t$@L
A_A^A]A\_^]
\$ UVWATAUAVAWH
E`H+EXH
A_A^A]A\_^]
\$ UVWATAUAVAWH
E`H+EXH
A_A^A]A\_^]
\$ UVWATAUAVAWH
E`H+EXH
A_A^A]A\_^]
\$ UVWATAUAVAWH
D$8E3
L9g8u
EHH+E@H
A_A^A]A\_^]
T$@H;
T$@H;
T$@H;
T$@H;
T$@H;
T$@H;
T$@H;
T$@H;
T$@H;
T$@H;
T$@H;
T$@H;
UVWATAUAVAWH
l$hL9y 
ChH+9H
L9SPL
|$XL9{(
ChH+q
L9KXL
ChL+A
L$(L9[`L
|$`I;
L$(I;
t$0L;
(|$pL
A_A^A]A\_^]
UVWATAUAVAWH
)t$`D
)D$PN
l$@L9I 
CPH+)H
H9S@H
CPL+q
H9KHH
|$(E3
|$(L;
(t$`L
A_A^A]A\_^]
@SUVWATAUAVAWH
l$`M9Y@
l$pM;Q`s7I
}/I;Ihs I
A_A^A]A\_^][
M9Q8~tI
I;QHs
M;Q8|
@SUVWATAUAVAWH
t$pM9Y@
ApL+Q
t$hM;Q`s<I
}4I;Ihs I
A_A^A]A\_^][
I9Y8~
I;QHs
I;Y8|
@SUVWATAUAVAWH
ApH+9H
}SM;Q`s?I
}7I;Ihs(I
A_A^A]A\_^][
M9Q8~iI
I;QHs
M;Q8|
@SUVWATAUAVAWH
ApH+9H
}SM;Q`s?I
}7I;Ihs(I
A_A^A]A\_^][
M9Q8~iI
I;QHs
M;Q8|
SUVWATAUAVAWH
$$M9A@
M;YpsvH
}nI;Qxs_I
8A_A^A]A\_^][
SUVWATAUAVAWH
M;YpsvH
}nI;Qxs_I
8A_A^A]A\_^][
SUVWATAUAVAWH
\$ H+y
,$I;ppsrH
}jI;Pxs[I
\$ L;
HA_A^A]A\_^][
SUVWATAUAVAWH
\$ H+y
,$I;ppsrH
}jI;Pxs[I
\$ L;
HA_A^A]A\_^][
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
SUVWATAUAVAWH
HA_A^A]A\_^][
X UVWATAUAVAWH
A_A^A]A\_^]
X UVWATAUAVAWH
A_A^A]A\_^]
X UVWATAUAVAWH
A_A^A]A\_^]
X UVWATAUAVAWH
A_A^A]A\_^]
X UVWATAUAVAWH
A_A^A]A\_^]
X UVWATAUAVAWH
A_A^A]A\_^]
X UVWATAUAVAWH
A_A^A]A\_^]
X UVWATAUAVAWH
A_A^A]A\_^]
X UVWATAUAVAWH
A_A^A]A\_^]
X UVWATAUAVAWH
A_A^A]A\_^]
X UVWATAUAVAWH
Lc8E3
A_A^A]A\_^]
UATAVH
A^A\]
UATAVH
A^A\]
UATAVH
A^A\]
UATAVH
A^A\]
` UAVAWH
*L$HH
A_A^]
` UAVAWH
A_A^]
UATAVH
A^A\]
UATAVH
A^A\]
A@A!AHI
|$ UAVAWH
A_A^]
UVWATAUAVAWH
)D$@H
d$ E3
A_A^A]A\_^]
UVWATAUAVAWH
*L$0L
*L$0I
A_A^A]A\_^]
UVWATAUAVAWH
H;D$0u
H;D$0u
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
\$PE3
*L$0I
*L$0M
A_A^A]A\_^]
UVWATAUAVAWH
\$PE3
A_A^A]A\_^]
UATAUAVAWH
t$0L+
D$pL+
t$HE3
A_A^A]A\]
\$ UVWH
9rHuVL
\$ UVWH
9rHuVL
t$ WATAUAVAWH
D$ fH
\$ I;
A_A^A]A\_
t$ WATAUAVAWH
D$ fH
\$ I;
A_A^A]A\_
@SVWH
UWAVH
H!\$0H
h UAVAWH
H!\$0D
H!\$0L
A_A^]
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
\$ UVWAVAWH
pA_A^_^]
\$ UVWAVAWH
pA_A^_^]
\$ UVWAVAWH
pA_A^_^]
\$ UVWAVAWH
pA_A^_^]
@USVWATAUAVAWH
D$ 8UwI
A_A^A]A\_^[]
@USVWATAUAVAWH
D$ 8UwI
A_A^A]A\_^[]
@USVWATAUAVAWH
D$ 8UwI
A_A^A]A\_^[]
@USVWATAUAVAWH
D$ 8UwI
A_A^A]A\_^[]
USVWATAUAVAWH
A_A^A]A\_^[]
t$ WATAUAVAWH
\$xH;
l$8E3
D$`E3
\$hu~H
D$pH;l$`|
_@uAN
l$8H;
A_A^A]A\_
|$ UATAUAVAWH
A_A^A]A\]
USVWATAUAVAWH
A_A^A]A\_^[]
t$ WATAUAVAWH
\$xH;
l$8E3
D$`E3
\$hu~H
D$pH;l$`|
_@uAN
l$8H;
A_A^A]A\_
|$ UATAUAVAWH
A_A^A]A\]
USVWATAUAVAWH
A_A^A]A\_^[]
t$ WATAUAVAWH
\$xH;
l$8E3
D$`E3
D$pH;l$`|
_@uBN
l$8H;
A_A^A]A\_
|$ UATAUAVAWH
A_A^A]A\]
USVWATAUAVAWH
A_A^A]A\_^[]
t$ WATAUAVAWH
\$xH;
l$8E3
D$`E3
D$pH;l$`|
_@uBN
l$8H;
A_A^A]A\_
|$ UATAUAVAWH
A_A^A]A\]
USVWATAUAVAWH
A_A^A]A\_^[]
t$ WATAUAVAWH
\$xH;
l$8E3
D$`E3
\$hu~H
D$pH;l$`|
_@uAN
l$8H;
A_A^A]A\_
|$ UATAUAVAWH
A_A^A]A\]
USVWATAUAVAWH
A_A^A]A\_^[]
t$ WATAUAVAWH
\$xH;
l$8E3
D$`E3
\$hu~H
D$pH;l$`|
_@uAN
l$8H;
A_A^A]A\_
|$ UATAUAVAWH
A_A^A]A\]
USVWATAUAVAWH
A_A^A]A\_^[]
t$ WATAUAVAWH
\$xH;
l$8E3
D$`E3
D$pH;l$`|
_@uBN
l$8H;
A_A^A]A\_
|$ UATAUAVAWH
A_A^A]A\]
USVWATAUAVAWH
A_A^A]A\_^[]
t$ WATAUAVAWH
\$xH;
l$8E3
D$`E3
D$pH;l$`|
_@uBN
l$8H;
A_A^A]A\_
|$ UATAUAVAWH
A_A^A]A\]
x AVM
|$(A^
x AVM
|$(A^
x AVM
|$(A^
x AVM
|$(A^
\$ VWAVH
 A^_^
t$ WAVAWH
 A_A^_
@SUVWAVH
l$8H+
L$@H3
PA^_^][
\$ UVWATAUAVAWH
 A_A^A]A\_^]
t$ WAVAWH
 A_A^_
@SUVWAVH
l$8H+
L$@H3
PA^_^][
\$ UVWATAUAVAWH
 A_A^A]A\_^]
t$ WAVAWH
 A_A^_
@SUVWAVH
l$8H+
L$@H3
PA^_^][
\$ UVWATAUAVAWH
 A_A^A]A\_^]
t$ WAVAWH
 A_A^_
@SUVWAVH
l$8H+
L$@H3
PA^_^][
\$ UVWAVAWH
 A_A^_^]
t$ WAVAWH
 A_A^_
@SUVWAVH
l$8H+
L$@H3
PA^_^][
\$ UVWAVAWH
 A_A^_^]
t$ WAVAWH
 A_A^_
@SUVWAVH
l$8H+
L$@H3
PA^_^][
\$ UVWAVAWH
 A_A^_^]
t$ WAVAWH
 A_A^_
@SUVWAVH
l$8H+
L$@H3
PA^_^][
\$ UVWAVAWH
 A_A^_^]
t$ WAVAWH
 A_A^_
@SUVWAVH
l$8H+
L$@H3
PA^_^][
x AVAWL
|$0A_A^
x AVAWL
|$0A_A^
x AVAWL
|$0A_A^
x AVAWL
|$0A_A^
\$ VWI
z4u2I;
z2u0L;
z1u/I;
z1u/L;
\$0_^
\$ VWI
\$0_^
\$ VWI
\$0_^
\$ VWI
\$0_^
\$ VWI
\$0_^
SVWAVAWH
(D$Pf
A_A^_^[
\$ VWAVH
z3u1I;
\$ VWAVH
z3u1I;
\$ VWAVH
z5u3I;
\$ VWAVH
\$ UVWAVH
A^_^]
\$ UVWAVH
A^_^]
\$ UVWAVH
A^_^]
\$ UVWAVH
A^_^]
(D$ f
(D$ f
\$ UVWATAUAVAWH
UUUUUUU
y(H;]
t$XH;]
^ I+^
D$`H+
(D$`f
]8H+]0H
T$pE3
I;X(u
t6M;Z(t+
M;Z(t
L;R(u
t=I;S(t.
c8H;u
MHL;MPt0H
\$pH;u
l$XL;}
A_A^A]A\_^]
\$ UVWATAUAVAWH
L9H }
L$@H9A }
UUUUUUU
z(H;]
t$XH;]
^ I+^
D$`H+
(D$`f
]8H+]0H
T$pE3
L;Z(u
t"M;Q(t
M;Q(t
L;H(u
t*I;B(t
b8H;u
MHL;MPt0H
\$pH;u
A_A^A]A\_^]
\$ UVWATAUAVAWH
D8H }
MP8A }
UUUUUUU
b(H;]
t$PH;]
_ I+_
H;} t
D$@H+
(D$@f
]HH+]@H
t$pE3
L;Z(u
t M;Q(t
M;Q(t
L;H(u
t'I;B(t
D$`L;
j8L;u
\$`L;u
A_A^A]A\_^]
\$ UVWATAUAVAWH
]@H+]8H
|$HH;]
H;t$@
_ I+_
D$@H;
t$`I+
(D$`f
]@H+]8H
T$PE3
E8H;M
t6M;w(t+I
M;w(t
t>M;w(t/I
o8H;]0t
|$PH;]0t
l$HH;|$@
A_A^A]A\_^]
VWAVH
|$0M;
L$8H3
WAVAWH
 A_A^_
UVWATAUAVAWH
A_A^A]A\_^]
\$ UVWATAUAVAWH
t$ I+
(D$ f
T$pE3
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
\$ UVWATAUAVAWH
t$ I+
(D$ f
T$pE3
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
\$ UVWATAUAVAWH
D$XE3
T$`L9\$Xt
|$8M;
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
\$ UVWATAUAVAWH
t$0H+
(D$0f
D$hE3
D$ E3
T$pL9\$ht
|$HM;
A_A^A]A\_^]
@UVWATAUAVAWH
PA_A^A]A\_^]
l$ VWATAVAWH
 A_A^A\_^
VWAVH
 H;k(u
 A^_^
\$ UVWH
\$ UVWH
\$ UVWH
\$ UVWH
(D$ f
(D$ f
D$PH+
D$hH+
(D$ f
(D$ f
D$PH+
D$hH+
UVWAVAWH
pA_A^_^]
UVWAVAWH
A_A^_^]
@USVWATAUAVAWH
A_A^A]A\_^[]
USVWAVH
A^_^[]
USVWAVH
A^_^[]
@USVWATAVAWH
A_A^A\_^[]
@USVWATAVAWH
A_A^A\_^[]
|$ UH
|$ UH
@USVWH
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
|$ UH
@USVWATAVAWH
A_A^A\_^[]
(D$ f
(D$ f
(D$ f
D$`H+
D$xH+
USVWAVH
@A^_^[]
UWAVH
\$ UVWATAUAVAWH
A_A^A]A\_^]
t$ WH
@USVWATAUAVAWH
A H+A
|$t@8|$pt
HcL$|;
|$tHc
A_A^A]A\_^[]
@USVWATAUAVAWH
A H+A
|$t@8|$pt
HcL$|;
|$tHc
A_A^A]A\_^[]
@USVWATAUAVAWH
A H+A
|$t@8|$pt
HcL$|;
|$tHc
A_A^A]A\_^[]
@USVWATAUAVAWH
A H+A
|$t@8|$pt
HcL$|;
|$tHc
A_A^A]A\_^[]
@USVWATAUAVAWH
A H+A
A_A^A]A\_^[]
@USVWATAVAWH
A_A^A\_^[]
@USVWATAVAWH
A_A^A\_^[]
UVWATAUAVAWH
0A_A^A]A\_^]
|$ UH
\$ UVWAVAWH
\$0H;
A_A^_^]
UVWATAUAVAWH
L$xE3
l$XL9
A_A^A]A\_^]
@USVWATAVAWH
T$`H+
A_A^A\_^[]
UAVAWH
A_A^]
\$ UVWATAUAVAWH
L$ H+
U(H;U0t
E(H9E u L
ExH9Epu!H
E`H9EXu!H
A_A^A]A\_^]
x UATAUAVAWH
H+D$8H
A_A^A]A\]
|$ UH
|$ UH
\$ UVWATAUAVAWH
P I+P
d$ E3
V H;V(t
A_A^A]A\_^]
X UVWATAUAVAWH
A_A^A]A\_^]
L+ExI
L+ExI
H H+H
@USVWATAVAWH
A_A^A\_^[]
L$PH3
UVWATAUAVAWH
A_A^A]A\_^]
X UVWATAUAVAWH
A_A^A]A\_^]
O L+O
EXL+EPI
I;^(H
|$ UH
|$ UH
UWAVH
UVWATAUAVAWH
A_A^A]A\_^]
|$ UH
|$ UH
UVWATAUAVAWH
\$xE3
(D$Pf
\$xE3
(D$Pf
EXL+EPI
;D$@u
\$xE3
A_A^A]A\_^]
t$ UWAVH
|$ UH
UVWATAUAVAWH
L$8H;
A_A^A]A\_^]
UVWAVAWH
{(H;w
A_A^_^]
|$ UH
UVWATAUAVAWH
D$HH;
D$XL+
A_A^A]A\_^]
\$ UVWATAUAVAWH
T$HD+
HcT$8H
D$@Mc
|$HI+
D$xI;
A_A^A]A\_^]
UVWAVAWH
A_A^_^]
|$ UH
VWAVH
L$xH3
L$(H3
L$XH3
@UVWATAUAVAWH
L!m@I
upL;}xA
A_A^A]A\_^]
x AVM
|$(A^
` UAVAWH
A_A^]
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
PA_A^A]A\_^]
UVWATAUAVAWH
D$PI+
`A_A^A]A\_^]
VWAUAVH
D$PI+
H;T$H|
l$8A^A]_^
X UVWAVAWH
@A_A^_^]
WATAUAVAWH
)d$pD
)l$`D
)t$PD
)|$@H
D$ L;
L$(I;
L$0H3
(t$PD
(|$@I
A_A^A]A\_
@UVWATAUAVAWH
A_A^A]A\_^]
@USVWAVAWH
A_A^_^[]
X UVWAVAWH
@A_A^_^]
H;T$0|
t$ A^
t$ WATAUAVAWH
 A_A^A]A\_
\$ WL
WATAUAVAWH
l$ I+
H;L$h
H;L$p
A_A^A]A\_
WATAUAVAWH
(t$pL
(|$`I
A_A^A]A\_
|$ UH
|$ UH
\$ UVWATAUAVAWH
D$08X
A_A^A]A\_^]
L$hE3
l$HL9l$0
l$HIc
H;|$0
UVWATAUAVAWH
D$0bodyD
D$@H+
D$@H+
T$`H;
T$`H;
A_A^A]A\_^]
\$ UVWATAUAVAWH
D$HL;
A_A^A]A\_^]
\$ UVWAUAVH
A^A]_^]
fffffff
t$ UWAWH
UWAVH
t$ UWAVH
P H+P
WHH;WPt
UVWATAUAVAWH
T$`E3
t$pL+
(D$pf
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
\$ UVWAVAWH
A_A^_^]
\$ UVWATAUAVAWH
E`H;Eht
E`H;Eht
A_A^A]A\_^]
UVWATAUAVAWH
T$HE3
P H+P
A_A^A]A\_^]
|$ UH
|$ UH
pq_PT'
t$ WATAUAVAWH
t$@I;
L$HH3
A_A^A]A\_
t$ WH
.H+9H
WAVAWH
pq_PT'
 A_A^_
pq_PT'
WAVAWH
 A_A^_
t$ UWAVH
UVWAVAWH
A_A^_^]
@SVWATAUAVAWH
\$PHi
\$`L;
d$hL;
L$pH3
A_A^A]A\_^[
@SUVWATAVAWH
 A_A^A\_^][
@SUVWATAVAWH
 A_A^A\_^][
qq_PT'
pq_PT'
@SUVWAVH
A^_^][
@USVWATAUAVAWH
A_A^A]A\_^[]
|$ UH
UVWATAUAVAWH
C L+C
A_A^A]A\_^]
USVWAVAWH
A_A^_^[]
|$ UH
|$ UH
|$ UH
VWAVH
 A^_^
x AVH
x AVH
UVWATAUAVAWH
A_A^A]A\_^]
D$0H+D$(H
H+L$(H
@USVWATAUAVAWH
H;Ew}nL
A_A^A]A\_^[]
@USVWATAVAWH
D$pE3
d$@L9
L$PH;
L$PIc
A_A^A\_^[]
@USVWATAUAVAWH
H;Ew}nL
A_A^A]A\_^[]
@USVWATAUAVAWH
A_A^A]A\_^[]
@USVWATAVAWH
H;]g}tIc
H;]gL
H;Eo}nL
A_A^A\_^[]
@USVWATAUAVAWH
D$xE3
d$@L9
L$XI;
L$XIc
A_A^A]A\_^[]
@USVWATAUAVAWH
t$@H9
d$XI;
d$XHc
A_A^A]A\_^[]
@USVWATAUAVAWH
t$@H9
d$XI;
d$XHc
A_A^A]A\_^[]
@USVWATAUAVAWH
t$@H9
d$PI;
d$PHc
A_A^A]A\_^[]
@USVWATAUAVAWH
t$@H9
d$PI;
d$PHc
A_A^A]A\_^[]
@USVWATAUAVAWH
H;Ew}nL
A_A^A]A\_^[]
@USVWATAUAVAWH
t$@H9
d$PI;
d$PHc
A_A^A]A\_^[]
@USVWATAUAVAWH
t$@H9
d$XI;
d$XHc
A_A^A]A\_^[]
@USVWATAUAVAWH
t$@H9
|$XI;
|$XHc
A_A^A]A\_^[]
@SUVWAVH
A^_^][
UVWATAUAVAWH
A_A^A]A\_^]
|$ UH
|$ UH
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
UUUUUUU
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
)T$PA
)\$@A
)d$0D
)l$ D
A_A^A]A\_^]
UVWATAUAVAWH
)T$PA
)\$@A
)d$0D
)l$ D
A_A^A]A\_^]
t$ UWAWH
UVWAVAWH
A_A^_^]
UVWAVAWH
A_A^_^]
UATAUAVAWH
A_A^A]A\]
UATAUAVAWH
A_A^A]A\]
t$8Hc
@USVWATAUAVAWH
L9:u{I
A_A^A]A\_^[]
\$ UVWATAUAVAWH
D$hE3
A_A^A]A\_^]
S H;S(t
S H;S(t
S H;S(t
\$ UVWATAUAVAWH
C L+C
I;F0u
I;FHu
A_A^A]A\_^]
x UATAUAVAWH
A_A^A]A\]
\$ UVWAVAWH
 A_A^_^]
|$ UH
\$ UVWATAUAVAWH
A_A^A]A\_^]
O L+O
ExL+EpI
I;F0u
I;FHu
@SUVWAVH
 A^_^][
\$ UVWATAUAVAWH
V H;V(t
A_A^A]A\_^]
p AWH
p AWH
@USVWATAUAVAWH
XA_A^A]A\_^[]
UWAVH
\$ UVWATAUAVAWH
M9uht
A_A^A]A\_^]
VWAVH
3333333
L!t$ H
0A^_^
X UVWATAUAVAWH
A_A^A]A\_^]
UUUUUUU
t$ WH
UUUUUUU
WAVAWH
 A_A^_
l$ VWATAVAWH
UUUUUUU
 A_A^A\_^
|$ UH
UVWATAUAVAWH
\$PH9
A_A^A]A\_^]
\$ UVWATAUAVAWH
|$PH;u
A_A^A]A\_^]
t$ WAVAWH
D$RH9D$@u
L$XH3
A_A^_
x AVAWH
|$8E3
L;T$H
|$0A_A^
x AVL
H;D$@
|$(A^
t$ WH
u>D!D$ 
@USVWATAUAVAWH
D8qhu
A_A^A]A\_^[]
@USVWATAUAVAWH
D8yhu
EpH;}
A_A^A]A\_^[]
E8Mjt
E8Mjt
UVWATAUAVAWH
H9Mwu H
\$@H;
|$ H+
(D$ f
|$@L;
t$PM+
(D$Pf
CL$PL
D$`L;
CD$PH
A_A^A]A\_^]
UVWATAUAVAWH
|$HH;
t$ H+
(D$ f
t$HH;
|$`H+
(D$`f
CL$`L
D$pL;
CD$`H
 L;|$ 
A_A^A]A\_^]
UVWATAUAVAWH
L9uwu
\$8H;
|$ H+
(D$ f
t$8L;
|$ M+
(D$ f
A_A^A]A\_^]
\$ UVWATAUAVAWH
UUUUUUU
@A_A^A]A\_^]
UVWATAUAVAWH
0A_A^A]A\_^]
t$ WH
H;{ H
SUVWATAUAVAWH
L9|$x
L9|$xu
(A_A^A]A\_^][
@SVWAVAWH
A_A^_^[
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
VWATAVAWH
)\$`D
p#UP:
A_A^A\_^
SUVWATAUAVAWH
z8u6E3
p#UP:
L$XH;
A_A^A]A\_^][
D8T$XL
USVWATAUAVAWH
W H;W(t
L$PH+
A_A^A]A\_^[]
USVWATAUAVAWH
L$HH;
W8H;W@t
W H;W(t
L$PH+
A_A^A]A\_^[]
UWAVH
\$ UVWATAUAVAWH
V H+V
^ H+^
A_A^A]A\_^]
V H+V
(D$0f
\$ UVWATAUAVAWH
V H+V
^ H+^
A_A^A]A\_^]
V H+V
(D$0f
\$ UVWATAUAVAWH
V H+V
^ H+^
A_A^A]A\_^]
V H+V
(D$0f
l$ VWATAVAWH
 A_A^A\_^
USVWATAUAVAWH
E8UtL
C I+C
E8UtL
E8UtH
E8UtH
A_A^A]A\_^[]
USVWATAUAVAWH
E8UtL
C I+C
E8UtL
E8UtH
E8UtH
A_A^A]A\_^[]
USVWATAUAVAWH
E8UtL
C I+C
E8UtL
E8UtH
E8UtH
A_A^A]A\_^[]
@SUVWATAUAVAWH
z+u)H
L$hH3
xA_A^A]A\_^][
UVWAVAWH
A_A^_^]
@USVWATAVAWH
A_A^A\_^[]
UVWATAUAVAWH
(t$0L
(|$ I
A_A^A]A\_^]
UVWATAUAVAWH
D$pH;
|$`I;
H#NHH
H;F tDL
DF H;F t!L
D$XI;
H;|$p
A_A^A]A\_^]
@SUVWATAUAVAWH
z+u)H
L$hH3
xA_A^A]A\_^][
UVWAVAWH
A_A^_^]
@SUVWATAUAVAWH
D$HI97
A_A^A]A\_^][
@USVWATAVAWH
A_A^A\_^[]
UVWATAUAVAWH
(t$@L
(|$0I
A_A^A]A\_^]
@SUVWATAUAVAWH
z+u)H
L$hH3
xA_A^A]A\_^][
UVWAVAWH
A_A^_^]
@SUVWATAUAVAWH
A_A^A]A\_^][
@USVWATAVAWH
A_A^A\_^[]
UVWATAUAVAWH
(t$0L
(|$ I
A_A^A]A\_^]
t$ WH
H;{ H
H;{ H
SUVWATAUAVAWH
)t$ I
L#b0M
L#r0M
(t$ H
8A_A^A]A\_^][
VWATAUAVAWL
l$PA_A^A]A\_^
UVWATAUAVAWH
I98~1L
A_A^A]A\_^]
VWATAUAVAWH
l$PA_A^A]A\_^
UVWATAUAVAWH
H99~RD
A_A^A]A\_^]
H98~.L
L9)~oE
l$PE3
x ATAUAVAWH
|$@A_A^A]A\
UVWATAUAVAWH
H99~RD
A_A^A]A\_^]
H98~.L
L9)~oE
l$PE3
@SUVWATAVAWH
(D$ f
@A_A^A\_^][
@SUVWATAVAWH
fffffff
(D$ f
@A_A^A\_^][
@SUVWATAVAWH
fffffff
(D$ f
@A_A^A\_^][
|$ AVH
z-u+H
|$ AVH
z-u+H
H;{ H
UVWATAUAVAWH
)t$ I
M#u0M
H9l$xu
(t$ H
0A_A^A]A\_^]
H9l$xu
M#u0H
H;\$xt
@SUVWATAVAWH
(D$ f
0A_A^A\_^][
x ATAVAWH
|$8A_A^A\
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
@USVWATAUAVAWH
\$ E3
A_A^A]A\_^[]
@USWH
L$XH3
A H9YPD
A H9YPH
L$`H3
I L9AP
Q L9APL
L$hH3
I L9AP
Q L9APL
L$pH3
UWATAVAWH
]0H9YPA
A_A^A\_]
UWATAVAWH
]0H9YPA
A_A^A\_]
\$ UVWH
t$ WH
L$ H;
L$ H;
L$`H;
L$`H;
x AVL
|$(A^
UVWATAUAVAWH
A(I+A H
t+L9 
QHH+Q@H
A8H+A0H
A(H+A H
H;\$0rMH;]
A(H+A H
A_A^A]A\_^]
x UATAUAVAWH
OHH+O@M
GHH+G@I
A_A^A]A\]
\$ UVWATAUAVAWH
D$(L;
t$0|0I
t$XHc
A_A^A]A\_^]
\$ UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
L;|$@t
H98uJH
A_A^A]A\_^]
UVWATAUAVAWH
L;d$@t
H90uJL
A_A^A]A\_^]
@USWH
@SUVWATAUAVAWH
(D$ f
8A_A^A]A\_^][
@SUVWATAUAVAWH
HA_A^A]A\_^][
UVWATAUAVAWH
 A_A^A]A\_^]
H!|$0H
UWAVH
\$ UVWH
L$HH3
USVWATAVAWH
D$@H+
T$@H+
T$@H+
T$@H+
T$@H+
A_A^A\_^[]
VWAVH
L$0H3
@A^_^
VWAVH
T$xL+
/D$xv
0A^_^
UWAVH
@A^_]
(D$ f
(D$ f
(D$ f
(D$ f
UWAVH
UWAVH
UWAVH
UWAVH
UWAVH
\$ UVWAVAWH
tHE9>u
t+fE9>u
pA_A^_^]
t$ UH
(D$ f
(D$ f
D$PH+
D$hH+
\$ UVWH
@ H+A
|$ UH
H!T$8H
t$ UWAVH
4A9r8t
@SVWH
t$ UWATAUAVH
H!T$0D
D$0body
T$`H;
T$`H;
A^A]A\_]
\$ UVWATAUAVAWH
D$HL;
A_A^A]A\_^]
\$ UVWAUAVH
A^A]_^]
UWATAUAVH
A^A]A\_]
t$ UWATAVAWH
A_A^A\_]
UVWATAUAVAWH
VXH;V`t
VXH;V`t
A_A^A]A\_^]
\$ UVWATAUAVAWH
A_A^A]A\_^]
\$ UVWATAUAVAWH
T$hE3
L;o }gH
L;o |
A_A^A]A\_^]
|$ UH
x6H;A8}0H;
@SVWATAUAVAWH
\$HMi
\$0I;
PA_A^A]A\_^[
@USVWATAUAVAWH
A_A^A]A\_^[]
t$ WATAUAVAWH
 A_A^A]A\_
|$ UH
X UVWATAUAVAWH
D+t$0A
D$XA;
D$XA;
D$`A;
D$`E;
Hc|$<H
d$8E;
A_A^A]A\_^]
UVWATAVH
A^A\_^]
|$ UH
|$ UH
H!|$0H
H!|$0H
WATAUAVAWH
0A_A^A]A\_
WATAUAVAWH
0A_A^A]A\_
\$(E3
\$(E3
\$(E3
\$(E3
@USVWATAVAWH
V H+V
W I+W
A_A^A\_^[]
|$ UH
UVWATAUAVAWH
D$@E3
A_A^A]A\_^]
UVWATAUAVAWH
D$8meanD
D$8seedD
(D$`f
A_A^A]A\_^]
USVWAUAVAWH
H!D$0H
D$0mean
H!t$PH
H!t$0H
D$0seed@
A_A^A]_^[]
UVWATAUAVAWH
D$8highD
D$8seedD
(D$`f
A_A^A]A\_^]
USVWAUAVAWH
H!D$0H
D$0high
H!t$PH
H!t$0H
D$0seed@
A_A^A]_^[]
USVWATAUAVH
L!t$0H
D$0seedD
A^A]A\_^[]
\$ UVWAVAWH
pA_A^_^]
\$ UVWAVAWH
pA_A^_^]
\$ UVWATAUAVAWH
A_A^A]A\_^]
)|$PI
(|$PH
X UVWATAUAVAWH
A_A^A]A\_^]
d$PE3
X UVWATAUAVAWH
A_A^A]A\_^]
d$PE3
WAVAWH
)d$PD
)l$@D
)t$0D
(t$0L
(|$ I
A_A^_
WAVAWH
)d$PD
)l$@D
)t$0D
(t$0L
(|$ I
A_A^_
UAVAWH
A_A^]
I;A8}
(D$ B
L$@H3
|$ UAVAWH
A_A^]
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
H;t$ u
I;OXt
DOXH;
@A_A^A]A\_^]
|$ UATAWH
A_A\]
|$ UATAWH
A_A\]
UVWAVAWH
t$0H+
L$@H3
PA_A^_^]
UVWATAUAVAWH
A_A^A]A\_^]
@SUVWAUAVAWH
L$@H3
PA_A^A]_^][
UVWATAUAVAWH
A_A^A]A\_^]
\$ UVWAVAWH
L$@H3
PA_A^_^]
UVWATAUAVAWH
A_A^A]A\_^]
UVWAVAWH
L$@H3
PA_A^_^]
UVWATAUAVAWH
A_A^A]A\_^]
UVWAVAWH
L$@H3
PA_A^_^]
UVWATAUAVAWH
A_A^A]A\_^]
@SUVWAUAVAWH
L$@H3
PA_A^A]_^][
UVWATAUAVAWH
A_A^A]A\_^]
\$ UVWAVAWH
L$@H3
PA_A^_^]
UVWATAUAVAWH
A_A^A]A\_^]
t$ WH
)t$ H
H;S8v
(t$ H
t$ WH
)t$ H
H;S8v
(t$ H
VWAVH
 A^_^
t$ UWATAVAWH
A_A^A\_]
@SUVWATAVAWH
(D$ f
@A_A^A\_^][
@SUVWATAVAWH
(D$ f
0A_A^A\_^][
@SUVWATAVAWH
(D$ f
@A_A^A\_^][
@SUVWATAVAWH
@A_A^A\_^][
@SUVWATAVAWH
(D$ f
PA_A^A\_^][
@SUVWATAVAWH
(D$ f
@A_A^A\_^][
|$ AVH
z-u+H
H;{ H
UVWATAUAVAWH
)t$ I
M#u0M
H9l$xu
(t$ H
0A_A^A]A\_^]
H9l$xu
M#u0H
H;\$xt
UATAUAVAWH
D$PE3
D$`H+
C H9C
A_A^A]A\]
X UVWATAUAVAWH
d$XI;
U8I+U0H
U I+U
D$(L9H }
UUUUUUU
D$PE3
A_A^A]A\_^]
VWAVH
VWAVH
t$0H+
L$8H3
@A^_^
L$8H3
SVWAVAWH
0A_A^_^[
t$ WATAUAVAWH
d$PM;
L$XH3
A_A^A]A\_
BRANCH_LI
:LEAFu
BRANCH_GI
BRANCH_EH9
8NONEu
LOGISTICL9
8SOFTu
SOFTMAX_H9
|$ UH
|$ UH
USVWATAUAVAWH
NONED
A_A^A]A\_^[]
USVWATAUAVAWH
NONED
A_A^A]A\_^[]
x UATAUAVAWH
D9Q |
E;P |
t$8D8hH
D9Y$|
A;Q |
E;Y$}
x&L;_(} M
D9I$|
A;P |
E;H$}
x&L;O(} M
L9o(v\I
PH;w(r
D9H$|
D;I$}
A_A^A]A\]
APH+AHH
D$PI+
APH+AHH
D$PI+
APH+AHH
D$PI+
APH+AHH
D$PI+
x UATAUAVAWH
D9Q |
E;P |
t$8D8hH
D9Y$|
A;Q |
E;Y$}
x&L;_(} M
D9I$|
A;P |
E;H$}
x&L;O(} M
L9o(v\I
PH;w(r
D9H$|
D;I$}
A_A^A]A\]
APH+AHH
D$PI+
APH+AHH
D$PI+
APH+AHH
D$PI+
APH+AHH
D$PI+
L$09Q |
UUUUUUU
t$ WH
VWAVH
t$0H+
L$8H3
@A^_^
l$ VWAVH
L$0D9A |
UUUUUUU
USVWATAUAVAWH
[hHcCtL;
A_A^A]A\_^[]
HcCxH;
ChHcCtL;
H9{h~(H
H;{h|
HcCxH;
CPH+CHH
CPH+CHH
USVWATAUAVAWH
[hHcCtL
A_A^A]A\_^[]
HcCxH;
T$`H;
T$`H;
ChHcCtL;
H9{h~(H
H;{h|
T$`H;
HcCxH;
CPH+CHH
CPH+CHH
T$`H;
T$`H;
T$`H;
USVWATAUAVAWH
[hHcCtL;
A_A^A]A\_^[]
HcCxH;
ChHcCtL;
L9[h~eH
H0H;H8t<H
I;I8u
L;[h|
HcCxH;
H0H;H8t<H
I;I8u
x AVH
USVWATAUAVAWH
[hHcCtL;
A_A^A]A\_^[]
HcCxH;
ChHcCtL;
L9[h~iH
H0H;H8t@H
I;I8u
L;[h|
HcCxH;
H0H;H8t@H
I;I8u
x AVH
USVWATAUAVAWH
[hHcCtL;
A_A^A]A\_^[]
HcCxH;
ChHcCtL;
H9{h~(H
H;{h|
HcCxH;
CPH+CHH
CPH+CHH
USVWATAUAVAWH
[hHcCtL
A_A^A]A\_^[]
HcCxH;
T$`H;
T$`H;
ChHcCtL;
H9{h~(H
H;{h|
T$`H;
HcCxH;
CPH+CHH
CPH+CHH
T$`H;
T$`H;
T$`H;
USVWATAUAVAWH
[hHcCtL;
A_A^A]A\_^[]
HcCxH;
ChHcCtL;
L9[h~eH
H0H;H8t<H
I;I8u
L;[h|
HcCxH;
H0H;H8t<H
I;I8u
x AVH
USVWATAUAVAWH
[hHcCtL;
A_A^A]A\_^[]
HcCxH;
ChHcCtL;
L9[h~iH
H0H;H8t@H
I;I8u
L;[h|
HcCxH;
H0H;H8t@H
I;I8u
x AVH
l$ VWATAVAWH
 A_A^A\_^
UVWATAUAVAWH
 A_A^A]A\_^]
VWAVH
L$8H3
@A^_^
VWAVH
H!t$ H
0A^_^
@0I;@8t9I
I;A8u
~;A8Y$t
@0I;@8t9I
I;A8u
~;A8Y$t
D8Qqt7HcB
D8RIt
D8RHu
D8RHu
D8Qqt7HcB
D8RIt
R D8RHu
R D8RHu
D8Qqt5HcB
D8RIt
D8RHu
R D8RHu
D8Qqt5HcB
D8RIt
D8RHu
R D8RHu
D8Qqt;HcB
D8RIt
D8RHu
R D8RHu
D8Qqt8HcB
D8RIt
D8RHu
R D8RHu
D8RHu
D8RIt
D8Qqt@
D8RIt
D8RHu
D8RHu
D8Qqt@
D8RIt
R D8RHu
R D8RHu
D8Qqt>
D8RIt
D8RHu
R D8RHu
D8Qqt>
D8RIt
D8RHu
R D8RHu
D8Qqt@
D8RIt
D8RHu
R D8RHu
D8Qqt=
D8RIt
D8RHu
R D8RHu
D8RHu
D8RIt
@USVWATAVAWH
A_A^A\_^[]
\$ UVWH
\$ UVWH
\$ UVWH
\$ UVWH
@USWH
\$ UVWH
@USWH
\$ UVWH
\$ UVWH
\$ UVWH
\$ UVWH
\$ UVWH
@USWH
\$ UVWH
@USWH
\$ UVWH
l$ WATAUAVAWH
3333333
l$(M+
\$0L;
L$8H3
A_A^A]A\_
h WAVAWH
(t$@L
(|$0I
A_A^_
UVWAVAWH
 A_A^_^]
(t$pL
(|$`I
l$ VWATAVAWH
 A_A^A\_^
|$ UH
APH+AHH
H0H;H8t@H
I;I8u
APH+AHH
x AVH
G8LcG H
D9w ~/H
HcG I
UVWAVAWH
Hcy H
H;H8t=H
I;I8u
0A_A^_^]
HcN H
H0H;H8t@H
I;J8u
G0LcG
VWAVH
LcA H
 A^_^
|$ UH
APH+AHH
H0H;H8t<H
I;I8u
APH+AHH
x AVH
G8LcG H
D9w ~/H
HcG I
UVWAVAWH
Hcy H
H;H8t9H
I;I8u
0A_A^_^]
HcN H
H0H;H8t<H
I;J8u
G0LcG
VWAVH
LcA H
 A^_^
UWAVH
API+AHH
API+AHH
VWAVH
F8LcF H
D9v ~/H
HcF I
 A^_^
UVWAVAWH
Hcy H
0A_A^_^]
HcN H
G0LcG
t$ WAVAWH
LcA H
 A_A^_
APH+AHH
APH+AHH
T$PH+
L$XH3
x AVH
G8LcG H
D9w ~/H
HcG I
G0LcG
|$ UH
APH+AHH
H0H;H8t@H
I;I8u
APH+AHH
x AVH
G8LcG H
D9w ~/H
HcG I
UVWAVAWH
Hcy H
H;H8t=H
I;I8u
0A_A^_^]
HcN H
H0H;H8t@H
I;J8u
VWAVH
LcA H
 A^_^
|$ UH
APH+AHH
H0H;H8t<H
I;I8u
APH+AHH
x AVH
G8LcG H
D9w ~/H
HcG I
UVWAVAWH
Hcy H
H;H8t9H
I;I8u
0A_A^_^]
HcN H
H0H;H8t<H
I;J8u
VWAVH
LcA H
 A^_^
UWAVH
API+AHH
API+AHH
VWAVH
F8LcF H
D9v ~/H
HcF I
 A^_^
UVWAVAWH
Hcy H
0A_A^_^]
HcN H
t$ WAVAWH
LcA H
 A_A^_
APH+AHH
APH+AHH
T$PH+
L$XH3
x AVH
G8LcG H
D9w ~/H
HcG I
VWAVH
 A^_^
USVWATAUAVAWH
NONED
A_A^A]A\_^[]
@USVWAUAVAWH
`A_A^A]_^[]
USVWATAUAVAWH
NONED
A_A^A]A\_^[]
@USVWAUAVAWH
`A_A^A]_^[]
USVWATAUAVAWH
NONED
A_A^A]A\_^[]
@USVWAUAVAWH
`A_A^A]_^[]
USVWATAUAVAWH
NONED
A_A^A]A\_^[]
@USVWAUAVAWH
`A_A^A]_^[]
UVWATAUAVAWH
A_A^A]A\_^]
@USVWATAUAVAWH
APH+AHH
D$pH+
(D$0f
CPH+CHH
A_A^A]A\_^[]
UVWATAUAVAWH
A_A^A]A\_^]
@USVWATAUAVAWH
APH+AHH
D$pH+
(D$0f
CPH+CHH
A_A^A]A\_^[]
UVWATAUAVAWH
A_A^A]A\_^]
@USVWATAUAVAWH
APH+AHH
D$pH+
(D$0f
CPH+CHH
A_A^A]A\_^[]
UVWATAUAVAWH
A_A^A]A\_^]
@USVWATAUAVAWH
APH+AHH
D$pH+
(D$0f
CPH+CHH
A_A^A]A\_^[]
x UATAUAVAWH
D9Q |
E;P |
t$8D8hH
D9Y$|
A;Q |
E;Y$}
x&L;_(} M
D9I$|
A;P |
E;H$}
x&L;O(} M
L9o(v\I
PH;w(r
D9H$|
D;I$}
A_A^A]A\]
x UATAUAVAWH
D9Q |
E;P |
t$8D8hH
D9Y$|
A;Q |
E;Y$}
x&L;_(} M
D9I$|
A;P |
E;H$}
x&L;O(} M
L9o(v\I
PH;w(r
D9H$|
D;I$}
A_A^A]A\]
USVWATAUAVAWH
[hHcCtL;
A_A^A]A\_^[]
HcCxH;
ChHcCtL;
H9{h~(H
H;{h|
HcCxH;
CPH+CHH
CPH+CHH
\$8H;
VWAVH
 A^_^
USVWATAUAVAWH
[hHcCtL;
A_A^A]A\_^[]
HcCxH;
ChHcCtL;
H9{h~(H
H;{h|
HcCxH;
CPH+CHH
CPH+CHH
\$8H;
VWAVH
 A^_^
USVWATAUAVAWH
{hHcCtH;
A_A^A]A\_^[]
HcCxH;
ChHcCtL;
H9{h~(H
H;{h|
HcCxH;
CPH+CHH
CPH+CHH
\$0H;
VWATAVAWH
 A_A^A\_^
USVWATAUAVAWH
{hHcCtH;
A_A^A]A\_^[]
HcCxH;
ChHcCtL;
H9{h~(H
H;{h|
HcCxH;
CPH+CHH
CPH+CHH
\$0H;
VWATAVAWH
 A_A^A\_^
\$ UVWH
UVWAVAWH
C(@8{1H
@A_A^_^]
@0I;@8t9I
I;A8u
\$ UVWH
@0I;@8t9I
I;A8u
\$ UVWH
\$ UVWH
D8A0t
D8A1u!
D8A0tFD8A1t+
D8A0t
D8A1u!
D8A0tFD8A1t+
D8A0t
D8A1u!
D8A0tFD8A1t+
D8A0t
D8A1u!
D8A0tFD8A1t+
D8Yqt*HcB
D8ZHu
D8ZHu
D8Yqt*HcB
R D8ZHu
R D8ZHu
D8Yqt(HcB
D8ZHu
R D8ZHu
D8Yqt(HcB
D8ZHu
R D8ZHu
D8Yqt,HcB
D8ZHu
R D8ZHu
D8Yqt)HcB
D8ZHu
R D8ZHu
D8ZHu
D8Yqt*HcB
D8ZHu
D8ZHu
D8Yqt*HcB
R D8ZHu
R D8ZHu
D8Yqt(HcB
D8ZHu
R D8ZHu
D8Yqt(HcB
D8ZHu
R D8ZHu
D8Yqt,HcB
D8ZHu
R D8ZHu
D8Yqt)HcB
D8ZHu
R D8ZHu
D8ZHu
\$ UVWH
\$ UVWH
\$ UVWH
\$ UVWH
\$ UVWAVAWH
A_A^_^]
\$ UVWH
\$ UVWAVAWH
A_A^_^]
\$ UVWH
UVWAVAWH
(t$@L
(|$0I
A_A^_^]
h VWAVH
(t$@L
(|$0I
(|$@D
(D$0D
(L$ H
APH+AHH
APH+AHH
T$PH+
L$XH3
x AVH
G8LcG H
9o ~/H
HcG H
UVWAVAWH
Hcy H
0A_A^_^]
HcN H
A0LcA
VWATAVAWH
LcA H
 A_A^A\_^
APH+AHH
APH+AHH
T$PH+
L$XH3
x AVH
G8LcG H
9o ~/H
HcG H
UVWAVAWH
Hcy H
0A_A^_^]
HcN H
VWATAVAWH
LcA H
 A_A^A\_^
APH+AHH
APH+AHH
T$PH+
L$XH3
x AVH
G8LcG H
9o ~/H
HcG H
APH+AHH
APH+AHH
T$PH+
L$XH3
x AVH
G8LcG H
9o ~/H
HcG H
x AVH
x AVH
t$ UWAUAVAWH
A_A^A]_]
|$ UH
USVWATAUAVAWH
D$8E3
D$`H+
D$PNONED
G L9w0
GpH+GhH
A_A^A]A\_^[]
X UVWATAUAVAWH
]@H9]Hu
(L$pf
8^ t7H
A_A^A]A\_^]
UVWATAUAVAWH
.5ut3
zAu?H
z>u<H
A_A^A]A\_^]
UVWATAUAVAWH
X%zn3
`A_A^A]A\_^]
UVWATAUAVAWH
D$`H+
D$`H+
D$`H+
D$`H+
D$PNONE
t$pH9
GPH+GHH
WhH;Wpt
A_A^A]A\_^]
\$ UVWATAUAVAWH
A_A^A]A\_^]
USVWATAUAVAWH
M`H!Ep
!D$XM
D$X;D$`u
d$hL;
W(Hc\$PH
O8Hc\$PH
Hc\$PH
H;}0|
H;E0|
A_A^A]A\_^[]
x UATAUAVAWH
t$PE3
7L9g8
u0D8o
u*D8o
OHE8n@t"H
A_A^A]A\]
D$(Lc
L$(H3
p UWATAVAWH
A_A^A\_]
T$0H;
T$0H;
(|$@D
(D$0D
(L$ H
UVWAVAWH
D$ ;D$ u
0A_A^_^]
W5#H3
(t$PD
(D$0D
(|$@H
|$ UH
|$ UH
|$ UH
|$ UH
UWATAUAVH
D$`H+
D$`H+
A^A]A\_]
UVWATAUAVAWH
O0H+O(H
(D$ H
A_A^A]A\_^]
t$ WH
L$0H3
UWATAUAVH
D$`H+
D$`H+
A^A]A\_]
UVWATAUAVAWH
O0H+O(H
(D$ H
A_A^A]A\_^]
UWATAUAVH
D$`H+
D$`H+
A^A]A\_]
UVWATAUAVAWH
O0H+O(H
(D$ H
A_A^A]A\_^]
UWATAUAVH
D$`H+
D$`H+
A^A]A\_]
UVWATAUAVAWH
O0H+O(H
(D$ H
A_A^A]A\_^]
|$ UH
|$ UH
|$ UH
|$ UH
\$ UVWATAUAVAWH
I;GXt
A_A^A]A\_^]
UVWATAUAVAWH
D\$hH
D$`H+
A_A^A]A\_^]
\$ UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
D\$hH
D$`H+
A_A^A]A\_^]
\$ UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
D\$hH
D$`H+
A_A^A]A\_^]
\$ UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
D\$hH
D$`H+
A_A^A]A\_^]
|$ UH
D$0norm
\$ UVWH
@SUVWAUAVAWH
PA_A^A]_^][
@SUVWAUAVAWH
PA_A^A]_^][
@SUVWAUAVAWH
PA_A^A]_^][
@SUVWAUAVAWH
z%u#3
PA_A^A]_^][
z)u'3
z)u'3
WATAUAVAWH
z(u&3
(t$ H
0A_A^A]A\_
z*u(3
z*u(3
WATAUAVAWH
z)u'3
(t$ H
0A_A^A]A\_
z*u(3
z*u(3
WATAUAVAWH
z)u'3
(t$@L
(|$0I
A_A^A]A\_
z&u$3
x ATAVAWH
z%u#3
(t$@L
(|$0I
A_A^A\
|$ UH
UVWATAUAVAWH
D$`H+
D$0NONE
G8I+G0H
A_A^A]A\_^]
@USVWAUAVAWH
A_A^A]_^[]
\$ UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
D$:E3
D$0NONED
T$hH;
T$HH;
T$HH;
D$`H+
T$HH;
T$HH;
D$`H+
G`I9GX
GHI+G@H
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
X UVWATAUAVAWH
O`H+OXH
GxH+GpH
A_A^A]A\_^]
UVWATAUAVAWH
D$@H+
D$@H+
A_A^A]A\_^]
UVWATAUAVAWH
\$ H+
(t$0H
@A_A^A]A\_^]
UVWATAUAVAWH
D$(I9
T$(H+
@A_A^A]A\_^]
UVWAVAWH
|$0H9
A_A^_^]
\$ UVWATAUAVAWH
A_A^A]A\_^]
;D$0u
t$ WATAUAVAWH
0A_A^A]A\_
t$ WATAUAVAWH
0A_A^A]A\_
t$ WATAUAVAWH
0A_A^A]A\_
t$ WATAUAVAWH
0A_A^A]A\_
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
VWAVH
D$0H+G
L9P }
@A^_^
VWAVH
D$0H+G
L9P }
@A^_^
t$ UWATAVAWH
L9P }
A_A^A\_]
UVWATAUAVAWH
)t$PI
D$@H+G
(t$PH
`A_A^A]A\_^]
UVWATAUAVAWH
)t$PI
D$@H+G
(t$PH
`A_A^A]A\_^]
UVWAVAWH
D$ I+G
D$0I;
L$@H3
PA_A^_^]
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
H;t$ u
I;OXt
DOXH;
@A_A^A]A\_^]
H SWH
H;\$ t*
L$(H3
UVWATAWH
map_formH
A_A\_^]
)t$0H
(t$0H
\$ UVATAVAWH
A_A^A\^]
p UWATAVAWH
tQL9P uK
A_A^A\_]
UVWATAUAVAWH
A_A^A]A\_^]
t$ UWATAVAWH
tTL9P uN
A_A^A\_]
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
|$ UH
)t$@H
(t$@H
UVWATAUAVAWH
A_A^A]A\_^]
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
UVWATAUAVAWH
(D$@f
A_A^A]A\_^]
UVWATAUAVAWH
(D$@f
A_A^A]A\_^]
UVWATAUAVAWH
(D$@f
A_A^A]A\_^]
UVWATAUAVAWH
(D$@f
A_A^A]A\_^]
UVWATAUAVAWH
(D$@f
A_A^A]A\_^]
@SUVWATAUAVAWH
8A_A^A]A\_^][
UVWAVAWH
 A_A^_^]
@USVWATAUAVAWH
LcMwI
A_A^A]A\_^[]
\$ UVWATAUAVAWH
L$@H;
L$@H;
A_A^A]A\_^]
t$ UWAVH
X UVWATAUAVAWH
D$8Lc
HcD$8H
A_A^A]A\_^]
 Hct$PI
 Hct$PI
HcD$(H
UVWAVAWH
(|$PH
pA_A^_^]
D$@Hc
L$@H3
LcT$0H
UVWAVAWH
(|$PH
pA_A^_^]
UVWATAVH
(|$PH
pA^A\_^]
(|$PH
\$ UVWATAUAVAWH
A_A^A]A\_^]
UVWAVAWH
A_A^_^]
UVWAVAWH
A_A^_^]
@SVWATAUAVAWH
gfffffffH
fffffff
D$0I;
|$`M;
L$hH3
pA_A^A]A\_^[
WAVAWH
 A_A^_
L$8H3
@SUVWAVH
A^_^][
l$ VWAVH
@A^_^
l$ VWAVH
@A^_^
VWATAVAWH
 A_A^A\_^
VWATAVAWH
 A_A^A\_^
@USVWATAUAVAWH
A_A^A]A\_^[]
@USVWATAVAWH
A_A^A\_^[]
\$ UVWATAUAVAWH
A_A^A]A\_^]
L9uPu
L9u`u
@USVWATAUAVAWH
A_A^A]A\_^[]
L9u0u
@USVWATAUAVAWH
A_A^A]A\_^[]
@USVWATAUAVAWH
A_A^A]A\_^[]
X UVWATAUAVAWH
HcL$pH
HcD$pH
A_A^A]A\_^]
T$8H+T$0H
L$8H+L$0H
L$pH3
@USVWAVAWH
A_A^_^[]
UVWATAUAVAWH
M H!E0H
HcD$XI;
u2H;U
A_A^A]A\_^]
USVWATAUAVAWH
L$XH+
L+|$pI
C H+C
E H+E
H9t$XvaL
A_A^A]A\_^[]
\$ UVWATAUAVAWH
G L+G
(D$Pf
A_A^A]A\_^]
UVWATAUAVAWH
M H!E0H
HcD$XI;
u2H;U
A_A^A]A\_^]
USVWATAUAVAWH
L$XH+
L+|$pI
C H+C
E H+E
H9t$XvaL
A_A^A]A\_^[]
\$ UVWATAUAVAWH
G L+G
(D$Pf
A_A^A]A\_^]
UVWATAUAVAWH
M H!E0H
HcD$XI;
u2H;U
A_A^A]A\_^]
USVWATAUAVAWH
L$XH+
L+|$pI
C H+C
E H+E
H9t$XvaL
A_A^A]A\_^[]
\$ UVWATAUAVAWH
G L+G
(D$Pf
A_A^A]A\_^]
UVWATAUAVAWH
M H!E0H
HcD$XI;
u2H;U
A_A^A]A\_^]
USVWATAUAVAWH
L$XH+
L+|$pI
C H+C
E H+E
H9t$XvaL
A_A^A]A\_^[]
\$ UVWATAUAVAWH
G L+G
(D$Pf
A_A^A]A\_^]
UVWATAUAVAWH
D$8H+
PA_A^A]A\_^]
UVWATAUAVAWH
D$8H+
PA_A^A]A\_^]
UVWATAUAVAWH
@A_A^A]A\_^]
UVWATAUAVAWH
QHH+Q@H
@8|$P
A_A^A]A\_^]
UVWATAUAVAWH
D$XE3
L$8E3
t$PH;
L$PH;
D$XL;
A_A^A]A\_^]
UVWAVAWH
A_A^_^]
UVWATAUAVAWH
T$HE3
L$HM;
A_A^A]A\_^]
UVWATAUAVAWH
N H+N
H H+H
D$PL;E
A_A^A]A\_^]
l$ VWATAVAWH
 A_A^A\_^
t$ WH
WAVAWH
 A_A^_
x ATAVAWH
 A_A^A\
UVWAVAWH
A_A^_^]
t$ WATAUAVAWH
T$8H;
\$(I;
A_A^A]A\_
\$ UVWAVAWH
 A_A^_^]
x AVH
UVWATAUAVAWH
A_A^A]A\_^]
@USVWATAUAVAWH
H!D$HL
D$PM;
A_A^A]A\_^[]
@USVWATAUAVAWH
A_A^A]A\_^[]
t$ WAVAWH
0A_A^_
UVWAVAWH
A_A^_^]
t$ UWAVH
WATAUAVAWH
A_A^A]A\_
@USVWATAUAVAWH
@XH9BX
A_A^A]A\_^[]
WATAUAVAWH
A_A^A]A\_
@USVWATAUAVAWH
@XH9BX
A_A^A]A\_^[]
WATAUAVAWH
A_A^A]A\_
@USVWATAUAVAWH
@XH9BX
A_A^A]A\_^[]
WATAUAVAWH
A_A^A]A\_
@USVWATAUAVAWH
@XH9BX
A_A^A]A\_^[]
p P6!
@SUVWAVH
A^_^][
s WATAUAVAWH
(D$pf
A_A^A]A\_
VWAVH
@USVWAVH
T$XH+
A^_^[]
@USVWATAUAVAWH
uhH;M
A_A^A]A\_^[]
UVWATAUAVAWH
A_A^A]A\_^]
@USVWATAUAVAWH
M+<$I
|$hH9
\$hH;
A_A^A]A\_^[]
UVWAVAWH
t$ H;W
t$ H;W
0A_A^_^]
VWAVH
VWAVH
PA^_^
s WAVAWH
A_A^_
t$ UWAVH
UVWATAUAVAWH
l$0D8
A_A^A]A\_^]
\$ UVWH
t$ WH
t$ UWAVH
ATAVAWH
gfffffffI
fffffff
 A_A^A\
@SVWATAUAVAWH
|$PHi
|$`I;
t$hH;
pq_PT'
>HiL$8
L$pH3
A_A^A]A\_^[
VAVAWH
fffffff
 A_A^^
\$ UVWATAUAVAWH
f9Bvu$A
PA_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
x UAVAWH
A_A^]
UVWAVAWH
{(H;w
A_A^_^]
UWAUAVAWH
A_A^A]_]
UVWAVAWH
{(H;w
A_A^_^]
UVWATAUAVAWH
D$XL+
A_A^A]A\_^]
\$ UVWATAUAVAWH
L$0fD
LcT$@I
HcT$DH
|$XI+
D$HI;
A_A^A]A\_^]
VWAVH
L$xH3
|$ ATAVAWH
L$ fE#
0A_A^A\
L$(H3
|$ UH
@UVWATAUAVAWH
L!}@I
U@L;}hA
A_A^A]A\_^]
UVWATAUAVAWH
D$PH;
D$XL+
A_A^A]A\_^]
X UVWATAUAVAWH
HcT$8H
D$@Mc
|$HI+
D$xI;
A_A^A]A\_^]
` UAVAWH
A_A^]
UVWAVAWH
A_A^_^]
UVWATAUAVAWH
D$ fD
A_A^A]A\_^]
WATAUAVAWH
L;L$P|
t$@A_A^A]A\_
UVWATAUAVAWH
D$xI+
A_A^A]A\_^]
WATAVAWH
D$PI+
L;D$H|
t$8A_A^A\_
@SUVWATAVAWH
@A_A^A\_^][
UVWATAUAVAWH
L$|L;
D$xfD
A_A^A]A\_^]
p AWH
t$XfA
t$ WH
H;{(}UI
H;{(|
UVWAVAWH
A_A^_^]
|$ UH
VWAVH
L$xH3
L$(H3
@UVWATAUAVAWH
A_A^A]A\_^]
L$XH3
USVWATAUAVAWH
MXI;u
hA_A^A]A\_^[]
@USVWATAVAWH
A_A^A\_^[]
VWAVH
L$xH3
VWAVH
L$xH3
@SUVWATAVAWH
@A_A^A\_^][
@UVWATAUAVAWH
L!m@I
upL;}xA
A_A^A]A\_^]
L;D$0|
t$ A^
x AVM
|$(A^
UVWATAUAVAWH
D$ fD
D$ fD
D$ fD
T$DfD
|$@fD
t$0fD
D$$H;
D$ fD
D$ fD
D$,H;
T$$fD
D$,H;
D$ fD
D$(L;
D$(H;
D$$L;
D$$H;
A_A^A]A\_^]
` UAVAWH
A_A^]
UVWATAUAVAWH
A_A^A]A\_^]
WATAVAWH
L;L$H|
t$8A_A^A\_
UVWATAUAVAWH
D$xI+
A_A^A]A\_^]
VWAUAVH
D$PI+
H;T$H|
l$8A^A]_^
X UVWAVAWH
@A_A^_^]
@UVWATAUAVAWH
A_A^A]A\_^]
x ATAVAWH
)t$pD
)|$`H
)|$@D
pG Df
pG@Df
pG`Df
pWpDfD
(D$@fE
L$0H;
L$PH3
(t$pD
(|$`I
A_A^A\
UVWATAUAVAWH
L$(L;
mHL;U`
L$4L;
L$0L;
L$,L;
L$(L;
L$(L;
A_A^A]A\_^]
WAVAWH
D$8E3
A_A^_
@UVWATAUAVAWH
A_A^A]A\_^]
@USVWAVAWH
A_A^_^[]
VWAVH
L$xH3
VWAVH
L$xH3
VWAVH
L$xH3
VWAVH
L$xH3
X UVWAVAWH
@A_A^_^]
H;T$0|
t$ A^
t$ WATAUAVAWH
 A_A^A]A\_
\$ WL
WATAUAVAWH
T$0I+
\$8I+
t$(M+
l$(M+
L+,$H
L$HH;
L$@H;
A_A^A]A\_
@UVWATAUAVAWH
A_A^A]A\_^]
@UVWATAUAVAWH
A_A^A]A\_^]
WATAUAVAWH
)\$ f
(t$pL
(|$`I
A_A^A]A\_
|$ UH
|$ UH
UVWATAUAVAWH
A_A^A]A\_^]
(t$ H
UVWATAUAVAWH
A_A^A]A\_^]
(t$ H
\$ UVWH
\$ UVWH
|$ UH
|$ UH
|$ UH
|$ UH
H!|$0H
\$ UVWATAUAVAWH
t$@H+
(D$@f
(D$@f
A_A^A]A\_^]
x AVH
H!|$0H
\$ UVWATAUAVAWH
|$@H+
(D$@f
T$`E3
H;]h|
H;Eh|
A_A^A]A\_^]
x AVH
H!|$0H
\$ UVWATAUAVAWH
t$@H+
(D$@f
(D$@f
A_A^A]A\_^]
x AVH
H!|$0H
\$ UVWATAUAVAWH
|$@H+
(D$@f
T$`E3
H;]h|
H;Eh|
A_A^A]A\_^]
x AVH
l$ VWATAVAWH
 A_A^A\_^
|$ UH
|$ UH
p UWAVH
H!|$0H
|$ UH
|$ UH
|$ UH
|$ UH
t$ UWAVH
H;]'|
H;E'|
x UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UH
x UATAUAVAWH
T$pH;
A_A^A]A\]
|$ UH
|$ UH
|$ UH
X UVWATAUAVAWH
E I9E
A_A^A]A\_^]
UWAVH
H!t$0H
UAVAWH
A_A^]
|$ UH
|$ UH
\$ UVWATAUAVAWH
O(H;O0t
(D$Pf
T$8M;
A_A^A]A\_^]
UWAVH
UVWATAUAVAWH
(D$`f
D$8H;
d$HL+
L9|$@
H;\$P
d$0I+
4;L;t$@}|H
|$HL+
H;|$8H
A_A^A]A\_^]
|$ UH
\$ UVWATAUAVAWH
T$PH;
A_A^A]A\_^]
|$ UH
x AVH
|$ UH
|$ UH
UVWATAUAVAWH
L9eptg
A_A^A]A\_^]
C L+C
L$hHc
p UWATAVAWH
A_A^A\_]
C L+C
L$hHc
\$ UVWH
\$ UVWH
\$ UVWH
|$ UH
|$ UH
UWATAVAWH
A_A^A\_]
UWATAVAWH
A_A^A\_]
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
USVWATAUAVAWH
A_A^A]A\_^[]
USVWATAUAVAWH
A_A^A]A\_^[]
l$ VWAVH
 A^_^
(|$ H
|$ UH
UVWATAUAVAWH
bilinearH
A_A^A]A\_^]
X UVWATAUAVAWH
@ H+A
HcD$`H
HcD$dH
(D$pf
A_A^A]A\_^]
SUVWATAUAVAWH
)t$pD
)|$`E
d$(E3
bilinearH9
(t$pL
(|$`I
A_A^A]A\_^][
|$ UH
|$ UH
UVWATAUAVAWH
(D$0f
A_A^A]A\_^]
\$ UVWATAUAVAWH
(D$Pf
MxH+MpH
A_A^A]A\_^]
u88_0t
|$ UH
T$@H;
T$@H;
T$@H;
UVWATAUAVAWH
M9iPM
AhH+y
M9aXM
0A_A^A]A\_^]
l$ VWAVI
M9Q@M
l$8A^_^
UVWATAUAVAWH
D$8I9z(
I9ZXI
D$(I9z0
BxL+I
M9b`M
BxH+i
I9BhI
L$0L;
@A_A^A]A\_^]
|$ UH
|$ UH
positiveH
(D$ f
(D$ f
UVWATAUAVAWH
A_A^A]A\_^]
UWATAUAVH
A^A]A\_]
(D$ f
(D$ f
D$PH+
D$hH+
UVWATAUAVAWH
H9w H
G H+G
G L+G
A_A^A]A\_^]
UVWAVAWH
A_A^_^]
@USVWATAUAVAWH
A_A^A]A\_^[]
UVWATAUAVAWH
@80tx@
A_A^A]A\_^]
UVWATAUAVAWH
@80tx@
A_A^A]A\_^]
UVWATAUAVAWH
@80tx@
A_A^A]A\_^]
UVWATAUAVAWH
@80tx@
A_A^A]A\_^]
UVWATAUAVAWH
@80tx@
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
L9d$@
\$xL;
H;L$@
A_A^A]A\_^]
UVWATAUAVAWH
L9d$@
\$xL;
H;L$@
A_A^A]A\_^]
UVWATAUAVAWH
L9d$@
\$xL;
H;L$@
A_A^A]A\_^]
UVWATAUAVAWH
H9\$H
H;T$H
A_A^A]A\_^]
\$hE3
UVWATAUAVAWH
H9T$H
H;T$H
A_A^A]A\_^]
UVWATAUAVAWH
H9T$H
H;T$H
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
UVWAVAWH
A_A^_^]
l$ VWATAVAWH
|$(E3
A_A^A\_^
UVWATAUAVAWH
A_A^A]A\_^]
T$@L;
T$@H;
UVWATAUAVAWH
H;Cht
l$0E3
A_A^A]A\_^]
UVWATAUAVAWH
H;Cpt
A_A^A]A\_^]
UVWATAUAVAWH
L$(L;
H;CXt
L$hH3
pA_A^A]A\_^]
UVWATAUAVAWH
H;C`t
L$pH3
A_A^A]A\_^]
UVWATAUAVAWH
H;Cht
l$0E3
A_A^A]A\_^]
UVWATAUAVAWH
H;Cpt
A_A^A]A\_^]
UVWATAUAVAWH
L$(L;
H;CXt
L$hH3
pA_A^A]A\_^]
UVWATAUAVAWH
H;C`t
L$pH3
A_A^A]A\_^]
UVWATAUAVAWH
H;Cht
t$8E3
A_A^A]A\_^]
UVWATAUAVAWH
H;Cpt
A_A^A]A\_^]
UVWATAUAVAWH
L$0L;
H;CXt
L$hH3
pA_A^A]A\_^]
UVWATAUAVAWH
H;C`t
L$hH3
pA_A^A]A\_^]
UVWATAUAVAWH
t$@E3
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
L$pH3
A_A^A]A\_^]
UVWATAUAVAWH
L$pH3
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
)t$pH
L$hH3
(t$pH
A_A^A]A\_^]
UVWATAUAVAWH
D$XE3
L;D$(|
L$hH3
pA_A^A]A\_^]
l$ VWAVH
T$ L91H
L$0H3
@A^_^
UVWATAUAVAWH
S8H;S@~
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
l$ VWAVH
 A^_^
|$ UH
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
|$ UH
|$ UH
UVWATAVH
@A^A\_^]
UVWATAUAVAWH
PA_A^A]A\_^]
UVWATAUAVAWH
D$xI;
A_A^A]A\_^]
X UVWATAUAVAWH
A_A^A]A\_^]
|$ UH
p</w1I
|$ UH
UVWATAUAVAWH
|$PH9
F@A8^@t
VPI;VXt
A_A^A]A\_^]
\$ UVWATAUAVAWH
l$@I;
tdH;\$XH
B\$XH
|$0H;T$pt/H
D$@H+
(D$@f
T$pH+
L$ @8y
D$0H;T$pt
D$0H+
(D$0f
L$XI;
\$PH+
L;d$@t
T$pH+
A_A^A]A\_^]
@USVWATAUAVAWH
D$HL;X8r5L
D$HL;X8r0L
d$@M;
l$HI;
H!\$PH;U
t$PH;U
D$pL;
L;d$pI
A_A^A]A\_^[]
@USVWATAUAVAWH
D$@L;H8r3I
|$HH;U
D$PH+
(D$Pf
|$HH;U
D$PH+
(D$Pf
D$pE3
l$HL;l$Pt
A_A^A]A\_^[]
\$ UVWATAUAVAWH
FPH9FHH
A_A^A]A\_^]
L$0I;
L$8H3
UVWATAUAVAWH
 A_A^A]A\_^]
WAVAWH
 A_A^_
l$ VWATAVAWH
 A_A^A\_^
x ATAVAWH
 A_A^A\
\$ UVWATAUAVAWH
 A_A^A]A\_^]
\$(I;
L$8H3
UVWATAUAVAWH
\$XL9 u
D$pH+
D$pH+
L9d$X
tanhD
tanhD
A_A^A]A\_^]
@USVWATAUAVAWH
A_A^A]A\_^[]
@USVWATAUAVAWH
A_A^A]A\_^[]
\$ UVWATAUAVAWH
A_A^A]A\_^]
@SUVWAVH
A^_^][
|$ UH
UVWATAUAVAWH
A_A^A]A\_^]
|$ UH
t$ UWATAVAWH
D$pH;
(D$Pf
A_A^A\_]
|$ UH
t$ UWAVH
|$ UH
UVWATAUAVAWH
H9uXt:
(D$0f
(D$0f
EXMcd$
L!l$P
A_A^A]A\_^]
WAVAWH
HcI H
D;S$|
 A_A^_
\$ UVWH
UVWATAUAVAWH
A_A^A]A\_^]
|$ UH
UVWATAUAVAWH
VUUUA
9A tu
M9 t0H
`A_A^A]A\_^]
D$XHc
D$`Hc
D$hHc
(D$0f
X UVWATAUAVAWH
A_A^A]A\_^]
u0HcU
UVWATAUAVAWH
VUUUUUUUH
D$tLc)A
G Jc,
Lcd$tE
A_A^A]A\_^]
\$ UVWATAUAVAWH
^ I+^
A_A^A]A\_^]
@USVWATAUAVAWH
L$@E3
Hct$0H
W H;W(t
A_A^A]A\_^[]
UVWATAUAVAWH
A_A^A]A\_^]
USVWATAVAWH
t$PHc
A_A^A\_^[]
WATAUAVAWH
(t$`L
A_A^A]A\_
USVWH
UVWATAUAVAWH
t$0Hc
A_A^A]A\_^]
x AVH
l$ VWATAVAWH
\$ Lc
L$HH3
A_A^A\_^
|$ UH
t$ UWAUAVAWH
bilinearH
D\$pH
A_A^A]_]
UVWATAUAVAWH
epE8|$
A_A^A]A\_^]
UVWATAUAVAWH
Yd$HH
Y\$LH
YD$PH
D$pH;
A_A^A]A\_^]
L;L$X
L;D$P
|$ UH
x AVH
\$ UVWATAUAVAWH
D$`I9~ht
(D$Pf
A_A^A]A\_^]
~ppu%L
~tpu.H
l$PL;
MPI9~h
WAVAWH
(t$0H
@A_A^_
t$ WATAUAVAWH
0A_A^A]A\_
H;L$@|
UVWATAUAVAWH
pA_A^A]A\_^]
|$ UH
\$ UVWH
UVWATAUAVAWH
(D$0f
(D$0f
HcD$ L
A_A^A]A\_^]
p AWH
C@H9{Ht
H9{8~{D
H9{Ht
H;C8|
H9{8~&H
H;C8|
H9{8~zH
H;C8|
UVWATAUAVAWH
(D$0f
(D$0f
HcD$ L
A_A^A]A\_^]
p AWH
C@H9{Ht
H9{8~{D
H9{Ht
H;C8|
H9{8~&H
H;C8|
H9{8~zH
H;C8|
\$ UVWATAUAVAWH
A_A^A]A\_^]
|$ UH
@USVWATAUAVAWH
A_A^A]A\_^[]
[ UVWATAUAVAWH
E8/tpH
S H;S(H
`A_A^A]A\_^]
\$ UVWATAUAVAWH
A_A^A]A\_^]
@USVWATAUAVAWH
A_A^A]A\_^[]
X UVWATAUAVAWH
D$DH;
UUUUUUU
A_A^A]A\_^]
X UVWATAUAVAWH
L9sht
A_A^A]A\_^]
X UVWATAUAVAWH
H9]`t7
8\$au
A_A^A]A\_^]
t$ WH
UUUUUUU
WAVAWH
 A_A^_
l$ VWATAVAWH
UUUUUUU
 A_A^A\_^
X UVWATAUAVAWH
H9s@H
A_A^A]A\_^]
UUUUUUU
\$ UVWATAUAVAWH
H9\$Hv`H
V H;V(t
A_A^A]A\_^]
l$ VWATAVAWH
UUUUUUU
 A_A^A\_^
|$ UH
|$ UH
|$ UH
|$ UH
L$@H3
L$hH3
L$hH3
L$@H3
L$hH3
L$hH3
L$@H3
L$XH3
L$XH3
L$@H3
L$XH3
L$XH3
UVWATAUAVAWH
d$ E3
A_A^A]A\_^]
UVWATAUAVAWH
d$ E3
A_A^A]A\_^]
A H9A
A H9A
A H9A
M;A M
|$ UH
|$ UH
|$ UH
|$ UH
UVWATAUAVAWH
D8q8t
A_A^A]A\_^]
UVWATAUAVAWH
D8q8t
A_A^A]A\_^]
UVWATAUAVAWH
@8p8t
A_A^A]A\_^]
UVWATAUAVAWH
@8p8t
A_A^A]A\_^]
UVWATAUAVAWH
T$pH;
D$pH;
A_A^A]A\_^]
UVWATAUAVAWH
T$pH;
D$pH;
A_A^A]A\_^]
UVWATAUAVAWH
T$pH;
D$pH;
A_A^A]A\_^]
UVWATAUAVAWH
T$pH;
D$pH;
A_A^A]A\_^]
|$ UH
|$ UH
|$ UH
|$ UH
t$ H;K
L$(H3
X UVWATAUAVAWH
A_A^A]A\_^]
|$ UH
T$@H;
H+] H
H+] H
l$ VWATAVAWH
H9K@H
C`D8`
0A_A^A\_^
UVWATAUAVAWH
t$pL9Q(
CpH+9H
L9CXL
|$hL9S0
CpH+i
L9c`L
CpL+Q
D$@H;
D$ L;
A_A^A]A\_^]
X UVWATAUAVAWH
H9WhH
H9GpH
H9_xH
A_A^A]A\_^]
UVWATAUAVAWH
CXH+1H
L9{HL
CXH+Q
CpD8P
@A_A^A]A\_^]
X UVWATAUAVAWH
ChL+1H
L9CXL
ChH+q
H9{`H
A_A^A]A\_^]
\$ UVWATAUAVAWH
)t$`H
C8H+9H
H9s0H
CPD8X
(t$`H
pA_A^A]A\_^]
s(HcD$
UVWATAUAVAWH
zCuAI
T$`H;
T$`H;
A_A^A]A\_^]
\$ UVWATAUAVAWH
zIuGI
T$pH;
T$pH;
A_A^A]A\_^]
D$PL;t
D$`E3
|$ UH
UVAVH
t$0H+
L$8H3
@A^^]
VWAVH
L$8H3
@A^_^
l$ VAVAWH
L$(I+
t$0M;
L$8H3
@A_A^^
WAVAWH
 A_A^_
UVWATAUAVAWH
A_A^A]A\_^]
|$ UH
@USWH
\$ UVWATAUAVAWH
A_A^A]A\_^]
\$ UVWH
D$`I+
T$`H+
L$hH3
|$ UH
UVWATAUAVAWH
D$@E3
A_A^A]A\_^]
l$PE3
HcEXH
HcMPH
UWATAVAWH
A_A^A\_]
t$ UWATAVAWH
A_A^A\_]
@UVWATAUAVAWH
A_A^A]A\_^]
` UAVAWH
A_A^]
@SUVWATAVAWH
@A_A^A\_^][
L;D$0|
t$ A^
UVWATAUAVAWH
T$xE3
\$xI;
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
WATAUAVAWH
T$(H;
t$`!T$
l$0I+
L$8H;
D$ H;
A_A^A]A\_
t$ WATAUAVAWH
 A_A^A]A\_
\$ UVWL
l$ VWATAVAWH
 A_A^A\_^
UWAVH
channelsH
C@H+C8H
t$ UWATAVAWH
A_A^A\_]
\$ UVWATAUAVAWH
A_A^A]A\_^]
\$ UVWAVAWH
A_A^_^]
\$ UVWATAUAVAWH
O L+O
A_A^A]A\_^]
@USVWATAUAVAWH
D8o t
A_A^A]A\_^[]
VWAVH
0A^_^
\$ UVWATAUAVAWH
E8f,t
A_A^A]A\_^]
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
|$ UH
t$ WATAUAVAWH
A_A^A]A\_
VWATAVAWH
0A_A^A\_^
UVWATAUAVAWH
D8{5H
D8{6tgLcC$M
D8{5t
A_A^A]A\_^]
USVWATAUAVAWH
HcG$H
HcW$H
Hc_$H
LcG$H
Hc_$H
LcG$H
Hc_$H
LcG$H
HcO$H
xA_A^A]A\_^[]
LcA$H
D$PLc
D$`Lc
D$pLc
E;J$|
X UVWATAUAVAWH
uG!D$t
D$t;D$pu
HcK$H
;D$tu
D;t$pu
A_A^A]A\_^]
\$ UVWATAUAVAWH
C(D90
E$IcM$
uJF94
uDD!u
~7D!u
D;0|`I
K0D;0
A_A^A]A\_^]
X UVWATAUAVAWH
uE!D$x
D$x;E
$HcK$H
;D$xu
A_A^A]A\_^]
\$ UVWATAUAVAWH
E$IcM$A
uLC94
;0|kH
A_A^A]A\_^]
@USVWAVAWH
L$0E3
D$@H;
D$@H;
T$@H;
A_A^_^[]
UVWATAUAVAWH
A_A^A]A\_^]
q1RX&
UVWATAUAVAWH
D8{=H
r`HcS$H
A_A^A]A\_^]
X UVWATAUAVAWH
M+,$L
F(IcF E
uRIcF$H
|JD;/|EA
IcN$H
A_A^A]A\_^]
@USVWATAUAVAWH
HcG$H
HcW$L
tTHc_$
LcG$H
tTHc_$
LcG$H
t[Hc_$
LcG$H
HcG$L
HcO$L
A_A^A]A\_^[]
LcA$H
D$`Lc
D$pLc
E;J$|
UWATAVAWH
A_A^A\_]
HcO0HcW(L
p WATAUAVAWH
Hcy0M
LcQ,H
l$p9n 
D$tHcD$t
LcV(LcF$A
t$x;n 
A_A^A]A\_
\$ UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
F H+F
A_A^A]A\_^]
T$0H;
L$pH3
T$0H;
L$pH3
USVWATAUAVAWH
A_A^A]A\_^[]
USVWATAUAVAWH
A_A^A]A\_^[]
@SUVWAVH
0A^_^][
@USVATAUAVAWH
D$PH+
A_A^A]A\^[]
qPZph?
qQY:D7
HcEOH
HcE_H
HcEwH
L$ UVWATAUAVAWH
D$XH;
H;|$X
`A_A^A]A\_^]
UVWATAUAVAWH
H!D$ L
H!D$ L
\$ H;]gH
A_A^A]A\_^]
UVWATAUAVAWH
H!D$ L
H!D$ L
\$ H;]gH
A_A^A]A\_^]
UVWATAUAVAWH
T$hH+
H;D$pr4M
A_A^A]A\_^]
UATAUAVAWH
T$ L;
H;T$@H
D$HE3
H!D$0L
H!D$0L
A_A^A]A\]
UVWATAUAVAWH
H!D$ L
H!D$ L
l$(M+
A_A^A]A\_^]
UVWATAUAVAWH
l$XL;
|$0L;
l$@I;
`A_A^A]A\_^]
WATAUAVAWH
)D$ H9i
T$ L+
A_A^A]A\_
@USVWAVH
A^_^[]
@USVWAVH
A^_^[]
@UVWATAUAVAWH
A_A^A]A\_^]
@SUVWATAVAWH
@A_A^A\_^][
L;D$0|
t$ A^
WATAUAVAWH
L;L$P|
t$@A_A^A]A\_
UVWATAUAVAWH
A_A^A]A\_^]
WATAUAVAWH
\$@H!T$ M
H!T$(H
H!T$0E3
H!T$83
t$8L;
L$HH;
A_A^A]A\_
t$ WATAUAVAWH
 A_A^A]A\_
\$ UVWH
WATAUAVAWH
L$(E3
t$8E3
@H;T$P
A_A^A]A\_
@UVWATAUAVAWH
A_A^A]A\_^]
WATAUAVAWH
 H;T$@
t$ M;
A_A^A]A\_
@UVWATAUAVAWH
A_A^A]A\_^]
WATAUAVAWH
L+d$xJ
H+D$xH
L$ H;
t$8I+
A_A^A]A\_
WATAUAVAWH
L+t$HB
T$PI;
D$0M+
L+T$HH
L$8H;
\$hI+
T$PI;
t$ H+
L$(H;
t$@I+
A_A^A]A\_
@USVWAVH
epA^_^[]
q"V>\?
|$ UAVAWH
A_A^]
UVWATAUAVAWH
A_A^A]A\_^]
UVWAVAWH
A_A^_^]
x ATAVAWH
0A_A^A\
L+IHL
UVWATAUAVAWH
pA_A^A]A\_^]
T$0E3
@SVWH
\$ WATAUAVAWH
L+|$8
L$hH3
pA_A^A]A\_
L$HH3
q:X6p
t$ WH
y H!E
@USVWAVH
A^_^[]
L$8H3
l$ VWATAVAWH
L$8H3
A_A^A\_^
|$ UAVAWH
A_A^]
\$ VWAVH
L$8H3
@A^_^
\$ UVWH
L$8H3
WAVAWH
A_A^_
qcP>w
H A9H A
H A9H A
uVH9Y
T$@H;
|$ UH
C 9P 
C 9H 
D9A(t
C L9@
D9A(t
C L9@
D9I(t
B L9H
qSV~0
L$(H3
UATAUAVAWH
D840u
A_A^A]A\]
L$`H3
L$`H3
L$`H3
UVWATAUAVAWH
A_A^A]A\_^]
qj[\x
qcP>w
qcP>w
qcP>w
qcP>w
qcP>w
qcP>w
qcP>w
qcP>w
qcP>w
qcP>w
qcP>w
qcP>w
qcP>w
qcP>w
qj[\x
qj[\x
qj[\x
UVWATAUAVAWH
0A_A^A]A\_^]
UVWATAUAVAWH
0A_A^A]A\_^]
@SUVWATAVAWH
(D$ f
0A_A^A\_^][
UVWATAUAVAWH
T$ M+
PA_A^A]A\_^]
SVWAVAWH
(D$Pf
A_A^_^[
H;\$(t
 H;\$(u
L$0H3
@USWATAUAVAWH
A_A^A]A\_[]
UVWATAUAVAWH
 L;|$hs
 A_A^A]A\_^]
VWAVH
0A^_^
@SUVWAVAWH
L$8H3
HA_A^_^][
@SUVWAVH
L$HH3
PA^_^][
@USVWATAVAWH
A_A^A\_^[]
\$ UVWAVAWH
D$@H+
A_A^_^]
@UVWAVAWH
D$`H+
D$@H+
A_A^_^]
@USVATAVAWH
D$`H+
A_A^A\^[]
@USVWAVH
(D$0f
D$`H+
A^_^[]
@USVWAVH
A^_^[]
t$ UWATAVAWH
(D$Pf
(D$0f
A_A^A\_]
@USVWATAUAVAWH
A_A^A]A\_^[]
\$ UVWATAUAVAWH
A_A^A]A\_^]
@USVWATAUAWH
D$PH+
D$pH+
A_A]A\_^[]
@USVWATAUAVAWH
(D$0f
A_A^A]A\_^[]
UWAVH
@USVWAVAWH
A_A^_^[]
@USVWAVH
D$@H+
D$`H+
D$XH9]
A^_^[]
@USVATAVAWH
D$`H+
A_A^A\^[]
@USVWAVAWH
A_A^_^[]
@USVWATAUAVAWH
L$lA99
I9~xt
\$xH;E
I9~xtZH
A_A^A]A\_^[]
(D$@f
\$ UVWH
t$ WH
D$ HcH
t$ UWAVH
@USVWATAUAVAWH
A_A^A]A\_^[]
@USVWATAUAVAWH
A_A^A]A\_^[]
@SVWH
D$ HcH
l$ VWAVH
D$ H;
0A^_^
D$0H;
D$0I;
D$0M+
(D$0f
VWAWH
L$(H3
\$ WH
@USVWATAUAVAWH
E8,8u
E8,>u
A_A^A]A\_^[]
@USVWATAUAVAWH
xA_A^A]A\_^[]
@SUVWATAVAWH
@A_A^A\_^][
@SUVWAVH
0A^_^][
@SUVWAVH
{HcH(H
0A^_^][
Hc@(H;
E9A(~
E;A(|
@SUVWAVH
HcH@H
PA^_^][
P Hc@
P0Hc@(H
D$ H;
L$(H3
@USVWAVH
U'9P@
HcP@H
HcC@H
H;M't
A^_^[]
@USVWATAUAVAWH
A_A^A]A\_^[]
WAVAWH
0A_A^_
H;PHsaH
\$ UH
@USVWAVH
p"Rz-
A^_^[]
@SVWH
L$0H3
UAVAWH
A_A^]
WATAUAVAWH
0A_A^A]A\_
D$ A8z
L9A s
uFL;B r@I
L9A s
L;B s
L$(9J(
D$ A8{
L9A s
uHL;B rBI
LcT$(I
L9Q s
L;R s
@SUVWATAVAWH
 A_A^A\_^][
UWAWH
h VWAVH
@A^_^
t$ WH
VWAVH
UVWATAUAVAWH
0A_A^A]A\_^]
@USVWATAUAVAWH
hA_A^A]A\_^[]
\$ UVWATAUAVAWH
L$ L+
0A_A^A]A\_^]
@SUVWATAUAVAWH
\$ L;
8A_A^A]A\_^][
@SVWH
UWAVH
WAVAWH
D$ H;
0A_A^_
\$ UVWATAUAVAWH
L;t$H
A_A^A]A\_^]
@USVWATAUAVAWH
\$(H;\$0
H;\$0
\$xH;
CT$XL
A_A^A]A\_^[]
\$ UVWATAUAVAWH
CT$@L
|$HD8{
A_A^A]A\_^]
\$ UVWATAUAVAWH
@A_A^A]A\_^]
x UATAUAVAWH
T$0E3
A_A^A]A\]
UVWATAUAVAWH
A_A^A]A\_^]
\$ UVWATAUAVAWH
(D$ f
0A_A^A]A\_^]
x ATAVAWH
 A_A^A\
@USVWATAUAVAWH
FXI+FPH
FXI+FPH
bfloat16H
H Hc@
L9x s
L;y r
A_A^A]A\_^[]
\$ UVWAVAWH
L9H s
L;J r
A_A^_^]
@SUVWATAVAWH
(D$ f
0A_A^A\_^][
@SUVWATAVAWH
(D$ f
0A_A^A\_^][
@SUVWATAVAWH
(D$ f
0A_A^A\_^][
A H;B t
UWATAVAWH
A_A^A\_]
@USVWATAUAVAWH
L$XH+
A_A^A]A\_^[]
@USVWAVH
A^_^[]
\$ UVWH
(D$0f
HcB(H
HcJ(I;
HcJ(H
L$`H3
L$`H3
Ic@8H
IcH8H;
IcH8I
L$`H3
HcB8H
HcJ8I;
HcJ8H
L$`H3
HcB8H
HcJ8I;
HcJ8H
L$`H3
HcB8H
HcJ8I;
HcJ8H
L$`H3
HcB8H
HcJ8I;
HcJ8H
L$`H3
HcBhH
HcJhI;
HcJhH
L$`H3
L$`H3
L$`H3
t$ WH
[HcJXH;L$ht
HcB8H
6HcB8H;D$ht
HcB8H
HcB8H;
HcB8H
HcB8H;
D$@HcR
D$8H+
(D$ f
T$@H+
L$HH3
@USVWATAUAVAWH
A_A^A]A\_^[]
@USVWATAUAVAWH
C H+C
A_A^A]A\_^[]
N HcF
@USVWATAUAVAWH
A_A^A]A\_^[]
\$ UVWATAUAVAWH
L;t$(u
@A_A^A]A\_^]
USVWATAUAVAWH
A_A^A]A\_^[]
UVWATAUAVAWH
HcFhH;
HcF8H;
A_A^A]A\_^]
@USVWATAUAVAWH
L$0E3
L9j(H
N HcF
w IcG
H;u u
D$@IcV
t$0E3
A_A^A]A\_^[]
UVWATAUAVAWH
A_A^A]A\_^]
@USVWATAUAVAWH
A_A^A]A\_^[]
@USVWATAVAWH
A8x u
@0I9@(u
Lc{8J
Lc{8K
Lc{8K
Lc{8I
Lc{8I
Lc{(J
Lc{hJ
Lc{8K
Lc{8K
Lc{8I
A_A^A\_^[]
L$0H3
p WATAUAVAWH
L$XE3
=t4A+
 A_A^A]A\_
t$ WH
@SUVWAVH
@A^_^][
UVWATAUAVAWH
D$ H+
@A_A^A]A\_^]
UVWATAUAVAWH
D$ H+
@A_A^A]A\_^]
UVWATAUAVAWH
D$ H+
@A_A^A]A\_^]
@USVWATAVAWH
A_A^A\_^[]
UVWATAUAVAWH
pHZR4
A_A^A]A\_^]
A;A0t
A;A,u
A;A(u
)t$`I
(t$`H
@SUVWATAVAWH
fffffff
(D$ f
0A_A^A\_^][
@SUVWATAVAWH
fffffff
(D$ f
0A_A^A\_^][
|$ AVH
t$ WH
H;{ H
SUVWATAUAVAWH
L#q0M
(A_A^A]A\_^][
L#q0L
D9Y0t
x AVH
H,A;I
H(A;I
u^A;p0t
@,A9A
@(A9A
@,A9A
@(A9A
A9@,u
A9@(u
T$0@8j
uS;r0t
@USVWATAUAVAWH
A_A^A]A\_^[]
@SUVWAVH
ugH;M
L$`H3
pA^_^][
UVWAVAWH
A_A^_^]
t$ UWAVH
\$ UVWAVAWH
A_A^_^]
t$ WAVAWH
H,A9I
H(A9I
D;Q0t
A,A9A
A(A9A
L$ H3
0A_A^_
@SVWAVAWH
)t$`I
L$0@8q
(t$`H
pA_A^_^[
@SUVWATAVAWH
(D$ f
0A_A^A\_^][
@USVWATAUAVAWH
E M+E
(D$Pf
(D$Pf
(D$Pf
(D$Pf
A_A^A]A\_^[]
@USVWATAUAVAWH
l$hI;
A_A^A]A\_^[]
@USVWATAUAVAWH
H9|$p~AO
L;|$pL
H9|$p
L;|$pL
A_A^A]A\_^[]
@USVWATAUAVAWH
D$`E3
F L+F
(D$Pf
(D$Pf
(D$Pf
(D$Pf
A_A^A]A\_^[]
UVWATAUAVAWH
D$0E3
T$ L!l$(I
L$(H;W
D$(H;U
t$(H;S
D$(H;W
L$0H3
@A_A^A]A\_^]
UVWATAUAVAWH
D$8E3
T$ L!l$0I
L$0H;W
D$0H;U
D$(H;S
D$0H;W
L$8H3
@A_A^A]A\_^]
UVWATAUAVAWH
L$0H;W
D$0H;U
D$(H;S
D$0H;W
L$8H3
@A_A^A]A\_^]
UVWATAUAVAWH
D$0E3
T$ L!l$(I
L$(H;W
D$(H;U
D$(H;S
D$ H;W
L$0H3
@A_A^A]A\_^]
UVWATAUAVAWH
D$0E3
T$ L!l$(I
L$(H;W
D$(H;U
D$(H;S
D$(H;W
L$0H3
@A_A^A]A\_^]
WATAUAVAWH
t$(H;S
t"L;O
t$ L;O
L$0H3
A_A^A]A\_
WATAUAVAWH
D$ H;S
t"L;O
l$0L;O
L$8H3
A_A^A]A\_
WATAUAVAWH
D$ H;S
t'L;O
l$0L;O
L$8H3
A_A^A]A\_
WATAUAVAWH
D$(H;S
t"L;O
l$(L;O
L$0H3
A_A^A]A\_
WATAUAVAWH
D$(H;S
t"L;O
l$ L;O
L$0H3
A_A^A]A\_
UVWATAUAVAWH
 A_A^A]A\_^]
\$ WH
L$(H3
UATAUAVAWH
A_A^A]A\]
UATAUAVAWH
A_A^A]A\]
@USVWATAUAWH
fD9Bv
D$u8Bu
D$t8Bt
Evf9Bv
Eu8Bu
Et8Bt
A_A]A\_^[]
@USVWATAUAWH
fD9Bv
D$u8Bu
D$t8Bt
Evf9Bv
Eu8Bu
Et8Bt
A_A]A\_^[]
@SUVWATAVAWH
 A_A^A\_^][
|$ UATAVH
D9f(ufH
A^A\]
tionProvH
CPUExecuH9
VitisAIEH9
xecutionH9H
ProviderH9H
ACLExecuH9
tionProvH9H
@USVWATAUAVAWH
(D$Pf
A_A^A]A\_^[]
@USVWATAUAVAWH
D$PH;
d$XI;}
A_A^A]A\_^[]
@USWH
HcT$0I
@USWH
@USVWAUAVAWH
A_A^A]_^[]
VWAVH
 A^_^
\$ UVWATAUAVAWH
q0L+q(I
H91t:H
`A_A^A]A\_^]
@USVWATAUAVAWH
A_A^A]A\_^[]
@USVWAVAWH
hA_A^_^[]
@USVWATAUAVAWH
A_A^A]A\_^[]
@USVWATAUAVAWH
I+4$H
fffffff
A_A^A]A\_^[]
L9@ s
L;A r
l$ VWAVH
0A^_^
@SVWATAUAVAWH
\$ I;
pA_A^A]A\_^[
UVWATAUAVAWH
 A_A^A]A\_^]
UVWATAUAVAWH
 A_A^A]A\_^]
x ATAVAWH
gfffffffI
fffffff
 A_A^A\
(D$ f
D$ H;
D$PH;Fxt
L$XH3
I;RPt
DRPI;RP
L$XH3
Q +Q0
H;{Hv
x UATAUAVAWH
D$hL;
D$ E3
T$(D8P
D$ D9H,t
F,A9G
F(A9G
"""""""
t$`E3
T$ H;
L;d$h
A_A^A]A\]
\$ UVWH
T$qfD9H6u
D8P5u
D8@4tfH
L$@H3
UATAUAVAWH
D$ L;
A_A^A]A\]
@USVWATAUAVAWH
A_A^A]A\_^[]
UVWATAUAVAWH
H;Gxu
A_A^A]A\_^]
USVWH
\$ UVWATAUAVAWH
A_A^A]A\_^]
\$ UVWAVAWH
F I+F
A_A^_^]
UVWATAUAVAWH
D$xH;
D$PE3
MPI;MP
D$PI;
H;\$xD
A_A^A]A\_^]
UVWATAUAVAWH
L$PE3
H9q }
I;p |
D$0H9p }
H;r }aH
UUUUUUU
L$`H3
pA_A^A]A\_^]
UAVAWH
A_A^]
@SUVWAVAWH
hA_A^_^][
u4H;Q
@USVWAVAWH
A_A^_^[]
UVWATAUAVAWH
L;@(u
t&I;Q(t
I;Q(unL;
A_A^A]A\_^]
T$PH;
L;H(u
"""""""
UVWATAUAVAWH
O(A+O0A
W(A+W0A
A8WPt
A_A^A]A\_^]
@USVWATAUAVAWH
H+F8H;
H+F8H;
~(+~0
V(+V0
D8fPt
V(+V0
V(+V0
D8fPt
V(+V0
f(D+f0D
V(+V0
V(+V0
V(+V0
A_A^A]A\_^[]
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
D$0E3
H#N0H
A_A^A]A\_^]
@USVWH
UVWATAUAVAWH
`A_A^A]A\_^]
@USVWATAUAVAWH
L$pH+
A_A^A]A\_^[]
x UAVAWH
:Loopt
8ScanuF
A_A^]
UVWATAUAVAWH
T$ I+T$
T$PH;
A_A^A]A\_^]
@USVWATAUAVAWH
D8d$ptxI
A_A^A]A\_^[]
M#C0I
t$ WH
H;W H
|$ AVH
UVWATAUAVAWH
I#u0H
H9D$ t-H;
0A_A^A]A\_^]
I#u0L
UVWATAUAVAWH
 A_A^A]A\_^]
t$ UWATAVAWH
A_A^A\_]
t$ WH
\$ UVWATAUAVAWH
(D$ f
0A_A^A]A\_^]
UVWAVAWH
A_A^_^]
@SUVWAVH
@A^_^][
UVWATAUAVAWH
 A_A^A]A\_^]
l$ VWATAVAWH
 A_A^A\_^
@SUVWATAVAWH
@A_A^A\_^][
@SUVWATAVAWH
(D$ f
@A_A^A\_^][
@SUVWATAVAWH
@A_A^A\_^][
@SUVWATAVAWH
fffffff
(D$ f
0A_A^A\_^][
@SUVWATAVAWH
(D$ f
0A_A^A\_^][
@SUVWATAVAWH
(D$ f
0A_A^A\_^][
A;@,t
A;@(u
l$ VWATAVAWH
 A_A^A\_^
l$ VWATAVAWH
 A_A^A\_^
UWATAUAWH
A_A]A\_]
|$ AVH
|$ AVH
|$ AVH
UVWATAUAVAWH
0A_A^A]A\_^]
UVWATAUAVAWH
0A_A^A]A\_^]
H;{ H
H;{ H
t$ WH
H;{ H
tjH9_
H;W H
t$ WH
H;{ H
SUVWATAUAVAWH
L9|$x
L9|$xu
(A_A^A]A\_^][
UVWATAUAVAWH
H#r0H
m@I#u0H
5L9}Pu
@A_A^A]A\_^]
SUVWATAUAVAWH
L9|$x
L9|$xu
(A_A^A]A\_^][
UVWATAUAVAWH
M#u0M
H;\$xt/H;
H9l$pu
 A_A^A]A\_^]
H9l$pu
M#u0H
H;\$pt
UVWATAUAVAWH
M#u0M
H;\$xt/H;
H9l$pu
 A_A^A]A\_^]
H9l$pu
M#u0H
H;\$pt
@SUVWATAVAWH
@A_A^A\_^][
@USVWATAVAWH
A_A^A\_^[]
VWAVH
 A^_^
UVWAVAWH
 A_A^_^]
@USVWAVH
A^_^[]
@USVWAVH
A^_^[]
UVWATAUAVAWH
0A_A^A]A\_^]
t$ WH
H;{ H
SUVWATAUAVAWH
L9|$x
L9|$xu
(A_A^A]A\_^][
\$ UVWATAUAVAWH
PA_A^A]A\_^]
@SUVWATAVAWH
(D$ f
0A_A^A\_^][
@SUVWATAUAVAWH
D$ L;
L$(H3
8A_A^A]A\_^][
\$ UVWATAUAVAWH
@A_A^A]A\_^]
@USVWAVH
A^_^[]
t$ WH
@SUVWAVH
t0H;E
@A^_^][
t$ UWATAVAWH
A_A^A\_]
L$8H3
UVWAVAWH
`A_A^_^]
@USVWATAUAVAWH
A_A^A]A\_^[]
@SVWH
t$ WH
UVWATAUAVAWH
 A_A^A]A\_^]
\$ UVWATAUAVAWH
@A_A^A]A\_^]
UVWATAUAVAWH
0A_A^A]A\_^]
t$ WH
H;{ H
SUVWATAUAVAWH
L9|$x
L9|$xu
(A_A^A]A\_^][
@USVWATAUAVAWH
hA_A^A]A\_^[]
\$ UVWH
@SUVWAVH
D$ L;
A^_^][
N H+N
UVWAVAWH
 A_A^_^]
t$ UWAVH
\$ UVWH
@USVWAUAVAWH
D$0E3
A_A^A]_^[]
@USVWATAUAVAWH
L$`H;
D$XH;D$`
A_A^A]A\_^[]
\$ UVWATAUAVAWH
A_A^A]A\_^]
@USVWATAUAVAWH
A_A^A]A\_^[]
USVWATAUAVAWH
T$`I;
A_A^A]A\_^[]
@SUVWATAUAVAWH
L$XH3
hA_A^A]A\_^][
@USVWATAUAVAWH
A_A^A]A\_^[]
t$ WH
|$0L;
L$8H3
x ATAVAWH
 A_A^A\
\$ UVWAVAWH
 A_A^_^]
l$ VWATAVAWH
 A_A^A\_^
\$ UVWATAUAVAWH
A_A^A]A\_^]
t$ WAVAWH
A_A^_
@USVWATAUAVAWH
A_A^A]A\_^[]
t$ WAVAWH
 A_A^_
@USVWATAVAWH
D$8L;
A_A^A\_^[]
UWATAVAWH
A_A^A\_]
t$ UWAVH
t$ UWAUAVAWH
A_A^A]_]
UVWATAUAVAWH
L9 spH
A_A^A]A\_^]
L$0H+
l$ VWATAVAWH
L$0H3
A_A^A\_^
UVWATAUAVAWH
I+<$H
D$@L;
D$@M;
H+D$8H
T$0I;
D$@I;
D$XI+
D$HI+
A_A^A]A\_^]
x AVH
W(H9h u@H
|$ UH
HcH0H
UVWATAUAVAWH
L$ H3
0A_A^A]A\_^]
\$ UVWATAUAVAWH
)t$`M
l$0@8}
fffffff
(t$`H
pA_A^A]A\_^]
WAVAWH
 A_A^_
UVWATAVH
mL+1I
 A^A\_^]
WATAUAVAWH
gfffffffI
fffffff
 A_A^A]A\_
WATAVH
A^A\_
SVWATAUAVAWH
T$`H;
L$ E3
A_A^A]A\_^[
VWAVH
H;A(s
@USVWATAUAVAWH
H;G8u
A_A^A]A\_^[]
\$ UVWH
H;q(swA
H;H@s
\$ UVWATAUAVAWH
G8I+G0H
p"Rz-
_8I+_0H
I;G8u
M94$t
p"Rz-
M94$t
p"Rz-
A_A^A]A\_^]
@USVWAUAVAWH
y8H+y0H
`A_A^A]_^[]
@SVWATAUAVAWH
d$HE3
I#M0I
d$HE8
H9x }
D$XH9x }
vb'vb'v
p"Rz-
A_A^A]A\_^[
@USVWATAUAVAWH
p"Rz-
p"Rz-
A_A^A]A\_^[]
@USVWATAUAVAWH
D$PH9
D$PH9
p"Rz-
A_A^A]A\_^[]
VWAVH
L$(H3
0A^_^
@USVWAVAWH
A_A^_^[]
@SUVWH
L$8H3
H_^][
@USVWATAUAVAWH
A_A^A]A\_^[]
I;G`u
DG`I;G`tkM
p"Rz-
pj[\x
q"Rz-
@SVWH
L$8H3
\$ UVWH
|$ UH
H;A8u
L$(H3
VWAVH
D$0E3
D$0D9P0t
3333333
A;A0t
A;A,u
A;A(u
t$ WH
t$ WH
@SUVWATAVAWH
(D$ f
@A_A^A\_^][
@SUVWAVH
 A^_^][
\$ UVWH
\$ UVWATAUAVAWH
A_A^A]A\_^]
@USVWATAVAWH
A_A^A\_^[]
t$ UWATAVAWH
A_A^A\_]
@USVWAVH
A^_^[]
UWATAUAVH
D$8L+D$0I
A^A]A\_]
@USVWAVH
A^_^[]
UWATAUAVH
D$8L+D$0I
A^A]A\_]
@USVWATAVAWH
A_A^A\_^[]
x AVH
@USVWATAUAVAWH
A_A^A]A\_^[]
@USVWATAUAVAWH
D$@H9E
D$@H9E
l$XE3
l$@L9m
T$@H;
T$@H;
T$@H;
A_A^A]A\_^[]
@SUVWATAVAWH
@A_A^A\_^][
UVWATAUAVAWH
0A_A^A]A\_^]
t$ WH
H;{ H
SUVWATAUAVAWH
L9|$x
L9|$xu
(A_A^A]A\_^][
x ATAVAW
\$ D3
A_A^A\
UVWAVAWH
A_A^_^]
t$ WH
D$(H;
L$HH3
@USVWATAUAVAWH
D8l$@
A_A^A]A\_^[]
@SVWATAUAVAWH
A`@80
@8t$@t
L9@ s
L;A r
L9@ s
L;A r
|$h@8t$@
L$@H9
A_A^A]A\_^[
providerH
@8t$@
x UAVAWH
H9CHt
A_A^]
WATAUAVAWH
~PM;~Xt6
L$`H3
A_A^A]A\_
UVWATAUAVAWH
 A_A^A]A\_^]
@SVWATAUAVAWH
L$'I;
|$ H;
`A_A^A]A\_^[
@USVWATAUAVAWH
A_A^A]A\_^[]
SVWATAUAVAWH
L;P(u
t"L;J(t
L;J(t
L$pH;
@8t$At
@8t$@t
L9@ s
L;A r
L9@ s
L;A r
\$h@8t$@u;H
@8t$@
@8t$Bt
\$`H9
D$pHcX
D$p;X
@8t$@t]I
A_A^A]A\_^[
v0I;Q
r-H;Q
@SUVWAVH
A^_^][
x AVH
UVWATAUAVAWH
 A_A^A]A\_^]
UVWAVAWH
9Z(ujH
A H;B tN
L$0H3
@A_A^_^]
@SUVWATAVAWH
GXH+GPHcK
N 9H uaA
 A_A^A\_^][
@USVWAVH
A^_^[]
UVWATAUAVAWH
l$@I;
H;Gxu
H9x s
H;y r
A_A^A]A\_^]
\$ UVWATAUAVAWH
H9A s
H;B r
L$0H;L$8u
A_A^A]A\_^]
USVWATAUAVAWH
L$`E3
H;Fxu
A_A^A]A\_^[]
UVWAUAVH
A^A]_^]
UVWATAUAVAWH
D$PE3
H98tgH
|$@I;
A_A^A]A\_^]
l$ VWATAVAWH
t;8J8t6H
t;8J8t6H
t;8J8t6H
A_A^A\_^
VWATAVAWH
H9Gpv!H
?H9Gp
tYHcs
HPH9i
A_A^A\_^
t$ UWAVH
@A^_]
x UATAUAVAWH
D$ H;
A_A^A]A\]
UVWAVAWH
 A_A^_^]
l$ VWATAVAWH
 A_A^A\_^
l$ VWAVH
 A^_^
l$ VWATAVAWH
0A_A^A\_^
@USVWAVH
PA^_^[]
@USVWATAUAVAWH
D$HH+
8L! H
A_A^A]A\_^[]
@USVWATAUAVAWH
A_A^A]A\_^[]
t$ UWAUAVAWH
HcT$ I
f9Hvu
A_A^A]_]
\$ UVWATAUAVAWH
D$0H;
HcT$8I
H;\$0
H;L$0
HcT$8I
A_A^A]A\_^]
UVWAVAWH
PA_A^_^]
@SUVWAVH
@A^_^][
l$ VWAVH
D$09P
@USVWAVH
pA^_^[]
@SUVWATAVAWH
(D$ f
0A_A^A\_^][
\$ UVWAVAWH
pA_A^_^]
\$ UVWAVAWH
A_A^_^]
)D$@f
D$ E3
@SUVWATAVAWH
@A_A^A\_^][
\$ UVWATAUAVAWH
@A_A^A]A\_^]
UVWATAUAVAWH
0A_A^A]A\_^]
t$ WH
H;{ H
SUVWATAUAVAWH
L9|$x
L9|$xu
(A_A^A]A\_^][
L$(H3
UVWATAUAVAWH
A_A^A]A\_^]
V(L;2
D$0H;
t$ UWATAVAWH
D9P0t
K,9H,u
K(9H(u
B09C0t
B,9C,u
B(9C(u
A_A^A\_]
@SUVWH
L$ H;
L$(H3
8_^][
\$ UVWH
D$ L;
L$(H3
UVWATAUAVAWH
G I;G(t
A_A^A]A\_^]
UVWATAUAVAWH
gfffffffH+
fffffff
 A_A^A]A\_^]
\$ UVWATAUAVAWH
d$ I;
0A_A^A]A\_^]
UVWAVAWH
 A_A^_^]
@SUVWATAVAWH
fffffff
(D$ f
0A_A^A\_^][
@SUVWATAVAWH
~(D+~0D
NXH;i
L$@H3
PA_A^A\_^][
D$@H;
k VWAVH
(;Qh|
|$ UH
UVWATAUAVAWH
PA_A^A]A\_^]
VWATAVAWH
 A_A^A\_^
t$ WH
UWATAVAWH
A_A^A\_]
@SVWATAVAWH
E9`(u
D9`(u 
E9`(uOD9't7H
C D9f(
D9`(t8H
E9f(u
A_A^A\_^[
t$ WH
VWATAVAWH
H D9c(t8H
~ HcF
D9f(t
0A_A^A\_^
L9I0H
EQ0D9J(taH
B L9H
A9H(u$I
@USVWH
UVWATAUAVAWH
A9}(u
H 9{(u
A9}(u
9{(t8H
9{(u1H
A_A^A]A\_^]
M9P0I
EP0D9R(t2H
L$(H3
UVWATAUAVAWH
~XHcnP3
HcNP;M
@A_A^A]A\_^]
@USVWATAUAVAWH
H+F8H;
M`H!EpI
UhH;Upt
V(+V0
~(D+~0D
D8nPt
V(+V0
V(+V0
D8nPt
V(+V0
V(+V0
D8nPt
A_A^A]A\_^[]
@SUVWAVH
T$8H+
L$@H3
PA^_^][
\$ UVWATAUAVAWH
g(D+g0D
tFL9oHs
W(+W0
tOL9oHs
W(+W0
W(+W0
`A_A^A]A\_^]
UUUUUUU
@USVWATAUAVAWH
L$@Ic
A_A^A]A\_^[]
@USVWATAUAVAWH
L$`H;
D$XE3
A_A^A]A\_^[]
@USVWATAVAWH
d$KHc
d$@L9e
d$LHc
d$@L9e
A_A^A\_^[]
UVWATAUAVAWH
t$8@8q
D9q(u
D;r(u
UUUUUUU
L$PH;
A_A^A]A\_^]
@SUVWATAUAVAWH
8A_A^A]A\_^][
t$ UWAVH
]/H;_
t$ WH
t$ WH
@SVWH
UVWAVAWH
A_A^_^]
t$ UWATAVAWH
A_A^A\_]
t$ UWATAVAWH
A_A^A\_]
t$ WH
VWAVH
D8r8t.
D8r8t.
D8r8t*
L$0H3
@A^_^
UVWATAUAVAWH
D9h u
D9hht
IcF H
IcN8;
IcNPA;M
t$hM;
IcF H
D8l$x
A_A^A]A\_^]
T$pH+
x UATAUAVAWH
HcB8H
HcBPH
HcAhH
@0I;W
A_A^A]A\]
UVWATAUAVAWH
A_A^A]A\_^]
@USVWATAVAWH
A_A^A\_^[]
` UAVAWH
A_A^]
` UAVAWH
A_A^]
\$ UVWATAUAVAWH
t$0I;
D$HL+
H;D$8
t$0E3
t$XI;
A_A^A]A\_^]
T$0H;
@USVWATAUAVAWH
T$@H;
A_A^A]A\_^[]
UVWATAUAVAWH
L$`L;
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
A H9X
A 9X t
A 9X 
@USWH
H!E7H
D$ H;
L$(H3
t 8X8t
H9X0H
l$ VWAVH
VpH;Vxt
@USVWATAUAVAWH
L9x0H
A_A^A]A\_^[]
@SVWATAUAVH
A^A]A\_^[
t$ UWATAUAWH
D!}'H
A_A]A\_]
@SVWATAUAVAWH
\$pH;
\$h@8s
A8vduaA
A_A^A]A\_^[
t$ WATAUAVAWH
tLD8h8tFH
L9h0H
D$xH;
T$xH;
A_A^A]A\_
t$ UWATAVAWH
A_A^A\_]
UVWATAUAVAWH
pA_A^A]A\_^]
\$ UVWH
D$(H;
L$HH3
@USVWAVAWH
hA_A^_^[]
@USVWAVH
A^_^[]
t$ UWATAVAWH
IcN8;
A_A^A\_]
UVWATAUAVAWH
Mcn8K
A_A^A]A\_^]
\$ WH
D$ H9
L$(H3
\$ UVWH
@SUVWATAUAVAWH
T$(E3
L$HH3
XA_A^A]A\_^][
X UVWAVAWH
Ic^P3
A_A^_^]
@USVWATAVAWH
A_A^A\_^[]
@SUVWAVH
T$8H+
L$@H3
PA^_^][
@USVWATAUAVAWH
L$0H9
}(A+}0A
U A+U0A
A+M0A
H9t$puJH
U A+U0A
A+M0A
H9t$puiH
U A+U0A
M A+M0+
U A+U0A
M A+M0+
M(A+M0A
U(A+U0A
A8UPt
tUI9]Hs
U(A+U0A
U(A+U0A
A8UPt
tTI9]Hs
U(A+U0A
U(A+U0A
E8uPt
E8uPt
tTI9]Hs
U(A+U0A
U(A+U0A
E8uPt
tUI9]Hs
U(A+U0A
U(A+U0A
E8uPt
A_A^A]A\_^[]
UVWATAUAVAWH
A_A^A]A\_^]
@USVWATAUAVAWH
A_A^A]A\_^[]
t$ UWAVH
UVWATAUAVAWH
Hc@8H
L$0H;
A_A^A]A\_^]
UVWATAUAVAWH
D9k ~%L
{(Lcc E
D9kP~%L
{XLccPM
D9kh~%L
{pLcchM
HcOPA;
HcOhA;
HcW ;
A_A^A]A\_^]
UVWATAUAVAWH
McUPL
A_A^A]A\_^]
t$ WAVAWH
 A_A^_
UVWATAUAVAWH
UPH+UHH
H;D$P
T$@I+
A_A^A]A\_^]
D$HH;
L$PH3
\$ UVWATAUAVAWH
D$@I;
t$HE3
D$@I;
A_A^A]A\_^]
@SUVWAVH
L$8H3
@A^_^][
\$ UVWATAUAVAWH
D$HH;
c(Lcs,H
0Lcc(D
|$@H;|$H
A_A^A]A\_^]
\$ UVWAVAWH
0A_A^_^]
\$ UVWATAUAVAWH
L$ fA
HcK8;
D$HH;
L;E8t
D$XI;
t$XI;
H;\$H
L;E8t I
D$HH;u
t$HH;u
A_A^A]A\_^]
t$ UWATAVAWH
A_A^A\_]
l$ VWAVH
 A^_^
@USVWATAUAVAWH
A_A^A]A\_^[]
@SUVWATAVAWH
`A_A^A\_^][
UVWATAUAVAWH
\$hE3
 A_A^A]A\_^]
X UVWATAUAVAWH
IcUP;
I;T$8v
IcMP;
H;P8v
L9C0H
D$0M;
D$PIc
T$HH;
t$`E3
A_A^A]A\_^]
@USVWATAVAWH
A_A^A\_^[]
t$ WH
)t$ H
H;S8v
(t$ H
H;{ H
t$ WH
H;{ H
t$ WAVAWH
A9A(u
L$ H3
0A_A^_
S H+S
l$ VWAVH
UUUUUUU
L$@H3
?H+)H
t$ WH
UUUUUUU
UVWATAUAVAWH
H;\$xt/H;
H9l$pu
 A_A^A]A\_^]
H9l$pH
H;\$pt
UVWATAUAVAWH
t$0I#
H9uPu9K
@A_A^A]A\_^]
UVWATAUAVAWH
 A_A^A]A\_^]
\$ UVWATAUAVAWH
)t$`M
l$0@8u
(t$`H
pA_A^A]A\_^]
WAVAWH
O(+O0
 A_A^_
D8APtfH
Q(+Q0
H+C8H
C(D+C0D
x AVH
H+C8H;
WAVAWH
O(+O0
 A_A^_
UWAVH
t$ WH
t$ WH
t$ WH
UVWAVAWH
A_A^_^]
UVWAVAWH
A_A^_^]
\$ UVWATAUAVAWH
@A_A^A]A\_^]
t$ UWAVH
h VWATAVAWH
A_A^A\_^
\$ UVWATAUAVAWH
@A_A^A]A\_^]
UVWAVAWH
A_A^_^]
l$ VWAVH
B(A9@
B,A9@
UUUUUUU
UVWATAUAVAWH
 A_A^A]A\_^]
UVWATAUAVAWH
 A_A^A]A\_^]
t$ UWAVH
@SVWATAUAVAWH
UUUUUUU
\$ M;
PA_A^A]A\_^[
UVWATAUAVAWH
UUUUUUU
0A_A^A]A\_^]
\$ UVWAVAWH
 A_A^_^]
l$ VWATAVAWH
 A_A^A\_^
\$ UVWATAUAVAWH
@A_A^A]A\_^]
\$ UVWATAUAVAWH
@A_A^A]A\_^]
UVWATAUAVAWH
 A_A^A]A\_^]
UVWATAUAVAWH
 A_A^A]A\_^]
@SUVWATAVAWH
 A_A^A\_^][
@SUVWATAVAWH
(D$ f
0A_A^A\_^][
@SUVWATAVAWH
G@!GDH
@A_A^A\_^][
UVWAVAWH
UUUUUUU
 A_A^_^]
@SUVWATAVAWH
@A_A^A\_^][
@SUVWATAVAWH
@A_A^A\_^][
@SUVWATAVAWH
(D$ f
0A_A^A\_^][
@SUVWATAVAWH
@A_A^A\_^][
@SUVWATAVAWH
 A_A^A\_^][
\$ UVWAVAWH
 A_A^_^]
@SUVWATAVAWH
@A_A^A\_^][
@SUVWATAVAWH
@A_A^A\_^][
WAVAWH
 A_A^_
x ATAVAWH
 A_A^A\
WAVAWH
 A_A^_
UVWATAUAVAWH
0A_A^A]A\_^]
t$ WH
H;{ H
SUVWATAUAVAWH
L9|$x
L9|$xu
(A_A^A]A\_^][
l$ VWATAVAWH
 A_A^A\_^
l$ VWATAVAWH
 A_A^A\_^
t$ WATAUAVAWH
D$0I;
D$0H+
A_A^A]A\_
\$ UVWATAUAVAWH
`A_A^A]A\_^]
@USVWATAUAVAWH
^ IcF
CT$@L
A_A^A]A\_^[]
UVWAVAWH
0A_A^_^]
l$ VWAVH
 A^_^
t$ WATAUAVAWH
A_A^A]A\_
l$ VWATAVAWH
UUUUUUU
 A_A^A\_^
t$ WAVAWH
 A_A^_
UVWATAUAVAWH
 A_A^A]A\_^]
UVWATAUAVAWH
UUUUUUU
@A_A^A]A\_^]
\$ UVWATAUAVAWH
PA_A^A]A\_^]
@SUVWH
H+D$(H
8_^][
@SUVWATAUAVAWH
L$0H;
XA_A^A]A\_^][
\$ VWI
x AVH
D9L$P}
)Hct$P
@USVWAVH
0A^_^[]
VWATAVAWH
0A_A^A\_^
@USVWATAVAWH
I9NXt
A_A^A\_^[]
@SVWATAUAVAWH
D9x }
CT$0L
A_A^A]A\_^[
UVWATAUAVAWH
OXI;OX
9p ~GD8x
@A_A^A]A\_^]
L$(H3
UVWATAUAVAWH
G 9B H
A_A^A]A\_^]
@USVWATAUAVH
H0;N 
A^A]A\_^[]
@USVWATAUAVH
H0;N$
A^A]A\_^[]
@USVWATAUAVAWH
uhD9}
A_A^A]A\_^[]
\$ UVWAVAWH
D$09P }
PA_A^_^]
@SUVWATAVAWH
@A_A^A\_^][
@SUVWATAVAWH
@A_A^A\_^][
UVWATAUAVAWH
D$8H+
PA_A^A]A\_^]
?H;58)*
\$ UVWATAUAVAWH
T$0H;
T$0H;
T$0H;
I+VHH
I+VHH
A_A^A]A\_^]
L$(H3
Gp9Fpt
L$(H3
UWATAVAWH
t$@L!
uLL!t$HH
A_A^A\_]
\$ UVWATAUAVAWH
D$HL9p
HcO ;
HcOh;
E H;E(t
A_A^A]A\_^]
\$ UVWATAUAVAWH
IcF8H
IcFhH
D$hI;
A_A^A]A\_^]
@SVATAUAVAWH
A_A^A]A\^[
\$ UVWAVAWH
A_A^_^]
@USVWATAVAWH
L9d$@t>
L9d$@t4
A_A^A\_^[]
t$ UWAVH
L$0E3
L9t$0t>
D$HfD
t$xfD
@USVWATAUAVAWH
IcE H
w(A+w0A
W(A+W0A
A+O0A
w A+w0A
W(A+W0A
A+O0A
w(A+w0A
W(A+W0A
W(A+W0A
A8WPt
W(A+W0A
W(A+W0A
W(A+W0A
W(A+W0A
A_A^A]A\_^[]
UVWATAUAVAWH
fD;:s
fD;:s
HcN8;
A_A^A]A\_^]
UVWAVAWH
3333333
|$0H!\$8
@A_A^_^]
D8APtgH
Q(+Q0
H+C8H
C(D+C0D
t$ UWAVH
|$ AVH
UVWATAUAVAWH
 A_A^A]A\_^]
t$ WH
UVWATAUAVAWH
 A_A^A]A\_^]
@SVAVH
L$`H3
pA^^[
WAVAWH
0A_A^_
@SUVWATAVAWH
3333333
@A_A^A\_^][
s AVH
VWATAVAWH
A_A^A\_^
uz9z(t7H
@SVWH
UVWATAUAVAWH
\$ A;
A_A^A]A\_^]
\$ UVWATAUAVAWH
A_A^A]A\_^]
UAVAWH
A_A^]
UWATAVAWH
D9`(u
D9`(u
X L9{
|$XA;
D9`(t
HcL$p;
HcL$p;
D9c(t
D9}`t
D9e`t'
A_A^A\_]
|$ UAVAWH
A_A^]
h VWAVH
UVWATAUAVAWH
 A_A^A]A\_^]
x AVH
x AVH
D9L$P}
)Hct$P
t$ UWATAUAVH
A^A]A\_]
UVWATAUAVAWH
x0Hc@(L
A_A^A]A\_^]
@USVWAVH
9r(u1H
z 9s(u
A^_^[]
HcSh;
t$ UWAVH
HcKh;
HcWP;
x UATAUAVAWH
D9h(u
LcepI
A_A^A]A\]
D9h(u
D9m(~\I
D$(H;
D;m(|
EHH+E@H
@8|$ tnL+
UWATAVAWH
D9{(t
A_A^A\_]
x UATAUAVAWH
D9x(t
D9x(t
D9}xu
A_A^A]A\]
UWATAVAWH
|$XA;
D9`(t
HcL$p;
HcL$p;
D9c(t
D9}`t
D9e`t'
A_A^A\_]
UVWATAUAVAWH
D9y(u
VUUUUUUUH
Hc\$ H
J 9y(
A_A^A]A\_^]
UWATAVAWH
A D9`(
A_A^A\_]
@USVWATAUAVAWH
A_A^A]A\_^[]
FastGeluH
UVWATAUAVAWH
@ HcX H
A_A^A]A\_^]
UATAUAVAWH
A_A^A]A\]
95@$)
x UAVAWH
HcM';
A_A^]
x UATAUAVAWH
D9h(t9H
D9h(u
I D9i(
D9h(t
D9h(t
A_A^A]A\]
x UATAUAVAWH
D9f(t
A_A^A]A\]
UWATAVAWH
A_A^A\_]
WATAUAVAWH
D9h(u
D9h(u
D9i(u
D9h(t8H
A_A^A]A\_
x UATAUAVAWH
D9`(u
D9`(u
HcOhH
A_A^A]A\]
9p(t7H
9p(t7H
UATAUAVAWH
D9`(t8H
D9`(t8H
D9`(u
u0Mcn
D9`(t8H
D9`(t8H
D9c(t
A_A^A]A\]
UVWATAUAVAWH
@ Hc@ H
A_A^A]A\_^]
UWAVH
WATAVH
D9`(u
D9`(t8H
D9c(t
@A^A\_
UVWATAVH
@ HcX H
A^A\_^]
UWATAVAWH
A_A^A\_]
|$ UH
UWAVH
UWAVH
\$ WH
Lc@(H
D$0H+
L$@H3
UWAVH
UWAVH
UWAVH
UWAVH
positiveI
L$8H3
\$ UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
(D$@f
L$0D8P8
D9x,I
L9P0H
A_A^A]A\_^]
x AVH
@SUVWATAVAWH
L$HH3
PA_A^A\_^][
l$ VWAVH
t$ WH
h UAVAWH
A_A^]
VWAVH
ai.of
u4f9O
 A^_^
D$ H;C
L$(H3
\$ VWAVH
D$ I+
L$8H3
@A^_^
@SUVWAVH
L$@H3
PA^_^][
@SUVWATAVAWH
D$8H+
L$@H3
PA_A^A\_^][
\$ UVWH
\$ H;
L$8H3
@SUVWATAVAWH
`A_A^A\_^][
\$ H;
\$ I;
L$(H3
UWATAUAWH
u:I;t$
r3w1HcC(H
A_A]A\_]
\$ H;
\$ I;
L$(H3
t$ WATAUAVAWH
L$ E3
HcK,H
0A_A^A]A\_
UVWATAUAVAWH
^ IcN
A_A^A]A\_^]
UVWAVAWH
D$@Mc
D$8H9D$0
\$0H;
L$HH3
PA_A^_^]
T$XH;
@SUVWATAVAWH
L$HH3
PA_A^A\_^][
D;H,tKH
UVWATAUAVAWH
L$ L;
`L;t$0
A_A^A]A\_^]
x UAVAWH
A_A^]
\$(H+\$ H
L$8H3
h VWAVH
h VWAVH
UVWAVAWH
HcC(I;
pA_A^_^]
UVWAVAWH
A_A^_^]
UVWATAUAVAWH
 A_A^A]A\_^]
` AUAVAWH
 A_A^A]
VWATAVAWH
L$@H3
A_A^A\_^
x ATAVAWH
 A_A^A\
UVWATAUAVAWH
T$ M+
PA_A^A]A\_^]
\$(H;
L$8H3
@SUVWAVH
T$8H+
L$@H3
PA^_^][
@USVWATAUAVAWH
LcOXI
I+F8H;
~(A+~0A
V(A+V0A
V(A+V0A
E8fPt
xA_A^A]A\_^[]
USVWATAUAVAWH
HcGXH
~(+~0
V(+V0
V(+V0
D8nPt
LcO@I
^(+^0
HcW(H
HcO(H
^(+^0
H+F8H;
~(+~0
V(+V0
V(+V0
D8nPt
^(+^0
V(+V0
V(+V0
^(+^0
^(+^0
^(+^0
^(+^0
D8nPtmH
V(+V0
H+F8H
F(D+F0D
A_A^A]A\_^[]
UVWATAUAVAWH
tffE;
t_fD;
A_A^A]A\_^]
@USVWATAUAWH
A_A]A\_^[]
@USVWATAUAVAWH
tgfE;
tbfD;
HcVX;
T$pI;
T$pI;
L$0L;
T$HI;
T$HI;
T$pI;
T$pI;
T$pI;
T$HI;
T$HI;
fD;1s
T$pI;
T$HI;
L$0H;
T$HI;
A_A^A]A\_^[]
x AVH
H+C8H;
@SVWH
D$0Ic
T$0H+
L$8H3
l$ VWAVH
T$0I;
T$0Hc
L$8H3
@A^_^
VWAVH
)t$@H
T$0I;
T$0Hc
L$8H3
(t$@H
PA^_^
@USVWATAUAVAWH
A_A^A]A\_^[]
@USVWATAUAVAWH
IcFPH
IcF H
D$0H;H
@0I;U
D$@H;H
IcV8H
A_A^A]A\_^[]
CXH;CXu
UVWATAUAVAWH
A_A^A]A\_^]
@USVWATAUAVAWH
Ic\$ 
IcT$ H
IcD$PH
IcD$hL
IcD$PH
E@H;G
D$XL;`
MHH;Mht
A9|$8~_3
]PH;]ht*I
A;|$8|
A_A^A]A\_^[]
D$ H;C
L$(H3
@USVWATAUAVAWH
HcQ H
IcU8H
IcEhL
L$X9O 
L$xH;A
H;L$X
H;D$X
G8H;G@t
G8H;G@t
l$`;G 
\$TE3
l$`9_8
L$pH;A
G8H;G@t
G8H;G@t
HcGPH
IcEPH
A_A^A]A\_^[]
@SVWATAUAVAWH
A_A^A]A\_^[
UVWATAUAVAWH
IcE H
t|HcP0H
9s8~pE3
HcP0I
Ic@PH
A_A^A]A\_^]
USVWATAUAVAWH
A_A^A]A\_^[]
@USVWATAUAVAWH
H+WHH
H+W`H
D$hL;
D$PL;
D$PL;
L;l$h
A_A^A]A\_^[]
@USVWATAUAVAWH
L$PH;
L;l$Pu
E8`8t
E8`8t
L$d@H
m8D9a 
HcK0H
D$PI;
HcK0H
D$PI;
HcK0H
tzHcH0H
L9`0H
D$PI;
D$PI;
d$XD9a8
HcK0H
D$PH;}(t
} HcK0H
D$PH;}(t
HcK0H
UpH9E
\$PH;}(t
D$PH;}(t
Ic@PH
}HH;}
]hI;]
}HH;}
A_A^A]A\_^[]
t$ WH
UVWAVAWH
|$ H!\$(
PA_A^_^]
VWAVH
0A^_^
@SUVWATAVAWH
@A_A^A\_^][
t$ WH
UUUUUUU
@SUVWATAUAVAWH
HA_A^A]A\_^][
UVWAVAWH
A_A^_^]
l$ VWATAVAWH
 A_A^A\_^
UVWATAUAVAWH
 A_A^A]A\_^]
UVWATAUAVAWH
 A_A^A]A\_^]
l$ VWATAVAWH
 A_A^A\_^
l$ VWATAVAWH
 A_A^A\_^
UVWATAUAVAWH
0A_A^A]A\_^]
t$ WH
H;{ H
SUVWATAUAVAWH
L9|$x
L9|$xu
(A_A^A]A\_^][
l$ VWAVH
 A^_^
VWAVH
(t$PI
@ 9p 
UVWAVAWH
A_A^_^]
WAVAWH
D9}(u
D9x(t8H
D9x(u
D9x(u
A_A^_
@USVWATAUAVAWH
A_A^A]A\_^[]
D95A:'
D95T*'
D95O)'
D95M$'
WAVAWH
D9x(t8H
D9x(t8H
D9x(u
D9x(u
A_A^_
x UATAUAVAWH
D9x(u
D9x(t9H
tRHcO(H
A_A^A]A\]
USVWATAUAVAWH
HcEwH
A_A^A]A\_^[]
UAVAWH
A_A^]
x UAVAWH
D9x(u
A_A^]
t$ WAVAWH
D9x(u
D9z(u
D9{8t9H
A_A^_
VWAVH
D$ E3
UVWATAUAVAWH
A_A^A]A\_^]
UWAVH
@ 9A 
C 9A 
|$ UAVAWH
D9x(u
D9x(t8H
A_A^]
t$ UWAUAVAWH
A_A^A]_]
D$PConv@
|$ UATAUAVAWH
D9`(u
D9`(t8H
D9f(u,H
D9g(t
A_A^A]A\]
UVWATAUAVAWH
channelsH
A_A^A]A\_^]
x UATAUAVAWH
A_A^A]A\]
t$ UATAVH
A^A\]
UVWATAUAVAWH
t$`L+
l$hH+
L!B 3
\$HA_A^A]A\_^]
L+QxH+
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
\$xL;
A_A^A]A\_^]
@SUVWATAUAVAWH
A_A^A]A\_^][
\$ UVWATAUAVAWH
L+CxH+
H;C r
H;s8u
A_A^A]A\_^]
@USVWATAVAWH
D$pL;
A_A^A\_^[]
UVWATAUAVAWH
PA_A^A]A\_^]
WATAUAVAWH
0A_A^A]A\_
p WATAUAVAWH
D$`M;
L$@L+
(H+D$XA
(t$pL
A_A^A]A\_
\$ UVWATAUAVAWH
T$`I;
L$hH;
D;|$ u
A_A^A]A\_^]
UVWATAUAVAWH
l$PM;
L$@H;
A_A^A]A\_^]
t$ WATAUAVAWH
 A_A^A]A\_
p WATAUAVAWH
D$xH;
T$0I+
l$8I+
A_A^A]A\_
t$ WATAUAVAWH
t$XM;
L$PH+
t$(H;
D$ I;
A_A^A]A\_
UVWATAUAVAWH
 A_A^A]A\_^]
SUVWATAUAVAWH
t$ H+
|$ H+
HA_A^A]A\_^][
(|$@D
(D$0D
(L$ D
(t$`L
R^*/m
(|$@D
(D$0D
(L$ D
l$ VWAVH
R^*/m
R^*/m
L$@H3
T$ L;
L$PH3
)t$0D
)|$ I
(|$ L
x ATAVAWL
|$8A_A^A\
UVWATAUAVAWH
)\$pD
)d$`D
)l$PD
)t$@I
(t$@L
A_A^A]A\_^]
UVWATAUAVAW
zFuDA
\$ L;
A_A^A]A\_^]
UVWATAUAVAW
D$hE2
t$ L;
\$pE2
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
VWATAUAVH
D$`I;
0A^A]A\_^
UVWATAUAVAWH
A0t:I
pA_A^A]A\_^]
WATAUAVAWH
(t$0L
(|$ I
A_A^A]A\_
\$ UVWATAUAVAWH
t$pO;,
D$hJ9
} 8\$0t
8\$1t
L;E(|
A_A^A]A\_^]
WATAVAWH
t$8A_A^A\_
WATAUAVAWH
A_A^A]A\_
WATAUAVAWH
L$@E3
T$(E3
L$@E3
A_A^A]A\_
WATAUAVAWD
t$@A_A^A]A\_
WATAUAVAWH
A_A^A]A\_
WATAUAVAWH
T$HE3
A_A^A]A\_
UVWATAUAVAW
A_A^A]A\_^]
UVWATAUAVAW
T$ L;
d$0L+
D$8L;
A_A^A]A\_^]
UVWATAUAVAW
\$0E3
A_A^A]A\_^]
UVWATAUAVAW
D$hM;
D$@L+
A_A^A]A\_^]
UVWATAUAVAWH
H+t$(I
H;t$8
L;D$PuLH
A_A^A]A\_^]
UVWATAUAVAWH
L+t$HO
L$`L;
H;t$h
A_A^A]A\_^]
SUVWATAUAVAWH
l$(H;
A_A^A]A\_^][
SUVWATAUAVAWH
D$pH;
A_A^A]A\_^][
@USVWATAUAVAWH
|$0L!L$(M
A_A^A]A\_^[]
SUVWATAUAVAWH
L$0H+
L$ H+
K0H9KHu
C(H9C@u
L9kHu
HA_A^A]A\_^][
\$ UVWATAUAVAWH
D$8L;
`A_A^A]A\_^]
s WATAUAVAWH
T$hH;
\$xI#
\$xL;
T$hM;
A_A^A]A\_
s WATAUAVAWH
L$pM;
A_A^A]A\_
UVWATAUAVAWH
0A_A^A]A\_^]
UVWATAUAVAWH
 A_A^A]A\_^]
USVWATAUAVAWH
A_A^A]A\_^[]
@USVWATAUAVAWH
U(v&L
A_A^A]A\_^[]
t$ WH
{ AVH
x ATAVAWH
|$8A_A^A\
x ATAVAWH
|$8A_A^A\
x AVH
|$(A^
x AVH
|$(A^
x AVH
|$(A^
L$0H3
L$@H3
L$0H3
(|$pE
L$@H3
@SVWAVAWH
)|$0H
~T$ f
L$(H3
(|$0H
PA_A^_^[
SUVWAVAWH
)T$0H
~L$ H
~T$ f
L$(H3
(|$`D
(D$PD
(L$@D
(T$0H
A_A^_^][
@SVWAVAWH
)|$0H
~T$ f
L$(H3
(|$0H
PA_A^_^[
SUVWAVAWH
)T$0H
~L$ H
~T$ f
L$(H3
(|$`D
(D$PD
(L$@D
(T$0H
A_A^_^][
UVWATAUAVAWH
A_A^A]A\_^]
USVWATAUAVAWH
D$hH;
D$x@8|$`t
A_A^A]A\_^[]
WATAUAVAWH
A_A^A]A\_
UVWATAUAVAW
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
@USWH
@SVWH
@USVWATAUAVAW
E8f1u
uFM9fHu@E8a
H;ExuQI
A_A^A]A\_^[]
@USVWATAUAVAW
A8^1u
} A8Y
A_A^A]A\_^[]
@USVWATAUAVAW
H;EhuQI
A_A^A]A\_^[]
@USVWATAUAVAW
A_A^A]A\_^[]
$bR}H|
(|$0D
(D$ D
$bR}H|
(|$0D
(D$ D
$]Y`H
x(D$@
x(L$0
x(T$ 
UVWATAUAVAWH
x)T$`H
zoU L
A_A^A]A\_^]
UVWATAUAVAWH
x)T$`H
zoU L
A_A^A]A\_^]
UVATAVAWH
)t$0M
)|$ H
(|$ H
@A_A^A\^]
UVWATAUAVAWH
)T$PH
oD$ D
oD$0f
L$@H3
A_A^A]A\_^]
WAVAWH
0A_A^_
@USVWATAUAVAW
H;EpuQI
A_A^A]A\_^[]
@USVWATAUAVAW
F(A8^1t
A_A^A]A\_^[]
USVWAVAWH
A_A^_^[]
USVWATAVAWH
A_A^A\_^[]
USVWAVAWH
A_A^_^[]
USVWATAVAWH
A_A^A\_^[]
\$ UVWAVAWH
 A_A^_^]
p WAVAWH
L$ H3
(|$0I
A_A^_
@USVWATAUAVAW
H;Epu]L
A_A^A]A\_^[]
VWAVH
VWAVH
L$PH3
UVWAVAWH
fD99sD
GfE98s
A_A^_^]
VWATAVAWH
A_A^A\_^
L$(H3
UWATAVAWH
HcT$xH
L$`D9v
t$XH;
HcT$@H
A_A^A\_]
UVWAVAWH
A_A^_^]
@USWH
@USVWAUAVAWH
9}`uH
A_A^A]_^[]
\$ UVWAVAWH
IZfD;
CL$(3
A_A^_^]
\$ UVWAVAWH
PA_A^_^]
UVWAVAWH
A_A^_^]
UVWATAUAVAWH
A_A^A]A\_^]
D848u
CL$(H
qaUvT
D$0H;
L$0H3
SVWAVAWH
(D$Pf
pA_A^_^[
x AVAWI
fF9<Ju
fA90u
|$0A_A^
t$ UWAVH
@USVWATAUAVAWH
A_A^A]A\_^[]
@SUVWAVH
A^_^][
l$ VWAVH
 A^_^
SVWAVAWH
(D$Pf
A_A^_^[
t$ UWAWH
t$ WAVAWH
A_A^_
L$xH3
t$ WH
L$@I+
WAVAWH
 A_A^_
@USVWATAUAVAWH
9zv2H
D8t$0
D8t$0
A_A^A]A\_^[]
@USVWATAVAWH
A_A^A\_^[]
UVWATAUAVAWH
l$0I+l$(H
L$pL;
0A_A^A]A\_^]
t$@@8q t]
F0H9F(t
l$ VWATAVAWH
L$8H3
A_A^A\_^
t$ WATAUAVAWH
fD9 t
D$ H;
0A_A^A]A\_
@SVWATAUAVAWH
t$ L;
tHL+d$(L+
`A_A^A]A\_^[
@SVWATAUAVAWH
D$XH+
\$xM;
A_A^A]A\_^[
|$ L;
L$@I;
\$xM;
WAVAWH
 A_A^_
qXQtf
q!YzX?A
\$ UVWATAUAVAWH
A_A^A]A\_^]
x AWH
S8H;S@t
SPH;SXt
t$ WH
APH9AHtMH
t$ WH
APH9AHt]H
t$ UWAUAVAWH
APH9AH
A_A^A]_]
VWAVH
UVWAVAWH
D$0L;
@A_A^_^]
@USVWATAUAVAWH
pi[Z[
A_A^A]A\_^[]
T$ H;
pXQtf
pXQtf
@SUVWAUAVAWH
A_A^A]_^][
pi[Z[
pi[Z[
SUVWAVH
)|$PE
L$0H3
(|$PD
(D$@H
pA^_^][
USVWAVH
A^_^[]
O@t'H
pi[Z[
pi[Z[
u5I!K
qi[Z[
UVWATAUAVAWH
o D9g
A_A^A]A\_^]
\$ UVWATAUAVAWH
D$(E3
A_A^A]A\_^]
qXQtf
UVWATAVH
T$0H;
A^A\_^]
H9s`v?3
H;s`r
pXQtf
 Hiy`@!
@USVWATAUAVAWH
T$`H;
A_A^A]A\_^[]
@SUVWAVH
L$ H3
0A^_^][
t$ UWAUAVAWH
A_A^A]_]
@SUVWATAVAWH
L$8H3
@A_A^A\_^][
@USVWATAUAVAWH
D$(Li
T$PH;
L$@D;
A_A^A]A\_^[]
l$ VWATAVAWH
 A_A^A\_^
UVWATAUAVAWH
 A_A^A]A\_^]
UVWATAUAVAWH
 A_A^A]A\_^]
@SUVWAUAVAWH
|$@vkL
9~Pvk
`A_A^A]_^][
UVWATAUAVAWH
T$`H;
A_A^A]A\_^]
D$0H;
L$pH3
UVWATAUAVAWH
A_A^A]A\_^]
H!A@H
t$ WH
t$ WH
t.;y@s)H
t$ WH
t$ WH
T$ H;
x AVH
WAVAWH
A_A^_
UVWATAUAVAWH
D$(L+
L$PH+
s~H;_
A_A^A]A\_^]
UVWATAUAVAWH
T$(E3
8{usH
t$8E3
A_A^A]A\_^]
t#HcQ
@SVWATAUAVAWH
L$pH3
A_A^A]A\_^[
VWAVH
L$HH3
@USVATAUAVAWH
L9l$Xu@H
fE9.u
fD9,Pu
A_A^A]A\^[]
D$0E3
L$PH3
D$0E3
L$PH3
UATAUAVAWH
M`H;Mhs
A_A^A]A\]
@USVWAVH
CT$ L
CT$ L
A^_^[]
VWAVH
T$(H+
t$ UWAWH
D9X0H
D9X0H
D9X0H
UVWAVAWH
D9X0H
D1X0I
A_A^_^]
gfffffffH
L$0H3
t$ WAVAWH
D8sPt_H
H+C8H
C(D+C0D
 A_A^_
USVWATAUAVAWH
~(E+~0E
V(A+V0A
V(A+V0A
A8VPt
V A+V0A
N A+N0+
^(A+^0A
M9~Hs
V(A+V0A
A8vPt
xA_A^A]A\_^[]
@USVWAUAVAWH
^(+^0
^(+^0
~(+~0
V(+V0
D8vPt
^(+^0
L9nHs
H+F8I;
F(D+F0D
V(+V0
V(+V0
D8vPt
A_A^A]_^[]
@USVWAVAWH
H9_(L
~(A+~0A
A_A^_^[]
UWATAVAWH
fE; s
t]fE; s
sBfD9z
uPfD;"s
A_A^A\_]
\$ UVWATAUAVAWH
fD;)s
A_A^A]A\_^]
@USVWAVH
L$0Hc
L9u't:
A^_^[]
@USVWAVH
L$0Hc
L9u't:
A^_^[]
\$ UVWAUAWH
tcfD;
D9k(t?H
D9K(t=H
A_A]_^]
\$ UVWAUAVH
fD;*s
t\fD;*s
t\fD;
L9u't7
A^A]_^]
\$ UVWATAUAVAWH
L$HE3
A_A^A]A\_^]
@SUVWAVH
L$@H3
PA^_^][
@SUVWAVAWH
(A_A^_^][
L$0H3
UVWATAUAVAWH
H;C(t
DC(H;C(
9x(ubH
L$XH9
H;C(t
DC(H;C(
A_A^A]A\_^]
UVWATAUAVAWH
pRT8Y
pRT8Y
.E9f ~ZA
T$XA;V |
d$XE9f8~VA
T$XA;V8|
IcFPH
D$XH;D$`
YE9gp
ME9gX
D$XD8`
X@D8cDt
A_A^A]A\_^]
UVWATAVH
A^A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
@USVWAWH
A__^[]
UAVAWH
A_A^]
\$ UVWAVAWH
A_A^_^]
UAVAWH
A_A^]
\$ UVWAVAWH
A_A^_^]
UAVAWH
A_A^]
\$ UVWAVAWH
A_A^_^]
UAVAWH
A_A^]
\$ UVWATAUAVAWH
A_A^A]A\_^]
X UVWATAUAVAWH
A_A^A]A\_^]
@USVWATAVAWH
A_A^A\_^[]
\$ UVWATAUAVAWH
A_A^A]A\_^]
\$ UVWATAUAVAWH
A_A^A]A\_^]
\$ UVWATAUAVAWH
A_A^A]A\_^]
\$ UVWATAUAVAWH
A_A^A]A\_^]
USVWATAUAVAWH
A_A^A]A\_^[]
@USVWATAUAVAWH
d$0L;
D8<1u
L;d$8t'
E8,7u
\$(I;
F8,6u
A_A^A]A\_^[]
USVWATAUAVAWH
I9L$@v
I;L$@r
D$ E3
t$(I;
t$(I;
A_A^A]A\_^[]
WAVAWH
 A_A^_
x UATAWH
A_A\]
x ATAVAWH
D8<*u
E8<0u
D$ E3
0A_A^A\
WATAUAVAWH
A_A^A]A\_
@SUVWATAUAVAWH
3333333
XA_A^A]A\_^][
@SUVWATAVAWH
(D$ f
0A_A^A\_^][
t$ WATAUAVAWH
HkL$PXH
 A_A^A]A\_
t$ WATAUAVAWH
l$pM+
 A_A^A]A\_
@SUVWATAVAWH
PA_A^A\_^][
t$ WATAUAVAWH
t$8Hi
t$PM;
L$XH3
A_A^A]A\_
t$ WH
qRT8Y
VWATAVAWH
0A_A^A\_^
x UATAUAVAWH
A_A^A]A\]
D$0!D$4H
L$PH3
|$ AVH
L$`H3
@USVWATAUAVAWH
A_A^A]A\_^[]
I;SHt
DSHI9SHt
x UATAUAVAWH
9s(t7H
A 9s(t7H
A_A^A]A\]
t$ WH
L$`H3
` UAUAWH
bfloat16H
A_A]]
TO_FLOATH
|$ UAUAVH
A^A]]
x UAVAWH
A_A^]
UWATAVAWH
A_A^A\_]
UWATAVAWH
D$8H+
D$XH+
D$xH+
A_A^A\_]
x UATAVH
NONED
A^A\]
|$ UATAUAVAWH
D9`(t9H
D9`(t9H
t$ L+
L+t$@I
D$PH+
D9p(u
D$0H+
A_A^A]A\]
x UAUAVH
NONED
A^A]]
UWATAUAVH
NONED
A^A]A\_]
x UATAVH
NONED
A^A\]
x UATAWH
NONED
A_A\]
UWAVH
\$ UVWAVAWH
Hc@@I
pA_A^_^]
\$ WH
D$0H+
L$@H3
x UAVAWH
A_A^]
t$ UWAUAVAWH
A_A^A]_]
UWAVH
9F(t:H
9F(t:H
t$ WH
|$ UH
D$<E3
UATAUH
A]A\]
@SUVWATAVAWH
 A_A^A\_^][
x UATAUAVAWH
A_A^A]A\]
x UATAUAVAWH
A_A^A]A\]
@USVWAUAVAWH
Hc@ H
A_A^A]_^[]
UWAVH
x UATAUAVAWH
D9x(t9H
D9x(u
l$$D9m
L$0D;E
A_A^A]A\]
UWAVH
|$ UATAUAVAWH
D9h(t9H
D9h(t9H
D9h(u
D9n(t
A_A^A]A\]
UWATAVAWH
A_A^A\_]
x UAUAVH
A^A]]
UWAVH
x UATAUAVAWH
A_A^A]A\]
UWAVH
x UATAUAVAWH
D9`(u
D9`(t8H
D9`(t8H
A_A^A]A\]
L$xH3
UWAUAVAWH
A_A^A]_]
x UATAUAVAWH
L$XH+L$PH
D$pH+D$hH
D9`(u
T$XH+T$PH
T$XH+T$PH
L$@H+L$8H
D$XH+D$PH
D$@H+D$8H
D$8L+
D$(I;
Lcd$(H
D$(H+D$0
D$HH+
D$xH+
T$`H+
A_A^A]A\]
|$ UH
x UATAUAVAWH
t!A9}
t%A9|$
~%H;]
A_A^A]A\]
UWAVH
UWAVH
E0perm
D$0H+
|$8H;\$0t
|$ E3
T$0H+
\$(H;
L$@H;
UWAUAVAWH
A_A^A]_]
x UATAUAVAWH
noneD
A_A^A]A\]
x UATAUAVAWH
noneD
A_A^A]A\]
x UAUAVH
A^A]]
x UATAUAVAWH
D9h(u
D9h(u
X L9c
D9h(t9H
A_A^A]A\]
x UAUAVH
A^A]]
UWAVH
x UATAUAVAWH
D9h(u
A_A^A]A\]
UWAVH
x UATAUAVAWH
D$@E3
L$@H;
D$hH+
L$xH9
D9`(t8H
D9`(u
\$(Ic
L;t$8D
t$0Ic
9p(t7H
A_A^A]A\]
UWAVH
x UATAVH
A^A\]
x UAUAVH
A^A]]
t$ WATAUAVAWH
D9p(u
D9p(t8H
\$ H+
T$0H+
A_A^A]A\_
UWATAVAWH
A_A^A\_]
UWATAVAWH
A_A^A\_]
|$ UH
x UAUAVH
A^A]]
x UATAUAVAWH
|$0E3
A_A^A]A\]
x UATAUAVAWH
t"D9`
D9a(u
D9a(u
D9`(u
A_A^A]A\]
x UATAVH
A^A\]
UWAUAVAWH
D9h(u
D9h(u
D9h(u
D9h(t8H
A_A^A]_]
UWAVH
UWATAVAWH
A_A^A\_]
x UATAVH
A^A\]
UWATAVAWH
D9g(t8H
D9g(t8H
D9g(t8H
D9c(t8H
A_A^A\_]
E9g(u
E9g(u
UWAVH
x UATAUAVAWH
D9`(u
D9`(u
X L9s
A_A^A]A\]
UWATAUAVH
A^A]A\_]
t$ WATAUAVAWH
\$ H+
9h(t7H
A9l$(u-H
T$0H+
A_A^A]A\_
x UAVAWH
A_A^]
VWAVH
 A^_^
VWAVH
t$0I;
@A^_^
UVWAVAWH
@A_A^_^]
L$xH3
L$xH3
x AVH
x AVH
D$ E3
L$@H3
VWAVH
@A^_^
VWATAVAWH
0A_A^A\_^
x UATAUAVAWH
UUUUUUU
T$`H;T$h
T$`H;T$ht
d$pE3
bodyD
|$@H;U
L;|$ps
A_A^A]A\]
x UATAUAVAWH
|$xM+
t$`I+
L$0L;
D$pI+
D$XH+
A_A^A]A\]
9C(t:H
|$ UATAUAVAWH
D$HE3
UUUUUUU
T$PH;T$Xt
T$PH;T$Xt
T$PH;T$Xt
bodyD
T$hH;T$pt
D$pH+
D$@L;m
T$XH+
A_A^A]A\]
|$ UH
UATAUH
A]A\]
x UAVAWH
A_A^]
l$ VWATAVAWH
UUUUUUU
 A_A^A\_^
\$ UVWATAUAVAWH
UUUUUUU
0A_A^A]A\_^]
UWATAUAWH
A_A]A\_]
D$`!D$dH
T$h!D$xH
L$PE3
L$PE3
{ AVH
UWATAVAWH
A_A^A\_]
|$ UH
UWAVH
UWAUAVAWH
A_A^A]_]
L$xH3
D$<E3
D$<E3
UVWATAUAVAWH
D$`E3
D;|$PL
A_A^A]A\_^]
UWAVH
UWAVH
UWAVH
UWAVH
UWAVH
X UVWATAUAVAWH
IcWh;
A_A^A]A\_^]
x UATAUAVAWH
A_A^A]A\]
UVWAVAWH
A_A^_^]
UWAVH
E H;]
@A^_]
UWAVH
UWAVH
x UAVAWH
A_A^]
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
x UAVAWH
A_A^]
t$ UWATAVAWH
D9`(u
D9`(u
X L9{
|$ A;
D9`(t
HcL$h;
HcL$8;
HcL$8;
D9c(t
D9e8u
D9ehu
E`H9E0
A_A^A\_]
x UAVAWH
A_A^]
x UATAUAVAWH
H L9)
D9o(u
D9k(t
A_A^A]A\]
UWAVH
x UATAUAVAWH
D9h(u
D9h(u
>D9o(t
A_A^A]A\]
UWAVH
UWAVH
x UATAUAVAWH
A_A^A]A\]
t$ WH
@ 9A 
C 9A 
L$xH3
UWATAVAWH
A_A^A\_]
x UAUAVH
A^A]]
x UAVAWH
H9D$p
A_A^]
\$ UVWATAUAVAWH
D9p(u
mean@
A_A^A]A\_^]
UAVAWH
A_A^]
x UATAUAVAWH
D9p(u
D9p(u
p L9f
meanD
A_A^A]A\]
UVWATAUAVAWH
D$`HcH
D$`HcH
D$`HcH
D$8H9t$@
D$`HcH
L9|$@
>.t/I
A_A^A]A\_^]
UWAVH
` UAVAWH
A_A^]
\$ UVWATAUAVAWH
E@perm@
E@perm@
A_A^A]A\_^]
UAVAWH
A_A^]
x UATAUH
A]A\]
WATAUAVAWH
yxxxxxxxH+
 A_A^A]A\_
t$ WATAUAVAWH
 A_A^A]A\_
t$ WH
L$xH3
UWATH
L$xH3
x AVH
s HcK
x UAVAWH
A_A^]
x UATAUAVAWH
}HH9|$`
H9|$p
H9|$h
(D$0f
H;T$xu
A_A^A]A\]
t#E9L$(
Ic\$(
t E9N
Hc[@A
T$PH+
I9~(I
UWAVH
UWAUAVAWH
D9h(t8H
D9h(t8H
7D9n(t
A_A^A]_]
x UATAUAVAWH
A_A^A]A\]
x UATAVH
A^A\]
x UATAVH
A^A\]
D$`!D$dH
T$h!D$xH
t$ WAVAWH
D9x(t8H
D9{(t
L$xH3
A_A^_
UWATAVAWH
A_A^A\_]
UVWATAUAVAWH
Lcx H
seed@
X_randomH
D$pIc
D$PE3
X_randomH
A_A^A]A\_^]
x AVH
(D$0f
(D$@f
D$0H;
\$ UVWAVAWH
A_A^_^]
\$ UVWAVAWH
A_A^_^]
\$ UVWAVAWH
A_A^_^]
\$ UVWAVAWH
A_A^_^]
\$ UVWATAUAVAWH
\$(L;
A_A^A]A\_^]
|$ UATAUAVAWH
D$hH+D$`H
D$@H+D$8H
@8|$ 
HcT$0I
X H9{
D9m ~eA
D$(H;
D;e |
d$@L+d$8I
H+D$xH
D$pH+
D$(L+
D9x(t9H
@8|$ t
HcT$0H
E9}(t
A^A]A\]
L$@E3
UVWAVAWH
L$`I;
A_A^_^]
T$pH;
x UATAUAVAWH
D$HH+D$@H
D$0H+D$(H
@ ;D$p
D$8H+
D$PH+
A_A^A]A\]
)L+$8L+d9
x UATAVH
D9`(u
D9`(u
|$ H+
D9`(t8H
7D9f(t
D9f(t
D$0H+
A^A\]
UAVAWH
A_A^]
UWAUAVAWH
A_A^A]_]
t$ WH
@ 9A 
C 9A 
UWATAVAWH
A_A^A\_]
x UATAUAVAWH
L$HE3
D9x(u
L$0H+
L$PH+
D$`H+
D$@H+
L$pH+
D$`H+
D$@H+
9u@~aH
D$(H;
D$xL+D$pI
D$`H+
D$@H+
D$(L+
D$hL9f
@8|$ ttL+
A_A^A]A\]
UAVAWH
A_A^]
9h(t7H
x UATAUAVAWH
A_A^A]A\]
x UATAUAVAWH
A_A^A]A\]
x UATAUAVAWH
A_A^A]A\]
UWAVH
UWAVH
x UAVAWH
D9x(t8H
D9x(u
D9{(t
A_A^]
UAVAWH
E0NONED
A_A^]
UWAVH
L$xH3
t$ WH
x UATAUAVAWH
A_A^A]A\]
UWAVH
x UATAUAVAWH
D9`(t8H
D9`(u
!|$(I
d$(E3
A_A^A]A\]
UWAVH
x UAUAVH
A^A]]
9h(t7H
9h(t7H
x UAUAVH
A^A]]
x UAUAVH
A^A]]
UWAVH
UWAVH
x UATAUAVAWH
D9`(u
D9`(t8H
D9`(t8H
A_A^A]A\]
x UATAUAVAWH
A_A^A]A\]
x UATAUAVAWH
L$XH+L$PH
D$pH+D$hH
D9`(u
T$XH+T$PH
T$XH+T$PH
L$@H+L$8H
D$XH+D$PH
D$@H+D$8H
D9b(u
r D9g(t
D$8L+
D$0H;
Lcl$0H
D$HH+
D$xH+
T$`H+
A_A^A]A\]
UWAVH
UWAVH
E0perm
t$8H;\$0t
|$ E3
T$0H+
|$ H;
L$@H;
x UATAVH
A^A\]
UWATAVAWH
A_A^A\_]
UWATAUAVH
A^A]A\_]
x UATAUAVAWH
A_A^A]A\]
UWAUAVAWH
A_A^A]_]
UWAUAVAWH
A_A^A]_]
UWAVH
UWATAUAVH
A^A]A\_]
UWAVH
x UATAUAVAWH
T$HH+
t$@I;
L$xH9
D9`(u
T$HH+
A_A^A]A\]
D9`(t9H
D9`(u
|$PE3
D9w(t
|$(H;|$0L
D9p(u
D9p(t8H
D9w(t
UWAVH
UWATAVAWH
A_A^A\_]
x UAUAVH
A^A]]
UWATAVAWH
A_A^A\_]
UWAVH
UWAVH
UWAVH
x UAVAWH
A_A^]
x UATAUAVAWH
A_A^A]A\]
x UATAUAVAWH
A_A^A]A\]
x UATAUAVAWH
A_A^A]A\]
UWAVH
|$ UATAUAVAWH
T$@E3
D9h(u
D9h(t9H
|$$H;
|$0L;|$H
HcD$$H
A_A^A]A\]
UWATAVAWH
A_A^A\_]
x UAVAWH
A_A^]
UWATAVAWH
A_A^A\_]
UWATAVAWH
A_A^A\_]
UWAVH
x UATAUAVAWH
L;t$0t
\$ H+
A_A^A]A\]
L+<0x
D$0H+
x UATAUAVAWH
A_A^A]A\]
x UATAUAVAWH
L$PH+L$HH
D$hH+D$`H
T$PH+T$HH
T$PH+T$HH
L$8H+L$0H
D$PH+D$HH
D$8H+D$0H
D$0L+
D$(I;
Lcl$(H
D$@H+
D$pH+
T$XH+
A_A^A]A\]
x UATAUAVAWH
A_A^A]A\]
UWAVH
x UATAUAVAWH
A_A^A]A\]
UWAVH
x UATAUAVAWH
D9p(u
D9p(t9H
D9p(u
s0I9M
A_A^A]A\]
UWAVH
|$ UATAUAVAWH
9p(t7H
A_A^A]A\]
x UATAUAVAWH
A_A^A]A\]
x UATAUAVAWH
t"D9`
D9a(u
D9a(u
D9`(u
A_A^A]A\]
UWAUAVAWH
A_A^A]_]
UWAVH
UWAUAVAWH
A_A^A]_]
UWAVH
t$ WATAUAVAWH
D9x(u
l$ Ic}
D9x(u
l$ Ic
A_A^A]A\_
|$ UH
x UATAVH
A^A\]
x UATAUAVAWH
A_A^A]A\]
UWAVH
x UATAUAVAWH
VarianceH
A_A^A]A\]
|$ UATAUAVAWH
D$`H+D$XH
D$8H+D$0H
@8|$ 
D9`(u
X H9{
D$(H;
d$8L+d$0I
D$xH+D$pH
D$hH+
D$(L+
D9x(t9H
@8|$ t
E9}(t
A_A^A]A\]
t$ UWAVH
T$pH;
T$pH;
UAVAWH
A_A^]
x UATAUAVAWH
L$0E3
L$8H+
L$PH+
D$`H+
D$HH+
L$pH+
D9eH~eL
D$(H;
D;mH|
D$xL+D$pI
D$`H+
D$HH+
D$(L+
@8|$ t}M+
A_A^A]A\]
UAVAWH
A_A^]
` UAUAVH
A^A]]
` UAUAWH
A_A]]
x UATAUAVAWH
A_A^A]A\]
` UAUAVH
A^A]]
UWAVH
` UAUAVH
A^A]]
@USVWAVH
A^_^[]
UWAVH
x UATAUAVAWH
A_A^A]A\]
UAVAWH
A_A^]
UWAVH
T$`H;
T$`H;
WAVAWH
0A_A^_
x UATAUAVAWH
UUUUUUU
t$8I;
L$hH;L$pt
T$PH;T$Xt
L$hH;L$pt
T$PH;T$Xt
T$PH;T$Xt
|$xE3
bodyD
D$8I+
9p(t;H
A 9s(H
H;|$xr=H
A9u(t8I
L;d$8L
D$XH+
A_A^A]A\]
x UATAUAVAWH
UUUUUUU
T$`H;T$h
T$`H;T$ht
d$pE3
bodyD
|$@H;
L;|$ps
A_A^A]A\]
UWAVH
|$ UATAUAVAWH
D$HE3
UUUUUUU
T$PH;T$Xt
T$PH;T$Xt
D9k(t9H
T$PH;T$Xt
bodyD
T$hH;T$pt
D$pH+
D$@L;m
T$XH+
A_A^A]A\]
UWATAUAVH
A^A]A\_]
UWAUAVAWH
A_A^A]_]
x UAVAWH
A_A^]
x UATAUAVAWH
L$XH;
E(A9D$(
D$XL;
D$pH+
A_A^A]A\]
x UAUAVH
A^A]]
x UATAUAVAWH
L$XH;
E(A9D$(
D$XL;
D$pH+
A_A^A]A\]
x UAUAVH
A^A]]
|$ UATAUAVAWH
D$HE3
UUUUUUU
T$PH;T$Xt
T$PH;T$Xt
T$PH;T$Xt
bodyD
T$hH;T$pt
D$pH+
D9s(t
D9s(u
E9t$(t;I
D$@L;m
T$XH+
A_A^A]A\]
UATAUH
A]A\]
` UAUAWH
A_A]]
UATAWH
A_A\]
UATAWH
A_A\]
9p(t7H
9p(t7H
UWAVH
UVWATAUAVAWH
D83tYL
A_A^A]A\_^]
x UATAUAVAWH
tSHcK(H
D9`(u
9x(t8H
A_A^A]A\]
x UATAVH
A^A\]
x UATAUAVAWH
D9x(u
D9x(t9H
A_A^A]A\]
x UAVAWH
A_A^]
UWAVH
x UATAUAVAWH
A_A^A]A\]
x UATAUAVAWH
A_A^A]A\]
UVWAVAWH
A_A^_^]
UAVAWH
@A_A^]
UWAVH
UWAVH
x UATAVH
A^A\]
UWATAVAWH
D9`(u
D9`(u
X L9{
|$ A;
D9`(t
HcL$h;
HcL$8;
HcL$8;
D9c(t
D9e8u
D9ehu
E`H9E0
A_A^A\_]
UWAVH
UWAVH
UWAVH
x UATAWH
A_A\]
\$ UVWATAUAVAWH
EhmeanD
A_A^A]A\_^]
` UAVAWH
meanD
A_A^]
` UAVAWH
meanD
A_A^]
x UATAVH
A^A\]
x UATAWH
A_A\]
` UAVAWH
A_A^]
|$ UATAUAVAWH
D9h(u
D9x(u
p L9f
D9x(t9H
D9h(u
A_A^A]A\]
UWATAVAWH
D9`(u
D9`(u
X L9{
|$ A;
D9`(t
HcL$h;
HcL$8;
HcL$8;
D9c(t
D9e8u
D9ehu
E`H9E0
A_A^A\_]
UWAUAVAWH
D9k(t
A_A^A]_]
UAVAWH
A_A^]
UVWAVAWH
A_A^_^]
x UATAVH
A^A\]
UWATH
UWAVH
UWAVH
UWAVH
x UATAUAVAWH
A_A^A]A\]
x AVH
x UAVAWH
A_A^]
UWAVH
UWAVH
|$ UAVAWH
A_A^]
H9^(H
UWAVH
x UATAUAVAWH
tRHcK(H
A_A^A]A\]
x UATAVH
A^A\]
|$ UATAUAVAWH
tRHcK(H
A_A^A]A\]
T$0H;
x UATAVH
A^A\]
|$ UATAUAVAWH
A_A^A]A\]
UVWATAVH
A^A\_^]
UVWATAVH
A^A\_^]
UVWAVAWH
A_A^_^]
UWAVH
UWATAVAWH
A_A^A\_]
x UATAUAVAWH
D9x(u
D9~(t8H
D9x(u>H
A_A^A]A\]
D9x(u
x UATAUAVAWH
A_A^A]A\]
t$ WATAUAVAWH
d$4A9~(u
D$0D;
A_A^A]A\_
x UATAUAVAWH
A_A^A]A\]
x UAUAVH
A^A]]
x UAUAVH
A^A]]
x UATAUAVAWH
A_A^A]A\]
UWAUAVAWH
D9k(t8H
E9n(u
E9n(u
D9k(t8H
~>D9k(t
A_A^A]_]
D9k(u
D9k(t8H
UWATAVAWH
A_A^A\_]
UWATAVAWH
A_A^A\_]
x UATAUAVAWH
new_axisH
p M9f
9p(t7H
A_A^A]A\]
L$xH3
L$xH3
UWAVH
UWAVH
T$PH;
x UATAWH
A_A\]
{ AVH
UATAWH
A_A\]
x UAVAWH
A_A^]
T$`H;
x UATAUAVAWH
A_A^A]A\]
x UAVAWH
A_A^]
x UAVAWH
A_A^]
T$`H;
T$`H;
x UATAUAVAWH
A_A^A]A\]
x UAVAWH
A_A^]
x UAVAWH
A_A^]
UATAWH
A_A\]
UWATAUAVH
A^A]A\_]
WAVAWH
0A_A^_
(t$0H
p WAVAWH
(t$@I
A_A^_
VWATAVAWH
A_A^A\_^
VWATAVAWH
0A_A^A\_^
t$ UWAVH
t$ UWAVH
t$ UWAVH
UVWAVAWH
L$`LcG
A_A^_^]
L$HH+
D$XH+
T$XE3
L$PL!D$PH
D$HE3
UVWATAUAVAWH
D$8H+
PA_A^A]A\_^]
UVWATAUAVAWH
T$ M+
PA_A^A]A\_^]
UVWATAUAVAWH
T$ M+
PA_A^A]A\_^]
UVWAVAWH
A_A^_^]
t$ UWAVH
L9A(L
t$ UWAUAVAWH
H!L$HE
F IcF
A_A^A]_]
\$ UVWH
C H98
x UATAUAVAWH
A_A^A]A\]
UVWAVAWH
A_A^_^]
@USVWATAVAWH
D9v(~
D9v@~
D9vX~
D9vp~
E8wHu
HcFXH
D9vp~vA
HcFpI
A_A^A\_^[]
\$ UVWATAUAVAWH
D$0I;G
HcFPH
A_A^A]A\_^]
{ ATAVAWH
HcFPH
HcFhH
HcFPH
L$hH9
HcF8H
L$hH9
HcF L
IcG H
L$hH9
IcG8H
L$hH9
L$xH+
A_A^A\
\$ UVWATAUAVAWH
@A_A^A]A\_^]
UAUAWH
bfloat16H
A_A]]
UAUAWH
A_A]]
x AVH
\$ UVWATAUAVAWH
PA_A^A]A\_^]
\$ UVWH
\$ UVWH
\$ UVWH
\$ UVWH
\$ UVWH
@SUVWAVH
T$ H;
L$HH3
PA^_^][
\$ UVWH
@USVWAUAVAWH
`A_A^A]_^[]
@USVWAVH
pA^_^[]
\$ UVWH
UWATAVAWH
0A_A^A\_]
@USVWATAUAVAWH
A_A^A]A\_^[]
@USVWATAUAVAWH
A_A^A]A\_^[]
\$ UVWH
D$8!D$<H
@USVWATAUAVAWH
A_A^A]A\_^[]
@USVWAVH
`A^_^[]
\$ UVWATAUAVAWH
T$@E3
L9d$pu
L9d$pu
D$0=A
A_A^A]A\_^]
D$0]A
UWATAVAWH
0A_A^A\_]
\$ UVWAVAWH
A_A^_^]
UWATAVAWH
0A_A^A\_]
\$ UVWH
@USVWATAVAWH
PA_A^A\_^[]
|$ AVH
D$8H;
|$ AVH
D$8H;
@USVWAVH
A^_^[]
t$ UWAVH
t$ UWAVH
t$ UWAVH
t$ UWAVH
t$ UWAVH
t$ UWAVH
t$ UWAVH
t$ UWAVH
t$ UWAVH
@USVWAVH
A^_^[]
t$ UWAVH
|$ UH
@USVWATAUAVAWH
|$ E3
IcEPH
IcE H
D$0H;H
@0I;W
t$ I;W
IcU8H
A_A^A]A\_^[]
UVWATAUAVAWH
tmI;O
A_A^A]A\_^]
t$p!t$tE3
|$x!u
D$0H+
D$8Nc$
D$0H+
\$ UVWH
T$ H;Qp
I;Pht}L
|$ UH
t$ WH
uA9C(
9C(t:H
WAUAVH
 A^A]_
WAUAVH
 A^A]_
9C(t;H
9C(t;H
C 9o(u
@SVWATAUAVAWH
IcGPH
IcGhH
IcG8H
d$DE9g
D$HA;O
D$pH;
M9e(I
M9e(I
d$DE9e
D$HA;M
L$hH;
IcG H
IcG L
IcEPH
IcG H
\$PH;\$h
D$pH;F
D$XH;
L$XH;
D$`D8`
D$`D9`
A_A^A]A\_^[
@USVWATAUAVAWH
Ic\$ !T$`L
IcT$ H
IcD$PH
IcD$hL
HcGPH
D$HD9
@ D9x u'
@ D9x u
H;D$ht
|$PL9
d$XE9|$8
\$XH;\$h
E;|$8
A_A^A]A\_^[]
\$ UVWATAUAVAWH
HcA8H
HcPhH
Hc@hH
H9p(H
A_A^A]A\_^]
\$ UVWATAUAVAWH
D$HE3
M9,$I
A_A^A]A\_^]
S H+S
WAVAWH
 A_A^_
F H;G 
F H;G 
UVWAVAWH
pA_A^_^]
UVWAVAWH
pA_A^_^]
x ATAVAWH
(D$ f
@A_A^A\
x ATAVAWH
@A_A^A\
UVWATAUAVAWH
 A_A^A]A\_^]
H;{ H
SUVWATAUAVAWH
L9|$x
L9|$xu
(A_A^A]A\_^][
UVWATAUAVAWH
IcVP;
0A_A^A]A\_^]
SUVWATAUAVAWH
HcUP;
8A_A^A]A\_^][
\$ UVWATAUAVAWH
A_A^A]A\_^]
x UATAUAVAWH
A_A^A]A\]
x UAVAWH
A_A^]
\$ UVWAVAWH
L$hH3
pA_A^_^]
UVWAVAWH
LcG(I
@A_A^_^]
VWAVH
D9sp~!H
sxHckpA
 A^_^
\$ UVWATAUAVAWH
HcVX;
HcVp;
PA_A^A]A\_^]
@SUVWATAVAWH
@8w8t
@8w8t
@8w8t
@8w8t
@8w8u
Lc}(M
@8w8u
Hc]@H
@8w8t
@t*H;
 A_A^A\_^][
UWAVH
t$ WAVAWH
LcF(I
 A_A^_
\$ UVWATAVH
0A^A\_^]
\$ UVWH
l$ VWAVH
0A^_^
x AVH
D9sP~!H
sXHckPA
@USVWAUAVAWH
HcWP;
FXE9nPt
0A_A^A]_^[]
\$ UVWAVAWH
Hci H
Hco8H
 A_A^_^]
t$ WH
VWAVH
 A^_^
t$ WH
q(Hci 3
s@Hck83
\$ UVWAVAWH
HcW8;
HcW ;
0A_A^_^]
@SUVWAVH
 A^_^][
VWAVH
0A^_^
l$ VWAVH
 A^_^
t$ WAVAWH
@A_A^_
VWAVH
D9q ~!H
q(Hci A
s D9s8~!H
s@Hck8I
s8D9sP~!H
sXHckPI
sPD9sh~!H
spHckhI
 A^_^
@USVWATAVAWH
HcVh;
HcVP;
HcV8;
HcV ;
PA_A^A\_^[]
\$ UVWATAVH
 t>I;
 A^A\_^]
VWAVH
HcUhH
VWAVH
 A^_^
\$ UVWAVAWH
0A_A^_^]
t$ WH
q(Hci 3
\$ UVWAVAWH
HcV ;
0A_A^_^]
@SUVWAVH
 A^_^][
t$ WH
l$ VWAVH
0A^_^
VWAVH
D9q ~!H
q(Hci A
D9sP~!H
sXHckPI
sPD9sh~!H
spHckhI
 A^_^
@USVWAUAVAWH
FXD9nPt
IcVh;
IcVP;
IcV8;
IcV ;
0A_A^A]_^[]
@SUVWATAVAWH
@8w8t
@8w8t
@8w8t
@8w8t
@8w8t
@8w8t
 A_A^A\_^][
VWAVH
VWAVH
 A^_^
\$ UVWAVAWH
@A_A^_^]
\$ UVWH
t$ WH
t$ WAVAWH
LcG(I
LcG8I
LcGhI
@A_A^_
VWAVH
 A^_^
\$ UVWATAUAVAWH
`A_A^A]A\_^]
\$ UVWATAUAVAWH
D8v8t
D8v8t
D8v8u
~hH;>r$D8v8u
D8v8u
}@HcE8M
D8v8u
Lc}XM
]pHcEhL
~FH;>r
 A_A^A]A\_^]
UWATAVAWH
McF8M
A_A^A\_]
t$ WAVAWH
LcF(I
LcF8I
LcFhI
 A_A^_
l$ VWAVH
@A^_^
\$ UVWATAUAVAWH
@A_A^A]A\_^]
@SUVWAVH
 A^_^][
l$ VWAVH
 A^_^
\$ UVWATAVH
@A^A\_^]
\$ UVWH
@8n8u
t$ WAVAWH
 A_A^_
UVWAVAWH
0A_A^_^]
@SUVWAVH
 A^_^][
t$ WH
t$ WH
t$ WH
\$ UVWAVAWH
@A_A^_^]
\$ UVWH
VWAVH
 A^_^
t$ WH
\$ UVWAVAWH
0A_A^_^]
@SVWH
\$ UVWAVAWH
@A_A^_^]
\$ UVWH
VWAVH
 A^_^
t$ WH
VWAVH
 A^_^
\$ UVWATAVH
D9g(t8H
PA^A\_^]
\$ UVWH
t$ WAVAWH
D9s(t:H
9C(t=H
D9s(t:H
D9s(t:H
D9s(t:H
D9{(t:H
 A_A^_
\$ UVWAVAWH
@A_A^_^]
@SVWH
l$ VWAVH
0A^_^
t$ WH
spHckh3
\$ UVWATAVH
HcVh;
0A^A\_^]
\$ UVWAVAWH
Hc^ H
Hc^8H
Hc^PH
 A_A^_^]
VWAVH
VWAVH
 A^_^
D$0H!X
D$0H!X
D$0H!X
!H !H$H!H(H
!H !H$H!H(H
H!H H
D$0H!X
D$0H!X
D$0H!X
D$0H!X
D$0H!X
D$0H!X
D$0H!X
D$0H!X
D$0H!X
D$0H!X
x AVH
uILc7M
x AVH
uILc7M
x AVH
uILc7M
x AVH
uILc7M
x AVH
uILc7M
x AVH
uILc7M
x AVH
uILc7M
x AVH
uILc7M
x AVH
uILc7M
x AVH
uILc7M
x AVH
uILc7M
x AVH
uILc7M
CX9{Pu
CX9{Pu
CX9{Pu
CX9{Pu
CX9{Pu
CX9{Pu
CX9{Pu
CX9{Pu
CX9{Pu
x AVH
D9L$P}
)Hct$P
x AVH
D9L$P}
)Hct$P
x AVH
D9L$P}
)Hct$P
x AVH
D9L$P}
)Hct$P
x AVH
D9L$P}
)Hct$P
x AVH
D9L$P}
)Hct$P
x AVH
D9L$P}
)Hct$P
x AVH
D9L$P}
)Hct$P
x AVH
D9L$P}
)Hct$P
x AVH
D9L$P}
)Hct$P
CL$ L
L$@H3
L$PH3
UVWATAUAVAWH
D9l$0
tID8G&tIA
D8G$A
VUUUUUUUH
A_A^A]A\_^]
t0H;=
t1H;=;
UVWATAUAVAWH
9qHtQ@8q&
)D$pfH
l$xL+
l$xH!t$PH!t$X
|$pM+
@8{%@
|$ E3
(D$pA
(D$PA
H9L$XwmD
A_A^A]A\_^]
t.HcS
t.HcS
VUUUUUUUH
t$ WH
T$0f9w
\$ UVWH
` AUAVAWH
u*fD9o
A_A^A]
t$ WH
t$ WATAUAVAWH
L$HH;
t7HcD$8H
t2HcT$HH
A_A^A]A\_
WAVAWH
 A_A^_
B H9G 
UVWATAUAVAWH
]PfE9e
@A_A^A]A\_^]
|$ UH
P2c7l
UVWATAUAVAWH
0A_A^A]A\_^]
u0A;H
t%A9X H
u'A;H
D$`+D$d
D$d+D$`
|$ UATAUAVAWH
A_A^A]A\]
T$0Ic
D$0H9P s
UUUUUUU
WAVAWH
L9A s
L;A H
H;H s
 A_A^_
SVWAVAWH
(D$Pf
A_A^_^[
u)A8p
u%A8p
x AVH
D$09H }
fffffff
x AVH
D$09H }
fffffff
WATAUAVAWH
 A_A^A]A\_
WATAUAVAWH
 A_A^A]A\_
|$ UATAUAVAWH
P2c7l
A_A^A]A\]
x AVH
x ATAVAWH
 A_A^A\
t.HcSPH
x ATAVAWH
LcG`H
+G`Lc
HcG`I
D$ L;
0A_A^A\
t$ WAVA
t$0A^_
VWAVH
 A^_^
WAVAWH
0A_A^_
VWAVH
@A^_^
t$ WH
D$`fB
|$ AVH
@Hc\$pH
H;Gxt
DGxH;Gxt
I;Bxt
DBxI;Bx
VWAUAVAWH
A_A^A]_^
WATAUAVAWH
0A_A^A]A\_
UVWAVAWH
8VHt^3
T$ E3
T$ E3
@A_A^_^]
D8A@t
UWAVH
(t$pL
` UAVAWH
(t$@~cM
(D$@f
d$ E3
A_A^]
t$ WATAUAVAWH
HcT$0H
A_A^A]A\_
t$ WATAUAVAWH
t2HcT$0H
A_A^A]A\_
x UAVAWH
(D$0f
(L$0f
A_A^]
t$ UWAVH
H;{ H
x ATAVAWH
(D$ f
0A_A^A\
UVWATAUAVAWH
o@fA9v
}UHcG
0fA9v
A_A^A]A\_^]
x ATAVAWH
 A_A^A\
VWAVH
G E9p
 A^_^
x AVH
VWAVH
 A^_^
P2c7l
x AUAVAWH
D$\@B
A_A^A]
t$ WAVAWH
A_A^_
UVWATAUAVAWH
D$0E3
HcK(;
H;|$@t'A
eH+|$0H
\$8HcC
A_A^A]A\_^]
SUVWATAUAVAWH
D$HI;
Hc|$4
D$XH;P
d$@E3
hA_A^A]A\_^][
SUVWATAUAVAWH
D$@L9d$8
D$PI;W
|$8E3
hA_A^A]A\_^][
UVWATAUAVAWH
L$@E3
d$PA;
D$8Hc
A_A^A]A\_^]
UVWATAUAVAWH
t2HcT$0H
@A_A^A]A\_^]
x ATAVAWH
 A_A^A\
8,u5H
VWATAVAWH
0A_A^A\_^
WAVAWH
 A_A^_
WAVAWH
L$0H;
9{(~3H
@A_A^_
?\u8A
UVWATAUAVAWH
8Anu$
(t$PH
`A_A^A]A\_^]
x ATAVAWH
0A_A^A\
WAVAWH
8-uuH
8]tmH
@A_A^_
x UATAUAVAWH
8^u7H
<]u"E
<-u!E
vd<[u)
(t$PI
A_A^A]A\]
UWATAVAWH
L9D$(
D$(I+
D$(I;
D$ H+
A_A^A\_]
UVWATAUAVAWH
T$@E3
)D$0E
t$@E3
)L$0A
L9l$8u
A_A^A]A\_^]
Dt$HD;
\$0I+
<QufH
(L$0L
UVWATAUAVAWH
UUUUUUU
 A_A^A]A\_^]
WATAUAVAWH
 A_A^A]A\_
K SUVWAUAVAWH
A_A^A]_^][
WAVAWH
 A_A^_
VWATAVAWH
L$ L;
0A_A^A\_^
t.HcSxH
UVWATAUAVAWH
HcT$0
D$(Lc@
A_A^A]A\_^]
H WATAUAVAWH
xhHc/A
u%LcC
A_A^A]A\_
@USVWATAUAVAWH
D$XfH
L$@@8x
T$hH;s s
@8|$@t
,A9<$u&I;
0H;s s
T$HA9>
A_A^A]A\_^[]
@SVWAVAWH
A_A^_^[
t.HcS
t.HcS
L9A t
L9A t
t.HcS
t.HcS
UVWATAUAVAWH
D$ L;
0A_A^A]A\_^]
x ATAVAWH
 A_A^A\
p WATAUAVAWH
A_A^A]A\_
t.HcS(H
UVWATAUAVAWH
D$(D;X(
D$(D;X(|
E IcE
PA_A^A]A\_^]
@SUVWATAUAVAWH
H;CXt
L$PH3
hA_A^A]A\_^][
x AVH
x ATAVAWH
LcC0A
A_A^A\
@SUVWATAVAWH
;E(|WA
LcG0A
A_A^A\_^][
t$ WATAUAVAWH
} IcE
E;E(D
A_A^A]A\_
L$0H3
VWAVH
VWAVH
 A^_^
UVWATAUAVAWH
t^H9]
A_A^A]A\_^]
|$ UH
x ATAVAWH
A_A^A\
H;{ H
WAVAWH
 A_A^_
UVWATAUAVAWH
T$PE3
T$PM;
|$ M;
A_A^A]A\_^]
UVWATAUAVAWH
T$PE3
T$PM;
|$ M;
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
|$HM;
A_A^A]A\_^]
UVWATAUAVAWH
|$HE3
|$HL;
L$8L;
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
A_A^A]A\_^]
x ATAVAWH
@A_A^A\
x ATAVAWH
M#^0M
t9A;p
 A_A^A\
uIIc@
WAVAWH
McFpH
D$ H;
0A_A^_
IpD;I`
KpD;K`|xA
ChA;|
u'McT
WATAUAVAWH
HhHcT
D9C8~73
D;C8|
x&;CP}!L
A_A^A]A\_
x ATAVAWH
~*L!D$ L!D$(
D;C8|
D$ L;
L$ L;
0A_A^A\
t*HcS`H
t*HcSPH
t*HcS@H
UVWATAUAVAWH
D$0fH
L$8H;
D$XE2
D$@Lc
D$$9|$(
H;\$XL
D$`H+
A_A^A]A\_^]
x UATAUAVAWH
D$ Mc
T$xHcE
L$@Hc
T$4Hc
d$<D;d$T
@@8t$0E
T$xHcM
A_A^A]A\]
UVWATAUAVAWH
D$0I+
PA_A^A]A\_^]
t&HcSpH;
t*HcS`H
t)HcSPH
UATAUAVAWH
~hHc3D
A_A^A]A\]
u1</w
x AVH
x UATAUAVAWH
)T$@3
L$(A9}
D$&H;
A_A^A]A\]
x UATAUAVAWH
HcE H
Hc} H
T$PL;
t$xH+t$pH
|$pH+
+D$DA
L$DE3
D$XH+
T$PE;
D9c(~
D$PHc
D;C(|
D$`H+
A_A^A]A\]
USVWATAUAVAWH
ExH;V
\$ Ic
F HcL
D$0L;
A_A^A]A\_^[]
UVWATAUAVAWH
C HcL
A_A^A]A\_^]
WATAUAVAWH
A_A^A]A\_
t$ WH
UATAUAVAWH
A_A^A]A\]
l$ VWATAVAWH
?H+)M
 A_A^A\_^
x ATAVAWH
 A_A^A\
WAVAWH
 A_A^_
|$ UATAUAVAWH
A_A^A]A\]
UVWATAUAVAWH
 A_A^A]A\_^]
WATAUAVAWH
 A_A^A]A\_
wet-A+
VWATAUAVH
 A^A]A\_^
D$XfA
t$ WATAUAVAWH
A_A^A]A\_
VWATAVAWH
A_A^A\_^
t$ WH
@USVWATAUAVAWH
D$4Hc
t6HcT$8H
t9HcM8H
t$4D;
A_A^A]A\_^[]
H UATAUAVAWH
g(fE9n
A_A^A]A\]
x ATAVAWH
UUUUUUU
 A_A^A\
P2c7l
w|tTA
H9^ tGA
t$ UWATAVAWH
|u"L;
A_A^A\_]
L$@H3
x AVH
UWATAUAVH
A^A]A\_]
UVWATAUAVAWH
A_A^A]A\_^]
UVWATAUAVAWH
H9D$pt$3
A_A^A]A\_^]
q:_0#
|$ AVH
VWAVH
p:_0#
p:_0#
p:_0#
@A^_^
q:_0#
q0R^G'
@SVWH
p:_0#
\$ UH
M H1E
 H3E H3E
ntelA
GenuD
D8L$0uP
|$(A^
VWATAVAWH
twIcF
T$PE3
 A_A^A\_^
WATAUAVAWH
A_A^A]A\_
x AVH
(t$ H
(t$ H
HcE_H
HcE_L
HcEgH
H;XXs
H;xXu5
tAHcP
WATAUAVAWH
|$8;3
 tDE3
A_A^A]A\_
t$ WH
L$@;|
tRLcY
AUAVAWH
u4I9}(
;I9}(tiH
0A_A^A]
AUAVAWH
u4I9}(
;I9}(tiH
0A_A^A]
UVWATAUAVAWH
`A_A^A]A\_^]
UVWATAUAVAWH
`A_A^A]A\_^]
@USVWATAUAVAWH
K0HcQ
L$xE3
D9o t
HcO H
D9o t
HcG H
A_A^A]A\_^[]
@USVWATAUAVAWH
t)IcV
K0HcQ
D$pHc
d$dD;d$ltY
L$h;M
A_A^A]A\_^[]
UVWATAUAVAWH
mwtMH
A_A^A]A\_^]
t-Lck
MGD;}
@USVWATAUAVAWH
L$`;M
A_A^A]A\_^[]
x AVH
x AVH
x AVH
HcK H
G0HcH
WAVAWH
9p@u+
t'HcW
9t$Pu
C0HcH
A_A^_
tEHcR
@SVWATAUAVAWH
|$ D!
L!|$(L!
D$8L9
D$0HcH
pA_A^A]A\_^[
SVWATAUAWH
d$ D!
L!d$(L!d$@D
D$HL9gXt
A_A]A\_^[
< u\A
B(I9A(u
SVWATAUAVAWH
0A_A^A]A\_^[
SVWATAUAVAWH
t@HcS
t$8L;
L;t$X
(D$0f
A_A^A]A\_^[
t$ WATAUAVAWH
E0Lc`
E0HcH
 A_A^A]A\_
9)~P3
UVWATAUAVAWH
99~CE
 A_A^A]A\_^]
ffffff
fffffff
fffff
LcA<E3
u HcA<H
x AVH
HUSVWATAUH
)D$ D
)L$0D
)T$@D
)\$PD
)d$`J
(D$ D
(L$0D
(T$@D
(\$PD
(d$`H
A]A\_^[]
HUSVWATAUH
)D$ D
)L$0D
)T$@D
)\$PJ
(D$ D
(L$0D
(T$@D
(\$PH
A]A\_^[]
HUSVWATAUH
)D$ D
)L$0D
)T$@D
)\$PD
)d$`D
)l$pD
(D$ D
(L$0D
(T$@D
(\$PD
(d$`D
(l$pD
A]A\_^[]
HUSVWATAUH
)D$ D
)L$0D
)T$@D
)\$PJ
(D$ D
(L$0D
(T$@D
(\$PH
A]A\_^[]
bQ|H[
bQ|H[
bq$HY
ba$@Y
bQ|H[
bQ|H[
bq,HY
ba,@Y
bQ|H[
bQ|H[
bq4HY
ba4@Y
o"bq=H
bQ|H[
bQ|H[
bq<HY
ba<@Y
bq$H_
ba$@_
bq$H]
ba$@]
bQ}H[
bQ}H[
bq,H_
ba,@_
bq,H]
ba,@]
bQ}H[
bQ}H[
bq4H_
ba4@_
bq4H]
ba4@]
bQ}H[
bQ}H[
bq<H_
ba<@_
bq<H]
ba<@]
bQ}H[
bQ}H[
$qbB~J
1bB~J
$0bR~J
HUSVWATAUAVAWH
)D$ D
)L$0D
)T$@D
)\$PD
)d$`D
)l$pD
}HX"b
}HX$"b
}HX$*b
}HX$2b
}HX$:b
}HX"b
}HX$"b
}HX$*b
}HX$2b
}HX$:b
}HX"b
}HX$"b
}HX$*b
}HX$2b
}HX$:b
}HX"b
}HX$"b
}HX$*b
}HX$2b
}HX$:b
(D$ D
(L$0D
(T$@D
(\$PD
(d$`D
(l$pD
A_A^A]A\_^[]
HUSVWATAUAVAWH
)D$ D
)L$0D
)T$@D
)\$PD
)d$`D
)l$pD
(D$ D
(L$0D
(T$@D
(\$PD
(d$`D
(l$pD
A_A^A]A\_^[]
HUSVWATAUAVAWH
)D$ D
)L$0D
)T$@D
)\$PD
)d$`D
)l$pD
}HX"br]HQ
br]HQ
br]HQ
br]HQ
br]HQ
br]HQ
br]HQ
br]HQ
}HX$"b
}HX$*b
}HX$2bb]HQ
bb]HQ
bb]HQ
bb]HQ
}HX$:bb]HQ
bb]HQ
bb]HQ
bb]HQ
}HX"br]HQ
br]HQ
br]HQ
br]HQ
br]HQ
br]HQ
}HX$"b
}HX$*b
}HX$2bb]HQ
bb]HQ
bb]HQ
}HX$:bb]HQ
bb]HQ
bb]HQ
}HX"br]HQ
br]HQ
br]HQ
br]HQ
}HX$"b
}HX$*b
}HX$2bb]HQ
bb]HQ
}HX$:bb]HQ
bb]HQ
}HX"br]HQ
br]HQ
}HX$"b
}HX$*b
}HX$2bb]HQ
}HX$:bb]HQ
(D$ D
(L$0D
(T$@D
(\$PD
(d$`D
(l$pD
A_A^A]A\_^[]
HUSVWATAUAVAWH
)D$ D
)L$0D
)T$@D
)\$PD
)d$`D
)l$pD
br}HQ
bruHQ
bb}HQ
bbuHQ
br}HQ
bruHQ
bb}HQ
bbuHQ
br}HQ
bruHQ
bb}HQ
bbuHQ
br}HQ
bruHQ
bb}HQ
bbuHQ
(D$ D
(L$0D
(T$@D
(\$PD
(d$`D
(l$pD
A_A^A]A\_^[]
HUSVWAWH
)D$ D
)L$0D
)T$@D
)\$PD
)d$`D
)l$pD
(D$ D
(L$0D
(T$@D
(\$PD
(d$`D
(l$pD
A__^[]
HUSVWAVAUATH
A\A]A^_^[]
HUSVWAVAUATH
A\A]A^_^[]
HUSVWAVAUATH
A\A]A^_^[]
HUSVWAVAUATH
wA\A]A^_^[]
HUSVWAVAUATH
wA\A]A^_^[]
HUSVWAVAUATH
wA\A]A^_^[]
)D$ D
)L$0D
)T$@D
)\$PD
)d$`D
)l$pD
(D$ D
(L$0D
(T$@D
(\$PD
(d$`D
(l$pD
HUSVWATAUH
)D$ D
)L$0D
)T$@D
)\$PD
)d$`D
)l$pD
~oL$ I
~o] I
~oL$ I
~o] I
~oL$ I
~o] I
(D$ D
(L$0D
(T$@D
(\$PD
(d$`D
(l$pD
A]A\_^[]
~oL$ I
~o] I
~oL$ I
~o] I
~oL$ I
~o] I
HUSVWAWAVAUATH
)D$ D
)L$0D
)T$@D
)\$PD
)d$`D
)l$pD
(D$ D
(L$0D
(T$@D
(\$PD
(d$`D
(l$pD
A\A]A^A__^[]
fffff
HUSVWAWAVAUATH
)D$ D
)L$0D
)T$@D
)\$PD
)d$`D
)l$pD
(D$ D
(L$0D
(T$@D
(\$PD
(d$`D
(l$pD
A\A]A^A__^[]
fffff
fffffff
HUSVWAWAVAUATH
)D$ D
)L$0D
)T$@D
)\$PD
)d$`D
)l$pD
(D$ D
(L$0D
(T$@D
(\$PD
(d$`D
(l$pD
A\A]A^A__^[]
HUSVWAVATH
)D$ D
)L$0D
)T$@D
)\$PD
)d$`D
)l$pD
(D$ D
(L$0D
(T$@D
(\$PD
(d$`D
(l$pD
A\A^_^[]
ffffff
fffffff
A<X@@
fffffff
A<X@@
A<X@@
A<X@@
)D$ D
)L$0D
)T$@D
)\$PD
)d$`D
)l$pD
(D$ D
(L$0D
(T$@D
(\$PD
(d$`D
(l$pD
)D$ D
)L$0D
)T$@D
)\$PD
)d$`D
)l$pD
(D$ D
(L$0D
(T$@D
(\$PD
(d$`D
(l$pD
)D$ D
)L$0D
)T$@D
)\$PD
)d$`D
)l$pD
(D$ D
(L$0D
(T$@D
(\$PD
(d$`D
(l$pD
HUSVWATAUAVH
br}H{
~HoD$
bReX@3b
]X@#bQ}H
breHQ
~HoT2
breHQ
~HoT2
breHQ
~HoT2
breHQ
breHQ
~HoT2
~HoT2
~HoT2
0bQ~I
bReX@3bQ}H
breHQ
breHQ
breHQ
breHQ
breHQ
~HoL$
~HoD$
bReX@3b
]X@#bBUX@
#bAmX
bbeHQ
breHQ
~HoL2
~HoTr
bbeHQ
breHQ
~HoL2
~HoTr
bbeHQ
breHQ
~HoL2
~HoTr
bbeHQ
breHQ
bbeHQ
breHQ
~HoL2
~HoTr
~HoL2
~HoTr
~HoL2
~HoTr
~HoD$
bReX@3b
]X@#bQ}H
bReX@{
#bQ}X
breHQ
breHQ
~HoT2
breHQ
breHQ
~HoT2
breHQ
breHQ
~HoT2
breHQ
breHQ
breHQ
breHQ
~HoT2
~HoT2
~HoT2
0bQ~I
bReX@3bQ}H
bReX@{
3bQ}X
breHQ
breHQ
breHQ
breHQ
breHQ
breHQ
breHQ
breHQ
breHQ
breHQ
~HoL$
~HoD$
bReX@3b
]X@#bBUX@
bReX@{
bBUX@[
'bQ}X
#bAmX
bbeHQ
breHQ
bbeHQ
breHQ
~HoL2
~HoTr
bbeHQ
breHQ
bbeHQ
breHQ
~HoL2
~HoTr
bbeHQ
breHQ
bbeHQ
breHQ
~HoL2
~HoTr
bbeHQ
breHQ
bbeHQ
breHQ
bbeHQ
breHQ
bbeHQ
breHQ
~HoL2
~HoTr
~HoL2
~HoTr
~HoL2
~HoTr
~HoD$
bReX@3b
]X@#bQ}H
bReX@{
(bQ}X
#bQ}X
breHQ
breHQ
@bQ~I
0bQ~I
bReX@3bQ}H
bReX@{
3bQ}X
breHQ
breHQ
~HoL$
~HoD$
bReX@3b
]X@#bBUX@
bReX@{
bBUX@[
bBUX@c
<bQ}X
#bAmX
bbeHQ
breHQ
bbeHQ
breHQ
IbbeHQ
$@bA~H
~HoD$
bReX@3b
]X@#bQ}H
bReX@{
6bQ}X
#bQ}X
breHQ
breHQ
~HoT2
breHQ
breHQ
}HX\I
~HoT2
breHQ
breHQ
}HX\I
~HoT2
breHQ
breHQ
}HX\I
breHQ
breHQ
~HoT2
}HX\I
~HoT2
}HX\I
~HoT2
}HX\I
0bQ~I
bReX@3bQ}H
bReX@{
3bQ}X
breHQ
breHQ
breHQ
breHQ
}HX\I
breHQ
breHQ
}HX\I
breHQ
breHQ
}HX\I
breHQ
breHQ
}HX\I
}HX\I
}HX\I
~HoL$
~HoD$
bReX@3b
]X@#bBUX@
bReX@{
bBUX@[
bBUX@c
bBUX@k
QbQ}X
#bAmX
bbeHQ
breHQ
bbeHQ
breHQ
IbbeHQ
bbeHQ
~HoL2
~HoTr
bbeHQ
breHQ
bbeHQ
breHQ
}HX\I
bbeHQ
bbeHQ
~HoL2
~HoTr
bbeHQ
breHQ
bbeHQ
breHQ
}HX\I
bbeHQ
bbeHQ
~HoL2
~HoTr
bbeHQ
breHQ
bbeHQ
breHQ
}HX\I
bbeHQ
bbeHQ
bbeHQ
breHQ
bbeHQ
breHQ
IbbeHQ
bbeHQ
~HoL2
~HoTr
}HX\I
~HoL2
~HoTr
}HX\I
~HoL2
~HoTr
}HX\I
+bA~H
$@ba~H
~HoD$
bReX@3b
]X@#bQ}H
bReX@{
bB]X@C
DbQ}X
#bQ}X
breHQ
breHQ
bbeHQ
;ba=@
;ba~H
0bQ~I
bReX@3bQ}H
bReX@{
"bQ}X
3bQ}X
breHQ
breHQ
~HoL$
~HoD$
bReX@3b
]X@#bBUX@
bReX@{
bBUX@[
bBUX@c
bBUX@k
bB]X@C
bBUX@s
fbQ}X
#bAmX
bbeHQ
breHQ
bbeHQ
breHQ
IbbeHQ
bbeHQ
bbeHQ
bbeHQ
u!bA-@
$@ba~H
+ba~H
~HoD$
bReX@3b
]X@#bQ}H
bReX@{
bB]X@C
bB]X@K
RbQ}X
#bQ}X
breHQ
breHQ
bbeHQ
KbbeHQ
~HoT2
breHQ
breHQ
}HX\I
bbeHQ
}HX\K
bbeHQ
~HoT2
breHQ
breHQ
}HX\I
bbeHQ
}HX\K
bbeHQ
~HoT2
breHQ
breHQ
}HX\I
bbeHQ
}HX\K
bbeHQ
breHQ
breHQ
bbeHQ
KbbeHQ
~HoT2
}HX\I
}HX\K
~HoT2
}HX\I
}HX\K
~HoT2
}HX\I
}HX\K
;ba=@
;ba~H
CbQ~I
0bQ~I
bReX@3bQ}H
bReX@{
)bQ}X
3bQ}X
breHQ
breHQ
breHQ
breHQ
}HX\I
}HX\K
breHQ
breHQ
}HX\I
}HX\K
breHQ
breHQ
}HX\I
}HX\K
breHQ
breHQ
}HX\I
}HX\K
}HX\I
}HX\K
}HX\I
}HX\K
~HoL$
~HoD$
bReX@3b
]X@#bBUX@
bReX@{
bBUX@[
bBUX@c
bBUX@k
bB]X@C
bBUX@s
bB]X@K
bBUX@{
{bQ}X
#bAmX
bbeHQ
breHQ
bbeHQ
breHQ
IbbeHQ
bbeHQ
bbeHQ
bbeHQ
KbbeHQ
bbeHQ
~HoL2
~HoTr
bbeHQ
breHQ
bbeHQ
breHQ
}HX\I
bbeHQ
bbeHQ
bbeHQ
bbeHQ
}HX\K
bbeHQ
bbeHQ
~HoL2
~HoTr
bbeHQ
breHQ
bbeHQ
breHQ
}HX\I
bbeHQ
bbeHQ
bbeHQ
bbeHQ
}HX\K
bbeHQ
bbeHQ
~HoL2
~HoTr
bbeHQ
breHQ
bbeHQ
breHQ
}HX\I
bbeHQ
bbeHQ
bbeHQ
bbeHQ
}HX\K
bbeHQ
bbeHQ
bbeHQ
breHQ
bbeHQ
breHQ
IbbeHQ
bbeHQ
bbeHQ
bbeHQ
KbbeHQ
bbeHQ
~HoL2
~HoTr
}HX\I
}HX\K
~HoL2
~HoTr
}HX\I
}HX\K
~HoL2
~HoTr
}HX\I
}HX\K
u(bA-@
<CbA~H
$@ba~H
+ba~H
(|$ H
0A^A]A\_^[]
HUSVH
)D$ H
A|U@ 
(D$ H
HUSVH
HUSVWAWAVAUATH
)D$ D
)L$0D
)T$@D
)\$PD
)d$`D
)l$pD
rbb}H
$IbB}H
<Nba|H
rbb}H
$Iba|H
rbb}H
rbb}H
$IbB}H
<Nba|H
rbb}H
$Iba|H
rbb}H
$IbB}H
<Nba|H
$Iba|H
IbR=P
(D$ D
(L$0D
(T$@D
(\$PD
(d$`D
(l$pD
A\A]A^A__^[]
fffffff
rbb}H
rbb}H
HUSVWAWAVAUATH
)D$ D
)L$0D
)T$@D
)\$PD
)d$`D
)l$pD
rbb}H
$IbB}H
<Nba|H
rbb}H
$Iba|H
rbb}H
rbb}H
$IbB}H
<Nba|H
rbb}H
$Iba|H
rbb}H
$IbB}H
<Nba|H
$Iba|H
IbR=P
$Nba|H
Iba|H
(D$ D
(L$0D
(T$@D
(\$PD
(d$`D
(l$pD
A\A]A^A__^[]
2bb}H
rbb}H
rbb}H
3bb}H
HUSVWAWAVAUATH
)D$ D
)L$0D
)T$@D
)\$PD
)d$`D
)l$pD
IbR=@
(D$ D
(L$0D
(T$@D
(\$PD
(d$`D
(l$pD
A\A]A^A__^[]
HUSVWAVATH
)D$ D
)L$0D
)T$@D
)\$PD
)d$`D
)l$pD
rbb}H
$IbB}H
<Nba|H
rbb}H
$Iba|H
rbb}H
rbb}H
3bb}H
rbb}H
$IbB}H
<Nba|H
rbb}H
$Iba|H
rbb}H
rbb}H
$IbB}H
<Nba|H
$Iba|H
2bb}H
IbR=P
$Nba|H
Iba|H
(D$ D
(L$0D
(T$@D
(\$PD
(d$`D
(l$pD
A\A^_^[]
bQ<HX@
bQ<@_
bQ<HX@
t*ba|H
bQ<@_
bQ<@_
fffff
fffff
t%ba|H
!ba|H
bQ<HX@
bQ4HXL
t1ba|H
!ba|H
bQ<@_
bQ<@_
bQ<HX@
bQ4HXL
tUba|H
!ba|H
bQ<@_
bQ<@_
bQ<@_
bQ<@_
bQ<@_
bQ<@_
t8ba|H
!ba|H
ffffff
bQ<HX@
bQ4HXL
bq,HXS
tJba|H
!ba|H
bQ<@_
bQ<@_
bQ<@_
bQ<HX@
bQ4HXL
bq,HXS
!ba|H
bQ<@_
bQ<@_
bQ<@_
bQ<@_
bQ<@_
bQ<@_
bQ<@_
fffff
tKba|H
!ba|H
bQ<HX@
bQ4HXL
bq,HXS
bq$HX\
tcba|H
!ba|H
bQ<@_
bQ<@_
bQ<@_
bQ<@_
bQ<HX@
bQ4HXL
bq,HXS
bq$HX\
!ba|H
bQ<@_
bQ<@_
bQ<@_
bQ<@_
bQ<@_
bQ<@_
bQ<@_
bQ<@_
HUSVWAWH
)D$ D
)L$0D
)T$@D
)\$PD
)d$`D
)l$pD
(D$ D
(L$0D
(T$@D
(\$PD
(d$`D
(l$pD
A__^[]
HUSVWAWH
)D$ D
)L$0D
)T$@D
)\$PD
)d$`D
)l$pD
b},4C
b}.<C
(D$ D
(L$0D
(T$@D
(\$PD
(d$`D
(l$pD
A__^[]
b"-@,
HUSVWH
)D$ D
)L$0D
)T$@H
(D$ D
(L$0D
(T$@H
X_^[]
HUSVWAWAVAUATH
)D$ D
)L$0D
)T$@D
)\$PD
)d$`D
)l$pD
(D$ D
(L$0D
(T$@D
(\$PD
(d$`D
(l$pD
A\A]A^A__^[]
HUSVWAWAVAUATH
)D$ D
)L$0D
)T$@D
)\$PD
)d$`D
)l$pD
(D$ D
(L$0D
(T$@D
(\$PD
(d$`D
(l$pD
A\A]A^A__^[]
HUSVWAWAVAUATH
)D$ D
)L$0D
)T$@D
)\$PD
)d$`D
)l$pD
(D$ D
(L$0D
(T$@D
(\$PD
(d$`D
(l$pD
A\A]A^A__^[]
HUSVWAVATH
)D$ D
)L$0D
)T$@D
)\$PD
)d$`D
)l$pD
(D$ D
(L$0D
(T$@D
(\$PD
(d$`D
(l$pD
A\A^_^[]
fffffff
HUSVWAWH
)D$ D
)L$0D
)T$@D
)\$PD
)d$`D
)l$pD
A4XH 
(D$ D
(L$0D
(T$@D
(\$PD
(d$`D
(l$pD
A__^[]
A4XH 
A4XH 
HUSVWAWAVAUATH
)D$ D
)L$0D
)T$@D
)\$PD
)d$`D
)l$pD
(D$ D
(L$0D
(T$@D
(\$PD
(d$`D
(l$pD
A\A]A^A__^[]
fffff
ffffff
HUSVWAWAVAUATH
)D$ D
)L$0D
)T$@D
)\$PD
)d$`D
)l$pD
(D$ D
(L$0D
(T$@D
(\$PD
(d$`D
(l$pD
A\A]A^A__^[]
fffffff
HUSVWAWAVAUATH
)D$ D
)L$0D
)T$@D
)\$PD
)d$`D
)l$pD
(D$ D
(L$0D
(T$@D
(\$PD
(d$`D
(l$pD
A\A]A^A__^[]
HUSVWAVATH
)D$ D
)L$0D
)T$@D
)\$PD
)d$`D
)l$pD
(D$ D
(L$0D
(T$@D
(\$PD
(d$`D
(l$pD
A\A^_^[]
HUSVWAWH
)D$ D
)L$0D
)T$@D
)\$PD
)d$`D
)l$pD
ba|H(
ba|H(
ba|H(
ba|H(
QbreH
SbreH
VbbeH
|H(L"
|H(L"
QbreH
SbreH
VbbeH
@bq|H
6bA|H
@bq|I
>bA|I
ba|H(
ba|H(
Qbr}X
Qbr}X
|H(L"
|H(L"
QbreH
|H(L"
|H(L"
QbreH
@bq|H
@bq|I
Qbr}X
Qbr}X
QbreH
SbreH
|H(L"
|H(L"
QbreH
SbreH
@bq|H
@bq|I
Qbr}X
Qbr}X
(D$ D
(L$0D
(T$@D
(\$PD
(d$`D
(l$pD
A__^[]
QbreH
|H(L"
|H(L"
QbreH
QbreH
|H(L"
|H(L"
QbreH
@bq|H
@bq|I
Qbr}X
Qbr}X
HUSVWAVAUATH
wA\A]A^_^[]
fffffff
HUSVWAVAUATH
wA\A]A^_^[]
fffff
HUSVWAVAUATH
*l$hb
wA\A]A^_^[]
HUSVWH
HUSVWAWH
)D$ D
)L$0D
)T$@D
)\$PD
)d$`D
)l$pD
b}-4C
b}/<C
(D$ D
(L$0D
(T$@D
(\$PD
(d$`D
(l$pD
A__^[]
HUSVWH
bb}HX'H
w_^[]
bb}HX
HUSVWH
bb}H{
bb}HX'H
w_^[]
bb}HX
HUSVWAWH
)D$ D
)L$0D
)T$@D
)\$PD
)d$`D
)l$pD
(D$ D
(L$0D
(T$@D
(\$PD
(d$`D
(l$pD
A__^[]
HUSVWAWH
)D$ D
)L$0D
)T$@D
)\$PD
)d$`D
)l$pD
A5XH 
(D$ D
(L$0D
(T$@D
(\$PD
(d$`D
(l$pD
A__^[]
A5XH 
A5XH 
HUSVWATAUH
)|$PD
)D$`D
)L$pD
o|$0J
(|$PD
(D$`D
(L$pD
A]A\_^[]
HUSVWH
)|$PD
)D$`D
)L$pH
(|$PD
(D$`D
(L$pH
HUSVWATAUH
)|$PD
)D$`D
)L$pH
}0t$ 
}0|$0J
(|$PD
(D$`D
(L$pH
A]A\_^[]
HUSVH
oD$ f
oL$0f
oT$@f
o\$PH
oD$ f
oL$0f
oT$@f
o\$PH
D$DDtRH
|$ UATAUAVAWH
t'HcG<
M09L8
H;|80u
A_A^A]A\]
oD$ f
oL$0f
oT$@f
o\$PH
SVWAVH
8A^_^[
WAVAWH
\$8H;
A_A^_
WAVAWH
0A_A^_
x AVH
A:8ucI
t&A88t
oD$ f
oL$0f
oT$@f
o\$PH
oD$ f
oL$0f
oT$@f
o\$PH
x AVH
SUVWH
(_^][
@UAUH
@UAUH
T;8H+
api-ms-win-core-synch-l1-2-0.dll
kernel32.dll
SleepConditionVariableCS
WakeAllConditionVariable
bad exception
8d3d12.dll
dxgi.dll
KERNEL32.DLL
AcquireSRWLockExclusive
ReleaseSRWLockExclusive
DirectML.dll
OLEAUT32.dll
api-ms-win-core-com-l1-1-0.dll
Unknown exception
bad array new length
string too long
D:\a\_work\1\s\engine\lotus\cmake\external\protobuf\src\google/protobuf/parse_context.h
(cannot determine missing fields for lite message)
D:\a\_work\1\s\engine\lotus\cmake\external\protobuf\src\google\protobuf\message_lite.cc
Can't 
 message of type "
" because it is missing required fields: 
parse
 exceeded maximum protobuf size of 2GB: 
vector too long
D:\a\_work\1\s\engine\lotus\cmake\external\protobuf\src\google\protobuf\io\zero_copy_stream.cc
This ZeroCopyOutputStream doesn't support aliasing. Reaching here usually means a ZeroCopyOutputStream implementation bug.
iostream
bad cast
bad locale name
ios_base::badbit set
ios_base::failbit set
ios_base::eofbit set
D:\a\_work\1\s\engine\lotus\cmake\external\protobuf\src\google\protobuf\io\zero_copy_stream_impl.cc
close() failed: 
CHECK failed: !is_closed_: 
iostream stream error
D:\a\_work\1\s\engine\lotus\cmake\external\protobuf\src\google\protobuf\io\zero_copy_stream_impl_lite.cc
CHECK failed: (count) >= (0): 
CHECK failed: backup_bytes_ == 0 && buffer_.get() != NULL: 
 BackUp() can only be called after Next().
CHECK failed: (count) <= (buffer_used_): 
 Can't back up over more bytes than were returned by the last call to Next().
 Parameter to BackUp() can't be negative.
CHECK failed: (backup_bytes_) == (0): 
CHECK failed: (buffer_used_) == (buffer_size_): 
WARNING
ERROR
FATAL
[libprotobuf %s %s:%d] %s
%I64u
Can't happen
D:\a\_work\1\s\engine\lotus\cmake\external\protobuf\src\google\protobuf\arena.cc
CHECK failed: (min_bytes) <= (std::numeric_limits<size_t>::max() - SerialArena::kBlockHeaderSize): 
false
\x%02x
invalid string position
INVALID_ARGUMENT
D:\a\_work\1\s\engine\lotus\cmake\external\protobuf\src\google/protobuf/repeated_field.h
CHECK failed: (index) < (current_size_): 
map/set too long
unordered_map/set too long
invalid hash bucket count
generic
device or resource busy
invalid argument
no such process
not enough memory
operation not permitted
resource deadlock would occur
resource unavailable try again
bad allocation
bad function call
address family not supported
address in use
address not available
already connected
argument list too long
argument out of domain
bad address
bad file descriptor
bad message
broken pipe
connection aborted
connection already in progress
connection refused
connection reset
cross device link
destination address required
directory not empty
executable format error
file exists
file too large
filename too long
function not supported
host unreachable
identifier removed
illegal byte sequence
inappropriate io control operation
interrupted
invalid seek
io error
is a directory
message size
network down
network reset
network unreachable
no buffer space
no child process
no link
no lock available
no message available
no message
no protocol option
no space on device
no stream resources
no such device or address
no such device
no such file or directory
not a directory
not a socket
not a stream
not connected
not supported
operation canceled
operation in progress
operation not supported
operation would block
owner dead
permission denied
protocol error
protocol not supported
read only file system
result out of range
state not recoverable
stream timeout
text file busy
timed out
too many files open in system
too many files open
too many links
too many symbolic link levels
value too large
wrong protocol type
unknown error
0123456789-+Ee
0123456789ABCDEFabcdef-+XxPp
0123456789ABCDEFabcdef-+Xx
0123456789abcdefghijklmnopqrstuvwxyz
:Sun:Sunday:Mon:Monday:Tue:Tuesday:Wed:Wednesday:Thu:Thursday:Fri:Friday:Sat:Saturday
:Jan:January:Feb:February:Mar:March:Apr:April:May:May:Jun:June:Jul:July:Aug:August:Sep:September:Oct:October:Nov:November:Dec:December
:Sun:Sunday:Mon:Monday:Tue:Tuesday:Wed:Wednesday:Thu:Thursday:Fri:Friday:Sat:Saturday
:Jan:January:Feb:February:Mar:March:Apr:April:May:May:Jun:June:Jul:July:Aug:August:Sep:September:Oct:October:Nov:November:Dec:December
%b %d %H : %M : %S %Y
%m / %d / %y
:AM:am:PM:pm
%I : %M : %S %p
%H : %M
%H : %M : %S
%d / %m / %y
%.0Lf
+v$x+v$xv$+xv+$xv$+x+$vx+$vx$v+x+$vx$+vx+v $+v $v $+v +$v $++$ v+$ v$ v++$ v$+ v+xv$+ v$v$ +v+ $v$ ++x$v+ $v$v ++ $v$ +v
:AM:am:PM:pm
0123456789-
0123456789abcdefghijklmnopqrstuvwxyz
 was false.
Stacktrace:
ai.onnx.ml
ai.onnx.training
ai.onnx.preview.training
VIWEF
D:\a\_work\1\s\engine\lotus\include\onnxruntime\core/framework/data_types_internal.h
D:\a\_work\1\s\engine\lotus\include\onnxruntime\core/framework/tensor.h
CPUExecutionProvider
Tensor sequence must contain only primitive types
elem_type_ != nullptr
onnxruntime::TensorSeq::SetType
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/TensorSeq.h
i < tensors_.size()
onnxruntime::TensorSeq::Get
Trying to get a Tensor, but got: 
IsTensor()
OrtValue::Get
D:\a\_work\1\s\engine\lotus\include\onnxruntime\core/framework/ort_value.h
OrtValue::GetMutable
Trying to get a TensorSeq, but got: 
IsTensorSequence()
Trying to get a SparseTensor, but got: 
IsSparseTensor()
ORT_LOAD_CONFIG_FROM_MODEL
Integer overflow
SafeIntExceptionHandler<class onnxruntime::OnnxRuntimeException>::SafeIntOnOverflow
D:\a\_work\1\s\engine\lotus\onnxruntime\core/common/safeint.h
tried creating tensor with negative value in shape
size overflow
, got 
not enough space: expected 
Not able to find appropriate IDataTransfer to copy sparse data
`anonymous-namespace'::GetDataTransfer
D:\a\_work\1\s\engine\lotus\onnxruntime\core\session\onnxruntime_c_api.cc
Strings can only reside in CPU memory
`anonymous-namespace'::ValidateFillInputArgs
tried Filling sparse tensor with negative value in values shape
OrtApis::FillSparseTensorCoo
OrtApis::FillSparseTensorCsr
tried Filling sparse tensor with negative value in block sparse indices shape
OrtApis::FillSparseTensorBlockSparse
Can not use strings in pre-allocated memory. Use CreateSparseTensorAsOrtValue() to allocate memory inside and copy
OrtApis::UseCooIndices
OrtApis::UseCsrIndices
OrtApis::UseBlockSparseIndices
the ort_value must contain a constructed tensor
Use GetStringTensor*() API to retrieve strings
RegisterCustomOpsLibrary: Failed to load library
RegisterCustomOps
RegisterCustomOpsLibrary: Entry point RegisterCustomOps not found in library
EnableOrtCustomOps: Custom operators in onnxruntime-extensions are not enabled
onnxruntime_profile_
input name cannot be empty
output name cannot be empty
lengths allocation failed
string buffer allocation failed
Output buffer allocation failed
input array doesn't equal tensor size
element index is out of bounds
OrtValue should contain a Tensor or a Sparse Tensor
Sparse Tensor does not contain sparse data
This API supports Tensors or SparseTensors
shape is invalid
index is out of bounds
offsets buffer is not equal to tensor size
output buffer is too small. Use GetStringTensorDataLength.
buffer size is too small for string element
out of index
internal error
index out of range
Input is not of one of the supported sequence types.
Input is not of type sequence or map.
input array is too short
Input is not of one of the supported map types.
Expecting all elements to be tensors. Got: 
in[idx]->IsTensor()
OrtCreateValueImplSeqHelper
Sequences must have tensors of the same data type. There was at least one tensor in the input that was different.
Each element of the sequence should be either tensor or map.
At least one element in the sequence is of a type different from others.
Unsupported input type
For map type num_values MUST be 2
Either the key tensor or the value tensor has NumDimensions > 1
Key and value tensors have unequal number of elements.
Key type is not supported yet.
Number of values should be at least 1.
opaque(
Specified domain and type names combination does not refer to a registered opaque type
ml_type != nullptr
OrtApis::CreateOpaqueValue
Opaque type is not a non_tensor type!!!
non_tensor_base != nullptr
OrtApis::GetOpaqueValue
Specified provider is not supported.
this API does not support strings
location dimensions do not match shape size
invalid location range
max_mem
arena_extend_strategy
initial_chunk_size_bytes
max_dead_bytes_per_chunk
initial_growth_chunk_size_bytes
Invalid key found: 
The given version [%u] is not supported, only version 1 to %u is supported in this build.
1.10.0
Tensor type mismatch. 
utils::IsPrimitiveDataType<T>(dtype_)
onnxruntime::Tensor::MutableData
onnxruntime::Tensor::DataAsSpan
Invalid index requested for map type.
Tensor must always contain primitive types. Found: 
value_type != nullptr
OrtCreateValueImplMapHelper
Value type is not supported yet: 
Map is missing type entry for its value
++index < c.size()
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<__int64,double,struct std::less<__int64>,class std::allocator<struct std::pair<__int64 const ,double> > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<__int64,float,struct std::less<__int64>,class std::allocator<struct std::pair<__int64 const ,float> > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<__int64,__int64,struct std::less<__int64>,class std::allocator<struct std::pair<__int64 const ,__int64> > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<__int64,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,struct std::less<__int64>,class std::allocator<struct std::pair<__int64 const ,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,double,struct std::less<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,double> > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,float,struct std::less<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,float> > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,__int64,struct std::less<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,__int64> > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,struct std::less<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > > > >::check
Sequence is missing type entry for its element
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::vector<class std::map<__int64,float,struct std::less<__int64>,class std::allocator<struct std::pair<__int64 const ,float> > >,class std::allocator<class std::map<__int64,float,struct std::less<__int64>,class std::allocator<struct std::pair<__int64 const ,float> > > > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::vector<class std::map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,float,struct std::less<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,float> > >,class std::allocator<class std::map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,float,struct std::less<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,float> > > > > >::check
invalid vector subscript
onnxruntime::DataTypeImpl::GetType<T>() == type_
len >= 0 && static_cast<uint64_t>(len) < std::numeric_limits<size_t>::max()
OrtCreateMapMLValue
onnxruntime::Tensor::Data
not implemented
execution_mode is not valid
graph_optimization_level is not valid
 is not implemented
ai.onnx
onnxruntime::OpKernel::ComputeAsync
' in custom op '
Result buffer is not large enough
onnxruntime::CustomOpKernel::CustomOpKernel
Unsupported version '
custom op registered at runtime
D:\a\_work\1\s\engine\lotus\onnxruntime\core\session\custom_ops.cc
Input
There must be one (and only one) dynamic typed input to the custom op. Its type info at runtime will be used to infer the type info of this dynamic typed output which is required for the success of the model loading step. More than one dynamic typed inputs are currently not supported as differing types at runtime means the output type cannot be inferred without which model loading cannot proceed.
all types
onnxruntime::CreateCustomRegistry
type_id_counter == 1
Output
onnxruntime::logging::LoggingManager::DefaultLogger
Attempt to use DefaultLogger but none has been registered.
D:\a\_work\1\s\engine\lotus\include\onnxruntime\core/common/logging/logging.h
Tensor type mismatch.
onnxruntime::Tensor::MutableDataRaw
type == dtype_
) != new size (
onnxruntime::Tensor::DataRaw
shape_.Size() == new_shape.Size()
Tensor size (
onnxruntime::Tensor::Reshape
IsSameDataType(tensor)
TensorSeq: tensor to be added has a different data type.
onnxruntime::TensorSeq::Add
Required output at index 
 is not present.
onnxruntime::OpKernelContext::RequiredOutput
output_ptr
Please fetch output tensor with specified shape.
D:\a\_work\1\s\engine\lotus\include\onnxruntime\core/framework/op_kernel_context.h
onnxruntime::OpKernelContext::Output
p_ml_value
onnxruntime_providers_shared.dll
D:\a\_work\1\s\engine\lotus\onnxruntime\core\session\provider_bridge_ort.cc
onnxruntime::ProviderSharedLibrary::Ensure
onnxruntime::ProviderSharedLibrary::Unload
Provider_SetHost
GetProvider
onnxruntime::ProviderLibrary::Get
onnxruntime_providers_cuda.dll
onnxruntime::ProviderLibrary::Unload
onnxruntime_providers_dnnl.dll
onnxruntime_providers_rocm.dll
onnxruntime_providers_tensorrt.dll
onnxruntime_providers_openvino.dll
SessionOptionsAppendExecutionProvider_OpenVINO: Failed to load shared library
SessionOptionsAppendExecutionProvider_Tensorrt: Failed to load shared library
OrtSessionOptionsAppendExecutionProvider_Cuda: Failed to load shared library
CUDA and/or ROCM execution provider is either not enabled or not available.
TensorRT execution provider is not enabled in this build.
OrtSessionOptionsAppendExecutionProvider_Rocm: Failed to load shared library
invalid unordered_map<K, T> key
onnxruntime::OpKernelContext::Input
Missing Input: 
input_ptr
Required input at index 
onnxruntime::OpKernelContext::RequiredInput
OrtMemoryInfo is null
Provided allocator is null
No requested allocator available
Env is null
Please register the allocator as OrtDeviceAllocator even if the provided allocator has arena logic built-in. OrtArenaAllocator is reserved for internal arena logic based allocators only.
Provided OrtMemoryInfo is null
env_ptr == p_instance_
OrtEnv::Release
D:\a\_work\1\s\engine\lotus\onnxruntime\core\session\ort_env.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\session\IOBinding.cc
onnxruntime::SyncProviders
onnxruntime::IOBinding::BindInput
p_provider
onnxruntime::IOBinding::SynchronizeInputs
onnxruntime::IOBinding::SynchronizeOutputs
 DeviceId:
 MemoryType:
 OrtAllocatorType:
 OrtMemType:
DeviceType:
Device:[
OrtMemoryInfo:[
name:
CUDAExecutionProvider
TensorrtExecutionProvider
DmlExecutionProvider
D:\a\_work\1\s\engine\lotus\include\onnxruntime\core/graph/graph.h
Validating no unexpected access using an invalid node_index. Got:
node_index < nodes_.size()
 Max:
 has already been registered.
Provider 
onnxruntime::Graph::NodeAtIndexImpl
onnxruntime::ExecutionProviders::Add
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/execution_providers.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/feeds_fetches_manager.h
Compute_
onnxruntime::FeedsFetchesInfo::FeedsFetchesInfo
Create_State_
Release_State_
IsOptionalTensor(type)
onnxruntime::utils::GetElementTypeFromOptionalTensor
Provided type is not an optional tensor
IsOptionalSeqTensor(type)
onnxruntime::utils::GetElementTypeFromOptionalSeqTensor
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/mldata_type_utils.h
Provided type is not an optional sequence tensor
MemcpyTransformer
[json.exception.
other_error
session.use_env_allocators
session.load_model_format
session.save_model_format
session.set_denormal_as_zero
session.intra_op.allow_spinning
session.use_ort_model_bytes_directly
session.inter_op.allow_spinning
memory.enable_memory_arena_shrinkage
optimization.save_runtime_optimizations
Could not find an implementation for 
%Y-%m-%d_%H-%M-%S
) node with name '
onnxruntime::`anonymous-namespace'::VerifyEachNodeIsAssignedToAnEp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\session\inference_session.cc
Node placements
 Provider: [
All nodes have been placed on [
The only supported values for the environment variable 
Reading the provided model for the ORT config
The environment variable contained the value: 
 are '0' and '1'. 
is_model_proto_parsed
onnxruntime::FinalizeSessionOptions
ModelProto needs to be parsed to check for ORT config within it
onnxruntime::InferenceSession::ConstructorCommon::<lambda_7cec36099d1ae29ef2b3cbe41ba5b9a0>::operator ()
Could not finalize session options while constructing the inference session. Error Message: 
Flush-to-zero and denormal-as-zero are 
Creating and using per session threadpools since use_per_session_threads_ is true
status.IsOK()
onnxruntime::InferenceSession::ConstructorCommon
custom join thread function not set for intra op thread pool
to.custom_join_thread_fn
-intra-op
session-
Failed to create the inter-op thread pool for the parallel executor, setting ExecutionMode to SEQUENTIAL
Using global/env threadpools since use_per_session_threads_ is false
-inter-op
custom join thread function not set for inter op thread pool
CastFloat16Transformer
Given model could not be parsed while creating inference session. Error message: 
When the session is not configured to use per session threadpools, the env must be created with the the CreateEnvWithGlobalThreadPools API.
session_env.EnvCreatedWithGlobalThreadPools()
result
onnxruntime::InferenceSession::InferenceSession
Could not parse model successfully while constructing the inference session
Unknown error during EndProfiling()
onnxruntime::InferenceSession::~InferenceSession
Error during EndProfiling(): 
onnxruntime::InferenceSession::{dtor}::<lambda_117bb6d1dfafcab7bc48c4431153f278>::operator ()
onnxruntime::InferenceSession::RegisterExecutionProvider
Execution providers must be registered before the session is initialized.
Received nullptr for exec provider
Execution providers must be registered before the session is initialized. 
So making the execution mode sequential for this session since it uses the DML Execution Provider.
Parallel execution mode does not support the DML Execution Provider. 
So disabling it for this session since it uses the DML Execution Provider.
Having memory pattern enabled is not supported while using the DML Execution Provider. 
onnxruntime::InferenceSession::AddCustomOpDomains
Received nullptr for custom registry
So making the execution mode sequential for this session since it uses the CUDA Execution Provider.
Parallel execution mode does not support the CUDA Execution Provider. 
onnxruntime::InferenceSession::RegisterGraphTransformer
Received nullptr for graph transformer
Graph transformers must be registered before the session is initialized.
This session already contains a loaded model.
onnxruntime::InferenceSession::Load
onnxruntime::InferenceSession::SaveToOrtFormat
Exception during loading: 
ModelProto corresponding to the model to be loaded has already been parsed. Invoke Load().
Unknown exception in Load()
Encountered unknown exception in Load()
model_loading_proto
Failed to load model because protobuf parsing failed.
model_loading_array
onnxruntime::InferenceSession::TransformGraph
ModelProto corresponding to the model to be loaded has not been parsed yet. This API should be called in conjunction with a ctor that takes a model abstraction.
model_loading_from_saved_proto
This session has already been initialized.
ORT model verification failed.
onnxruntime::InferenceSession::LoadOrtModel::<lambda_564e08730d4bd4a97b43e8c38a06cf59>::operator ()
onnxruntime::InferenceSession::LoadOrtModel
] is not supported this build 
The ORT format model version [
InferenceSession is null. Invalid ORT format model.
Serialized version info is null. Invalid ORT format model.
The session already has a PrePackedWeightsContainer instance
Missing Model. Invalid ORT format model.
SessionState is null. Invalid ORT format model.
) in kernel registries for 
onnxruntime::`anonymous-namespace'::PartitionOrtFormatModel
onnxruntime::`anonymous-namespace'::AssignNodesToEpsFromHashesImpl
Exception during initialization: 
onnxruntime::InferenceSession::Initialize::<lambda_b4dbd8b7631db100bde145756917a967>::operator ()
Failed to find kernel def hash (
onnxruntime::`anonymous-namespace'::AssignNodesToEpsFromHashes
onnxruntime::InferenceSession::Initialize
Model was not loaded
onnxruntime::InferenceSession::Initialize::<lambda_4f6bc740749357f1ee569964a854a405>::operator ()
Initializing session.
Adding default CPU execution provider.
This session will use the allocator registered with the environment.
Model was not loaded.
Session has already been initialized.
NchwcTransformer
Serializing optimized model with Graph Optimization level greater than ORT_ENABLE_EXTENDED and the NchwcTransformer enabled. The generated model may contain hardware specific optimizations, and should only be used in the same environment the model was optimized in.
loading_ort_format && serialized_session_state != nullptr
Unable to serialize model as it contains compiled nodes. Please disable any execution providers which generate compiled nodes.
session_initialization
 Please fix either the inputs or the model.
Session successfully initialized.
Encountered unknown exception in Initialize()
Invalid rank for input: 
 for the following indices
 Expected: 
 Got: 
Unexpected input data type. Actual: (
)) , expected: (
Got invalid dimensions for input: 
 index: 
elements, but feeds has 
Size mismatch: feed_names has 
 elements.
Input with name: 
tensor
Invalid Feed Input Name:
 is not expected to be of type tensor.
sparse_tensor
 is not expected to be of type tensor sequence.
onnxruntime::InferenceSession::ValidateInputs
 is not expected to be of type sparse tensor.
At least one output should be requested.
p_fetches->size(): 
Output vector pointer is NULL
Session was not initialized
onnxruntime::InferenceSession::Run
Output vector incorrectly sized: output_names.size(): 
Invalid Output Name:
Encountered unknown exception in Run()
model_run
Session not initialized.
Running with tag: 
onnxruntime::InferenceSession::GetOverridableInitializers
onnxruntime::InferenceSession::GetModelOutputs
onnxruntime::InferenceSession::GetModelMetadata
onnxruntime::InferenceSession::GetModelInputs
onnxruntime::InferenceSession::EndProfiling
Could not write a profile because no model was loaded.
onnxruntime::InferenceSession::NewIOBinding
Profiler is disabled.
Unsupported device specified in the memory arena shrink list: 
Unsupported device id in the memory arena shrink list: 
 combination is not an arena based allocator: 
The registered allocator for device-id 
 combination in the memory arena shrink list: 
Did not find an arena based allocator registered for device-id 
onnxruntime::InferenceSession::ShrinkMemoryArenas
Invalid run log severity level. Not a valid onnxruntime::logging::Severity value: 
 error message: 
Unable to shrink arena: 
Invalid session log severity level. Not a valid onnxruntime::logging::Severity value: 
session_options_.session_log_severity_level >= 0 && session_options_.session_log_severity_level <= static_cast<int>(logging::Severity::kFATAL)
run_options.run_log_severity_level >= 0 && run_options.run_log_severity_level <= static_cast<int>(logging::Severity::kFATAL)
onnxruntime::InferenceSession::CreateLoggerForRun
onnxruntime::InferenceSession::InitLogger
onnxruntime::InferenceSession::AddPredefinedTransformers
961c151d2e87f2686a955a9be24d316f1362bf21 3.9.1
model_loading_uri
 failed:
.json
 bytes were able to be read.
 failed. Only 
Load model from 
onnxruntime::LoadOrtModelBytes
list too long
localtime_s(&local_tm, &in_time_t) == 0
onnxruntime::`anonymous-namespace'::GetCurrentTimeString
com.microsoft
com.microsoft.dml
com.microsoft.nchwc
com.microsoft.experimental
Input 
 expected to have tensor or sparse tensor type. Got: 
[TypeInferenceError] 
Input type was null
 expected to have sequence type
Output 
Element type of input 
 unknown
 expected to have type but instead is null
Element type of optional input 
 expected to have optional type
Element type of sequence input 
 Target=
Unsupported Source/Target type=
[ShapeInferenceError] 
Mismatch between source and target type. Source=
unknown
tensor(int64)
tensor(int32)
tensor(uint64)
tensor(uint32)
tensor(bfloat16)
tensor(double)
tensor(float)
tensor(float16)
tensor(int16)
tensor(int8)
tensor(uint16)
tensor(uint8)
seq(tensor(uint64))
seq(tensor(uint32))
seq(tensor(uint16))
seq(tensor(uint8))
seq(tensor(int64))
seq(tensor(int32))
seq(tensor(int16))
seq(tensor(int8))
tensor(string)
seq(tensor(double))
seq(tensor(float))
seq(tensor(float16))
seq(tensor(string))
tensor(complex128)
tensor(complex64)
tensor(bool)
seq(tensor(complex128))
seq(tensor(complex64))
seq(tensor(bool))
, but it is already registered from file 
 line 
Schema error: 
Trying to register schema with name 
 (domain: 
 version: 
) from file 
forgot to update the version range in DomainToVersionRange 
in onnx/defs/schema.h).
, but its domain is not
 known by the checker.
, but its version is not 
in the inclusive range [
] (usually, this means you 
bumped the operator version but 
Only CPU allocators can be shared between multiple sessions for now.
No allocator for this device has been registered for sharing.
Received invalid value for arena extend strategy. Valid values can be either 0, 1 or -1.
Only CPU devices are supported for now.
An allocator for this device has already been registered for sharing.
Exception caught: 
seq(tensor(bfloat16))
string
MemcpyFromHost
D:\a\_work\1\s\engine\lotus\onnxruntime\core\session\environment.cc
inter-op
intra-op
output
input
MemcpyToHost
Constrain to all fixed size tensor and sequence types. If the dtype attribute is not provided this must be a valid output type.
value
parse error
 at line 
, column 
parse_error
ort_config
out_of_range
type_error
invalid_iterator
D:\a\_work\1\s\engine\lotus\onnxruntime\core\session\inference_session_utils.cc
onnxruntime::SetIntraOpNumThreads
Unsupported value for intra_op_num_threads: 
session_options
Setting inter_op_num_threads to 
onnxruntime::SetInterOpNumThreads
Unsupported value for inter_op_num_threads: 
Setting intra_op_num_threads to 
Parallel mode
Sequential mode
onnxruntime::SetExecutionMode
Unsupported execution_mode value in ORT config: 
Setting graph_optimization_level to ORT_ENABLE_BASIC
onnxruntime::SetGraphOptimizationLevel
Setting graph_optimization_level to ORT_DISABLE_ALL
Setting execution_mode to 
Unsupported value for enable_profiling option: 
Unsupported graph_optimization_level value in ORT config: 
Setting graph_optimization_level to ORT_ENABLE_ALL
Setting graph_optimization_level to ORT_ENABLE_EXTENDED
onnxruntime::inference_session_utils::JsonConfigParser::ParseOrtConfigJsonInModelProto::<lambda_f5703e30d66339144972d370855935b6>::operator ()
Json stored in the `ort_config` key cannot be parsed. Error message: 
Setting enable_profiling to 
onnxruntime::SetEnableProfiling
ORT config json from the model: 
onnxruntime::inference_session_utils::JsonConfigParser::ParseOrtConfigJsonInModelProto
Found session/run/environment configuration in the model file to be used while running the model
The Model Proto has already been checked for the ORT config json.
intra_op_num_threads
onnxruntime::inference_session_utils::JsonConfigParser::ParseSessionOptionsFromModelProto
Did not find session options in the model file to be used while running the model
The Model Proto hasn't been checked for the ORT config json.
execution_mode
inter_op_num_threads option in the model file must be an integer
inter_op_num_threads
intra_op_num_threads option in the model file must be an integer
enable_profiling
graph_optimization_level option in the model file must be an integer
graph_optimization_level
execution_mode option in the model file must be an integer
' not found
Ignoring unsupported session option in ORT config: 
enable_profiling option in the model file must be an integer
cannot use at() with 
key '
cannot use key() for non-object iterators
invalid map<K, T> key
binary
boolean
array
object
cannot compare iterators of different containers
number
discarded
cannot get value
syntax error 
unexpected 
; last read: '
while parsing 
invalid literal
invalid BOM; must be 0xEF 0xBB 0xBF if given
<U+%.4X>
; expected 
null literal
false literal
true literal
<uninitialized>
number literal
string literal
unknown token
'[', '{', or a literal
end of input
<parse error>
invalid number; expected digit after exponent sign
invalid number; expected '+', '-', or digit after exponent
invalid number; expected digit after '.'
invalid number; expected digit after '-'
invalid string: '\u' must be followed by 4 hex digits
invalid string: missing closing quote
invalid comment; expecting '/' or '*' after '/'
invalid comment; missing closing '*/'
invalid string: control character U+0000 (NUL) must be escaped to \u0000
invalid string: forbidden character after backslash
invalid string: surrogate U+DC00..U+DFFF must follow U+D800..U+DBFF
invalid string: surrogate U+D800..U+DBFF must be followed by U+DC00..U+DFFF
invalid string: control character U+0004 (EOT) must be escaped to \u0004
invalid string: control character U+0003 (ETX) must be escaped to \u0003
invalid string: control character U+0002 (STX) must be escaped to \u0002
invalid string: control character U+0001 (SOH) must be escaped to \u0001
invalid string: control character U+0008 (BS) must be escaped to \u0008 or \b
invalid string: control character U+0007 (BEL) must be escaped to \u0007
invalid string: control character U+0006 (ACK) must be escaped to \u0006
invalid string: control character U+0005 (ENQ) must be escaped to \u0005
invalid string: control character U+000C (FF) must be escaped to \u000C or \f
invalid string: control character U+000B (VT) must be escaped to \u000B
invalid string: control character U+000A (LF) must be escaped to \u000A or \n
invalid string: control character U+0009 (HT) must be escaped to \u0009 or \t
invalid string: control character U+0010 (DLE) must be escaped to \u0010
invalid string: control character U+000F (SI) must be escaped to \u000F
invalid string: control character U+000E (SO) must be escaped to \u000E
invalid string: control character U+000D (CR) must be escaped to \u000D or \r
invalid string: control character U+0014 (DC4) must be escaped to \u0014
invalid string: control character U+0013 (DC3) must be escaped to \u0013
invalid string: control character U+0012 (DC2) must be escaped to \u0012
invalid string: control character U+0011 (DC1) must be escaped to \u0011
invalid string: control character U+0018 (CAN) must be escaped to \u0018
invalid string: control character U+0017 (ETB) must be escaped to \u0017
invalid string: control character U+0016 (SYN) must be escaped to \u0016
invalid string: control character U+0015 (NAK) must be escaped to \u0015
invalid string: control character U+001C (FS) must be escaped to \u001C
invalid string: control character U+001B (ESC) must be escaped to \u001B
invalid string: control character U+001A (SUB) must be escaped to \u001A
invalid string: control character U+0019 (EM) must be escaped to \u0019
invalid string: ill-formed UTF-8 byte
invalid string: control character U+001F (US) must be escaped to \u001F
invalid string: control character U+001E (RS) must be escaped to \u001E
invalid string: control character U+001D (GS) must be escaped to \u001D
vector<bool> too long
number overflow parsing '
object separator
object key
iterator out of range
iterator does not fit current value
cannot use erase() with 
type must be number, but is 
^|*W?
ReturnHr
Exception
%hs!%p: 
%hs(%u)\%hs!%p: 
FailFast
LogHr
Msg:[%ws] 
%hs(%d) tid(%x) %08X %ws
(caller: %p) 
[%hs]
[%hs(%hs)]
CallContext:[%hs] 
RaiseFailFastException
kernelbase.dll
std::exception: %hs
D:\a\_work\1\s\engine\lotus\cmake\external\wil\include\wil\resource.h
WilError_03
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\dml_provider_factory.cc
Local\SM0:%d:%d:%hs
DML_OPERATOR_ELEMENT_WISE_ADD
DML_OPERATOR_ELEMENT_WISE_ACOS
DML_OPERATOR_ELEMENT_WISE_ABS
DML_OPERATOR_ELEMENT_WISE_IDENTITY
DML_OPERATOR_ELEMENT_WISE_CLIP
DML_OPERATOR_ELEMENT_WISE_CEIL
DML_OPERATOR_ELEMENT_WISE_ATAN
DML_OPERATOR_ELEMENT_WISE_ASIN
DML_OPERATOR_ELEMENT_WISE_COS
DML_OPERATOR_ELEMENT_WISE_CLIP_GRAD1
DML_OPERATOR_ELEMENT_WISE_CLIP_GRAD
DML_OPERATOR_ELEMENT_WISE_CLIP1
DML_OPERATOR_ELEMENT_WISE_LOG
DML_OPERATOR_ELEMENT_WISE_FLOOR
DML_OPERATOR_ELEMENT_WISE_EXP
DML_OPERATOR_ELEMENT_WISE_DIVIDE
DML_OPERATOR_ELEMENT_WISE_LOGICAL_LESS_THAN
DML_OPERATOR_ELEMENT_WISE_LOGICAL_GREATER_THAN
DML_OPERATOR_ELEMENT_WISE_LOGICAL_EQUALS
DML_OPERATOR_ELEMENT_WISE_LOGICAL_AND
DML_OPERATOR_ELEMENT_WISE_LOGICAL_OR
DML_OPERATOR_ELEMENT_WISE_LOGICAL_NOT
DML_OPERATOR_ELEMENT_WISE_LOGICAL_LESS_THAN_OR_EQUAL
DML_OPERATOR_ELEMENT_WISE_LOGICAL_GREATER_THAN_OR_EQUAL
DML_OPERATOR_ELEMENT_WISE_MIN
DML_OPERATOR_ELEMENT_WISE_MEAN
DML_OPERATOR_ELEMENT_WISE_MAX
DML_OPERATOR_ELEMENT_WISE_LOGICAL_XOR
DML_OPERATOR_ELEMENT_WISE_RECIP
DML_OPERATOR_ELEMENT_WISE_CONSTANT_POW
DML_OPERATOR_ELEMENT_WISE_POW
DML_OPERATOR_ELEMENT_WISE_MULTIPLY
DML_OPERATOR_ELEMENT_WISE_ATAN_YX
DML_OPERATOR_ELEMENT_WISE_DIFFERENCE_SQUARE
DML_OPERATOR_ELEMENT_WISE_SQRT
DML_OPERATOR_ELEMENT_WISE_SIN
DML_OPERATOR_ELEMENT_WISE_QUANTIZE_LINEAR
DML_OPERATOR_ELEMENT_WISE_THRESHOLD
DML_OPERATOR_ELEMENT_WISE_TAN
DML_OPERATOR_ELEMENT_WISE_SUBTRACT
DML_OPERATOR_REDUCE
DML_OPERATOR_GEMM
DML_OPERATOR_CONVOLUTION
DML_OPERATOR_ELEMENT_WISE_DEQUANTIZE_LINEAR
DML_OPERATOR_LP_POOLING
DML_OPERATOR_AVERAGE_POOLING
DML_OPERATOR_ARGMAX
DML_OPERATOR_ARGMIN
DML_OPERATOR_SLICE
DML_OPERATOR_ROI_POOLING
DML_OPERATOR_MAX_POOLING1
DML_OPERATOR_MAX_POOLING
DML_OPERATOR_PADDING
DML_OPERATOR_JOIN
DML_OPERATOR_SPLIT
DML_OPERATOR_CAST
DML_OPERATOR_GATHER
DML_OPERATOR_UPSAMPLE_2D
DML_OPERATOR_VALUE_SCALE_2D
DML_OPERATOR_PADDING1
DML_OPERATOR_TOP_K
DML_OPERATOR_TILE
DML_OPERATOR_DEPTH_TO_SPACE
DML_OPERATOR_SPACE_TO_DEPTH
DML_OPERATOR_MEAN_VARIANCE_NORMALIZATION
DML_OPERATOR_BATCH_NORMALIZATION_TRAINING_GRAD
DML_OPERATOR_BATCH_NORMALIZATION_GRAD
DML_OPERATOR_BATCH_NORMALIZATION
DML_OPERATOR_RNN
DML_OPERATOR_LP_NORMALIZATION
DML_OPERATOR_LOCAL_RESPONSE_NORMALIZATION_GRAD
DML_OPERATOR_LOCAL_RESPONSE_NORMALIZATION
DML_OPERATOR_ELEMENT_WISE_IS_NAN
DML_OPERATOR_ELEMENT_WISE_SIGN
DML_OPERATOR_GRU
DML_OPERATOR_LSTM
DML_OPERATOR_ELEMENT_WISE_COSH
DML_OPERATOR_ELEMENT_WISE_SINH
DML_OPERATOR_ELEMENT_WISE_ERF
DML_OPERATOR_ELEMENT_WISE_NEGATE
DML_OPERATOR_ELEMENT_WISE_ATANH
DML_OPERATOR_ELEMENT_WISE_ACOSH
DML_OPERATOR_ELEMENT_WISE_ASINH
DML_OPERATOR_ELEMENT_WISE_TANH
DML_OPERATOR_DIAGONAL_MATRIX
DML_OPERATOR_MAX_UNPOOLING
DML_OPERATOR_ELEMENT_WISE_ADD1
DML_OPERATOR_ELEMENT_WISE_IF
DML_OPERATOR_ELEMENT_WISE_BIT_SHIFT_LEFT
DML_OPERATOR_RESAMPLE
DML_OPERATOR_ONE_HOT
DML_OPERATOR_SCATTER
DML_OPERATOR_ELEMENT_WISE_MODULUS_TRUNCATE
DML_OPERATOR_ELEMENT_WISE_IS_INFINITY
DML_OPERATOR_ELEMENT_WISE_ROUND
DML_OPERATOR_ELEMENT_WISE_BIT_SHIFT_RIGHT
DML_OPERATOR_CUMULATIVE_SUMMATION
DML_OPERATOR_FILL_VALUE_SEQUENCE
DML_OPERATOR_FILL_VALUE_CONSTANT
DML_OPERATOR_ELEMENT_WISE_MODULUS_FLOOR
DML_OPERATOR_GATHER_ND
DML_OPERATOR_GATHER_ELEMENTS
DML_OPERATOR_REVERSE_SUBSEQUENCES
DML_OPERATOR_CUMULATIVE_PRODUCT
DML_OPERATOR_TOP_K1
DML_OPERATOR_SLICE1
DML_OPERATOR_MAX_POOLING2
DML_OPERATOR_SCATTER_ND
DML_OPERATOR_RESAMPLE1
DML_OPERATOR_MEAN_VARIANCE_NORMALIZATION1
DML_OPERATOR_SPACE_TO_DEPTH1
DML_OPERATOR_DEPTH_TO_SPACE1
DML_OPERATOR_QUANTIZED_LINEAR_CONVOLUTION
DML_OPERATOR_CONVOLUTION_INTEGER
DML_OPERATOR_QUANTIZED_LINEAR_MATRIX_MULTIPLY
DML_OPERATOR_MATRIX_MULTIPLY_INTEGER
DML_OPERATOR_ELEMENT_WISE_BIT_NOT
DML_OPERATOR_ELEMENT_WISE_BIT_XOR
DML_OPERATOR_ELEMENT_WISE_BIT_OR
DML_OPERATOR_ELEMENT_WISE_BIT_AND
DML_OPERATOR_MAX_POOLING_GRAD
DML_OPERATOR_AVERAGE_POOLING_GRAD
DML_OPERATOR_ACTIVATION_RELU_GRAD
DML_OPERATOR_ELEMENT_WISE_BIT_COUNT
DML_OPERATOR_SLICE_GRAD
DML_OPERATOR_RESAMPLE_GRAD
DML_OPERATOR_NONZERO_COORDINATES
DML_OPERATOR_RANDOM_GENERATOR
DML_OPERATOR_GATHER_ND1
DML_OPERATOR_ROI_ALIGN1
DML_OPERATOR_ROI_ALIGN
DML_OPERATOR_ADAM_OPTIMIZER
DML_OPERATOR_BATCH_NORMALIZATION_TRAINING
DML_OPERATOR_ROI_ALIGN_GRAD
DML_OPERATOR_ELEMENT_WISE_QUANTIZED_LINEAR_ADD
DML_OPERATOR_DYNAMIC_QUANTIZE_LINEAR
OutputTensor
InputTensor
BTensor
ATensor
ScaleBias
OutputGradientTensor
InputGradientTensor
MinMaxDataType
ZeroPointTensor
ScaleTensor
Exponent
ExponentTensor
Direction
BiasTensor
FilterTensor
StartPadding
Dilations
Strides
DimensionCount
FusedActivation
GroupCount
OutputPadding
EndPadding
Alpha
TransB
TransA
CTensor
AxisCount
Function
IncludePadding
WindowSize
AxisDirection
PooledSize
SpatialScale
ROITensor
OutputIndicesTensor
OutputTensors
OutputCount
Sizes
Offsets
PaddingMode
InputTensors
InputCount
ChannelCount
Scale
PaddingValueDataType
PaddingValue
IndicesTensor
InterpolationMode
ScaleSize
Repeats
RepeatsCount
BlockSize
IndexDimensions
MeanTensor
OutputIndexTensor
OutputValueTensor
OutputScaleGradientTensor
Epsilon
Spatial
VarianceTensor
LocalSize
NormalizeVariance
CrossChannel
OutputBiasGradientTensor
SequenceLengthsTensor
HiddenInitTensor
RecurrenceTensor
WeightTensor
ActivationDescs
ActivationDescCount
OutputSingleTensor
OutputSequenceTensor
ClipThreshold
OutputCellSingleTensor
PeepholeTensor
CellMemInitTensor
ConditionTensor
LinearBeforeReset
CoupleInputForget
UseClipThreshold
ValuesTensor
UpdatesTensor
Value
Offset
InfinityMode
RoundingMode
Scales
ScaleCount
HasExclusiveSum
ValueDelta
ValueStart
ValueDataType
InputWindowOffsets
IndicesDimensionCount
InputDimensionCount
HasExclusiveProduct
InputPixelOffsets
Order
InputWindowStrides
InputWindowSizes
AScaleTensor
BZeroPointTensor
AZeroPointTensor
OutputPixelOffsets
InputZeroPointTensor
OutputZeroPointTensor
OutputScaleTensor
BScaleTensor
InputStateTensor
FilterScaleTensor
InputScaleTensor
FilterZeroPointTensor
OutputCoordinatesTensor
OutputCountTensor
OutputStateTensor
GradientTensor
InputSecondMomentTensor
InputFirstMomentTensor
InputParametersTensor
OutputSecondMomentTensor
OutputFirstMomentTensor
OutputParametersTensor
TrainingStepTensor
BatchIndicesTensor
Beta2
Beta1
LearningRate
OutOfBoundsInputValue
SpatialScaleY
SpatialScaleX
ReductionFunction
OutputPixelOffset
InputPixelOffset
MaximumSamplesPerOutput
MinimumSamplesPerOutput
FusedAddTensor
OutputROIGradientTensor
BatchDimensionCount
AlignRegionsToCorners
DML_OPERATOR_ACTIVATION_CELU
DML_OPERATOR_ACTIVATION_ELU
OutputVarianceTensor
OutputMeanTensor
DML_OPERATOR_ACTIVATION_LEAKY_RELU
DML_OPERATOR_ACTIVATION_IDENTITY
DML_OPERATOR_ACTIVATION_HARD_SIGMOID
DML_OPERATOR_ACTIVATION_HARDMAX
DML_OPERATOR_ACTIVATION_PARAMETERIZED_RELU
SlopeTensor
DML_OPERATOR_ACTIVATION_LOG_SOFTMAX
DML_OPERATOR_ACTIVATION_LINEAR
DML_OPERATOR_ACTIVATION_SCALED_ELU
Gamma
DML_OPERATOR_ACTIVATION_RELU
DML_OPERATOR_ACTIVATION_PARAMETRIC_SOFTPLUS
Steepness
DML_OPERATOR_ACTIVATION_SOFTMAX
DML_OPERATOR_ACTIVATION_SIGMOID
DML_OPERATOR_ACTIVATION_SCALED_TANH
DML_OPERATOR_ACTIVATION_THRESHOLDED_RELU
DML_OPERATOR_ACTIVATION_TANH
DML_OPERATOR_ACTIVATION_SOFTSIGN
DML_OPERATOR_ACTIVATION_SOFTPLUS
DML_OPERATOR_ACTIVATION_SHRINK
Threshold
equation
fused_activation_since_version
fused_activation_domain
fused_activation
Sigmoid
Dml::GraphTransformer::ApplyImpl
fused op (
) + (
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\GraphTransformer.cpp
decomposed_QLinearSigmoid_DequantizeLinear_
decomposed_QLinearSigmoid_output_
decomposed_QLinearSigmoid_input_
QLinearSigmoid
QuantizeLinear
decomposed_QLinearSigmoid_QuantizeLinear_
decomposed_QLinearSigmoid_Sigmoid_
DequantizeLinear
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\GpuEvent.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/dml/OperatorAuthorHelper/MLOperatorAuthorHelper.h
Dml::ExecutionProviderImpl::CopyTensors
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\ExecutionProvider.cpp
D:\a\_work\1\s\engine\lotus\cmake\external\wil\include\wil/wrl.h
lambd
LeakyRelu
ThresholdedRelu
fused_alpha
fused_beta
fused_gamma
Softplus
BILINEAR
ScaledTanh
HardSigmoid
Softsign
BatchNormalization
InstanceNormalization
MeanVarianceNormalization
ConvTranspose
ParametricSoftplus
Dropout
Fused
MatMul
PRelu
nearest
NEAREST
linear
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\OperatorUtility.cpp
Linear
Shrink
fused_
bilinear
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\GraphPartitioner.cpp
DmlFusedNode_
DmlFusedNodeDomain
Dml::ComputationCapacityFromPartition
auto_pad
border
ceil_mode
dilations
kernel_shape
output_shape
sample_size
scale
output_padding
split
strides
VALID
NOTSET
SAME_UPPER
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\OperatorAuthorHelper\OperatorHelper.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\OperatorAuthorHelper\MLOperatorAuthorHelper.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\External/DirectMLHelpers/GeneratedSchemaHelpers.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\MLOperatorAuthorImpl.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\MLOperatorAuthorImpl.cpp
tensor(complext64)
tensor(complext128)
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\AbiCustomRegistry.cpp
DML CPU
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\BucketizedBufferAllocator.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\PooledUploadHeap.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\DmlCommon.cpp
batch_dims
alpha
broadcast
blocksize
hidden_size
group
select_last_index
keepdims
output_width
output_height
pooled_shape
scales
transB
transA
starts
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/dml/OperatorAuthorHelper/OperatorHelper.h
LpPool
GlobalMaxPool
MaxRoiPool
GlobalLpPool
AveragePool
MaxPool
GlobalAveragePool
Split
ConvTransposeWithDynamicPads
RoiAlign
LpNormalization
DepthToSpace
SpaceToDepth
Expand
Concat
Transpose
Slice
ScatterElements
Scatter
EyeLike
ScatterND
Gather
ConstantOfShape
GatherND
GatherElements
Reshape
Reciprocal
Flatten
Identity
Unsqueeze
Squeeze
Floor
Asinh
Atanh
Acosh
Affine
IsNaN
ReduceProd
ReduceMean
ReduceLogSumExp
ReduceLogSum
Where
Einsum
ReduceSum
ArgMax
ReduceMin
ArgMin
ReduceL1
ReduceSumSquare
ReduceMax
ReduceL2
Equal
Greater
LessOrEqual
GreaterOrEqual
Resize
LogSoftmax
Softmax
Upsample
ImageScaler
FusedConvTranspose
FusedConv
FusedBatchNormalization
FusedInstanceNormalization
Hardmax
OneHot
IsInf
FusedSum
BitShift
FusedGemm
FusedMeanVarianceNormalization
FusedAdd
FusedMatMul
QLinearAdd
MaxUnpool
QLinearMatMul
QLinearConv
ReverseSequence
Round
Range
CumSum
ConvInteger
MatMulInteger
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\OperatorRegistration.cpp
DynamicQuantizeLinear
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\ReadbackHeap.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\ExecutionContext.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\FusedGraphKernel.cpp
Sbad variant access
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\External/DirectMLHelpers/ApiHelpers.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\GraphDescBuilder.cpp
FusedNode's nodeArgList does not contain one of the nodeArg
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\External/DirectMLHelpers/ApiTraits.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\External/DirectMLHelpers/SchemaHelpers.h
deque<T> too long
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorGather.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorMaxUnpool.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorGemm.cpp
detect_negative
direction
detect_positive
RIGHT
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorElementWise.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorExpand.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorConvolution.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorReduce.cpp
count_include_pad
storage_order
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorPooling.cpp
gamma
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorActivation.cpp
epsilon
spatial
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorBatchNormalization.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorScatter.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorQLinearConv.cpp
coordinate_transformation_mode
nearest_mode
exclude_outside
extrapolation_value
round_prefer_floor
round_prefer_ceil
floor
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorResize.cpp
cubic
half_pixel
pytorch_half_pixel
align_corners
asymmetric
tf_half_pixel_for_nn
tf_crop_and_resize
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorSplit.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorConstantOfShape.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorQLinearAdd.cpp
constant
reflect
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorPadding.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorMemcpy.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorRoiPooling.cpp
spatial_scale
activation_beta
activation_alpha
linear_before_reset
input_forget
activations
forward
bidirectional
reverse
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorRecurrentNeuralNetwork.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorSlice.cpp
sampling_ratio
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorRoiAlign.cpp
batch_axis
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorReverseSequence.cpp
time_axis
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorLpNormalization.cpp
largest
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorTopk.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorCopy.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorEyeLike.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorInstanceNormalization.cpp
across_channels
normalize_variance
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorMeanVarianceNormalization.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorMatMul.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorEinSum.cpp
exclusive
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorCumSum.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorAffine.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorConvInteger.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorDepthToSpace.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorNeg.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorOneHot.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorCast.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorMatMulInteger.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorSpaceToDepth.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorCrop.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorDynamicQuantizeLinear.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorTile.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorLocalResponseNormalization.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorConcat.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorQLinearMatMul.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorRange.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorTranspose.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorValueScale2D.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\CommandAllocatorRing.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\DmlCommandRecorder.cpp
ZvD:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\CommandQueue.cpp
_DmlExecutionProvider
_token_
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\GraphKernelHelper.cpp
_DmlExecutionProvider_
Bad optional access
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperator.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\TensorDesc.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\DescriptorPool.cpp
C++/WinRT version:2.0.201201.7
Fop_name
provider
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/dml/OperatorAuthorHelper/SchemaInferenceOverrider.h
Kernel
Session
File not found!
Engine failed to create a model!
Failed to parse model stream!
Failed to parse model file!
Model file not found!
IdentityTo
unused
Undefined tensor type!
Expected a single string value!
Expected a single float value!
Expected a single int64 value!
The model contains a 16-bit input (
), but the current device does not support 16-bit float.
Failed to serialize model!
DATA_BATCH
DMLCreateDevice1
DirectML.dll
D:\a\_work\1\s\engine\lotus\winml\adapter\winml_adapter_dml.cpp
RoOriginateLanguageException
length
GetSessionGetInputDevice
D:\a\_work\1\s\engine\lotus\winml\adapter\winml_adapter_session.cpp
Windows::AI::MachineLearning::Adapter::SessionRegisterCustomRegistry
Sequential execution should be enabled when using DML execution provider.
Mem pattern should be disabled when using DML execution provider.
combase.dll
Windows.Foundation.Collections.IKeyValuePair`2<String, UInt32>
Windows.Foundation.Collections.IMap`2<String, UInt32>
Windows.Foundation.Collections.IIterator`1<Windows.Foundation.Collections.IKeyValuePair`2<String, UInt32>>
D:\a\_work\1\s\engine\lotus\onnxruntime\core/session/inference_session.h
onnxruntime::InferenceSession::GetSessionState
session_state_ != nullptr
Session must be initialized to create session state.
Out of memory
dtype_attribute->second.has_i()
InsertCastTransformer works on the assumption that `dtype` attribute holds an integer.
dtype
onnxruntime::InsertCastTransformer::ApplyImpl
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\insert_cast_transformer.cc
onnxruntime::RemoveDuplicateCastTransformer::ApplyImpl
RemoveDuplicateCastTransformer
cast node to cast from float16 to float32 on cpu
InsertedCast_
D:\a\_work\1\s\engine\lotus\include\onnxruntime\core/optimizer/graph_transformer.h
onnxruntime::GraphTransformer::Recurse
CastElimination
BiasSoftmaxFusion
BiasGeluFusion
BiasDropoutFusion
AttentionFusion
ROCMExecutionProvider
ArmNNExecutionProvider
ACLExecutionProvider
CommonSubexpressionElimination
ConvActivationFusion
LayerNormFusion
EliminateIdentity
GemmTransposeFusion
GemmSumFusion
GemmActivationFusion
GeluFusion
GeluApproximation
FastGeluFusion
ExpandElimination
EmbedLayerNormFusion
DynamicQuantizeMatMulFusion
EliminateDropout
DivMulFusion
ConvMulFusion
ConvBNFusion
ConvAddFusion
ReluQuantRewrite
QDQS8ToU8Transformer
QDQPropagationTransformer
NotWhereFusion
NoopElimination
NhwcTransformer
MatmulTransposeFusion
MatMulScaleFusion
MatMulIntegerToFloatFusion
MatMulAddFusion
SimplifiedLayerNormFusion
onnxruntime::optimizer_utils::GenerateRewriteRules
Unsupported level
Level
_RuleBasedTransformer
optimization.enable_gelu_approximation
session.disable_quant_qdq
UnsqueezeElimination
TransposeOptimizer
EliminateSlice
SkipLayerNormFusion
ReshapeFusion
FuseReluClip
onnxruntime::optimizer_utils::GenerateRuleBasedGraphTransformer
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\graph_transformer_utils.cc
Unsupported optimization level: 
onnxruntime::optimizer_utils::GenerateTransformersForRuntimeOptimizations
onnxruntime::optimizer_utils::GenerateTransformers
VitisAIExecutionProvider
onnxruntime::MemcpyTransformer::ApplyImpl
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\transformer_memcpy.cc
onnxruntime::TransformerMemcpyImpl::ProcessDefs
' doesn't support memcpy 
Execution type '
Memcpy
Copy from/to host memory
onnxruntime::Node::ForEachWithIndex
dup_replacements.find(&arg) == dup_replacements.end()
onnxruntime::TransformerMemcpyImpl::ProcessInitializers::<lambda_e2d9c65f3423a9c8d6e3cd7ee2b26307>::operator ()
onnxruntime::TransformerMemcpyImpl::ProcessInitializers
onnxruntime::GraphTransformer::Apply
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\graph_transformer.cc
onnxruntime::GraphTransformerManager::ApplyTransformers
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\graph_transformer_mgr.cc
This transformer is already registered 
onnxruntime::Initializer::Initializer
D:\a\_work\1\s\engine\lotus\onnxruntime\core/optimizer/initializer.h
size is different
size_ == size
unsupported data type: 
ReadExternalRawData() failed: 
can't constant fold 
Could not find a CPU kernel and hence 
fetches.size() == node->OutputDefs().size()
. Can't constant fold 
Unsupported output type of 
Could not find OrtValue with name '
ConstantFolding
start
onnxruntime::ConstantFolding::ApplyImpl
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\constant_folding.cc
Shape
 node '
Unsupported data type: 
onnxruntime::`anonymous-namespace'::GetInputNodeMerges
output_node.OutputDefs().size() == 1
onnxruntime::`anonymous-namespace'::GetOutputNodeMerges
Fused MatMul and Scale
_FusedMatMulAndScale
onnxruntime::MatMulScaleFusion::ApplyImpl
Constant initializer NodeArg shape should not be null. NodeArg: 
shape
onnxruntime::`anonymous-namespace'::GetScalarConstantInitializer
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\matmul_scale_fusion.cc
div_inputs.size() == 2
onnxruntime::`anonymous-namespace'::GetScaleFromNode
mul_inputs.size() == 2
input_node.InputDefs().size() == 2 && scale_and_index->second < 2
onnxruntime::utils::mltype_dispatcher_internal::UnsupportedTypeDefaultPolicy<class onnxruntime::common::Status>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<struct onnxruntime::BFloat16>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<double>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<float>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<struct onnxruntime::MLFloat16>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<__int64>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<int>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<unsigned __int64>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<unsigned int>::operator ()
Gemm does not have 3 inputs
Gemm bias is not constant
Gemm bias shape not expected
CheckSliceParameters return false
concat first input value is not -1
Faild to match concat node for Gather paths
concat_after_gather does not have expected number of inputs or output edges
concat_after_gather input 2 does not have expected value
Faild to match gemm gather path
Output edge count not expected for nodes in gemm gather path
unsqueeze_after_gather axes value not expected
gather axis value not expected
gather input 1 value is not expected
Pass MatchGemmSubgraph
Start ValidateGemmInitializer
onnxruntime::AttentionFusionHelper::ValidateGemmInitializer
input_indices.size() == expected_values.size() && input_indices.size() > 0
onnxruntime::AttentionFusionHelper::CheckSliceParameters
D:\a\_work\1\s\engine\lotus\onnxruntime\core/optimizer/attention_fusion_helper.h
Slice does not have enough number of inputs
Slice ends is less than INT_MAX
Expected value:
Slice parameter is not expected. Input index:
Start MatchGemmSubgraph
onnxruntime::AttentionFusionHelper::MatchGemmSubgraph
Faild to match gemm path
Input of reshape_before_gemm is not the input of subgraph
Output edge count not expected for nodes in gemm path
Div and Shape1 does not have edge
CheckSliceParameters returns false for last_slice
CheckSliceParameters returns false for mask_slice
ValidateUnidirMask returns false for mask_slice
CheckSliceParameters returns false for slice1
Faild to match path 2 for unidirectional mask
Output edge count not expected for unsqueeze2 of unidirectional mask
Faild to match path 3 for unidirectional mask
Output edge count not expected for unsqueeze3 of unidirectional mask
Faild to match path 4 for unidirectional mask
Div and Shape does not have edge
Output edge count not expected for squeeze_2/slices2/shape2 of unidirectional mask
CheckSliceParameters return false for slice2
Pass MatchUnidirMaskSubgraph
Start MatchInputMaskSubgraph
onnxruntime::AttentionFusionHelper::MatchInputMaskSubgraph
Gemm bias is not constant initializer
Gemm bias shape is not expected
Gemm weight is not constant initializer
Gemm weight shape is not expected
Pass ValidateGemmInitializer
unidir mask is not constant
onnxruntime::AttentionFusionHelper::ValidateUnidirMask
unidir mask shape not expected
This optimizer does not support external data for unidirectional mask right now
Mask is neither unidirectional nor all ones
Expect mask data type is uint8 or float
Start MatchUnidirMaskSubgraph
onnxruntime::AttentionFusionHelper::MatchUnidirMaskSubgraph
Faild to match the path (Div-->Where-->Add) for unidirectional mask
Faild to match path 1 for unidirectional mask
Output edge count not expected for nodes in path 1 of unidirectional mask
Failed to find reshape shape path 2
gather indices not matched.
Pass MatchInputMaskSubgraphDistilBert
Start MatchPastSubgraph
onnxruntime::AttentionFusionHelper::MatchPastSubgraph
Failed to find path for past_k
Failed to find path for present_k
Failed to find path for present_v and past_v
Failed to match v_concat
past_k_transpose perm attribute not matched
present_k_transpose perm attribute not matched
present_k_unsqueeze axes value not expected
present_v_unsqueeze axes value not expected
past_v_gather indices != 1
past_k_gather indices != 0
past_v_gather and past_k_gather does not have same past input
Failed to find Softmax node
Output edge count not expected for Softmax
Failed to find path for mask
Output edge count not expected for mask nodes
Softmax attribute axis is expected to be 3
mask_unsqueeze_1 axes not matched. Expect: 1
mask_unsqueeze_2 axes not matched. Expect: 2
mask_sub const input not matched
mask_mul const input not matched
Pass MatchInputMaskSubgraph
Start MatchInputMaskSubgraphDistilBert
Failed to find mask path
where const not matched.
Failed to find shape path
equal const not matched.
Failed to find reshape shape path 1
q_transpose perm attribute not matched
Pass CheckNodesInPathQ
Start CheckNodesInPathK
onnxruntime::AttentionFusionHelper::CheckNodesInPathK
k_transpose has not perm attribute
k_transpose perm attribute not matched
k_reshape const not matched
Pass CheckNodesInPathK
Mask_Int32
Cast mask from int64 to int32
Start FuseGptAttention
onnxruntime::AttentionFusionHelper::FuseGptAttention
Faild to find path to qkv_matmul
Faild to find path v to Split
CheckNodesInPathV return false
Output edge count not expected for nodes in past subgraph
Pass MatchPastSubgraph
onnxruntime::AttentionFusionHelper::CheckDistilBertReshapeShape
Start CheckNodesInPathV
onnxruntime::AttentionFusionHelper::CheckNodesInPathV
Output edge count not expected for nodes in path v
Failed in match Transpose attribute perm. Expected: 0, 2, 1, 3
Failed in match v_transpose attribute perm. Expected: 0, 2, 1, 3
hidden_size != num_heads * head_size
v_reshape initializer value is not expected
Pass CheckNodesInPathV
reshape initializer value is not expected
Start CheckNodesInPathQ
onnxruntime::AttentionFusionHelper::CheckNodesInPathQ
q_reshape const not matched
qk_div const not matched.
qkv_weights
qkv_bias
Mask shape is unknown or not 2D, or data type unknown
onnxruntime::ConvertMaskToInt32
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\attention_fusion.cc
Mask data type is not int32 or int64 or float32
onnxruntime::AttentionFusion::ApplyImpl
LayerNormalization
shape of layer norm bias tensor not expected
Total fused Attention node count: 
onnxruntime::FuseSubGraphQKImpl
q root should be layer normalization
q_matmul and q_add shape not matched
k root is not layer norm
k_matmul and k_add shape not matched
Failed to load Q, K and V weights, or data type is not float or float16.
MatchInputMaskSubgraph returns false
MatchUnidirMaskSubgraph returns NULL
Failed to find path for q
q and v are not from same Split node
CheckNodesInPathQ returns false
Using transpose optimized pattern
opt_k_transpose perm attribute not matched
Failed to find path for k
k and v are not from same Split node
CheckNodesInPathK returns false
MatchPastSubgraph returns false
Fused Attention subgraphs 
Attention
num_heads
unidirectional
Fused an attention node for GPT.
Failed to load Q, K and V bias tensors, or data type is not float or float16.
Failed to convert mask to int32
onnxruntime::FuseSubGraphQK
Fused an attention node.
onnxruntime::FuseSubGraphQKDistilBert
Faild to find path v
onnxruntime::AttentionFusion::FuseSubGraph
Output edge count not expected for Add or MatMul in path v
Failed in match v_matmul and v_add input shape
Failed in match input mask subgraph
data type is not supported
onnxruntime::Initializer::ToProto
bn_scale_tensor_proto
onnxruntime::ConvBNFusion::Apply
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\conv_bn_fusion.cc
bn_B_tensor_proto
bn_mean_tensor_proto
bn_var_tensor_proto
conv_W_tensor_proto
conv_B_tensor_proto
ConvBnFusion_W_
ConvBnFusion_BN_B_
onnxruntime::QDQS8ToU8Transformer::ApplyImpl
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\qdq_transformer\qdq_s8_to_u8.cc
qdq_s8_to_u8_zp_conversion
qdq_s8_to_u8_quant
QDQSelectorActionTransformer
onnxruntime::`anonymous-namespace'::GetClipConstantMinMax::<lambda_2c3e38ae2de27c60397226fcef4474a0>::operator ()
Unexpected data type for Clip input of 
onnxruntime::ConvActivationFusion::ApplyImpl
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\conv_activation_fusion.cc
with activation 
activation
fused 
fused Conv 
activation_params
UnsqueezeElimination cannot remove node 
UnsqueezeElimination_
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\unsqueeze_elimination.cc
onnxruntime::UnsqueezeElimination::Apply
is not supported.
data type 
Unexpected data type for Clip 'min' input of 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\relu_clip_fusion.cc
onnxruntime::FuseReluClip::Apply
FuseReluClip_
_min_zero_constant
BiasGelu
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\gelu_approximation.cc
onnxruntime::GeluApproximation::ApplyImpl
FastGelu
Gelu approximation
Total Gelu Approximation (FastGelu) node count: 
onnxruntime::ReshapeFusion::ApplyImpl
allowzero
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\reshape_fusion.cc
Total fused reshape node count: 
Fused reshape node: 
onnxruntime::ReshapeFusion::Fuse_Subgraph
Cannot replace concat node with initializer:
!sum_input_moved
sum_output_edge.src_arg_index == 0
sum_node.GetOutputEdgesCount() == 0
graph.RemoveNode(gemm_node.Index())
other_sum_input != nullptr
graph.RemoveNode(sum_node.Index())
onnxruntime::GemmSumFusion::SatisfyCondition
new_gemm_input_defs.size() == 3
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\gemm_sum_fusion.cc
onnxruntime::GemmSumFusion::Apply
Fused Gemm with Sum
new_gemm_output_defs.size() == 1
gemm_input_edge.src_arg_index < 2
_sum_transformed
up_node should have only one Edge that points to down_node and its output is not graph output
onnxruntime::SwapAdjacentNodes
optimizer_utils::CheckOutputEdges(graph, up_node, 1)
up_node should be parent of down_node and NodeArg slots of the edge between up_node and down_node should be (0, 0).
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\qdq_transformer\qdq_propagation.cc
SwapAdjacentNodes
edge_it->GetDstArgIndex() == 0 && edge_it->GetSrcArgIndex() == 0 && edge_it->GetNode().Index() == down_node.Index()
onnxruntime::QDQPropagationTransformer::ApplyImpl
Second input of Gather in path 1 of position shape should be a constant with value 0.
Output edge count not expected for nodes in path 1 of position shape.
Output edge count not expected for nodes in path 2 of position shape.
Failed to find path 2 of position shape.
Second input of Gather in path 2 of position shape should be a constant with value 1.
Gather node in path 2 is not linked to another subgraph.
two paths share the same shape
The parent of two shape nodes are expected to be input_ids.
Output edge count not expected for nodes in path1.
NonZero
The first input of Range should be a constant with value 0.
onnxruntime::MatchPositionEmbeddingSubgraphsFromGather
Second input of Gather should be a constant with value 1. 
The third input of Range should be a constant with value 1.
The parent of shape nodes are expected to be input_ids.
Failed to match Shape node. 
_Int32
_Cast
Cast Input from int64 to int32
onnxruntime::CheckInput
Input shape is unknown or not 2D, or data type unknown
Input data type is not int32 or int64
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\embed_layer_norm_fusion.cc
onnxruntime::MatchInputToConcatSubgraph
Failed to find path 1 of position shape.
Segment id is not valid. 
Input id is not valid. 
Gamma should be of shape (hidden_size). 
Input_ids and segment id should have the same shape. 
onnxruntime::FuseSubGraphDistilBert
Beta should be of shape (hidden_size). 
onnxruntime::EmbedLayerNormFusion::ApplyImpl
Optional position subgraph nodes Where node is expected to be the parent of Reshape.
Optional position subgraph nodes number of outputs unexpected.
position_embeddings
Failed to match position subgraph.
fused EmbedLayerNorm subgraphs 
mask_index
Word embedding shape not expected.
EmbedLayerNormalization
Input is expected to have dim value in all dimensions.
onnxruntime::FuseSubGraph
Position embedding shape not matched.
Failed to get initializer tensor.
Failed to match position embedding subgraph.
Position embedding data type shall be float or float16.
Position embedding shape is not expected.
Failed to get position embedding weights.
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\dynamic_quantize_matmul_fusion.cc
onnxruntime::DynamicQuantizeMatMulFusion::ApplyImpl
DynamicQuantizeMatMul
MatMulIntegerToFloat
FreeDimensionOverrideTransformer
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\free_dim_override_transformer.cc
Conflicting free dimension overrides.
Invalid free dimension override.
onnxruntime::FreeDimensionOverrideTransformer::FreeDimensionOverrideTransformer
The model has input '
which does not equal the specified override of 
with a fixed dimension size 
onnxruntime::FreeDimensionOverrideTransformer::ApplyImpl
onnxruntime::MatMulAddFusion::ApplyImpl
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\matmul_add_fusion.cc
fused Matmul and Add 
onnxruntime::GemmActivationFusion::ApplyImpl
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\gemm_activation_fusion.cc
fused Gemm 
activation_
_transformed
Fused Gemm with Transpose
onnxruntime::GetTransposePerms
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\matmul_transpose_fusion.cc
transpose_node.InputDefs().size() == 1
cast != nullptr
onnxruntime::ReorderCastAndTranspose
!node_consumers.empty()
onnxruntime::UpdateConsumerCount
onnxruntime::MatmulTransposeFusion::ApplyImpl
fused MatMul and Transpose 
Created a new Cast node to interchange Cast and Transpose nodes
Created a new Transpose node to interchange Cast and Transpose nodes
MatMul_With_Transpose
onnxruntime::ConvMulFusion::Apply
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\conv_mul_fusion.cc
ConvMulFusion_Mul_B_
mul_B_tensor_proto
ConvMulFusion_W_
cast output of layer norm
SimplifiedLayerNormalization
layer_norm_out
onnxruntime::LayerNormFusion::ApplyImpl
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\layer_norm_fusion.cc
Cast_Scale
cast scale of layer norm
fused LayerNorm subgraphs 
onnxruntime::SimplifiedLayerNormFusion::ApplyImpl
'7onnxruntime::SkipLayerNormFusion::ApplyImpl
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\skip_layer_norm_fusion.cc
fused SkipLayerNorm subgraphs 
SkipLayerNormalization
reorder
ReorderInput
_nchwc
reshape
channels_last
SAME_LOWER
_bn_nchwc
ReorderOutput
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\nchwc_transformer.cc
channels
onnxruntime::NchwcTransformer::ApplyImpl
 and 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\bias_softmax_fusion.cc
axis 
axis >= -tensor_rank && axis <= tensor_rank - 1
 is not in valid range [-
onnxruntime::HandleNegativeAxis
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/common.h
 into softmax(input + bias)
broadcast_axis
onnxruntime::BiasSoftmaxFusion::ApplyImpl
BiasSoftmax
softmax_axis
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\common_subexpression_elimination.cc
onnxruntime::CommonSubexpressionElimination::ApplyImpl
 of node 
] because it's the graph's output.
representative.output_index != kInvalidOutputIndex
Not eliminating output 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\transpose_optimizer\ort_transpose_optimizer.cc
onnxruntime::TransposeOptimizer::ApplyImpl
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\nhwc_transformer.cc
onnxruntime::NhwcTransformer::ApplyImpl
BiasDropout
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\bias_dropout_fusion.cc
onnxruntime::BiasDropoutFusion::ApplyImpl
fused Add-Dropout-(Add) for 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\matmul_integer_to_float.cc
onnxruntime::MatMulIntegerToFloatFusion::ApplyImpl
onnxruntime::RuleBasedGraphTransformer::ApplyImpl
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\rule_based_graph_transformer.cc
onnxruntime::RuleBasedGraphTransformer::ApplyRulesOnNode
fused Gelu subgraphs 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\gelu_fusion.cc
onnxruntime::GeluFusion::ApplyImpl
fused Add and Gelu
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\bias_gelu_fusion.cc
onnxruntime::BiasGeluFusion::ApplyImpl
onnxruntime::FastGeluFusion::ApplyImpl
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\fast_gelu_fusion.cc
fast_gelu_output
fused GPT2Gelu subgraphs 
GPT2Gelu
add_B_tensor_proto
ConvAddFusion_B_
ConvAddFusion_Add_B_
onnxruntime::ConvAddFusion::Apply
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\conv_add_fusion.cc
model_path must not be empty. Ensure that a path is provided when the model is created or loaded.
, external_data.length: 
Computed size: 
TensorProto external data size mismatch. 
External data type must not be UNDEFINED or STRING.
onnxruntime::Initializer::ReadExternalRawData
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\initializer.cc
RandomUniform
RandomNormal
RandomUniformLike
RandomNormalLike
Multinomial
onnxruntime::OptimizerExecutionFrame::Info::Info
Tried to allocate without valid type information, ort_value index=
onnxruntime::OptimizerExecutionFrame::Info::{ctor}::<lambda_11b6908471b85c842f67ae917be35bc8>::operator ()
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\optimizer_execution_frame.cc
Failed to get allocator for optimizer
allocator_ptr_
QLinear
b33fd0fa-cd7b-4b10-ae5a-df64cabfe1f8
b33f88f7-c464-43e3-8692-97ac832bb14a
generated at runtime
static_cast<size_t>(index) < nodes_.size() && ((node = nodes_[index]) != nullptr || !required)
onnxruntime::NodesToOptimize::GetNode
D:\a\_work\1\s\engine\lotus\onnxruntime\core/optimizer/selectors_actions/helpers.h
Existing registration with name 
selectors_and_actions_map_.find(name) == selectors_and_actions_map_.cend()
onnxruntime::SelectorsAndActions::RegisterSelectorAndAction
Matched 
Multiple entries for operator is not supported. OpType=
inserted
onnxruntime::SelectorActionTransformer::SelectorActionTransformer
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\selectors_actions\selector_action_transformer.cc
onnxruntime::SelectorActionTransformer::MatchAndProcess
Saving runtime optimizations is not enabled in this build.
onnxruntime::SelectorActionTransformer::ApplyImpl
Failed to set node op schema.
Failed to remove node.
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\selectors_actions\actions.cc
onnxruntime::CreateReplacementNode
onnxruntime::ReplaceWithNew::Run
onnxruntime::ReplaceWithNew::RunForSave
onnxruntime::MergeIntoTarget::Run
com.microsoft.QLinearSigmoid
com.microsoft.QLinearReduceMean
com.microsoft.QLinearConcat
com.microsoft.QLinearLeakyRelu
NhwcMaxPool
noop_with_empty_axes
CastLike
HardSwish
com.microsoft.
com.microsoft.QLinearMul
com.microsoft.QLinearAdd
com.microsoft.QLinearGlobalAveragePool
com.microsoft.QLinearAveragePool
onnxruntime::ApiValueInfo::PermuteDims
perm.size() == gsl::narrow_cast<size_t>(shape_proto->dim_size())
 out of bounds for shape 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\transpose_optimizer\api_impl.cc
Permutation length 
 does not match rank 
onnxruntime::ApiTensor::Data
onnxruntime::ApiTensor::NumElements
node_arg_ != nullptr
No NodeArg found for name 
0 <= p && p_int < shape_proto->dim_size()
Permutation entry 
size >= 0
Failed to get size of TensorProto
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/tensorprotoutils.h
onnxruntime::utils::GetShape
TypeProto must have shape for this to run
onnxruntime::ApiGraph::ReshapeInitializer
Failed to find initializer to reshape with name 
Cannot reshape initializer 
 to have different number of elements
Failed to find initializer for name: 
onnxruntime::ApiGraph::GetValueInfo
onnxruntime::ApiGraph::TransposeInitializer
success
onnxruntime::ApiGraph::CopyValueInfo
new_num_elts == old_num_elts
const_transpose_optimizer
Added in transpose optimizer
node_idx <= NodesToOptimizeIndices::kEmptyNodeIndex
Node index value is too large to save to ORT format model: 
A target node must be set.
onnxruntime::GetNodesToOptimizeIndices::<lambda_2b3ea6407275864e85e8d3a338349287>::operator ()
Index out of range
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\selectors_actions\helpers.cc
onnxruntime::`anonymous-namespace'::MoveInputOutputImpl
onnxruntime::NodesToOptimizeIndicesBuilder::Build
target_node != NodesToOptimizeIndices::kEmptyNodeIndex
onnxruntime::MoveInputOutput
CoreMLExecutionProvider
NnapiExecutionProvider
NupharExecutionProvider
MIGraphXExecutionProvider
RknpuExecutionProvider
OpenVINOExecutionProvider
DnnlExecutionProvider
onnxruntime::ml::RegisterOnnxMLOperatorKernels
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\cpu_execution_provider.cc
onnxruntime::CPUExecutionProvider::GetKernelRegistry
onnxruntime::RegisterCPUKernels
onnxruntime::RegisterOnnxOperatorKernels
 is repeated.
Attribute perm of Transpose has an invalid value. Value 
 does not align with rank of input data: 
onnxruntime::TransposeBase::TransposeBase
v >= 0 && static_cast<uint64_t>(v) <= std::numeric_limits<size_t>::max()
 is outside range.
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/transpose.h
perm: 
Method IncrementIndexAndComputeOffset assumes this value is strictly positive.
Transpose not implemented for empty tensors.
onnxruntime::DoTransposeImpl
(local_source >= source) && (local_source < source + num_blocks * num_elts_in_block)
num_axes > 0
onnxruntime::IncrementIndexAndComputeOffsetSetup
naxes > 0
(local_source >= source) && (local_source < source + num_blocks * blocksize)
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\transpose.cc
input_tensor_ptr != nullptr
Mismatched data types between input and output Tensors. 
onnxruntime::Transpose::Compute
onnxruntime::DoTransposeEltWise
Transpose of element size not supported in this build. Size=
(local_source >= source) && (local_source < source + num_blocks)
(local_source >= source) && (local_source < source + sizeof(T) * num_blocks)
onnxruntime::TypedDoTransposeEltWise
output != nullptr
onnxruntime::NonMaxSuppression::Compute
onnxruntime::NonMaxSuppressionBase::GetThresholdsFromInputs
iou_threshold must be in range [0, 1].
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\object_detection\non_max_suppression.h
onnxruntime::NonMaxSuppressionBase::NonMaxSuppressionBase
boxes_tensor
NonMaxSuppression
center_point_box
0 == center_point_box_ || 1 == center_point_box_
center_point_box only support 0 or 1
boxes and scores should have same num_batches.
scores must be a 3D tensor.
The most inner dimension in boxes must have 4 data.
boxes and scores should have same spatial_dimension.
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\object_detection\non_max_suppression.cc
onnxruntime::NonMaxSuppressionBase::PrepareCompute
boxes must be a 3D tensor.
scores_tensor
onnxruntime::Gather::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\gatherbase.h
Gather Tind type not supported in this build.
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\gather.cc
Missing/Invalid 'axis' attribute value
onnxruntime::GatherBase::GatherBase
info.GetAttr<int64_t>("axis", &axis_).IsOK()
 must be within the inclusive range [
indices element out of data bounds, idx=
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/unsqueeze.h
onnxruntime::UnsqueezeBase::UnsqueezeBase
info.GetAttrs("axes", axes_).IsOK()
Missing/Invalid 'axes' attribute value
axes_tensor != nullptr
Axes input is null
axes_tensor->Shape().NumDimensions() == 0 || axes_tensor->Shape().NumDimensions() == 1
An axes tensor must be a scalar or a 1-D tensor.
X != nullptr
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\unsqueeze.cc
onnxruntime::UnsqueezeBase::PrepareCompute
'axes' has a duplicate axis
'axes' has an out of range axis
onnxruntime::Unsqueeze::Compute
nullptr != p.output_tensor
 inputs. Found:
static_cast<size_t>(num_subgraph_inputs) == subgraph_inputs.size()
Graph in 'body' attribute of Loop should have 
num_subgraph_outputs - 1 == num_outputs
'Loop' node has 
 Expected:
 Got:
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\controlflow\loop.cc
onnxruntime::Loop::Info::Info
 outputs so the subgraph requires 
 but has 
onnxruntime::LoopImpl::Execute::<lambda_bd8174261d52a27a5d071d22ebdb87ce>::operator ()
onnxruntime::LoopImpl::ConcatenateLoopOutput
 was not found. Defaulting to a rank 1 shape of {0}.
onnxruntime::LoopImpl::Execute
All scan outputs MUST be tensors
Loop subgraph input 1 has unknown shape: 
onnxruntime::LoopImpl::SaveOutputsAndUpdateFeeds
last_outputs[j + 1].IsTensor()
Loop had zero iterations and the shape of subgraph output 
info_ == nullptr
SetupSubgraphExecutionInfo should only be called once for each subgraph.
Subgraph SessionState was not found for 'body' attribute.
onnxruntime::Loop::SetupSubgraphExecutionInfo
Inconsistent shape in loop output for output. 
onnxruntime::Loop::Init
info.GetAttr<ONNX_NAMESPACE::GraphProto>("body", &proto).IsOK()
'Loop' input 'cond' should be a scalar tensor. Got shape of 
'Loop' input 'M' should be a scalar tensor. Got shape of 
onnxruntime::LoopImpl::Initialize
Loop subgraph input 0 has unknown shape: 
onnxruntime::Loop::Compute
session_state
feeds_fetches_manager_
CreateFeedsFetchesManager must be called prior to execution of graph.
info.GetAttr<ONNX_NAMESPACE::GraphProto>("then_branch", &proto).IsOK()
onnxruntime::If::Init
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\controlflow\if.cc
then_branch
num_subgraph_outputs == static_cast<size_t>(num_outputs)
onnxruntime::If::Info::Info
 outputs which doesn't match the subgraph's 
'If' node has 
' attribute.
Subgraph SessionState was not found for '
then_feeds_fetches_manager_ && else_feeds_fetches_manager_
onnxruntime::If::Compute
info == nullptr
onnxruntime::If::SetupSubgraphExecutionInfo
else_branch
info.GetAttr<ONNX_NAMESPACE::GraphProto>("else_branch", &proto).IsOK()
 outputs.
onnxruntime::IfImpl::Execute
Only tensors, tensor sequence, optional tensor, and optional tensor sequence types are supported
Failed to create output tensor for If output 
onnxruntime::IfImpl::Initialize
Failed to create output tensor for 
onnxruntime::IdentityOp<1>::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/identity_op.h
Unable to get an allocator
onnxruntime::IdentityOp<0>::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\optional\optional_ops.h
attr->has_tp()
onnxruntime::Optional::Optional
Optional op must have a TypeProto in the 'type' attribute if the attribute is present
Optional
OptionalHasElement
onnxruntime::OptionalGetElement::Compute
onnxruntime::Optional::Compute
Trying to use OptionalGetElement on an optional type OrtValue which contains no data
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\optional\optional_ops.cc
Only Optional type OrtValues containing Tensors and Sequence Tensors are acceptable
OptionalGetElement
onnxruntime::PropagateInputOrtValueToFirstOutput
onnxruntime::BatchNorm<double>::Compute
is_spatial_
onnxruntime::BatchNorm<double>::BatchNorm
momentum
Training mode only supports spatial BN
!is_train_ || ((!saved_mean && !saved_inv_std) || (saved_mean && saved_inv_std))
training_mode
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/batch_norm.h
Invalid number of outputs for BN training
Invalid input scale: 
Invalid input B: NumDimensions() != 
Invalid input scale: 0th dimension != 
 dimension != 
Invalid input scale: NumDimensions() != 
Invalid input var: 0th dimension != 
Invalid input var: 
Invalid input mean: 
Invalid input var: NumDimensions() != 
Invalid input mean: NumDimensions() != 
Invalid input mean: 0th dimension != 
Invalid input B: 0th dimension != 
Invalid input B: 
onnxruntime::BatchNorm<float>::Compute
onnxruntime::BatchNorm<float>::BatchNorm
onnxruntime::utils::mltype_dispatcher_internal::CallableDispatchableHelper::CheckCalledOnce
called_ == 1
 is invalid.
Can broadcast 0 by 0 or 1. 
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/math/element_wise_ops.h
onnxruntime::BroadcastIterator::Append
axis == 1 || axis == largest
onnxruntime::BroadcastIterator::Init
Attempting to broadcast an axis by a dimension other than 1. 
Invalid start/ending offset [
start_offset >= 0 && real_end >= 0 && start_offset <= real_end && real_end <= len
onnxruntime::InputBroadcaster::AdvanceBy
) for tensor of length:
InputBroadcaster can only start at span boundary!
offset % span_size_ == 0
largest <= 1
onnxruntime::Broadcaster::Broadcaster
onnxruntime::TensorAllocator::TensorAllocator
Broadcast Output range [
start_offset % span_size == 0 && real_end % span_size == 0
onnxruntime::OutputBroadcaster::OutputBroadcaster
) are not at boundary of span with size:
!helper.HaveTwoTensorInputs()
onnxruntime::ExpandBroadcastLooper
data_1.Shape() == shape
ExpandBroadcastLooper should only have a shape for the second input.
onnxruntime::Max_6<float>::Compute
onnxruntime::Mean_6<float>::Compute
All inputs must have the same shape
data_n.Shape() == shape
fmod attribute must be true for floating point types
onnxruntime::mod_internal::CallModImpl<struct onnxruntime::MLFloat16,void>::operator ()
(fmod == 0) || (fmod == 1)
onnxruntime::Mod::Mod
onnxruntime::UntypedExpand
fmod must have value either 0 or 1
Tensor with shape information must be 1 dimensional.
shape_data_tensor.Shape().GetDims().size() == 1
onnxruntime::Min_6<float>::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\element_wise_ops.cc
Must have 1 or more inputs
inputCount >= 1
Unsupported X type: 
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<__int64> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<__int64> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Floor<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Floor<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Ceil<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Ceil<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Reciprocal<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Reciprocal<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<double> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<signed char> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<signed char> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<int> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<int> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Exp<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Exp<double> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Log<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Log<float> >::ElementWiseKernel
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/element_wise_ranged_transform.h
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Log<double> >::ElementWiseKernel
input_size < std::numeric_limits<std::ptrdiff_t>::max()
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Log<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Reciprocal<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Reciprocal<double> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Sqrt<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Sqrt<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Sqrt<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Sqrt<double> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Exp<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Exp<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<double> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned char> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned char> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned short> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned short> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned int> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned int> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned __int64> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned __int64> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<signed char> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<signed char> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<short> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<short> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<int> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<int> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<__int64> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<__int64> >::ElementWiseKernel
input_count >= 1
onnxruntime::Sum_6<float>::Compute
cur_out == end_out
onnxruntime::BitShift<unsigned int>::BitShift
cur1 == end1
onnxruntime::BitShift<unsigned char>::Compute::<lambda_1bac9798e02ce2daa410e33c40a170ad>::operator ()
'. Valid values are 'LEFT' or 'RIGHT'.
Invalid direction value of '
onnxruntime::Sum_6<double>::Compute
onnxruntime::BitShift<unsigned char>::BitShift
BroadcastLooper requires two tensors as input.
helper.HaveTwoTensorInputs()
Invalid usage. Input 1 is a shape with no data.
onnxruntime::Expand_8<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::Compute::<lambda_c61de806568fb8e84995e9be0077f048>::operator ()
onnxruntime::BitShift<unsigned __int64>::Compute::<lambda_2dfbb49707127c93d1d3311367fcbf5e>::operator ()
Unsupported Y type: 
onnxruntime::BitShift<unsigned int>::Compute::<lambda_ede6396af27e2bad4c061429d30c1587>::operator ()
onnxruntime::BitShift<unsigned __int64>::BitShift
onnxruntime::BroadcastLooper
onnxruntime::mod_internal::CallModImpl<float,void>::operator ()
onnxruntime::mod_internal::CallModImpl<double,void>::operator ()
p7M}6p7M
info.GetAttr("direction", &direction_).IsOK()
layout
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\rnn\rnn.cc
onnxruntime::RNN<float>::Compute
allowed_activations.find(activations_[direction]) != allowed_activations.end()
RNN op: Invalid activation attribute - 
activations_.size() == static_cast<size_t>(num_directions)
info.GetAttr("hidden_size", &hidden_size_).IsOK()
info.GetAttrs("activations", activations_).IsOK()
allowed_directions.find(direction_) != allowed_directions.end()
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/rnn/rnn.h
onnxruntime::RNN<float>::RNN
layout_ == 0
Batchwise recurrent operations (layout == 1) are not supported. If you need support create a github issue with justification.
info.GetAttr("hidden_size", &int64_value).IsOK() && int64_value > 0
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\rnn\lstm_base.h
onnxruntime::LSTMBase::LSTMBase
info.GetAttr("direction", &direction).IsOK()
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/rnn/rnn_helpers.h
onnxruntime::rnn::detail::MakeDirection
Invalid data type for LSTM operator of 
LSTM operator does not support double yet
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\rnn\deep_cpu_lstm.cc
onnxruntime::DeepCpuLstmOp::PrePack
activation_func_names.size() == static_cast<size_t>(num_directions_) * 3
sigmoid
clip_ > 0.f
Invalid 'direction' argument of '
'. Must be one of 'forward', 'reverse', or 'bidirectional'.
onnxruntime::DeepCpuLstmOp::Compute
GRU operator does not support double yet
activation_func_names.size() == static_cast<size_t>(num_directions_) * 2
info.GetAttr("linear_before_reset", &int64_value).IsOK()
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/rnn/deep_cpu_gru.h
onnxruntime::DeepCpuGruOp::DeepCpuGruOp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\rnn\deep_cpu_gru.cc
onnxruntime::DeepCpuGruOp::Compute
Invalid data type for GRU operator of 
onnxruntime::DeepCpuGruOp::ComputeImpl
onnxruntime::Tensor::MutableDataAsSpan
cur + size <= end
C + (M * ldc - (ldc - N)) <= C_end
B + (N * ldb - (ldb - K)) <= B_end
A + (M * lda - (lda - K)) <= A_end
onnxruntime::rnn::detail::ComputeGemm
lda >= K && ldb >= K && ldc >= N
onnxruntime::rnn::detail::SafeRawPointer
offset + size <= size_t(span.size())
onnxruntime::rnn::detail::SafeRawConstPointer
!allow_zero
The input tensor cannot be reshaped to the requested shape. Input shape:
, requested shape:
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\reshape_helper.h
onnxruntime::ReshapeHelper::ReshapeHelper
requested_shape[i] >= -1
A dimension cannot be less than -1, got 
gsl::narrow_cast<int64_t>(input_shape.Size()) == size
size != 0 && (input_shape.Size() % size) == 0
i < input_shape.NumDimensions()
The dimension with value zero exceeds the dimension size of the input tensor.
unknown_dim == -1
At most one dimension can be -1.
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/reshape.h
onnxruntime::Reshape::Compute
shapeTensor->Shape().NumDimensions() == 1
A shape tensor must be a vector tensor.
onnxruntime::Reshape_1::Reshape_1
Attribute shape is not set.
onnxruntime::Trilu::Trilu
info.GetAttr<int64_t>("upper", &temp).IsOK()
upper
Unsupported input data type of 
Input tensor should have a rank of at least 2
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\trilu.cc
onnxruntime::Trilu::Compute
IsScalarOr1ElementVector(k)
k should be a 1-D or 0-D tensor.
Trilu
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\trilu.h
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Sigmoid<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Sigmoid<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Softplus<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Softsign<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Relu<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Relu<signed char> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Relu<int> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Selu<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Selu<float> >::Compute
No attribute with name:'
'is defined.
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Tanh<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Tanh<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Celu<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Celu<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::ThresholdedRelu<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::ThresholdedRelu<float> >::Compute
Attribute name and type don't match for '
onnxruntime::functors::HardSigmoid<float>::Init
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/activation/activations.h
onnxruntime::functors::Selu<float>::Init
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Elu<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::HardSigmoid<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::HardSigmoid<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::LeakyRelu<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::LeakyRelu<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Relu<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Elu<float> >::ElementWiseKernel
unknown kernel type
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\activation\activations.cc
onnxruntime::functors::ElementWiseRangedTransform<float>::Create
onnxruntime::functors::ParametricSoftplus<float>::Init
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops/cpu/activations.h
onnxruntime::functors::ScaledTanh<float>::Init
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/utils.h
onnxruntime::SliceSkips::SliceSkips
dims.size() == extents.size() && dims.size() >= steps.size()
Axis tensor should be of type `int32_t` or `int64_t`
Axis tensor should be 0D or 1D
Axis tensor must be provided to the CumSum op
onnxruntime::SliceIteratorBase::Init
dims.size() == starts.size() && dims.size() == extents_.size() && dims.size() >= steps.size()
onnxruntime::CumSum<float>::Compute
Cannot apply CumSum operator on a scalar
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\cumsum.cc
onnxruntime::CumSum<__int64>::Compute
onnxruntime::CumSum<int>::Compute
onnxruntime::CumSum<double>::Compute
steps.size()=
dims.size() == extents_.size()
extents.size()=
onnxruntime::WritableSliceIterator<__int64>::Init
dims.size() == starts.size()
dims.size()=
starts.size()=
onnxruntime::WritableSliceIterator<float>::Init
onnxruntime::WritableSliceIterator<double>::Init
onnxruntime::WritableSliceIterator<int>::Init
dims.size() == steps.size()
onnxruntime::Softmax<double>::ComputeImplOpset13
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\softmax.cc
onnxruntime::Softmax<float>::ComputeImplOpset13
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\hardmax.cc
onnxruntime::Hardmax<float>::Compute
Hardmax inputs N, D and N * D must be < 
info.GetAttr<std::string>("mode", &mode).IsOK()
Scale value should be greater than 0.
onnxruntime::UpsampleBase::ScalesValidation
scale >= 1
Scale value should be greater than or equal to 1.
onnxruntime::UpsampleBase::StringToNearestMode
nearest_mode:[
] is not supported!
scales size should be greater than 0.
scales.size() == 2 || (scales.size() == 4 && scales[0] == 1 && scales[1] == 1)
'Cubic' mode only support 2-D inputs ('Bicubic') or 4-D inputs with the corresponding outermost 2 scale values being 1 in the 
scales.size() == 2 || (scales.size() == 4 && scales[0] == 1 && scales[1] == 1) || (scales.size() == 4 && scales[0] == 1 && scales[3] == 1) || scales.size() == 3 || (scales.size() == 5 && scales[0] == 1 && scales[1] == 1)
'Linear' mode only support:
  * 2-D inputs or
  * 3-D inputs ('Bilinear', 'Trilinear') or
  * 4-D inputs with the corresponding outermost 2 scale values being 1 or the corresponding outermost and innermost scale values being 1 or
  * 5-D inputs with the corresponding outermost 2 scale values being 1in the 
Upsample operator
Resize operator
scale > 0
exclude_outside can be set to 1 only when mode is CUBIC. Current mode is set to 
cubic_coeff_a
`tf_half_pixel_for_nn` is deprecated since opset 13, 
yet this opset 
 model uses the deprecated attribute
info.GetAttrs<float>("scales", scales_).IsOK()
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/upsample.h
onnxruntime::UpsampleBase::UpsampleBase
onnxruntime::UpsampleBase::StringToCoordinateTransformationMode
coordinate_transform_mode:[
] is not supportted!
onnxruntime::UpsampleBase::StringToUpsampleMode
mode attribute is 
. It can only be 
(default) or 
onnxruntime::UpsampleBase::ParseScalesData
scales_size > 0
last_loop_red_size > 0
onnxruntime::ResultsNoTransposePrepareForReduce::ValidateNotEmpty
info.GetAttr("keepdims", &keepdims).IsOK()
onnxruntime::ReduceKernelBase<0>::ReduceKernelBase
count == 1
onnxruntime::ValidateNoTransposeReduce
onnxruntime::ValidateCommonFastReduce
An axes tensor must be a vector tensor.
axes_tensor->Shape().NumDimensions() == 1
Can't reduce on dim with value of 0 if 'keepdims' is false. Invalid output shape would be produced. input_shape:
onnxruntime::ValidateKeepDims
Output size mismatch.
fast_shape[0] == output.Shape().Size()
onnxruntime::ValidateFastReduceRK
fast_shape[1] == output.Shape().Size()
fast_shape.size() == 3
onnxruntime::ValidateFastReduceKRK
fast_shape[0] * fast_shape[2] == output.Shape().Size()
Reduction on all axes, output size should be 1.
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\reduction\reduction_ops.cc
last_loop_size > 0
projected_index.size() > 0
must be overloaded.
onnxruntime::ValidateMustBeOverloaded
Only works on matrices with two dimensions.
fast_shape.size() == 2
onnxruntime::ValidateFastReduceKR
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/reduction/reduction_ops.h
onnxruntime::ReduceKernelBase<1>::ReduceKernelBase
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\padbase.h
Invalid 'pads' attribute value
Invalid 'mode' attribute value
onnxruntime::PadBase::PadBase
value_tensor->DataType() == data_type && value_tensor->Shape().Size() == 1
Pads tensor should be an INT64 tensor
pads_tensor.IsDataType<int64_t>()
onnxruntime::Pad::Compute
Pads tensor should be a 1D tensor of shape [2 * input_rank] or a 2D tensor of shape [1, 2 * input_rank]
pads_tensor_dims.size() == 1 || (pads_tensor_dims.size() == 2 && pads_tensor_dims[0] == 1)
Pads tensor size should be equal to twice the input dimension count 
pads_size == 2 * data_rank
Value tensor should be a 1D tensor of size 1 with the same type as that of the input tensor
Cannot use 'edge' mode to pad dimension with a value of 0. Input shape:
Cannot use 'reflect' mode to pad dimension with a value of 0. Input shape:
Unexpected mode of 
onnxruntime::PadValueFromFloat
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\pad.cc
Input tensor has no dimensions
data_rank > 0
onnxruntime::PadImpl
'pads' has wrong number of values
data_rank * 2 == pads.size()
onnxruntime::PadInputWithDimValueOfZero
onnxruntime::GatherND::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\gather_nd.cc
last dimension of indices must not be larger than rank of input tensor
indices tensor data type not supported
GatherNDBase PrepareForCompute: Input count mismatch
input_tensor != nullptr && indices_tensor != nullptr
invalid index found, index = 
indices tensor must has rank larger than 0
X input is required!
onnxruntime::NonZero<bool>::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\nonzero_op.cc
failed to get first output!
onnxruntime::NonZero<unsigned char>::Compute
onnxruntime::NonZero<int>::Compute
onnxruntime::NonZero<__int64>::Compute
onnxruntime::NonZero<float>::Compute
Null input ptr
onnxruntime::ScatterND::Compute
input shape: 
input tensor and indices tensor must has rank larger than 0. 
, data shape: 
updates shape: 
updates tensor should have shape equal to indices.shape[:-1] + data.shape[indices.shape[-1]:]. 
onnxruntime::ScatterNDBase::PrepareForCompute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\scatter_nd.cc
invalid indice found, indice = 
, indices shape: 
onnxruntime::Scatter<struct onnxruntime::TypeList<float,double,__int64,unsigned __int64,int,unsigned int,short,unsigned short,signed char,unsigned char,struct onnxruntime::MLFloat16,struct onnxruntime::BFloat16,bool,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > >::Scatter
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\scatter.cc
data type is different from updates type
Indices and updates must have the same rank
Indices vs updates dimensions differs at position=
. Input rank=
Indices must have the same rank as Input. Indices rank=
 is greater than input dim=
 at pos=
Indices dim=
Indices type is not supported.
input count mismatch
onnxruntime::SpaceToDepth::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\space_depth_ops.cc
Unsupported input type in SpaceToDepth op: 
onnxruntime::DepthToSpace::Compute
Unsupported input type in DepthToSpace op: 
DepthToSpace op: only 'DCR' and 'CRD' modes are supported
onnxruntime::DepthToSpace::DepthToSpace
Attribute blocksize is not set.
info.GetAttr("blocksize", &blocksize_).IsOK()
onnxruntime::SpaceDepthBase::SpaceDepthBase
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/space_depth_ops.h
SpaceDepth ops require a 4-D input. Provided rank: 
SpaceToDepth requires input height to be a multiple of block_size
SpaceToDepth requires input width to be a multiple of block_size
DepthToSpace requires input depth to be a multiple of (block_size * blok_size)
onnxruntime::GatherElements::GatherElements
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\gather_elements.h
GatherElements op: Cannot operate on scalar input
GatherElements op: Rank of input 'data' needs to be equal to rank of input 'indices'
GatherElements op: 'indices' shape should have values within bounds of 'data' shape. Invalid value in indices shape is: 
GatherElements op: Data type of input 'data' should match the data type of the output
]. Actual value is 
GatherElements op: Value in indices must be within bounds [
onnxruntime::core_impl::<lambda_cc28a3a6e50de277c1640406cde367e7>::operator ()
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\gather_elements.cc
onnxruntime::core_impl::<lambda_57cfefc55869a6904acbae1315b5b1ba>::operator ()
onnxruntime::core_impl::<lambda_d225b397643c22bd72de44da053cd583>::operator ()
onnxruntime::core_impl::<lambda_ff722d1aa0e133d0aa24feb7c90fe947>::operator ()
Input count of Tile OP mismatch, the second one is empty
Input count of Tile OP mismatch, the first one is empty
'repeat' input tensor must be 1 dimensional
the tensor to be tiled using Tile OP must be atleast 1 dimensional
Tile doesn't support string type yet
'repeat' input tensor must have the same length as the 'input' tensor
onnxruntime::Tile::Compute
!input_tensor.IsDataType<std::string>()
Tile doesn't have an implementation yet for the type: 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\tile.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/squeeze.h
onnxruntime::SqueezeBase::ComputeOutputShape
onnxruntime::Squeeze::Compute
 must be 1 instead of 
. shape=
input_shape[i] == 1
Dimension of input 
std::all_of(split_sizes_.cbegin(), split_sizes_.cend(), [](int64_t value) { return value >= 0; })
Invalid value in 'split' attribute. All values must be > 0
 Input shape=
 Num entries in 'split' (must equal number of outputs) was 
Cannot split using values in 'split' attribute. Axis=
onnxruntime::Split::Compute
Split operator does not support 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\split.cc
 Axis=
 NumOutputs=
 Sum of sizes in 'split' (must equal size of selected axis) was 
Input cannot be split evenly on selected axis. Input shape=
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/split.h
onnxruntime::SplitBase::SplitBase
An split tensor must be a vector tensor.
onnxruntime::Split::ComputeImpl
split_tensor->Shape().NumDimensions() == 1
'axes' has duplicates
'axes' has an axis outside of the tensor dimension count
'step' value cannot be 0
Missing or invalid starts and ends attribute
onnxruntime::SliceBase::SliceBase
has_starts && has_ends && attr_starts_.size() == attr_ends_.size()
Invalid axes attribute, axes attribute (if present) should have the same size as starts/ends attributes
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/slice.h
!has_axes || attr_axes_.size() == attr_starts_.size()
Cannot slice scalars
Data type for starts and ends inputs' is not supported in this build. Got 
onnxruntime::SliceBase::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\slice.cc
onnxruntime::SliceBase::PrepareForCompute
onnxruntime::SliceBase::FillVectorsFromInput
Starts must be a 1-D array
Starts and ends shape mismatch
Ends must be a 1-D array
Starts and steps shape mismatch
Starts and axes shape mismatch
onnxruntime::SliceIteratorBase::CopyInnermostAxisNonSolitaryInnerStep
Unexpected element size of 
output == output_end
onnxruntime::SliceImpl::<lambda_bc7a68c171e484232baed5d83da3897a>::operator ()
onnxruntime::SliceImpl::<lambda_bcecb15eb8cf7fa8e3b34d315d7dea50>::operator ()
onnxruntime::SliceImpl::<lambda_2f378bba7d73ec2c16a1e9a755eda4f7>::operator ()
onnxruntime::SliceImpl::<lambda_93ca941f50a0a49b2b4c0ced3b0c592e>::operator ()
onnxruntime::SliceImpl::<lambda_66f06e233b0a64f25a2f272d85a883a3>::operator ()
new_axis
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\concat.cc
onnxruntime::ConcatBase::PrepareForCompute
Must have valid 'axis' attribute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\concatbase.h
onnxruntime::ConcatBase::ConcatBase
onnxruntime::ConcatBase::ComputeImpl
Cannot concatenate scalars
input != nullptr
Ranks of input data are different, cannot concatenate them. expected rank: 
 got: 
 has mismatched dimensions of 
input_rank == reference_rank
Data type mismatch
Non concat axis dimensions must match: Axis 
dst.DataType() == src.DataType()
src and dst types must match
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/copy.h
onnxruntime::DispatchStridedCopy
onnxruntime::StridedCopy
dst_strides.size() == src_strides.size() && src_strides.size() == copy_shape.size() && !copy_shape.empty()
last >= first
counter.current_offset == last
onnxruntime::StridedCopy::<lambda_b9d9aa3891683438224b1c5285b28a19>::operator ()
src and dst must have same shape and not be rank 0.
onnxruntime::StridedCopy::<lambda_2db246e2ce365efa8cc2cf5d40338a67>::operator ()
onnxruntime::StridedCopy::<lambda_4b2e976425c8109a05bca3d80983d4bb>::operator ()
onnxruntime::StridedCopy::<lambda_9162731c94ecaf479f99be17541de90c>::operator ()
onnxruntime::StridedCopy::<lambda_ff1ccdcb7441249452b8640b2e9dfe31>::operator ()
onnxruntime::StridedCopy::<lambda_4b6703e7c30870568a65367efbb3ac2f>::operator ()
onnxruntime::StridedCopy::<lambda_11e4c67999de7fde8000f344864dbedb>::operator ()
onnxruntime::StridedCopy::<lambda_414b2f36cd6ead6cc1f643a6940e8f0c>::operator ()
onnxruntime::StridedCopy::<lambda_c883cb8d41543031ae666c752948f20d>::operator ()
onnxruntime::StridedCopy::<lambda_9a0964aea19bb920233e6fe5b8fa95e2>::operator ()
X and mask should have the same shape
ratio input should have a single value.
onnxruntime::`anonymous-namespace'::GetRatioOrDefault
ratio_tensor->Shape().Size() == 1
onnxruntime::Dropout<float,float>::Compute
!mask || mask->Shape() == X_shape
onnxruntime::Dropout<float,double>::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/dropout_op.h
onnxruntime::Dropout<double,double>::Compute
onnxruntime::Dropout<double,float>::Compute
0.0f <= ratio_value && ratio_value < 1.0f
ratio must be in the range [0, 1)
Input is expected to have four dimensions corresponding to [N,C,H,W]
onnxruntime::MeanVarianceNormalization_0<float>::MeanVarianceNormalization_0
info.GetAttr<int64_t>("across_channels", &across_channels_).IsOK()
info.GetAttr<int64_t>("normalize_variance", &normalize_variance_).IsOK()
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/mean_variance_normalization.h
beta_ > 0.0f
onnxruntime::LRN<float>::LRN
info.GetAttr<int64_t>("size", &size).IsOK()
size_ > 0
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/lrn.h
info.GetAttr<float>("alpha", &alpha_).IsOK()
size_ % 2 == 1
info.GetAttr<float>("beta", &beta_).IsOK()
alpha_ > 0.0f
X->Shape().NumDimensions() == 4
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\nn\lrn.cc
onnxruntime::LRN<float>::Compute
gsl::narrow_cast<int64_t>(X_shape.NumDimensions()) >= axis
The rank of input tensor must be >= axis
onnxruntime::Flatten::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/flatten.h
onnxruntime::Flatten::Flatten
zero_point_ptr == nullptr || IsScalarOr1ElementVector(zero_point_ptr)
x_zero_point must be null or a scalar or 1D tensor or size 1.
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\quantize_linear.cc
onnxruntime::PrepareForQDQ
scale.Shape().NumDimensions() == 1 && scale.Shape()[0] == broadcast_dim
scale must be 1D tensor with size 
zero_point_ptr == nullptr || (zero_point_ptr->Shape().NumDimensions() == 1 && zero_point_ptr->Shape()[0] == broadcast_dim)
x_zero_point must be null or 1D tensor with size 
DequantizeLinear with type int32 should have no zero point or all zero points should be 0
zero_point == nullptr || std::all_of(zero_point, zero_point + x_zero_point->Shape().Size(), [](int32_t zp) { return zp == 0; })
onnxruntime::DequantizeLinear<int>::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\gemm_base.h
info.GetAttr<int64_t>("transB", &temp).IsOK()
info.GetAttr<int64_t>("transA", &temp).IsOK()
onnxruntime::GemmBase::GemmBase
Gemm: Invalid bias shape for broadcast
M_ >= 0 && K_ > 0 && N_ >= 0
onnxruntime::GemmHelper::GemmHelper
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\gemm_helper.h
left.NumDimensions() == 2 || left.NumDimensions() == 1
GEMM: Dimension mismatch, W: 
right.NumDimensions() == 2
c_shape != nullptr
onnxruntime::GemmBroadcastBias
c_shape is required if c_data is provided
left_num_dims and right_num_dims must be >= 1
MatMul dimension mismatch
left operand cannot broadcast on dim 
onnxruntime::MatMulComputeHelper::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/math/matmul_helper.h
onnxruntime::MatMul<float>::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\matmul.cc
M_ == 1 && N_ == 1 was false
num_dims_with_pad - 1 != num_output_dims
right operand cannot broadcast on dim 
num_dims_with_pad != num_output_dims
num_dims_with_pad - 2 != num_output_dims
onnxruntime::MatMul<double>::Compute
onnxruntime::MatMul<int>::Compute
onnxruntime::MatMul<__int64>::Compute
min_ <= max_
onnxruntime::clip_internal::Clip_6Base<float>::Clip_6Base
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/math/clip.h
min->Shape().IsScalar()
onnxruntime::Clip::ComputeImpl<unsigned __int64>::operator ()
min should be a scalar.
max->Shape().IsScalar()
onnxruntime::Clip::ComputeImpl<__int64>::operator ()
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\clip.cc
max should be a scalar.
onnxruntime::Clip::ComputeImpl<double>::operator ()
onnxruntime::Clip::ComputeImpl<float>::operator ()
onnxruntime::Clip::ComputeImpl<unsigned char>::operator ()
onnxruntime::Clip::ComputeImpl<signed char>::operator ()
stod argument out of range
invalid stod argument
invalid stoll argument
stoull argument out of range
stoll argument out of range
invalid stoull argument
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\cast_op.cc
Attribute to is not set.
onnxruntime::`anonymous-namespace'::GetIntermediateMLFloat16ToFloatTensor
onnxruntime::`anonymous-namespace'::Cast::Cast
snprintf_result > 0 && gsl::narrow_cast<size_t>(snprintf_result) == buffer_span.size() - 1
onnxruntime::`anonymous-namespace'::CastToString
Failed to write value with snprintf().
snprintf() failed with return value: 
snprintf_result > 0
invalid expand shape
onnxruntime::Einsum::DeviceCompute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\einsum.cc
Einsum op: There must be atleast one input
There was a problem acquiring temporary memory allocator in Einsum op
 is not supported yet
Einsum op: An implementation for the input type 
onnxruntime::Einsum::Einsum
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\einsum.h
Missing 'equation' attribute
info.GetAttr<std::string>("equation", &equation_).IsOK()
info.GetAttr<int64_t>("p", &p_).IsOK()
onnxruntime::PoolProcessContext::init
Unsupported AutoPad Type.
onnxruntime::PoolAttributes::ComputeSizePadDilations
onnxruntime::PoolBase::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\nn\pool.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/pool_base.h
Input dimension cannot be less than 3.
dilations.size() == kernel_shape.size()
Invalid input shape. Only N can be zero. Got:
strides.size() == kernel_shape.size()
Dilations dimensions should match kernel shape
input_dims.size() >= 2
onnxruntime::PoolAttributes::InferOutputSize
input_shape.Size() > 0 || input_shape[0] == 0
onnxruntime::PoolAttributes::SetOutputSize
info.GetAttr<std::string>("auto_pad", &auto_padding).IsOK()
info.GetAttr<int64_t>("count_include_pad", &temp).IsOK()
onnxruntime::PoolAttributes::PoolAttributes
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/pool_attributes.h
Pad should be smaller than kernel.
pads[dim] < kernel_shape[dim] && pads[dim + kernel_shape.size()] < kernel_shape[dim]
info.GetAttr("storage_order", &storage_order).IsOK()
kernel_shape[dim] > 0
onnxruntime::StringToAutoPadType
Unknown AutoPadType String
No kernel shape is set.
info.GetAttrs<int64_t>("kernel_shape", kernel_shape).IsOK()
onnxruntime::Pool<float,class onnxruntime::LpPool>::Compute
Unsupported pooling size : 
Unsupported pooling size.
kernel_shape num_dims is not compatible with X num_dims.
onnxruntime::MaxPoolV8::ComputeImpl
Invalid argument for values; either it's rank is more than 1 or it has more than 2 elements
Invalid argument for depth; it's not a scalar.
onnxruntime::OneHotOp<__int64,__int64,__int64>::Compute
onnxruntime::OneHotOp<__int64,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,__int64>::Compute
onnxruntime::OneHotOp<float,__int64,__int64>::Compute
Depth is negative.
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\onehot.cc
onnxruntime::OneHotOp<int,float,float>::Compute
onnxruntime::OneHotOp<int,float,int>::Compute
onnxruntime::OneHotOp<__int64,float,__int64>::Compute
onnxruntime::OneHotOp<float,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,__int64>::Compute
onnxruntime::OneHotOp<__int64,float,int>::Compute
onnxruntime::OneHotOp<__int64,float,float>::Compute
onnxruntime::OneHotOp<__int64,int,float>::Compute
onnxruntime::OneHotOp<float,float,float>::Compute
sorted
op_kernel_info.GetAttr<int64_t>("largest", &largest_temp).IsOK()
onnxruntime::TopkOpset11ConstructorCommon
onnxruntime::TopkOpset10ConstructorCommon
op_kernel_info.GetAttr<int64_t>("sorted", &sorted_temp).IsOK()
op_kernel_info.GetAttr<int64_t>("k", &k_temp).IsOK()
op_kernel_info.GetAttr<int64_t>("axis", &axis_temp).IsOK()
k_temp > 0
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\top_k.cc
onnxruntime::TopkOpset9ConstructorCommon
value of k must not be negative
k tensor should be a 1D tensor of size 1
input count mismatch, expected 2 inputs - the tensor to be processed and a tensor containing k value
input count mismatch, expected 1 input - the tensor to be processed
output count mismatch, expected 2 outputs to be present for TopK operator
k argument [
] should not be greater than specified axis dim value [
Unsupported tensor type of 
Unique
delta in Range operator should be scalar like tensor, yet got shape:
limit in Range operator should be scalar like tensor, yet got shape:
start in Range operator should be scalar like tensor, yet got shape:
delta in Range operator can not be zero!
SequenceConstruct
onnxruntime::SequenceErase::Compute
SequenceErase
onnxruntime::SequenceInsert::Compute
SplitToSequence
Violation of the requirment that all input tensors must have the same data type.
onnxruntime::SequenceConstruct::Compute
num_inputs >= 1
onnxruntime::SequenceEmpty::Compute
Unsupported 'dtype' value: 
SequenceEmpty
Invalid sequence index (
Data type of the input tensor MUST be same as that of the input sequence. Sequence data type (
), input tensor data type (
onnxruntime::CreateCopyAndAppendCpuTensor
SequenceInsert
SequenceLength
) specified for sequence of size (
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\sequence\sequence_ops.cc
onnxruntime::GetSeqIdx
SequenceAt
Invalid data type for split tensor 
split_size_sum (
) != split_dim_size (
SplitToSequence operator does not support 
onnxruntime::GetSplitSizesInput
onnxruntime::GetScalarSplitInput
std::all_of(split_sizes.cbegin(), split_sizes.cend(), [](int64_t value) { return value >= 0; })
Invalid value in 'split' input. All values must be >= 0
onnxruntime::SplitToSequence::ComputeImpl
split_scalar > 0
Split should be > 0
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\sequence\concat_from_sequence.cc
onnxruntime::ConcatFromSequence::Compute
Got nullptr for sequence input.
ConcatFromSequence
 num_input_channels: 
group count is <= 0
Invalid input shape: 
onnxruntime::ConvTransposeAttributes::PrepareForCompute
Input channels is not divisible by group.
filter number not equal to input channel number.
 filter_number: 
kernel_shape is not compatible with W shape.
kernel_shape num_dims is not compatible with W num_dims.
 kernel_shape: 
 group: 
X num_dims does not match W num_dims.
A Conv/ConvTranspose node has both 'auto_pad' and 'pads' attributes
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/conv_attributes.h
onnxruntime::ConvAttributes::ConvAttributes
auto_pad == AutoPadType::NOTSET
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\nn\conv_transpose.cc
onnxruntime::ConvTranspose<float>::DoConvTranspose
*out_size >= 0
onnxruntime::ConvTransposeAttributes::ComputePadsAndOutputShape
dim_size > 0
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/conv_transpose_attributes.h
onnxruntime::ConvTransposeAttributes::ComputeTransposePadAndOutputShape
Not enough elements in kernel shape. Expected: 
Not enough elements in strides. Expected: 
Output channels M is not divisible by group.
onnxruntime::ConvAttributes::InferOutputShape
Not enough elements in pads. Expected: 
Not enough elements in dilations. Expected: 
ComputePad: pad type not supported.
Dilation not supported for AutoPadType::SAME_UPPER or AutoPadType::SAME_LOWER.
Input channels C is not equal to kernel channels * group.
 kernel channels: 
onnxruntime::ComputePadAndOutputShape
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\nn\conv.cc
onnxruntime::Conv<float>::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/unpool.h
onnxruntime::MaxUnpool::MaxUnpool
info.GetAttrs<int64_t>("kernel_shape", kernel_shape_).IsOK()
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\nn\Unpool.cc
onnxruntime::MaxUnpool::Compute
output_shape is smaller than minimum required. output_shape:
 inferred output shape:
Shape must be 1 dimensional as it's tensor data of a shape
Index tensor shape should be same as that of the input data tensor to unpool.
strides_.size() == kernel_shape_.size()
pads_[dim] < kernel_shape_[dim] && pads_[dim + kernel_shape_.size()] < kernel_shape_[dim]
kernel_shape_[dim] > 0
Matrix dimensions are not equal. Square matrix is expected
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\det.cc
onnxruntime::Det<float>::Compute
onnxruntime::Compress::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\compress.cc
Compress
axes_right_stride >= 0 && static_cast<uint64_t>(axes_right_stride) < std::numeric_limits<size_t>::max()
onnxruntime::scan::detail::OutputIterator::GetOutput
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/controlflow/scan_utils.h
Attempt to retrieve final output before it was set.
final_output_mlvalue_
feeds_fetches_manager_ && info_
onnxruntime::Scan<9>::Compute
onnxruntime::ScanImpl::Initialize
 dimensions or more but input had shape of 
scan_output_axes
Number of entries in 'scan_output_axes' was 
gsl::narrow_cast<int64_t>(output_axes_.size()) == num_scan_outputs
onnxruntime::Scan<9>::SetupSubgraphExecutionInfo
scan_input_axes
 but expected 
Number of entries in 'scan_input_axes' was 
gsl::narrow_cast<int64_t>(input_axes_.size()) == num_scan_inputs_
num_scan_inputs
info.GetAttr<int64_t>("num_scan_inputs", &num_scan_inputs_).IsOK()
scan_input_directions
scan_output_directions
onnxruntime::Scan<9>::Init
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\controlflow\scan_9.cc
Outputs from Scan are not optional and should never be null.
onnxruntime::ScanImpl::TransposeOutput
output_mlvalue
onnxruntime::ScanImpl::Execute
. Output tensor rank was 
Invalid value in scan_output_axes for output 
Subgraph in 'body' produces 
onnxruntime::ScanImpl::AllocateOutputTensors
onnxruntime::ScanImpl::CreateLoopStateVariables
Output OrtValue has not been created for loop state variable output 
Invalid value in scan_input_axes for input 
onnxruntime::ScanImpl::ValidateInput
onnxruntime::ScanImpl::SetupInputs
 outputs but Scan expects 
 but input '
Scan inputs have inconsistent sequence lengths. Previous value was 
. Input tensor rank was 
 Expected 
Invalid scan input:
 has length of 
' dimension 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\dynamicquantizelinear.cc
x_ptr != nullptr
onnxruntime::DynamicQuantizeLinear<unsigned char>::Compute
Invalid time_axis of 
time_axis < 2
time_axis and batch_axis must have different values but both are 
batch_axis != time_axis
info.GetAttr<int64_t>("time_axis", &time_axis).IsOK()
. Must be 0 or 1
Invalid batch_axis of 
batch_axis < 2
info.GetAttr<int64_t>("batch_axis", &batch_axis).IsOK()
onnxruntime::ReverseSequenceOp::ReverseSequenceOp
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/reverse_sequence.h
onnxruntime::ReverseSequenceOp::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\reverse_sequence.cc
. batch_size=
sequence_lens shape must be {batch_size}. Got:
Unknown tensor type of 
. Value must be in range [0,
Invalid sequence length: 
Number of dimensions for rois should be exactly 
Second dimension for rois should be exactly 
First dimension (num_rois) of batch_indices and rois don't match
Null input X ptr
Null rois_ptr
Null batch_indices_ptr
Number of dimensions for batch indices should be exactly 1
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\object_detection\roialign.h
Sampling ratio should be >=0, but it was 
sampling_ratio_ >= 0
 specified. It should be either avg or max
Invalid mode of value 
onnxruntime::RoiAlignBase::RoiAlignBase
onnxruntime::IsInf::IsInf
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\isinf.cc
Failed to obtain detect_negative
Failed to obtain detect_positive
QLinearConv : input scale must be a scalar or 1D tensor of size 1
IsScalarOr1ElementVector(X_scale)
onnxruntime::QLinearConv::ComputeOutputScale
QLinearConv : result scale must be a scalar or 1D tensor of size 1
QLinearConv : filter zero point shape invalid
IsValidQuantParam(W_zero_point, M)
QLinearConv : zero point of per-channel filter must be same
W_zero_point_data[i] == W_zero_point_value
onnxruntime::QLinearConv::ComputeOffset
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\nn\qlinearconv.cc
QLinearConv : result zero point must be a scalar or 1D tensor of size 1
IsScalarOr1ElementVector(Y_zero_point)
QLinearConv : input zero point must be a scalar or 1D tensor of size 1
IsScalarOr1ElementVector(X_zero_point)
onnxruntime::QLinearConv::UseSharedPrePackedBuffers
onnxruntime::QLinearConv::Compute
IsScalarOr1ElementVector(Y_scale)
QLinearConv : filter scale shape invalid
IsValidQuantParam(W_scale, M)
prepacked_buffers[0].get() == nullptr
IsScalarOr1ElementVector(W_Zero_Point)
IsScalarOr1ElementVector(X_Zero_Point)
onnxruntime::ConvInteger::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\nn\conv_integer.cc
Non per-tensor quantization is not supported now.
Must be a scalar or 1D tensor or size 1.
Per-column quantization parameter of batched matrix should have same dimension as the matrix,and its size by K should be equal to the matrix's size.
onnxruntime::MatMulComputeHelper::Compute::<lambda_aebe3ddf278f554a396f06ee5c1fe5fb>::operator ()
MatmulInteger : B zero point is not valid
IsBQuantParamSupported(b_zero_point->Shape(), b ? b->Shape() : b_shape_)
MatmulInteger : input1 zero point must be a scalar or 1D tensor of size 1
IsScalarOr1ElementVector(a_zero_point)
onnxruntime::MatMulInteger::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\matmul_integer.cc
IsScalarOr1ElementVector(y_offset)
QLinearMatmul : input scale must be a scalar or 1D tensor of size 1
IsScalarOr1ElementVector(a_scale)
QLinearMatmul : weight scale must be a scalar, 1D tensor of size 1, or last to second dimension is 1
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\quantize_linear_matmul.cc
QLinearMatmul : weight zero point must be a scalar, 1D tensor of size 1, or last to second dimension is 1
IsBQuantParamSupported(b_offset->Shape(), b ? b->Shape() : b_shape_)
QLinearMatmul : result zero point must be a scalar or 1D tensor of size 1
QLinearMatmul : input zero point must be a scalar or 1D tensor of size 1
IsScalarOr1ElementVector(a_offset)
onnxruntime::QLinearMatMul::Compute
IsBQuantParamSupported(b_scale->Shape(), b ? b->Shape() : b_shape_)
QLinearMatmul : result scale must be a scalar or 1D tensor of size 1
IsScalarOr1ElementVector(y_scale)
Failed to construct locale with name:
:Please, install necessary language-pack-XX and configure locales
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\nn\string_normalizer.cc
onnxruntime::string_normalizer::Locale::Locale
StringNormalizer
Conversion Error
Conversion Error
bad conversion
Input contains invalid utf8 chars
wstr != wconv_error
Stopword contains invalid utf8 chars
Input dimensions are either[C > 0] or [1][C > 0] allowed
Single dimension value must be greater than 0
!sw.empty()
Empty stopwords not allowed
p.second
Duplicate stopwords not allowed
attribute case_change_action has invalid value
stopwords
locale
attribute case_change_action is not set
case_change_action
UPPER
LOWER
is_case_sensitive
en-US
onnxruntime::StringNormalizer::StringNormalizer
attribute is_case_sensitive is not set
Upsample: input shape needs to be at least a single dimension.
Resize: input/output value's dimension mismatch
Upsample: input/output value is nullptr
Resize: input shape needs to be at least a single dimension
Upsample: input/output value's dimension mismatch
output_dims[i] == 0
Input dim is zero but required output dim is non-zero. 
Resize: input/output value is nullptr
onnxruntime::UpsampleBase::ParseScalesDataFromOutputSize
 Input dim value: 
 Output dim value: 
Cannot scale 0 by any factor to generate a non-zero value. 
Dimension: 
onnxruntime::Upsample<unsigned char>::Compute
Resize: input tensor's rank does not match the output tensor's rank.
sizes != nullptr && sizes->Shape().Size() != 0
onnxruntime::Upsample<int>::Compute
X->Shape().GetDims().size() == output_dims.size()
Only one of scales or sizes must be provided as input.
roi_input_idx_ > 0
Either scales or sizes MUST be provided as input.
sizes == nullptr
onnxruntime::Upsample<float>::Compute
Invalid roi input index.
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\upsample.cc
Upsample: unexpected mode
Resize: unexpected mode
onnxruntime::Upsample<unsigned char>::BaseCompute
onnxruntime::Upsample<int>::BaseCompute
Resize: size of roi array should be 2 * N where N is the rank of input tensor X.
Upsample: input tensor's dimension does not match the scales.
: 'Cubic' mode only support 2-D inputs ('Bicubic') or 4-D inputs with the corresponding outermost 2 scale values being 1.
: 'Linear' mode only support 2-D inputs or 3-D inputs ('Bilinear', 'Trilinear') or 4-D inputs or 5-D inputs with the corresponding outermost 2 scale values being 1.
Rank of input and output tensor should be same.
onnxruntime::Upsample<float>::BaseCompute
Resize: input tensor's dimension does not match the scales.
output_dims.size() == dims.size()
onnxruntime::UpsampleNearest
min_gram_length must be inbounds of ngram_counts: 
max_gram_length must be inbounds of ngram_counts: 
size_t(impl_->min_gram_length_) <= impl_->ngram_counts_.size()
ngram_counts
impl_->max_skip_count_ >= 0
status.IsOK() && !impl_->ngram_counts_.empty()
Non-empty ngram_counts is required
max_skip_count
impl_->max_gram_length_ >= impl_->min_gram_length_
max_skip_count must be non-negative: 
max_skip_count is required
max_gram_length
impl_->min_gram_length_ > 0
min_gram_length >= max_gram_length required: 
min_gram_length
impl_->weighting_criteria_ != kNone
Required min_gram_length must be positive: 
min_gram_length is required
TFIDF
mode: 
 is unrecognized, acceptable values are TF,IDF,TFIDF
onnxruntime::TfIdfVectorizer::TfIdfVectorizer
mode is required
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\nn\tfidfvectorizer.cc
TfIdfVectorizer
(items % ngram_size == 0)
Number of items must compose whole 
Input shape must have either [C] or [B,C] dimensions with B > 0.
-grams
status.IsOK() && !pool_int64s.empty()
end_idx >= start_idx && end_idx <= total_items
n-gram counts out of bounds for 
!pool_strings.empty()
pool_strings must not be empty if specified
non-empty pool_int64s is required if pool_strings not provided
pool_int64s
Got weights of size: 
 but ngram_indexes size: 
pool_strings
impl_->weights_.size() == impl_->ngram_indexes_.size()
std::all_of(impl_->ngram_indexes_.cbegin(), impl_->ngram_indexes_.cend(), [](int64_t i) { return i >= 0; })
Negative ngram_indexes values are not allowed
 must be of equal size
weights
ngram_indexes
size_t(impl_->max_gram_length_) <= impl_->ngram_counts_.size()
status.IsOK() && !impl_->ngram_indexes_.empty()
Non-empty ngram_indexes is required
p.first->second->id_ == 0
Duplicate ngram detected, size: 
onnxruntime::ngram_details::PopulateGrams
 id: 
op_kernel_info.GetAttr<float>("bias", &bias_temp).IsOK()
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/shrink.h
onnxruntime::Shrink::Shrink
op_kernel_info.GetAttr<float>("lambd", &lambd_temp).IsOK()
EyeLike : Input tensor dimension is not 2
t_proto_p->dims_size() == 1
Must have a single dimension
Must have a single dimension of 1
onnxruntime::ConstantOfShapeBase<struct onnxruntime::TypeList<__int64,struct onnxruntime::MLFloat16,float,double,signed char,short,int,unsigned char,unsigned short,unsigned int,unsigned __int64,bool> >::ConstantOfShapeBase
Must have a valid input shape.
Unsupported output datatype with size: 
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/generator/constant_of_shape_base.h
onnxruntime::ConstantOfShapeBase<struct onnxruntime::TypeList<__int64,struct onnxruntime::MLFloat16,float,double,signed char,short,int,unsigned char,unsigned short,unsigned int,unsigned __int64,bool> >::PrepareCompute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\generator\constant_of_shape.cc
onnxruntime::`anonymous-namespace'::ConstantOfShape::Compute
onnxruntime::ConstantOfShapeBase<struct onnxruntime::TypeList<__int64,struct onnxruntime::MLFloat16,float,double,signed char,short,int,unsigned char,unsigned short,unsigned int,unsigned __int64,bool> >::SetValueFromTensorProto
utils::HasDataType(t_proto)
onnxruntime::ConstantOfShapeBase<struct onnxruntime::TypeList<__int64,struct onnxruntime::MLFloat16,float,double,signed char,short,int,unsigned char,unsigned short,unsigned int,unsigned __int64,bool> >::SetValue
Unsupported value attribute datatype with size: 
t_proto_p->dims()[0] == 1
Tensor proto with external data for value attribute is not supported.
ONNX_NAMESPACE::TensorProto::DataType_IsValid(t_proto.data_type())
Unsupported value attribute datatype: 
!utils::HasExternalData(t_proto)
position_ >= 0 && position_ < sequence_length_
onnxruntime::Scan8Impl::Execute
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/ort_value_tensor_slicer.h
onnxruntime::OrtValueTensorSlicer<struct OrtValue const >::Iterator::operator *
onnxruntime::Scan8Impl::AllocateOutputTensors
onnxruntime::Scan8Impl::CreateLoopStateVariables
p_mlvalue
 did not match batch size of 
onnxruntime::Scan8Impl::ValidateInput
Invalid entries in sequence_lens. Max sequence length was 
sequence_lens length of 
 has batch size of 
onnxruntime::Scan8Impl::Initialize
Scan inputs have inconsistent batch size. Previous value was 
 but 
directions
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\controlflow\scan_8.cc
onnxruntime::Scan<8>::Compute
onnxruntime::Scan<8>::SetupSubgraphExecutionInfo
onnxruntime::Scan<8>::Init
Scan<8> spec does not support transpose of output. This should never be called.
info.GetAttr<float>("spatial_scale", &spatial_scale_).IsOK()
pooled_width_ > 0
spatial_scale_ > 0
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/roi_pool.h
onnxruntime::RoiPool<float>::RoiPool
pooled_height_ > 0
pooled_shape.size() == 2
roi_batch_id >= 0
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\nn\roi_pool.cc
info.GetAttrs<int64_t>("pooled_shape", pooled_shape).IsOK()
roi_batch_id < batch_size
onnxruntime::RoiPool<float>::Compute
R->Shape()[1] == 5
onnxruntime::LpNorm<float>::LpNorm
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/lp_norm.h
onnxruntime::LpNorm<double>::LpNorm
p_ == 1 || p_ == 2
op_kernel_info.GetAttr<int64_t>("p", &p_).IsOK()
op_kernel_info.GetAttr<int64_t>("axis", &axis_).IsOK()
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/instance_norm.h
onnxruntime::InstanceNorm<float>::InstanceNorm
onnxruntime::InstanceNorm<float>::Compute
op_kernel_info.GetAttr<float>("epsilon", &epsilon_).IsOK()
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\nn\instance_norm.cc
Mismatch between input data and scale: size of scale != input channel count 
 vs. 
Mismatch between input data and B: size of B != input channel count 
Invalid input B: number of dimensions is not 1: 
Invalid input scale: number of dimensions is not 1: 
Invalid input data: number of dimensions is less than 3: 
onnxruntime::Multinomial::Multinomial
ONNX_NAMESPACE::TensorProto::DataType_IsValid(output_dtype_) && output_dtype_ != ONNX_NAMESPACE::TensorProto::UNDEFINED
onnxruntime::RandomUniformLike::RandomUniformLike
info.GetAttr<int64_t>("sample_size", &num_samples_).IsOK()
info.GetAttr<float>("low", &low_).IsOK()
info.GetAttr<float>("high", &high_).IsOK()
onnxruntime::RandomUniform::RandomUniform
onnxruntime::RandomNormalLike::RandomNormalLike
ONNX_NAMESPACE::TensorProto::DataType_IsValid(dtype_) && dtype_ != ONNX_NAMESPACE::TensorProto::UNDEFINED
info.GetAttrs<int64_t>("shape", shape).IsOK()
info.GetAttr<int64_t>("dtype", &dtype).IsOK()
Invalid dtype of 
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/generator/random.h
info.GetAttr<float>("scale", &scale_).IsOK()
info.GetAttr<float>("mean", &mean_).IsOK()
onnxruntime::RandomNormal::RandomNormal
Invalid data type of 
Output type not supported in this build: 
num_classes is < 1
num_samples is < 1
Empty dimensions for input tensor
batch_size is < 1
Could not infer data type from input tensor with data type 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\generator\random.cc
onnxruntime::MultinomialCompute
onnxruntime::contrib::RegisterQuantizationKernels
onnxruntime::contrib::RegisterCpuContribKernels
onnxruntime::contrib::RegisterNchwcKernels
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\cpu_contrib_kernels.cc
info.GetAttrs<std::string>("classes_strings", string_classes).IsOK()
onnxruntime::ml::LabelEncoder::LabelEncoder
classes_strings
onnxruntime::ml::LabelEncoder_2<__int64,float>::LabelEncoder_2
onnxruntime::ml::LabelEncoder_2<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,float>::LabelEncoder_2
onnxruntime::ml::LabelEncoder_2<__int64,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::LabelEncoder_2
onnxruntime::ml::LabelEncoder_2<float,__int64>::LabelEncoder_2
num_keys == num_values
onnxruntime::ml::LabelEncoder_2<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,__int64>::LabelEncoder_2
 attribtues in LabelEncoder 
) must have the same length. 
(name: 
 and the number of 
However, the number of key is 
info.GetAttrs<TValue>(_value_field_name, values).IsOK()
values is 
info.GetAttrs<TKey>(_key_field_name, keys).IsOK()
onnxruntime::ml::LabelEncoder_2<__int64,__int64>::LabelEncoder_2
keys_int64s
values_int64s
values_floats
default_float
_Unused
keys_strings
keys_floats
values_strings
Input of tensor(string) must have output of tensor(int64)
Input of tensor(int64) must have output of tensor(string)
info.GetAttr<int64_t>("default_int64", &default_int_).IsOK()
LabelEncoder
info.GetAttr<std::string>("default_string", &default_string_).IsOK()
default_int64
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/ml/label_encoder.h
default_string
onnxruntime::ml::LabelEncoder_2<float,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::LabelEncoder_2
classlabels_strings_.empty() ^ classlabels_int64s_.empty()
onnxruntime::ml::ZipMapOp::ZipMapOp
classlabels_strings
Must provide classlabels_strings or classlabels_int64s but not both.
ZipMap
classlabels_int64s
Input features_per_batch[
Zipmap only supports 1D or 2D input tensors
] != number of classlabels[
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\zipmap.cc
Zipmap does not support empty dim count
TreeEnsembleRegressor
AVERAGE
SOFTMAX
SOFTMAX_ZERO
LOGISTIC
nodes_values
nodes_truenodeids
target_ids
post_transform
target_treeids
target_nodeids
target_weights
n_targets_or_classes > 0
Input shape needs to be at least a single dimension.
base_values
aggregate_function
nodes_falsenodeids
n_targets
nodes_hitrates
nodes_featureids
nodes_modes
nodes_missing_value_tracks_true
nodes_treeids
nodes_nodeids
onnxruntime::ml::detail::TreeEnsembleCommon<double,float>::TreeEnsembleCommon
onnxruntime::ml::detail::TreeEnsembleCommon<double,float>::compute
Unknown aggregation function in TreeEnsemble.
onnxruntime::ml::detail::TreeEnsembleCommon<float,float>::compute
 (falsenode).
 (weights).
Unable to find node 
One falsenode is pointing either to itself, either to another tree.
Node 
 (truenode).
 is already there.
 in tree 
target_class_ids.size() == target_class_nodeids.size()
target_class_ids.size() == target_class_treeids.size()
nodes_falsenodeids.size() == nodes_truenodeids.size()
nodes_falsenodeids.size() == nodes_values.size()
nodes_falsenodeids.size() == nodes_nodeids.size()
nodes_falsenodeids.size() == nodes_treeids.size()
nodes_falsenodeids.size() == nodes_featureids.size()
nodes_falsenodeids.size() == nodes_modes.size()
onnxruntime::ml::detail::TreeEnsembleCommon<float,float>::TreeEnsembleCommon
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\tree_ensemble_common.h
onnxruntime::ml::detail::TreeAggregatorSum<float,float>::ProcessTreeNodePrediction
onnxruntime::ml::detail::TreeAggregator<float,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorAverage<float,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorSum<float,float>::MergePrediction
onnxruntime::ml::detail::TreeAggregatorMax<float,float>::MergePrediction
onnxruntime::ml::detail::TreeAggregatorMin<float,float>::MergePrediction
predictions.size() == (size_t)n_targets_or_classes_
onnxruntime::ml::detail::TreeAggregator<double,float>::FinalizeScores
it->i < (int64_t)predictions.size()
onnxruntime::ml::detail::TreeAggregatorSum<double,float>::ProcessTreeNodePrediction
onnxruntime::ml::detail::TreeAggregatorAverage<double,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorSum<double,float>::MergePrediction
onnxruntime::ml::detail::TreeAggregatorMin<double,float>::MergePrediction
this->base_values_.size() == predictions.size()
onnxruntime::ml::detail::TreeAggregatorMax<double,float>::MergePrediction
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\tree_ensemble_aggregator.h
predictions.size() == predictions2.size()
TreeEnsembleClassifier
X dims is empty.
class_nodeids
class_ids
class_weights
class_treeids
onnxruntime::ml::detail::TreeEnsembleCommonClassifier<int,float>::compute
onnxruntime::ml::detail::TreeEnsembleCommonClassifier<double,float>::compute
onnxruntime::ml::detail::TreeEnsembleCommonClassifier<__int64,float>::compute
onnxruntime::ml::detail::TreeEnsembleCommonClassifier<float,float>::compute
onnxruntime::ml::detail::TreeAggregatorClassifier<float,float>::_set_score_binary
onnxruntime::ml::detail::TreeAggregatorClassifier<__int64,float>::_set_score_binary
onnxruntime::ml::detail::TreeAggregatorClassifier<double,float>::_set_score_binary
classes.size() == 2 || classes.size() == 1
onnxruntime::ml::detail::TreeAggregatorClassifier<int,float>::_set_score_binary
onnxruntime::ml::detail::TreeAggregatorClassifier<double,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorClassifier<float,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorSum<__int64,float>::MergePrediction
onnxruntime::ml::detail::TreeAggregatorSum<__int64,float>::ProcessTreeNodePrediction
onnxruntime::ml::detail::TreeAggregatorSum<int,float>::ProcessTreeNodePrediction
onnxruntime::ml::detail::TreeAggregatorClassifier<__int64,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorClassifier<int,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorSum<int,float>::MergePrediction
predictions.size() == 2
onnxruntime::ml::detail::TreeEnsembleCommon<int,float>::TreeEnsembleCommon
onnxruntime::ml::detail::TreeEnsembleCommon<__int64,float>::TreeEnsembleCommon
LINEAR
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\svmclassifier.h
SVMRegressor
info.GetAttrs<float>("kernel_params", kernel_params).IsOK()
onnxruntime::ml::SVMCommon::SVMCommon
kernel_type
kernel_params
onnxruntime::ml::SVMRegressor<float>::Compute
Unexpected mode:
one_class
num_features == feature_count_
info.GetAttrs<float>("coefficients", coefficients_).IsOK()
!coefficients_.empty()
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\svmregressor.cc
coefficients
info.GetAttrs<float>("rho", rho_).IsOK()
onnxruntime::ml::SVMRegressor<float>::SVMRegressor
support_vectors
n_supports
classlabels_strings_.size() > 0 || classlabels_ints_.size() > 0
coefficients_.size() > 0
classlabels_ints
info.GetAttrs<std::string>("classlabels_strings", classlabels_strings_).IsOK() || info.GetAttrs<int64_t>("classlabels_ints", classlabels_ints_).IsOK()
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\svmclassifier.cc
proba_.size() == probb_.size()
prob_b
onnxruntime::ml::SVMClassifier::SVMClassifier
vectors_per_class
prob_a
SVMClassifier
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\ml_common.h
scores.size() == static_cast<size_t>(expected_num_scores)
onnxruntime::ml::batched_update_scores_inplace
onnxruntime::ml::SVMClassifier::Compute
Unsupported data type of 
Unexpected value for 'add_second_class' of 
Scaler
scale_.size() == offset_.size()
Invalid argument: input has empty dimensions.
) != (
Scale size: (
onnxruntime::ml::ScalerOp<float>::ScalerOp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\scaler.cc
Empty scale in attributes
!scale_.empty()
offset
onnxruntime::ml::ScalerOp<int>::ScalerOp
onnxruntime::ml::ScalerOp<double>::ScalerOp
onnxruntime::ml::ScalerOp<__int64>::ScalerOp
) or 1
Either both scale and offset can be of feature size (
Unknown Category and zeros = 0.
OneHotEncoder
onnxruntime::ml::OneHotEncoderOp<double>::OneHotEncoderOp
onnxruntime::ml::OneHotEncoderOp<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::OneHotEncoderOp
num_categories_ > 0
onnxruntime::ml::OneHotEncoderOp<float>::OneHotEncoderOp
onnxruntime::ml::OneHotEncoderOp<__int64>::OneHotEncoderOp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\onehotencoder.cc
One and only one of the 'cats_*' attributes must be defined
tmp_cats_int64s.empty() || tmp_cats_strings.empty()
cats_int64s
cats_strings
zeros
onnxruntime::ml::MakeNormalize
Invalid normalize value of 
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/ml/ml_common.h
Normalizer
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/ml/normalizer.h
onnxruntime::ml::Normalizer::Normalizer
info.GetAttr<std::string>("norm", &norm).IsOK()
Unexpected NORMALIZE value of 
Rank of input to Normalized must be less than 2. Got 
intercepts
LinearRegressor
Input shape had more than 2 dimension. Dims=
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\linearregressor.cc
onnxruntime::ml::LinearRegressor::LinearRegressor
info.GetAttr<int64_t>("targets", &num_targets_).IsOK()
targets
Unsupported input element type of 
onnxruntime::ml::LinearClassifier::ComputeImpl
scores_output_data.length() >= scores_output_size
Scores output is incorrect size. Expected:
 Found:
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\linearclassifier.cc
onnxruntime::ml::LinearClassifier::LinearClassifier
multi_class
LinearClassifier
onnxruntime::ml::CastInputToFloat
shape_size == out.length()
Invalid type
onnxruntime::ml::ImputerOp::Compute
imputed_values_float_.empty() ^ imputed_values_int64_.empty()
Must provide imputed_values_float_ or imputed_values_int64_ but not both.
Expected 'replace_value_int64' attribute since 'imputed_values_int64' is specified
replaced_value_int64
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\imputer.cc
onnxruntime::ml::ImputerOp::ImputerOp
Expected 'replaced_value_float' attribute since 'imputed_value_floats' is specified
replaced_value_float
imputed_value_int64s
imputed_value_floats
Imputer
Empty input dimensions.
Empty value of imputed values.
status.IsOK() && !input_dimensions_.empty()
inputdimensions attribute must be provided
inputdimensions
Invalid input type:
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\feature_vectorizer.cc
onnxruntime::ml::FeatureVectorizer::Compute
input_count >= 0 && static_cast<size_t>(input_count) == input_dimensions_.size()
Number of inputs (
) does not match number of inputdimensions values (
FeatureVectorizer
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/ml/feature_vectorizer.h
onnxruntime::ml::FeatureVectorizer::FeatureVectorizer
onnxruntime::ml::DictVectorizerOp<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,__int64>::DictVectorizerOp
onnxruntime::ml::DictVectorizerOp<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,float>::DictVectorizerOp
onnxruntime::ml::DictVectorizerOp<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,double>::DictVectorizerOp
onnxruntime::ml::DictVectorizerOp<__int64,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::DictVectorizerOp
onnxruntime::ml::DictVectorizerOp<__int64,float>::DictVectorizerOp
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/ml/dictvectorizer.h
onnxruntime::ml::DictVectorizerOp<__int64,double>::DictVectorizerOp
info.GetAttrs(std::is_same<AttrType, std::string>::value ? "string_vocabulary" : "int64_vocabulary", vocabulary_).IsOK()
int64_vocabulary
string_vocabulary
DictVectorizer
Input of int64 must have output of string 
Input of string must have output of int64
CategoryMapper
num_entries == int_categories.size()
info.GetAttrs<int64_t>("cats_int64s", int_categories).IsOK()
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/ml/category_mapper.h
onnxruntime::ml::CategoryMapper::CategoryMapper
info.GetAttrs<std::string>("cats_strings", string_categories).IsOK()
stof argument out of range
invalid stof argument
TO_FLOAT
Unexpected CAST_TO value of 
Invalid input type of value: 
 Expected std::map<int64_t, float> or std::map<int64_t, std::string>
CastMap
map_form_ != PACK_MAP::SPARSE || max_map_ > 0
max_map must be > 0 if map_form is SPARSE
info.GetAttr<int64_t>("max_map", &max_map_).IsOK()
max_map
info.GetAttr<std::string>("map_form", &attr).IsOK()
map_form
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/ml/cast_map.h
onnxruntime::ml::CastMap::CastMap
info.GetAttr<std::string>("cast_to", &attr).IsOK()
cast_to
onnxruntime::ml::MakePack
Invalid PACK_MAP value of 
 Expected DENSE or SPARSE
SPARSE
DENSE
onnxruntime::ml::MakeCast
Invalid CAST_TO value of 
 Expected TO_FLOAT, TO_STRING or TO_INT64
TO_INT64
TO_STRING
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\cast_map.cc
onnxruntime::ml::CastMap::ComputeImpl
cur_input == end_input || cur_input->first >= 0
Negative index values are not permitted. First entry in map has index value of 
Binarizer
Input data with index: 
 is NaN
threshold
ArrayFeatureExtractor
Invalid Y argument: index is out of range: Y[
Invalid Y argument: num_indices = 0
Invalid argument: X input has empty dimensions.
Unsupported type
Input X must have 3 dimensions only. Actual:
}. Actual:
Input W must have shape {
Input R must have shape {
Input B must have shape {
Input sequence_lens must have shape {
Invalid value/s in sequence_lens. All values must be > 0 and < seq_length. seq_length=
Input initial_h must have shape {
affine
leakyrelu
thresholdedrelu
scaledtanh
hardsigmoid
softsign
softplus
Expecting activation to be one of Affine, Relu, LeakyRelu, ThresholdedRelu, Tanh, ScaledTanh, Sigmoid, HardSigmoid, Elu, Softsign, Softplus. Got 
onnxruntime::rnn::detail::NormalizeActivationArgumentAndGetAlphaBetaCount
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\rnn\rnn_helpers.cc
A + (M * K) <= A_end
weights.quant_para_
Quantized GEMM only support alpha equal to 1.0f and beta equal to 0.0f or 1.0f
alpha == 1.0f && (beta == 0.0f || beta == 1.0f)
Invalid activation function of 
onnxruntime::rnn::detail::deepcpu::ActivationFuncByName
Invalid LSTM merge activation function of 
onnxruntime::rnn::detail::deepcpu::LstmMergeGatesFuncByName
Invalid GRU reset gate activation function: 
onnxruntime::rnn::detail::deepcpu::GruResetGateFuncByName
Invalid GRU hidden gate activation function: 
onnxruntime::rnn::detail::deepcpu::GruOutputGateFuncByName
onnxruntime::LSTMBase::ComputeImpl
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\rnn\lstm_base.cc
Input initial_c must have shape {
Input P must have shape {
SoftmaxCPU inputs N, D and N * D must be < 
 Left shape override: 
Left shape: 
The override dims are not compatible with given tensor's shape. 
left.Shape().Size() == left_shape_override.Size()
onnxruntime::EinsumTypedComputeProcessor<float>::PairwiseOperandProcess
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\einsum_utils\einsum_typed_compute_processor.cc
 Right shape override: 
Right shape: 
right.Shape().Size() == right_shape_override.Size()
 Right shape: 
Ranks of pair-wise operands must be equal. 
left_rank == right_rank
Einsum op: Input dimensions must be equal along an axis to be reduced across all inputs
left_dim == right_dim
Einsum op: Input shapes do not align
Einsum op: The candidate output cannot be reshaped into the op's output
candidate_output.Shape().Size() == output_shape.Size()
onnxruntime::EinsumTypedComputeProcessor<float>::FinalizeOutput
Not all dimensions to be reduced have been reduced in the candidate output. Candidate output dims: 
candidate_output_dims[iter] == 1
Einsum op: Could not copy the intermediate output's buffer into the op's output buffer. Error: 
onnxruntime::EinsumTypedComputeProcessor<int>::PairwiseOperandProcess
onnxruntime::EinsumTypedComputeProcessor<int>::FinalizeOutput
onnxruntime::EinsumTypedComputeProcessor<double>::PairwiseOperandProcess
onnxruntime::EinsumTypedComputeProcessor<double>::FinalizeOutput
onnxruntime::EinsumTypedComputeProcessor<__int64>::PairwiseOperandProcess
onnxruntime::EinsumTypedComputeProcessor<__int64>::FinalizeOutput
onnxruntime::EinsumComputePreprocessor::Run
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\einsum_utils\einsum_compute_preprocessor.cc
Number of subscripts in the input equation does not match number of input tensors
Found a '.' not part of an ellipsis in input: 
Einsum subscripts string contains too many subscript labels when compared to the rank of the input
Ellipsis must indicate a fixed number of dimensions across all inputs
Found '.' not part of an ellipsis in input: 
The only subscript labels allowed are lower-cased letters (a-z) and upper-cased letters (A-Z)
 in the same dimension
Another operand has a dim value of 
. The shape is: 
 is incompatible in the dimension 
Einsum operands could not be broadcast together. Please check input shapes/equation provided.Input shape of operand 
Einsum subscripts string contains too many subscript labels when compared to the rank of the input 
Einsum subscripts does not contain enough subscript labels and there is no ellipsis for input 
num_broadcasted_indices < num_of_ellipsis_dims_
onnxruntime::EinsumComputePreprocessor::PostProcessBroadcastedDims
The broadcasted dimensions of the inputs are incompatible
dim_iter == rank
Inputs have ellipses in them but the provided output subscript does not contain an ellipsis
Found a '.' not part of an ellipsis in the output subscript provided
Found '.' not part of an ellipsis in the output subscript provided
Output subscript contains repeated letters
Output subscript contains letters not seen in the inputs
Rank of the input must match number of subscript labels corresponding to the input
onnxruntime::EinsumOp::IsTransposeRequired
Length of permutation must match the rank of the input to be permutated
onnxruntime::EinsumOp::Transpose
Einsum op: Transpose failed: 
Einsum op: The candidate output does not match the actual output's shape
output.SizeInBytes() == input.SizeInBytes()
onnxruntime::EinsumOp::DeviceHelpers::CpuDeviceHelpers::DataCopy
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\einsum_utils\einsum_auxiliary_ops.cc
The innermost dims should have the same dim value to parse the diagonal elements
input_dims[rank - 2] == input_dims[rank - 1]
onnxruntime::EinsumOp::DeviceHelpers::CpuDeviceHelpers::DiagonalInnermostDims
Einsum op: Unsupported data type for Diagonal 
 for input shape 
Cannot parse the diagonal elements along dims 
rank >= 2 && dim_1 != dim_2 && input_dims[dim_1] == input_dims[dim_2]
onnxruntime::EinsumOp::DeviceHelpers::CpuDeviceHelpers::Diagonal
The rank of the input must match permutation size for Transpose
input_rank == permutation.size()
Data types of the inputs must match for MatMul
input_1.DataType() == input_2.DataType()
onnxruntime::EinsumOp::MatMul
Only 1 batch dimension is allowed for MatMul
input_shape_1_override.size() == 3 && input_shape_2_override.size() == 3
Batch dimension should match for MatMul;
input_shape_1_override[0] == input_shape_2_override[0]
Incompatible matrix dimensions for matMul
input_shape_1_override[2] == input_shape_2_override[1]
Einsum op: Exception during MatMul operation: 
onnxruntime::scan::detail::OutputIterator::operator *
Expected AllocateFinalOutput to have been called to before we read the OrtValue from the iterator.
is_concrete_shape_
Expected AllocateFinalOutput to have been called to before we increment the iterator
onnxruntime::scan::detail::OutputIterator::operator ++
onnxruntime::OrtValueTensorSlicer<struct OrtValue>::Iterator::operator *
 inputs but Scan was only given 
The subgraph in 'body' requires 
num_variadic_inputs == num_subgraph_inputs
onnxruntime::scan::detail::Info::Info
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\controlflow\scan_utils.cc
' was 
Number of entries in '
directions.size() == num_entries
onnxruntime::scan::detail::ReadDirections
'. 0 == forward. 1 == reverse.
Invalid values in '
valid
 did not.
Subgraph must have the shape set for all outputs but 
onnxruntime::scan::detail::AllocateOutput
onnxruntime::scan::detail::CreateFeedsFetchesManager
onnxruntime::scan::detail::IterateSequence::<lambda_e4de1e6d15c53abac777de14725b2d0a>::operator ()
onnxruntime::scan::detail::IterateSequence
Misuse of LoopStateVariable. Attempt to move beyond end of sequence
iteration_num_ < sequence_len_
onnxruntime::scan::detail::LoopStateVariable::Next
 is not compatible with 
Mismatch between expected shape and shape from first output
onnxruntime::scan::detail::OutputIterator::Initialize
Failed to create output tensor for output #
onnxruntime::scan::detail::OutputIterator::AllocateFinalBuffer
If shape was concrete we shouldn't be using a custom allocator
!is_concrete_shape_
onnxruntime::scan::detail::OutputIterator::AllocateFinalOutput
cur_iteration_ < num_iterations_
Inverse
onnxruntime::contrib::SkipLayerNorm<float>::SkipLayerNorm
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\skip_layer_norm.cc
epsilon_ >= 0
input is expected to have 3 dimensions, got 
skip is expected to have same shape as input
gamma is expected to have 1 dimension, got 
Last dimension of gamma and input does not match
beta is expected to have 1 dimension, got 
Last dimension of beta and input does not match
bias is expected to have 1 dimension, got 
Last dimension of bias and input does not match
onnxruntime::contrib::SkipLayerNorm<double>::SkipLayerNorm
op_kernel_info.GetAttr("axis", &axis_).IsOK()
onnxruntime::contrib::LayerNorm<float,0>::LayerNorm
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\layer_norm.cc
onnxruntime::contrib::LayerNorm<float,0>::Compute
onnxruntime::contrib::LayerNorm<float,1>::LayerNorm
onnxruntime::contrib::LayerNorm<float,1>::Compute
onnxruntime::contrib::LayerNorm<double,0>::LayerNorm
onnxruntime::contrib::LayerNorm<double,0>::Compute
onnxruntime::contrib::LayerNorm<double,1>::LayerNorm
onnxruntime::contrib::LayerNorm<double,1>::Compute
onnxruntime::contrib::Scale<float>::Scale
info.GetAttr("scale", &scale_).IsOK()
info.GetAttr("alpha", &alpha_).IsOK()
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\element_wise_ops.h
info.GetAttr("beta", &beta_).IsOK()
onnxruntime::contrib::Affine<float>::Affine
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::ScaledTanh<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::ParametricSoftplus<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::ScaledTanh<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::ParametricSoftplus<float> >::ElementWiseKernel
) does not match the number of channels (
Input is expected to have four dimensions corresponding to [N,C,H,W], got 
onnxruntime::contrib::ImageScaler<float>::ImageScaler
Bias size (
info.GetAttrs<float>("bias", bias_).IsOK()
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\image_scaler.h
DynamicSlice
 input dimensions instead
Attribute border needs to be specified with four border elements, got 
) needs to be greater than or equal to the topBorder (
) + bottomBorder (
) + rightBorder (
Input's height (
Input's width (
) needs to be greater than or equal to the leftBorder (
) + scale_[1] (
) + scale_[0] (
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\crop.h
onnxruntime::contrib::Crop<float>::Compute
min_ngram_size
onnxruntime::contrib::BifurcationDetector::BifurcationDetector
info.GetAttr<int64_t>("min_ngram_size", &min_ngram_size_).IsOK()
min_ngram_size_ > 0
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\bert\bifurcation_detector.h
info.GetAttr<int64_t>("max_ngram_size", &max_ngram_size_).IsOK()
max_ngram_size
max_ngram_size_ >= min_ngram_size_
max_ngram_size_ > 0
onnxruntime::contrib::BifurcationDetector::Compute
src_tokens_len >= prev_suffix_match_idx_data
BifurcationDetector
pred_tokens_len == (src_tokens_len + 1 - prev_suffix_match_idx_data)
ngram_size
onnxruntime::contrib::NGramRepeatBlock::NGramRepeatBlock
info.GetAttr<int64_t>("ngram_size", &ngram_size_).IsOK()
ngram_size_ > 0
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\bert\ngram_repeat_block.h
onnxruntime::contrib::NGramRepeatBlock::Compute::<lambda_7f7ecb7c96871d43cc6d400ce6170f09>::operator ()
token_id < vocab_size
onnxruntime::contrib::NGramRepeatBlock::Compute
input_ids_dims.size() == 2
scores_dims[0] == batch_size
scores_dims.size() == 2
NGramRepeatBlock
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\bert\bias_gelu.cc
onnxruntime::contrib::BiasGelu<float,0>::Compute
onnxruntime::contrib::BiasGelu<float,1>::Compute
use_approximation
CDist
info.GetAttr<std::string>("metric", &metric).IsOK()
metric
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\cdist.h
onnxruntime::contrib::CDist<double>::CDist
euclidean
sqeuclidean
onnxruntime::contrib::CDist<float>::CDist
The second input of CDist kernel has wrong shape: 
The first input of CDist kernel has wrong shape: 
Input shape dimensions mismatch:
Number of dimensions for crop size should be exactly 1
Null crop_size_ptr
CropAndResize
onnxruntime::contrib::CropAndResize<float>::CropAndResize
 specified. It should be either bilinear or nearest
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops/cpu/crop_and_resize.h
Input tensor to Unique op should be 1D
onnxruntime::contrib::MaxpoolWithMask::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\maxpool_with_mask.h
MaxpoolWithMask
TransposeMatMul
positive
onnxruntime::contrib::MurmurHash3::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\murmur_hash3.cc
MurmurHash3
input_num_bytes % 4 == 0
Invalid assumption of output element size
sizeof(uint32_t) == output_element_bytes
SparseToDenseMatMul
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\math\sparse_dense_matmul.cc
 vs inner_B: 
Currently supporting only 2-D matrices
onnxruntime::contrib::SparseToDenseMatMul::Compute
Expecting 2xValues == indices
Expecting the same number NNZ == size of Inner indices
Can not multiply A and B as inner dimension does not match. inner_A: 
Expecting COO 2-D indices shape
Outer size must be M + 1
Currently support only COO and CSR(x64) formats
onnxruntime::contrib::`anonymous-namespace'::SparseToDenseCoo<unsigned __int64>::operator ()
 is out of bounds of lhs_right: 
COO indices must be 2-D, got: 
COO m index: 
onnxruntime::contrib::`anonymous-namespace'::SparseToDenseCoo<__int64>::operator ()
COO k index: 
 is out of bounds of out_left: 
onnxruntime::contrib::`anonymous-namespace'::SparseToDenseCoo<double>::operator ()
onnxruntime::contrib::`anonymous-namespace'::SparseToDenseCoo<float>::operator ()
onnxruntime::contrib::`anonymous-namespace'::SparseToDenseCoo<unsigned int>::operator ()
onnxruntime::contrib::`anonymous-namespace'::SparseToDenseCoo<int>::operator ()
embedding_size
conv_window_size
char_embedding_size
 conv filter size: 
 conv kernal size 1: 
 conv_window_size attribute: 
 embedding_size attribute: 
Conv filter size does not match embedding_size attribute.
 char_embedding_size attribute: 
Char embedding size does not match char_embedding_size attribute.
Conv kernal size 1 does not match conv_window_size attribute .
 Char embedding size: 
onnxruntime::contrib::WordConvEmbedding::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\word_conv_embedding.cc
 Conv kernal size 2 : 
Char embedding size does not match conv kernal size 2.
WordConvEmbedding
tensor(string) expected as input
Input dimensions are either [C] or [N][C] allowed
Match contains invalid utf8 chars: 
Tokenizer
onnxruntime::contrib::Tokenizer::Tokenizer
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\tokenizer.cc
attribute mark is not set
mincharnum
attribute mincharnum is not set
pad_value
attribute pad_value is not set
separators
tokenexp
attribute mincharnum must have a positive value
mincharnum_ > 0
!tokenexp.empty()
separators must not be empty
Either one of the separators OR tokenexp attributes required but none is set
Expecting a non-empty tokenexp
!char_tokenezation_ || mincharnum_ < 2
Can not digest separators: 
!separators.empty()
mincharnum is too big for char level tokenezation
Input string contains invalid utf8 chars: 
Can not digest tokenexp: 
Input string contains invalid utf8 chars
AttnLSTM
onnxruntime::contrib::DeepCpuAttnLstmOp::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\attnlstm\deep_cpu_attn_lstm.h
static_cast<int>(activation_func_names.size()) == num_directions_ * 3
Attention mechanism memory shape error! Expected: {
Attention mechanism memory sequence lengths must have shape {
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\attnlstm\deep_cpu_attn_lstm.cc
}, actural: 
Attention mechanism memory sequence lengths value must in (0, 
, am_attn_size}, Got:
 found!
], while 
Attention query layer weight shape error! Expected:{
}. Got: 
Attention memory layer weight shape error! Expected:{
}, Got: 
Attention layer weight shape error! Expected: {
onnxruntime::contrib::DeepCpuAttnLstmOp::ValidateInputs
Attention v weight shape error! Expected:{
, aw_attn_size}. Got:
onnxruntime::contrib::DeepCpuAttnLstmOp::DeepCpuAttnLstmOp
onnxruntime::contrib::DeepCpuAttnLstmOp::ComputeImpl
onnxruntime::contrib::FusedGemm<float>::FusedGemm
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\fused_gemm.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\fused_conv.cc
GetFusedActivationAttr(info, activation_).IsOK()
onnxruntime::contrib::FusedConvFloat::FusedConvFloat
onnxruntime::contrib::ExpandDims::Compute
axis_tensor->Shape().IsScalar()
An axis tensor must be a scalar tensor.
axis <= X_NumDims && axis >= -X_NumDims
Axis must be within range [
 Axis is 
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\expand_dims.h
ExpandDims
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\bert\embed_layer_norm.cc
onnxruntime::contrib::EmbedLayerNormBase::EmbedLayerNormBase
onnxruntime::contrib::EmbedLayerNorm<float>::Compute
input index out of range
Input 'bias' dimension 0 should have same length as dimension 1 of input 'weights'
qkv_sizes doesn't match the wights dimension
hidden_size should be divisiable by num_heads:
qkv_hidden_sizes first element should be same as the second
Inputs 'past' dimension 2 shall have length of num_heads
Inputs 'past' dimension 1 shall have same length as dimension 0 of input 0
Inputs 'past' dimension 0 shall have length of 2
Input 'past' is expected to have 5 dimension, got 
Inputs 'mask_index' with 3D data shall have shape batch_size x sequence_length x (past_sequence_length + sequence_length)
Inputs 'mask_index' with 2D data shall have shape batch_size x (past_sequence_length + sequence_length)
Inputs 'mask_index' with 1D data shall have length of batch_size or 2 * batch_size
Inputs 'past' dimension 2 shall have length of 
Input 'extra_add_qk' is expected to have 4 dimensions, got 
Input 'mask_index' is expected to have 1, 2, 3 or 4 dimensions, got 
Inputs 'mask_index' with 4D data shall have is_unidirectional_ set to false
Inputs 'mask_index' with 4D data shall have shape batch_size x 1 x max_sequence_length x max_sequence_length)
Input 'extra_add_qk' dimension 3 should be same as sequence_length, got 
Input 'extra_add_qk' dimension 2 should be same as sequence_length, got 
Input 'extra_add_qk' dimension 1 should be same as number of heads, got 
Input 'extra_add_qk' dimension 0 should be same as batch_size, got 
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\bert\attention.cc
onnxruntime::contrib::AttentionBase::GetPresent
Expect to have present state output when past state input is given
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\bert\attention_base.h
onnxruntime::contrib::AttentionBase::AttentionBase
info.GetAttr("num_heads", &num_heads).IsOK() && num_heads > 0
qkv_hidden_sizes
Input 1 dimension 0 should have same length as dimension 2 of input 0
Input 'weights' is expected to have 2 dimensions, got 
Input 'input' is expected to have 3 dimensions, got 
Attention cannot have past sequence and extra add qk
qkv_hidden_sizes attribute should have 3 elements
hidden_size should be divisiable by num_heads.
Input 1 dimension 1 should be 3 times of hidden dimension
Input 'bias' is expected to have 1 dimension, got 
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\bert\attention_cpu_base.h
onnxruntime::contrib::AttentionCPUBase::ApplyAttention
onnxruntime::contrib::Attention<float>::Compute
4D mask in attention cpu kernel is not supported
GridSample
padding_mode
mode_str == "bilinear" || mode_str == "nearest" || mode_str == "bicubic"
mode "
" not supported, expect bilinear, nearest or bicubic
bicubic
" not supported, expect zeros, border or reflection
reflection
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\grid_sample.cc
onnxruntime::contrib::GridSample<float>::GridSample
 does not match input batch size 
Only 4-D tensor is supported
padding_mode_str == "zeros" || padding_mode_str == "border" || padding_mode_str == "reflection"
padding_mode "
, expect 2
onnxruntime::contrib::GridSample<float>::Compute
grid_dims[0] == N
Grid batch size 
grid_dims[3] == 2
Last dimension of grid: 
SampleOp
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\quantization\quant_gemm.cc
onnxruntime::contrib::QGemm::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/math/gemm_helper.h
IsScalarOr1ElementVector(a_zp)
QGemm : zero point of input a must be a scalar or 1D tensor of size 1
onnxruntime::contrib::QGemm::CheckInputs
QGemm : scale of input a must be a scalar or 1D tensor of size 1
b_scale_shape.NumDimensions() == 0 || (b_scale_shape.NumDimensions() == 1 && (b_scale_shape[0] == 1 || b_scale_shape[0] == helper.N()))
QGemm : scale of input b must be a scalar or 1D tensor of size 1 or N
b_zp_shape.NumDimensions() == 0 || (b_zp_shape.NumDimensions() == 1 && (b_zp_shape[0] == 1 || b_zp_shape[0] == helper.N()))
QGemm : zero point of input b must be a scalar or 1D tensor of size 1 or N
y_zp == nullptr || IsScalarOr1ElementVector(y_zp)
QGemm : zero point of y must be null or a scalar or 1D tensor of size 1
b_scale_shape.NumDimensions() == b_zp_shape.NumDimensions() && (b_scale_shape.NumDimensions() == 0 || (b_scale_shape[0] == b_zp_shape[0]))
QGemm : zero point and scale of input b should have same shape size
y_scale == nullptr || IsScalarOr1ElementVector(y_scale)
QGemm : scale of y must be null or a scalar or 1D tensor of size 1
QGemm
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\bert\qembed_layer_norm.cc
onnxruntime::contrib::QEmbedLayerNorm<float>::Compute
Word embedding scale must be a scalar or 1D tensor of size 1
Beta scale must be a scalar or 1D tensor of size 1
Gamma scale must be a scalar or 1D tensor of size 1
Segment embedding scale must be a scalar or 1D tensor of size 1
Position embedding scale must be a scalar or 1D tensor of size 1
Gamma zero point must be a scalar or 1D tensor of size 1
Segment embedding zero point must be a scalar or 1D tensor of size 1
Position embedding zero point must be a scalar or 1D tensor of size 1
Word embedding zero point must be a scalar or 1D tensor of size 1
QEmbedLayerNormalization
Beta zero point must be a scalar or 1D tensor of size 1
input_shape.Size() > 0 || N == 0
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\quantization\nhwc_max_pool.cc
onnxruntime::contrib::NhwcMaxPool::Compute
} for per-channel quantization. Actual:
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\quantization\dynamic_quantize_lstm.cc
onnxruntime::contrib::DynamicQuantizeLSTM::PrePack
W_zero_point
 must have shape {
} for per-tensor/layer quantization or shape {
Weight zero point must be zero
R_scale
W_scale
R_zero_point
DynamicQuantizeLSTM
Recurrent
Weight point must be constant
DynamicQuantizeLSTM : 
MatmulInteger : b zero point is not valid
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\quantization\dynamic_quantize_matmul.cc
onnxruntime::contrib::MatMulIntegerToFloatBase::ComputeCommon
IsScalarOr1ElementVector(a_zero_point_tensor)
MatMulIntegerToFloat : input a zero point must be a scalar or 1D tensor of size 1. Per-Channel is not supported yet.
onnxruntime::contrib::DynamicQuantizeMatMul::Compute
IsBQuantParamSupported(b_zp_tensor->Shape(), b_tensor ? b_tensor->Shape() : b_shape_)
onnxruntime::contrib::MatMulIntegerToFloat::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\quantization\attention_quant.cc
onnxruntime::contrib::QAttention<float>::Compute
input zero point must be a scalar or 1D tensor of size 1.
input scale must be a scalar or 1D tensor of size 1
QAttention
QLinearMul
MatmulInteger : input1 A_scale must be a scalar or 1D tensor of size 1
MatmulInteger : input1 A_zero_point must be a scalar or 1D tensor of size 1 if given
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\qlinear_binary_op.cc
onnxruntime::contrib::`anonymous-namespace'::QLinearImpl
IsScalarOr1ElementVector(tensor_a_scale)
MatmulInteger : input1 B_zero_point must be a scalar or 1D tensor of size 1 if given
IsScalarOr1ElementVector(tensor_b_scale)
MatmulInteger : input1 B_scale must be a scalar or 1D tensor of size 1
tensor_a_zero_point == nullptr || IsScalarOr1ElementVector(tensor_a_zero_point)
MatmulInteger : input1 C_zero_point must be a scalar or 1D tensor of size 1 if given
IsScalarOr1ElementVector(tensor_c_scale)
MatmulInteger : input1 C_scale must be a scalar or 1D tensor of size 1
tensor_b_zero_point == nullptr || IsScalarOr1ElementVector(tensor_b_zero_point)
tensor_c_zero_point == nullptr || IsScalarOr1ElementVector(tensor_c_zero_point)
QLinearLeakyRelu
Input x_scale must be a scalar or 1D tensor of size 1
IsScalarOr1ElementVector(tensor_x_scale)
onnxruntime::contrib::QLinearAveragePool::Compute
IsScalarOr1ElementVector(tensor_y_scale)
input y_zero_point must be a scalar or 1D tensor of size 1 if given
tensor_y_zero_point == nullptr || IsScalarOr1ElementVector(tensor_y_zero_point)
Unsupported 'dtype' in QLinear Pooling:
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\qlinear_pool.cc
input x_zero_point must be a scalar or 1D tensor of size 1 if given
tensor_x_zero_point == nullptr || IsScalarOr1ElementVector(tensor_x_zero_point)
input y_scale must be a scalar or 1D tensor of size 1
QLinear Pooling unsupported pooling size!
QLinearAveragePool
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\qlinear_concat.cc
Input scale is not float for input def @
tensor_x_scale->IsDataType<float>()
Wrong input type encountered for zero point input def @
At least two inputs are needed, and each input must be (tensor, scale, zero_point) tuple!
input_def_count >= 8 && (input_def_count - 2) % 3 == 0
onnxruntime::contrib::QLinearConcat::QLinearConcat
Wrong input type encountered for zero point of quantized input @
QLinearConcat
tensor_x_zero_point->GetElementType() == tensor_y_zero_point->GetElementType()
input_count_x3 >= 6 && input_count_x3 % 3 == 0
onnxruntime::contrib::QLinearConcat::Compute
Input scale is not float for quantized input @
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\qlinear_global_average_pool.cc
IsScalarOr1ElementVector(tensor_x_zero_point)
IsScalarOr1ElementVector(tensor_y_zero_point)
QLinearGlobalAveragePool
onnxruntime::contrib::QLinearGlobalAveragePool::Compute
MatMulInteger16
A != nullptr && B != nullptr
onnxruntime::contrib::MatMulInteger16<short,short,int>::Compute
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\matmul_integer16.cc
info.GetAttr<int64_t>("channels_last", &channels_last_).IsOK()
onnxruntime::contrib::NchwcUpsample::Compute
invalid channel count
channels_ > 0
onnxruntime::contrib::NchwcConv::NchwcConv
pool_attrs_.kernel_shape.size() == 2
onnxruntime::contrib::ReorderInput::ReorderInput
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\nchwc_ops.h
info.GetAttr<int64_t>("channels", &channels_).IsOK()
onnxruntime::contrib::ReorderOutput::ReorderOutput
scales_[0] == 1 && scales_[1] == 1 && scales_[2] >= 1 && scales_[3] >= 1
info.GetAttr<std::string>("coordinate_transformation_mode", &transformation_mode).IsOK()
' for NCHWc Upsample
Unsupported transformation mode '
onnxruntime::contrib::NchwcPoolBase::NchwcPoolBase
info.GetAttrs<int64_t>("scales", scales_).IsOK()
onnxruntime::contrib::NchwcUpsample::NchwcUpsample
scales_.size() == 4
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\nchwc_ops.cc
(channels % 4) == 0
onnxruntime::contrib::ReorderOutput::Compute
channels_ <= X_shape[1]
transformation_mode_ == TransformationMode::ASYMMETRIC
Unsupported mode '
X_rank == 4
onnxruntime::contrib::ReorderInput::Compute
output and sum shape must match
onnxruntime::contrib::NchwcPoolBase::NchwcPool
(X_shape[1] % MlasNchwcGetBlockSize()) == 0
X_shape.size() == 4
onnxruntime::contrib::NchwcConv::Compute
X_shape.NumDimensions() == 4
(static_cast<size_t>(X_shape[1]) < nchwc_block_size) || ((X_shape[1] % nchwc_block_size) == 0)
Unsupported convolution size.
Input 0 is expected to have 1 or more dimensions, got 
Input 1 is expected to have 1 dimensions, got 
Input 1 dimension 0 should have same length as the last dimension of input 0
 rows[
 [seqno=
 row[
Real memory steps 
mem_steps <= max_memory_steps_ && mem_steps > 0
onnxruntime::contrib::BahdanauAttention<float>::PrepareMemory
!normalize_
onnxruntime::contrib::BahdanauAttention<float>::BahdanauAttention
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\attnlstm\bahdanau_attention.cc
 is not in (0, 
not support normalize yet.
activation_params count mismatch
unimplemented activation: 
word_embedding is expected to have 2 dimensions, got 
input_ids is expected to have 2 dimensions, got 
segment_embedding is expected to have 2 dimensions, got 
position_embedding is expected to have 2 dimensions, got 
input_ids and position_ids shall have same shape
Input 0 and 7 (mask) shall have same shape
Input 0 and 1 shall have same shape
beta is expected to have size of 
beta is expected to have 1 dimensions, got 
word_embedding and position_embedding shall have same dimension 1
word_embedding and segment_embedding shall have same dimension 1
gamma is expected to have size of 
gamma is expected to have 1 dimensions, got 
onnxruntime::contrib::QlinearBuildLookupTable
QlinearBuildLookupTable : input X_scale must be a scalar or 1D tensor of size 1
QlinearBuildLookupTable : input X_zero_point must be a scalar or 1D tensor of size 1
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\qlinear_lookup_table.cc
QlinearBuildLookupTable : input Y_zero_point must be a scalar or 1D tensor of size 1
QlinearBuildLookupTable : input Y_scale must be a scalar or 1D tensor of size 1
onnxruntime::concurrency::CreateThreadPoolHelper
custom join thread function not set
Received null OrtThreadingOptions
D:\a\_work\1\s\engine\lotus\onnxruntime\core\util\thread_utils.cc
Received invalid value for allow_spinning. Valid values are 0 or 1
onnxruntime::math::NextPosition
dims[d_i] < d_max
D:\a\_work\1\s\engine\lotus\onnxruntime\core\util\math_cpu.cc
Tensor types should have been handled already
OrtValue is TensorSequence type but has no element Tensor DataType.
 is not supported
OpenVINO_GPU
CudaPinned
Specified device is not supported.
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\allocator.cc
onnxruntime::IAllocator::CalcMemSizeForArrayWithAlignment::<lambda_c6b03386de95717d3eea3a793c94afde>::operator ()
Not implemented
`anonymous-namespace'::GetIndicesTensor
Unsupported indices_format passed
Argument is not a tensor
the ort_value must contain a constructed tensor or sparse tensor
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\tensor_type_and_shape.cc
OrtApis::GetTensorTypeAndShape
type_proto is not of type sequence!
type_proto is not of type map!
p_type != nullptr
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\tensor.cc
onnxruntime::Tensor::Tensor
Tensor is expected to contain one of the primitive data types. Got: 
onnxruntime::Tensor::Init
dtype_ != nullptr
tensor failed memory size calculation
shape.Size() must >=0
onnxruntime::Tensor::SizeInBytes
tensor size overflow
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\data_transfer.cc
onnxruntime::IDataTransfer::CopyTensors
src.SizeInBytes() == dst.SizeInBytes()
onnxruntime::IDataTransfer::CopySparseTensors
onnxruntime::CPUDataTransfer::CopyTensor
elem_proto != nullptr
D:\a\_work\1\s\engine\lotus\include\onnxruntime\core/framework/data_types.h
onnxruntime::data_types_internal::OptionalTypeHelper::Set
onnxruntime::data_types_internal::SequenceTypeHelper::Set
expected a registered ONNX type
onnxruntime::data_types_internal::MapTypeHelper::Set
value_proto != nullptr
onnxruntime::SequenceTensorTypeBase::GetElementType
onnxruntime::SparseTensorTypeBase::GetElementType
onnxruntime::TensorTypeBase::GetElementType
onnxruntime::OptionalTypeBase::GetElementType
onnxruntime::OptionalTypeBase::GetDeleteFunc
onnxruntime::data_types_internal::DataTypeRegistry::RegisterDataType
proto != nullptr
thisProto->value_case() == TypeProto::ValueCase::kTensorType
We do not expect duplicate registration of types for: 
onnxruntime::data_types_internal::IsCompatible
Only ONNX MLDataType can be registered
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\data_types.cc
 is not currently registered or supported
sparse tensor type 
MLDataType for: 
(unknown type)
bfloat16
tensor type 
Invalid DataTypeImpl TypeProto definition
onnxruntime::utils::ContainerChecker::ContainerChecker
thisProto->value_case() == TypeProto::ValueCase::kSequenceType
utils::HasElemType(thisProto->sparse_tensor_type())
utils::HasElemType(thisProto->sequence_type())
onnxruntime::SequenceTensorTypeBase::IsCompatible
utils::HasElemType(thisProto->tensor_type())
onnxruntime::TensorTypeBase::IsCompatible
onnxruntime::SparseTensorTypeBase::IsCompatible
thisProto->value_case() == TypeProto::ValueCase::kSparseTensorType
utils::HasKeyType(thisProto->map_type())
onnxruntime::NonTensorTypeBase::IsMapCompatible
onnxruntime::NonTensorTypeBase::IsSequenceCompatible
onnxruntime::OptionalTypeBase::IsCompatible
thisProto->value_case() == TypeProto::ValueCase::kOptionalType
thisProto->value_case() == TypeProto::ValueCase::kMapType
utils::HasElemType(thisProto->optional_type())
float
double
onnxruntime::NonTensorTypeBase::FromDataContainer
(null)
onnxruntime::NonTensorTypeBase::ToDataContainer
int64
uint32
float16
uint64
int16
uint8
int32
uint16
onnxruntime::SparseTensor::MakeCooStrings
Format() == SparseFormat::kUndefined
onnxruntime::SparseTensor::MakeCooData
Expecting data type to be set as string
Not expecting an allocator set
Use MakeCooStrings
Sparse format must not be set. Already contains format: 
onnxruntime::SparseTensor::UseCooIndices
format_data_.size() == 2U
dense shape must 2-D. Got: 
onnxruntime::SparseTensor::AsCsr
Expecting two indices. Got: 
Must contain Csr format. Contains: 
Format() == SparseFormat::kCsrc
This method should follow a call to constructor that supplies the allocator
allocator_ != nullptr
This method does not expect allocator to be set
onnxruntime::SparseTensor::UseCsrIndices
 rows: 
Outer index count must be rows + 1 or zero. Got: 
 the same as values size: 
Expecting inner index size: 
onnxruntime::SparseTensor::ValidateCsrIndices
Inner and Outer indices must either be both zero or non-zero
Expecting one index. Got: 
Expecting to have at lest 3-D shape. Got:
Format() == SparseFormat::kBlockSparse
onnxruntime::SparseTensor::AsBlockSparse
onnxruntime::SparseTensor::MakeCsrStrings
Must contain BlockSparse format. Got: 
Use MakeCsrStrings
onnxruntime::SparseTensor::MakeCsrData
onnxruntime::`anonymous-namespace'::CopyData
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\sparse_tensor.cc
 dst_size: 
Must have the same size. Got src_size: 
Divide by zero
SafeIntExceptionHandler<class onnxruntime::OnnxRuntimeException>::SafeIntOnDivZero
SparseTensor Allocation failed for size: 
Must contain Coo format. Got: 
Values size 
onnxruntime::SparseTensor::AllocateBuffer
this tensor already has populated sparse_indices
 must be less than total buffer size: 
the ort_value must contain a constructed sparse tensor
onnxruntime::SparseTensor::GetSparseTensorFromOrtValue
values_count == index_size
onnxruntime::SparseTensor::GetCooIndexDims
 must be equal to or twice the values size: 
Index size: 
Expecting to contain one index, got: 
format_data_.size() == 1U
Format() == SparseFormat::kCoo
onnxruntime::SparseTensor::AsCoo
Expecting fully sparse tensors to have indices shape {0}
onnxruntime::SparseTensor::UseBlockSparseIndices
Expecting index blocks: 
Expecting fully sparse tensors to have value shape {0}
Indices shape must have dim[0] == 2
 to be equal to values blocks: 
onnxruntime::SparseTensor::ValidateBlockSparseShapes
Expecting indices to have 2-D shape . Got: 
This instance should not be empty
Destination should be empty
Unable to find a data transfer for copying from device type: 
onnxruntime::SparseTensor::Copy
onnxruntime::SparseTensor::MakeBlockSparseStrings
 to device type: 
Use MakeBlockSparseStrings
onnxruntime::SparseTensor::MakeBlockSparseData
Src and Dst must be of the same type
Must have the same shape
Destination must have a CPU allocator set
X-device copy of strings not supported
Invalid dimension of 
dimension <= num_dims
 dimensions.
 for SizeFromDimension. Tensor has 
dimstart <= dimend && dimend <= values_.size()
onnxruntime::TensorShape::Slice
onnxruntime::TensorShape::SizeFromDimension
Invalid tensor shape slice argument.
onnxruntime::TensorShape::SizeToDimension
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\tensor_shape.cc
Config value is longer than maximum length 1024
]. It will be overwritten
Config key is empty or longer than maximum length 128
onnxruntime::ConfigOptions::AddConfigEntry
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\config_options.cc
] already exists with value [
Config with key [
An OrtValue for this name has already been added.
Received OrtValue is not a tensor. Only tensors are supported.
Buffer containing the initializer must be owned by the user.
Received nullptr for name.
Received nullptr for OrtValue.
Attribute: 
 but is of type: 
 expected to be of type: 
 is defined.
No attribute with name: 
values.size() == static_cast<size_t>(attr->ints_size())
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\op_node_proto_helper.cc
onnxruntime::OpNodeProtoHelper<struct onnx::InferenceContext>::GetAttrs
values.size() == static_cast<size_t>(attr->floats_size())
onnxruntime::OpNodeProtoHelper<class onnxruntime::ProtoHelperNodeContext>::GetAttrs
Attibute name and type don't match
No attribute with this name is defined.
Requested attribute: 
 is expected to have type: 
onnxruntime::NodeIndexInfo::GetNodeOffset
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/node_index_info.h
node_offsets_index < node_offsets_size_
Execution frame was null
frame != nullptr
Invalid arg_num of 
arg_num < arg_counts.size()
onnxruntime::OpKernelContext::OutputMLValue
. Num args is 
OpKernel was null
kernel != nullptr
onnxruntime::OpKernelContext::OpKernelContext
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\op_kernel.cc
onnxruntime::OpKernelContext::GetOrCreateOutputMLValue
onnxruntime::OpKernelContext::NumVariadicInputs
TempSpace allocator not found
onnxruntime::OpKernelInfo::GetMemoryInfo
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\op_kernel_info.cc
cannot find allocator
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\kernel_def_builder.cc
input_offset >= 0 && output_offset >= 0
onnxruntime::KernelDefBuilder::VariadicAlias
 kernel is not supported in 
Kernel not found
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\kernel_registry.cc
 Encountered following errors: (
 but the node in the model has the following type (
onnxruntime::KernelRegistry::TryCreateKernel
Found kernel for Op with name (
onnxruntime::KernelRegistry::Register
Failed to add kernel for 
: Conflict with existing kernel def hash.
kernel def can't be NULL
: Conflicting with a registered kernel with op versions.
 kernel_end_version: 
 kernel start version: 
 (node_version: 
 in the supported version range
 This op has been implemented only for the following types (
 However the types are incompatible.
 and type (
Op with name (
 node_version: 
 Version mismatch.
len <= op_schema.inputs().size()
onnxruntime::`anonymous-namespace'::TraverseFormalParametersWithTypeProto
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\fallback_cpu_capability.cc
kernel_info != nullptr
Candidate for fallback CPU execution: 
onnxruntime::GetCpuPreferredNodes::<lambda_21ca116bba4007835cdddfee631b7f69>::operator ()
 because the CPU execution path is deemed faster than overhead involved with execution on other EPs 
ORT optimization- Force fallback to CPU execution for node: 
onnxruntime::GetCpuPreferredNodes
 capable of executing this node
 type: 
UnpackTensor: the pre-allocated size does not match the raw data size, expected 
Tensor does not have external data to read from.
Invalid SparseTensor indices. Should be rank 0 or 1. Got:
onnxruntime::utils::SparseTensorProtoToDenseTensorProto
indices_shape[1] > 0 && static_cast<size_t>(indices_shape[1]) == dims.size()
cur_index == &*indices_data.cend()
Invalid SparseTensor indices. INT8 indices must be in the raw data of indices tensor
Invalid SparseTensor indices. Should one of the following types: int8, int16, int32 or int64
Sparse indices int32 data size does not match expected
Invalid SparseTensor indices. INT16 indices must be in the raw data of indices tensor
 data_type: 
onnxruntime::utils::UnpackInitializerData
HasDataType(dense_proto)
onnxruntime::utils::DenseTensorToSparseTensorProto
Unsupported sparse tensor data type of 
Must have a valid data type
 is not supported.
Element_size of: 
Unsupported type: 
onnxruntime::utils::UnpackTensorWithExternalDataImpl
`anonymous-namespace'::ReadExternalDataForTensor
nullptr == p_data
External data type cannot be UNDEFINED or STRING.
TensorProto external data size mismatch. Computed size: 
`anonymous-namespace'::GetExternalDataInfo
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\tensorprotoutils.cc
TensorProtoToTensor() tensor shape mismatch!
 can not be writen into Tensor type 
data overflow
onnxruntime::utils::GetFileContent
corrupted protobuf data: tensor shape size(
UnpackTensor: the pre-allocate size does not match the size in proto
) in proto
) does not match the data size(
string tensor can not use pre-allocated buffer
, Got 
Initialized tensor with unexpected type: 
TensorProtoToMLValue() must take a pre-allocated MemBuffer!
string tensor can not have raw data
tensor can't contain negative dims
TensorProto type 
onnxruntime::utils::TensorProtoToTensor
onnxruntime::utils::CopySparseData
Sparse indices int64 data size does not match expected
Unsupported attribute value type of 
Sparse Indices raw data size does not match expected.
onnxruntime::utils::ConstantNodeProtoToTensorProto
 in 'Constant' node '
The preallocated buffer is too small. Requires 
onnxruntime::utils::TensorProtoToMLValue
Invalid TensorProto
onnxruntime::AllocatorManager::InsertAllocator
duplicated allocator
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\allocatormgr.cc
onnxruntime::CreateAllocator
Received invalid value of arena_extend_strategy 
onnxruntime::IExecutionProvider::TryInsertAllocator
duplicated allocator: 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\execution_provider.cc
onnxruntime::IExecutionProvider::InsertAllocator
onnxruntime::IExecutionProvider::GenerateMetaDefId
metadef_id_generator_
IExecutionProvider constructor must be called with true for use_metadef_id_creator
IExecutionProvider::Compile with FusedNodeAndGraph is not implemented by 
IExecutionProvider::Compile with fused Node and dll path is not implemented by 
IExecutionProvider::Compile with fused Node is not implemented by 
onnxruntime::sparse_utils::DenseTensorToSparseCsr
Unable to convert strings tensor to a sparse tensor that not on CPU
Currently do not support dims higher than 2 dimensions: 
onnxruntime::sparse_utils::SparseCsrToDenseTensor
inner_num == src.Values().Shape().Size()
Expecting inner indices to be same as nnz. Got: 
Unable to convert strings tensor to a sparse tensor that is not on CPU
Input must be of CSR format
Support 2-D matrices only
Unsupported element size: 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\sparse_utils.cc
onnxruntime::sparse_utils::DenseTensorToSparseCoo
Invalid index: 
 > dense_size: 
onnxruntime::sparse_utils::SparseCooToDenseTensor
Expecting indices to be equal the number of values or be twice as many
Input must be of COO format
outer_num == (rows + 1)
Outer indices must be M + 1. Got: 
onnxruntime::DataTransferManager::CopySparseTensors
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\data_transfer_manager.cc
onnxruntime::DataTransferManager::CopyTensors
There's no data transfer registered for copying tensors from 
Tensor size mismatch
data_transfer registered is nullptr.
InternalTestingExecutionProvider
Unsupported OrtValue type to copy between device.
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\utils.cc
onnxruntime::utils::BatchOrCopyMLValue
allocator != nullptr
Failed to find allocator for device 
Unsupported OrtValue type.
invalid allocator.
fetch_alloc_info.size() == copy_info.size()
onnxruntime::utils::FinalizeCopyInfoForFeeds
feed_locations.size() == copy_info.size()
onnxruntime::utils::InitializeFeedFetchCopyInfo
onnxruntime::utils::CalculateStaticCopyInfoForFeeds
onnxruntime::utils::CalculateStaticCopyInfoForFeed
exec_plan_ptr
onnxruntime::utils::FindMemoryInfoForValue
onnxruntime::utils::ExecuteGraph
onnxruntime::utils::ExecuteGraphImpl
Only one thread was configured for parallel execution. Hence will use sequential execution.
onnxruntime::utils::CopyOutputsAcrossDevices
onnxruntime::utils::CopyOneInputAcrossDevices
onnxruntime::utils::CopyInputsAcrossDevices
copy_info.size() == num_feeds
onnxruntime::utils::FinalizeCopyInfoForFetches
Could not find OrtValue with idx '
onnxruntime::SessionState::SetupAllocators
Allocator already registered for 
. Ignoring allocator from 
session.disable_prepacking
. Do you have duplicated calls to SessionState::AddInitializedTensor function?
onnxruntime::SessionState::GetNodeKernelCreateInfo
entry != kernel_create_info_map_.cend()
onnxruntime::SessionState::PopulateKernelCreateInfo
Done saving OrtValue mappings.
onnxruntime::SessionState::CreateGraphInfo
SaveMLValueNameIndexMapping
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\session_state.cc
onnxruntime::SessionState::PrepackConstantInitializedTensors::<lambda_61c6200b3fa909ae7eb7a3a40a81346e>::operator ()
allocator_for_caching.get() != nullptr
The kernel corresponding to the node 
 doesn't have an implementation that can consume provided pre-packed weights
onnxruntime::KernelUseSharedPrePackedBuffers
duplicated ort_value index:
Unable to write the provided PrePackedWeights instance into the container
Using cached version of pre-packed weight for constant initializer: 
 used in the node: 
 which is of op type: 
!op_type.empty()
The op type of a node cannot be empty
weights_to_be_filled_in.buffers_.size() > 0
 doesn't have an implementation that can cache computed pre-packed weights
p_op_kernel
entry != node_to_subgraph_ss.second.cend()
Missing session state for subgraph. Node:'
' OpType:
 Index:
 Attribute:
onnxruntime::SessionState::FinalizeSessionStateImpl
onnxruntime::SessionState::AddOutputNameToNodeInfoMapping
output_names_to_nodeinfo.empty()
Only one node should produce an output. Existing entry for 
Failed to find input name in the mapping: 
Using an input in multiple nodes on different devices is not supported currently. Input:
 is used by node 
) and node 
onnxruntime::SessionState::UpdateToBeExecutedNodes
onnxruntime::SessionState::GetNodeIndexInfo
node_index_info_
SetGraphAndCreateKernels must be called prior to GetExecutionInfo.
onnxruntime::SessionState::AddSubgraphSessionState
existing_entries.find(attribute_name) == existing_entries.cend()
Entry exists in node 
 for attribute 
. Invalid ORT format model.
onnxruntime::SessionState::LoadFromOrtFormat
onnxruntime::SessionState::LoadFromOrtFormat::<lambda_c5c5b355d2a2c88bd85d8931b713a2e0>::operator ()
onnxruntime::SessionState::CreateSubgraphSessionState
subgraph
Main Graph instance should have populated all subgraphs when being resolved.
onnxruntime::SessionState::SaveToOrtFormat
onnxruntime::GetSubGraphSessionStatesOrtFormat
onnxruntime::AccumulateAllNestedSubgraphsInfo
subgraphs_kernel_create_info_maps.find(local_subgraph_kernel_create_info_map_key) == subgraphs_kernel_create_info_maps.end()
onnxruntime::OuterScopeNodeArgLocationAccumulator
onnxruntime::OuterScopeNodeArgLocationAccumulator::<lambda_f8822893c953bd42037266e93f73744e>::operator ()
onnxruntime::OuterScopeNodeArgLocationAccumulator::<lambda_10d9e40dd67492c4fb3c3f078abc9bd8>::operator ()
onnxruntime::SessionState::FinalizeSessionState
Unable to find compiled kernel hash for node '
Can't find node with index 
Kernel create info is null. Invalid ORT format model.
SessionState for subgraphs is null. Invalid ORT format model.
Subgraph SessionState for 
 is null. Invalid ORT format model.
Subgraph SessionState entry for 
 is missing. Invalid ORT format model.
Size mismatch for kernel create info node indexes and hashes. Invalid ORT format model.
Kernel create info hashes are null. Invalid ORT format model.
Kernel create info node indices are null. Invalid ORT format model.
Failed to find kernel for 
The node is not placed on any Execution Provider. 
 (node 
found duplicated provider 
 in KernelRegistryManager
nullptr != type_proto
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\mldata_type_utils.cc
onnxruntime::utils::GetMLDataType
onnxruntime::FeedsFetchesManager::SetDeviceCopyChecks
input_copy_needed != DeviceCopyCheck::Unknown && output_copy_needed != DeviceCopyCheck::Unknown
Error mapping output names: 
Error mapping feeds: 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\feeds_fetches_manager.cc
onnxruntime::FeedsFetchesInfo::MapNamesToMLValueIdxs
onnxruntime::FunctionKernel::FunctionKernel
fusion_style == IExecutionProvider::FusionStyle::Function
Must use Function based fusion when exporting compiled nodes to dll.
onnxruntime::PartitionOnnxFormatModelImpl
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\graph_partitioner.cc
onnxruntime::PlaceNode
1 == capability.nodes.size()
compute_info_->create_state_func(&context, &func_state_) == 0
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/func_kernel.h
. Execution Provider must generate unique names across the entire model.
single_node_compute_func should have 1 elements
 has Compile error: 
onnxruntime::PartitionOrtFormatModelImpl
onnxruntime::GraphPartitioner::PartitionOnnxFormatModel
onnxruntime::InlineNodes
 did not return correct number of compiled functions
compiled_kernel_hashes != nullptr
Compiled kernel hashes must be provided
onnxruntime::GraphPartitioner::Partition
No provider specified.
onnxruntime::GraphPartitioner::PartitionOrtFormatModel
Existing entry in compiled kernel hashes for 
p_int < base_int + memory_size_
onnxruntime::BFCArena::AllocationRegion::IndexFor
p_int >= base_int
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/bfc_arena.h
onnxruntime::BFCArena::AllocationRegion::AllocationRegion
0 == memory_size % kMinAllocationSize
 initial_growth_chunk_size_bytes: 
 memory limit: 
 arena_extend_strategy: 
onnxruntime::BFCArena::RegionManager::RegionFor
Could not find Region for 
onnxruntime::BFCArena::RegionManager::RemoveAllocationRegion
entry != regions_.end()
Could not find Region for: 
BinForSize(bin_size) == BinFromIndex(b)
Creating 
 bins of max chunk size 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\bfc_arena.cc
onnxruntime::BFCArena::BFCArena
Creating BFCArena for 
 with following configs: initial_chunk_size_bytes: 
 max_dead_bytes_per_chunk: 
 The total allocated bytes is now 
onnxruntime::BFCArena::SplitChunk
!c->in_use() && (c->bin_num == kInvalidBinNum)
onnxruntime::BFCArena::FindChunkPtr
!chunk->in_use()
Failed to find a free memory block despite calling Extend. rounded_bytes=
onnxruntime::BFCArena::InsertFreeChunkIntoBin
c2->prev == h1
onnxruntime::BFCArena::Merge
!c1->in_use() && !c2->in_use()
onnxruntime::BFCArena::DeallocateRawInternal
onnxruntime::BFCArena::Shrink
 BFC Arena shrunk by 
 bytes. 
onnxruntime::BFCArena::FreeAndMaybeCoalesce
c->in_use() && (c->bin_num == kInvalidBinNum)
BinFromIndex(c->bin_num)->free_chunks.erase(h) > 0
Could not find chunk in bin
onnxruntime::BFCArena::RemoveFreeChunkFromBin
onnxruntime::BFCArena::RemoveFreeChunkIterFromBin
!c->in_use() && (c->bin_num != kInvalidBinNum)
Incorrect arena extend strategy.
hipMalloc
cudaMalloc
onnxruntime::BFCArena::ChunkFromHandle
h < chunks_.size()
BinForSize(bin_size * 2) != BinFromIndex(b)
BinForSize(bin_size * 2 - 1) == BinFromIndex(b)
BinForSize(bin_size + 255) == BinFromIndex(b)
Total allocated bytes: 
onnxruntime::BFCArena::Extend
Extended allocation by 
 bytes.
Failed to allocate memory for requested buffer of size 
Available memory of 
 is smaller than requested bytes of 
onnxruntime::BFCArena::Extend::<lambda_1064b317e773adcb1310893e56d55e4e>::operator ()
h != kInvalidChunkHandle
reserved_chunks_.find(ptr) == reserved_chunks_.end()
onnxruntime::BFCArena::Reserve
Reserving memory in BFCArena for 
 size: 
Allocated memory at 
Extending BFCArena for 
. bin_num:
 (requested) num_bytes: 
 (actual) rounded_bytes:
onnxruntime::BFCArena::AllocateRawInternal
tried to allocate 0 bytes
Unloading DSO 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\ex_lib_loader.cc
onnxruntime::ExLibLoader::{dtor}::<lambda_c97304e2ece92b7381e847cbdb64609e>::operator ()
Caught exception while destructing CustomOpsLoader with message: 
onnxruntime::ExLibLoader::LoadExternalLib
A dso with name 
 has already been loaded.
Caught exception while loading custom ops with message: 
Failed to unload DSO: 
onnxruntime::ExLibLoader::~ExLibLoader
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/execution_frame.h
onnxruntime::IExecutionFrame::GetMLValue
ort_value_index >= 0 && static_cast<size_t>(ort_value_index) < all_values_size_
onnxruntime::IExecutionFrame::GetOrCreateNodeOutputMLValue
shape && tensor.Shape() == *shape
OrtValue shape verification failed. Current shape:
 Requested shape:
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\execution_frame.cc
onnxruntime::IExecutionFrame::IExecutionFrame
node_index_info_.GetMaxMLValueIdx() == ort_value_idx_map.MaxIdx()
node_index_info and ort_value_idx_map are out of sync and cannot be used
offset >= 0 && static_cast<size_t>(offset) < node_values_size_
onnxruntime::NodeIndexInfo::GetMLValueIndex
onnxruntime::ExecutionFrame::GetAllocationPlan
ort_value_idx >= 0 && static_cast<size_t>(ort_value_idx) < alloc_plan.size()
onnxruntime::ExecutionFrame::ReleaseMLValueImpl
onnxruntime::ExecutionFrame::VerifyOutputSizes
Expected shape from model of 
 does not match actual shape of 
 for output 
Invalid allocation kind: 
Memory pattern planner is not enabled on this execution framework.
TraceFree for ort_value_idx=
onnxruntime::ExecutionFrame::TraceFree
onnxruntime::ExecutionFrame::TraceAllocate
TraceAllocation for ort_value_idx=
 size=
 failed: 
 failed. Error:
Fetches vector passed to GetOutputs contains 
 entries which doesn't match the number of fetches the frame was initialized with of 
fetches.empty() || fetches.size() == fetch_mlvalue_idxs_.size()
onnxruntime::IExecutionFrame::Init
feeds.size() == feed_mlvalue_idxs.size()
invalid index 
shape && sp_tensor.DenseShape() == *shape
Tensor shape cannot contain any negative value
Trying to allocate memory for unused optional inputs/outputs
 returned nullptr
onnxruntime::ExecutionFrame::ExecutionFrame
buffers_.find(location) == buffers_.end()
onnxruntime::ExecutionFrame::{ctor}::<lambda_892836f21345417272512adea8711b39>::operator ()
Allocation of memory pattern buffer for 
Shape mismatch attempting to re-use buffer. 
. Validate usage of dim_value (values should be > 0) and dim_param (all values with the same string should equate to the same size) in shapes in the model.
For ort_value with index: 
, block in memory pattern size is: 
 but the actually size is: 
, fall back to default allocation behavior
onnxruntime::ExecutionFrame::AllocateMLValueTensorSelfOwnBufferHelper
ort_value.Fence() == nullptr
Allocation of tensor types requires a shape.
We don't expect custom allocators for non-tensor types, so a shape is mandatory here.
onnxruntime::ExecutionFrame::AllocateAsPerAllocationPlan
ort_value_index >= 0 && static_cast<size_t>(ort_value_index) < alloc_plan.size()
onnxruntime::ExecutionFrame::AllocateReusedOrtValueIfNotAllocatedHelper
onnxruntime::AllocateSparseTensor
mlvalue.Fence() == nullptr
onnxruntime::ExecutionFrame::AllocateMLValueTensorPreAllocateBuffer
parsing 
checksum
model format error!
model format error! Missing 'location'
model format error! Need a key for the external data info
model format error! Need a value for the external data info
location
 failed
onnxruntime::NodeIndexInfo::Init::<lambda_e256c24a178e1d4e49acb7fa7090a7cb>::operator ()
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\node_index_info.cc
onnxruntime::NodeIndexInfo::Init::<lambda_ca37d5ec199e35c30b080471c345d05e>::operator ()
Can't slice a non-tensor OrtValue. Type was 
ort_value.IsTensor()
onnxruntime::OrtValueTensorSlicer<struct OrtValue>::Create
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\ort_value_tensor_slicer.cc
OrtValue has not been allocated so can't be sliced.
onnxruntime::OrtValueTensorSlicer<struct OrtValue const >::Create
onnxruntime::OrtValueTensorSlicer<struct OrtValue const >::Iterator::Iterator
ort_value.IsAllocated()
. Shape:
Insufficient dimensions to slice on 
gsl::narrow_cast<int64_t>(tensor_shape.NumDimensions()) >= slice_dimension
. Dimension 0 is 
Invalid dim0_offset of 
dim0_offset < dim0_size
onnxruntime::OrtValueTensorSlicer<struct OrtValue>::Iterator::Iterator
 not found.
onnxruntime::FuncManager::GetFuncs
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\fuse_nodes_funcs.cc
 already exist.
func info for node: 
Can't use func with null ptr
source and destination buffer size mismatch
onnxruntime::utils::detail::CopyLittleEndian
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\endian_utils.cc
Multiple errors were found.
onnxruntime::ParallelExecutor::Execute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\parallel_executor.cc
ParallelExecutor::Execute
Begin execution
onnxruntime::ParallelExecutor::RunNodeAsync
Exiting due to terminate flag being set to true.
Got nullptr from GetKernel for node: 
 does not.
All implicit inputs should have OrtValue instances by now. 
entry != nullptr
onnxruntime::OpKernelContextInternal::OpKernelContextInternal
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/op_kernel_context_internal.h
Exception running nodes starting at 
_fence_before
' Status Message: 
 node. Name:'
Non-zero status code returned while running 
thread_scheduling_stats
_kernel_time
_fence_after
Unknown exception was caught by catch-all handler.
onnxruntime::SequentialExecutor::Execute
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\sequential_executor.cc
graph_index
exec_plan_index
activation_size
parameter_size
output_size
SequentialExecutor::Execute
onnxruntime::ReleaseNodeMLValues
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\allocation_planner.cc
SessionState should have saved the KernelCreateInfo prior to this running. NodeIndex:
entry != kernel_create_info_map.cend()
onnxruntime::GetKernelCreateInfo
reused != reused_for
onnxruntime::PlannerImpl::Reuse
nullptr != tensor_type_base
onnxruntime::PlannerImpl::GetElementSize
There is no location for this node arg in the outer scope location map
found_in_outer_scope_location_map
onnxruntime::PlannerImpl::ComputeUseCounts::<lambda_f3e7e1c16a77783b35cd48a7c1464d44>::operator ()
Can not find the node 
onnxruntime::PlannerImpl::Index
n >= 0 && static_cast<size_t>(n) < ort_value_info_.size()
onnxruntime::PlannerImpl::UseCount
onnxruntime::PlannerImpl::Buffer
n >= 0 && static_cast<size_t>(n) < plan_.allocation_plan.size()
onnxruntime::PlannerImpl::AllocPlan
id >= 0 && static_cast<size_t>(id) < ort_value_info_.size()
onnxruntime::PlannerImpl::ProcessDef
Previous entry was not terminated.
starts_.size() == ends_.size()
onnxruntime::AllocPlanPerValue::ProgramCounter::AddStart
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/sequential_execution_plan.h
Invalid 'start'. Value is smaller than previous 'end'.
starts_.empty() || start > ends_.back()
No matching 'start' entry.
starts_.size() == ends_.size() + 1
onnxruntime::AllocPlanPerValue::ProgramCounter::AddEnd
Invalid 'end'. Value is larger than 'start'.
end >= starts_.back()
onnxruntime::PlannerImpl::GeneratePlanForWeightsHelper
onnxruntime::PlannerImpl::ComputeReusePlan
Only tensors are supported for external outputs for now.
!IsNonTensor(*node_output)
Invalid program_counter entries at index 
entry.program_counter.HasValidEntries()
onnxruntime::PlannerImpl::VerifyMemoryTimeSchedule
AllocPlan(ml_value_idx).program_counter.Ends().back() == program_counter
Should not have entry in kernel create info with nullptr for kernel_def
p_kernel_def
onnxruntime::PlannerImpl::ComputeUseCounts
Can not find the execution provider 
allocator
onnxruntime::PlannerImpl::GetLocationForNodeInput
specific_subgraph_kernel_create_info_map != subgraphs_kernel_create_info_maps_.end()
onnxruntime::PlannerImpl::GenerateDeallocationPlan
onnxruntime::PlannerImpl::CreatePlan
onnxruntime::PrePackedWeights::GetHash
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\prepacked_weights.cc
buffers_.size() == buffer_sizes_.size()
Failed memory size calculation
DeserializeTensorProto() takes either pre-allocated buffer or an allocator!
Internal error. The preallocated buffer is too small. Requires 
onnxruntime::session_state_utils::DeserializeTensorProto
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\session_state_utils.cc
string tensor is not supported for copying between allocators
Failed to copy tensor to 
session.use_device_allocator_for_initializers
entry != initialized_tensors_to_allocate.end() && entry->second->data_type() != ONNX_NAMESPACE::TensorProto_DataType_STRING
 bytes for 
[Memory] SessionStateInitializer statically allocates 
Using user supplied initializer with name (
 failed.
Deserialize tensor 
Done saving initialized tensors
onnxruntime::session_state_utils::SaveInputOutputNamesToNodeMapping::<lambda_5f4d12f5625f87c7fb1ce2d7795c65fa>::operator ()
 ) is different from what is supplied (
) because the ORT planned memory location device 
Cannot use user supplied initializer with name: (
onnxruntime::session_state_utils::SaveInitializedTensors::<lambda_59a68c93b954c1615c2da7f82b7605e4>::operator ()
Saving initialized tensors.
onnxruntime::session_state_utils::SaveInitializedTensors
OrtValue indexes should have been populated.
ort_value_name_idx_map.MaxIdx() > -1
onnxruntime::session_state_utils::SaveInputOutputNamesToNodeMapping::<lambda_6f564a616944b22adda71fa903488618>::operator ()
onnxruntime::session_state_utils::SaveInputOutputNamesToNodeMapping
 is not used by any node.
 input with name 
Subgraph
Graph
Unsupported device allocator in the context of pre-packed weights caching: 
onnxruntime::PrepackedWeightsContainer::GetOrCreateAllocator
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\prepacked_weights_container.cc
!using_counters_
onnxruntime::MemPatternPlanner::TraceAllocation
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/mem_pattern_planner.h
current <= buffer_size_
 are '0' and '1'. The environment variable contained the value: 
ALLOW_RELEASED_ONNX_OPSET_ONLY
Constant
Can't merge shape info. Both source and target dimension have values but they differ. Source=
 Dimension=
Mismatch between number of source and target dimensions. Source=
fbs_node_arg_names cannot be null
onnxruntime::Node::SaveToOrtFormat
node_arg_name cannot be null
onnxruntime::Node::LoadFromOrtFormat::<lambda_9d082e5b6e5038c4215b2f4bff108ed6>::operator ()
LoadNodeArgsFromOrtFormat: Node [
], could not find NodeArg 
fbs_attr cannot be null
onnxruntime::Node::LoadFromOrtFormat
onnxruntime::NodeArg::UpdateTypeAndShape
Type mismatch. Current=
Optional Type mismatch. Expected: 
 . Got: 
Node [
] op_type [
does not have the graph for key 
Serialization of fused function body is not currently supported, 
This is an invalid model. The sum of input arg count is not equal to size of input defs in node (
output edges
graph_proto != nullptr
graph_proto cannot be null
' Model is invalid.
onnxruntime::Graph::Graph
Sparse initializer must have a name. This model is invalid
Duplicate constant node sparse initializer name: '
Node::LoadFromOrtFormat, input_arg_counts is missing
Serialization error. Graph attribute was serialized without Graph instance
onnxruntime::Node::LoadEdgesFromOrtFormat::<lambda_25bf04bd2587261b1112d5d221d28912>::operator ()
Node::LoadEdgesFromOrtFormat, edge is missing for 
input index: 
 is not the same as this node's index:
input edges
onnxruntime::Node::LoadEdgesFromOrtFormat
D:\a\_work\1\s\engine\lotus\onnxruntime\core/graph/model_load_utils.h
onnxruntime::model_load_utils::IsAllowReleasedONNXOpsetsOnlySet
 target:
. Falling back to lenient merge.
Error merging shape info for output. '
' source:
D:\a\_work\1\s\engine\lotus\onnxruntime\core\graph\graph.cc
onnxruntime::MergeShapeInfo
 Input=
Tensor element type mismatch. 
Source and target must both be tensors
Output:
 , or sparse tensors
 , or optional typed entities
onnxruntime::Graph::KahnsTopologicalSort
Some nodes are not included in the topological sort, graph have a cycle.
Graph attribute inferencing failed: 
This is an invalid model. Error: the graph is not acyclic.
No Graph instance was found for attribute 
 in node 
 inputs and requires 
 inputs. Either provide all subgraph inputs, or just the required inputs.
 Graph may not conform to the ONNX spec and contain initializers that are not graph inputs.
onnxruntime::Graph::BuildConnections
This is an invalid model. Failed to find NodeArg in all parent graphs. Name=
This is an invalid model. At top level graph without matching NodeArg that subgraph consumes. Name=
Invalid model. Node input '
' is not a graph input, initializer, or output of a previous node.
Node (
) Op (
) input arg (
) does not have type information set by parent node.
) is invalid.
This is an invalid model. Node (
) of operator (
) in node (
Size mismatch validating subgraph inputs. Got 
 inputs but subgraph has 
Subgraph input missing type.
Node:
onnxruntime::Graph::InferAndVerifySubgraphTypes
This may prevent some of the graph optimizations, like const folding. 
Move it out of graph inputs if there is no need to override it, 
Initializer 
 appears in graph inputs and will not be treated as constant value/weight. 
graph_inputs_excluding_initializers_.empty() && graph_inputs_including_initializers_.empty() && value_info_.empty() && graph_outputs_.empty()
Graph state to be loaded into must be empty.
Graph ctor should have created NodeArg for initializer. Missing:
onnxruntime::Graph::InitializeStateFromModelFileGraphProto
Duplicate sparse_tensor_initializer: '
utils::HasName(sparse_tensor)
' the model will use the latest encountered initializer
. Please, fix your model.
This is an invalid model. Tensor does not have type information.
Duplicate initializer (dense, sparse or ConstantNode): '
by either re-generating the model with latest exporter/converter 
or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.
Invalid destination node arg slot specified when adding edge.
Invalid source node arg slot specified when adding edge.
Invalid node indexes specified when removing edge.
Argument type mismatch when adding edge.
Invalid source node arg slot specified when removing edge.
onnxruntime::Graph::RemoveEdge
Argument mismatch when removing edge.
Invalid destination node arg slot specified when removing edge.
) does not exist in the graph.
node_arg
This is an invalid model. Error: two nodes with same node name (
This is an invalid model. Graph output (
onnxruntime::Graph::SetOuterScopeNodeArgs
This is an invalid model. Error: Duplicate definition of name (
onnxruntime::Graph::AddEdge
Invalid node indexes specified when adding edge.
outer_scope_node_args_consumed.empty()
Shouldn't be possible to have NodeArgs that haven't been handled already.
AddInitializedTensor already has tensor with name 
 but different TensorProto.
onnxruntime::Graph::AddInitializedTensor
existing->second == &tensor
sparse_tensor_names_.count(tensor_name) == 0
sparse_tensor_names_ not in sync with name_to_initial_tensor_
Function body initialization failed for node '
' optype 
Error: Duplicate definition-site for (
onnxruntime::Graph::InitFunctionBodyForNode
onnxruntime::Graph::PerformTypeAndShapeInferencing
onnxruntime::Graph::InitInputsInitializersOutputs
onnxruntime::Graph::Resolve
onnxruntime::Graph::ForThisAndAllSubgraphs
 as it still has output edges.
onnxruntime::Graph::SaveToOrtFormat
node->GetOutputEdgesCount() == 0
Can't remove node 
graph_proto_ is not in sync with name_to_initial_tensor_.
onnxruntime::Graph::RemoveInitializedTensor
!found
) does not match expected type (
) output arg (
) of output arg (
) of node (
) does not have type information.
Type Error: Type (
 but usage of initializer in graph expects 
This is an invalid model. Model input (
This is an invalid model. Type Error: Type '
' of input parameter (
) bound to different types (
 in node (
Type Error: Type parameter (
) of Optype (
) type inference failed
onnxruntime::Graph::InferAndVerifyTypeMatch
) is required but not specified.
Fatal error: 
) attribute (
. Error message 
. Execution will fail if ORT does not have a specialized kernel for this op
Type Error: Data in initializer '
' has element type 
Type Error: Shape of initializer 
 does not match. 
This is an invalid model. In Node, 
, Error 
 is not a registered function/op
onnxruntime::Graph::VerifyNodeAndOpMatch
onnxruntime::Graph::LoadFromOrtFormat::<lambda_dbbdad314ea6c338685c73dc6e5d5371>::operator ()
NodeArg Name is missing. Invalid ORT format model.
Duplicate initializer (dense or ConstantNode): '
Initializer tensor is missing. Invalid ORT format model.
NodeArg is missing. Invalid ORT format model.
Sparse Initializer tensor is missing. Invalid ORT format model.
Node index is out of range
Node is missing. Invalid ORT format model.
) -> (
onnxruntime::Graph::LoadFromOrtFormat
NodeEdge is missing. Invalid ORT format model.
Cannot find NodeArgs for [
onnxruntime::Graph::ToGraphProtoInternal
onnxruntime::Graph::CleanUnusedInitializersAndNodeArgs
initializer_node_arg != nullptr
Removing initializer '
'. It is not used by any node and should be removed from the model.
outer_scope_node_arg != nullptr
'. It is no longer used by any node.
onnxruntime::Graph::RemoveNode
onnxruntime::Graph::ToGraphProto
Failed to convert dense initializer to sparse
Outer scope node arg name '
'was added but does not exist. 
onnxruntime::Graph::InlineFunction
Input to set must exist.
onnxruntime::Graph::SetInputs
input->Exists()
) : (
 must be either specified in graph inputs or graph initializers.
Removing NodeArg '
onnxruntime::Graph::AllocateNode
nodes_.size() < static_cast<unsigned int>(std::numeric_limits<int>::max())
onnxruntime::Graph::CreateFusedSubGraphNode
nullptr != func_meta_def
dst_implicit_input_idx < (int)node->ImplicitInputDefs().size()
onnxruntime::Graph::FinalizeFuseSubGraph
onnxruntime::ConstPointerContainer<class std::vector<class onnxruntime::NodeArg *,class std::allocator<class onnxruntime::NodeArg *> > >::at
index < data_.size()
D:\a\_work\1\s\engine\lotus\include\onnxruntime\core/common/const_pointer_container.h
onnxruntime::OnnxRuntimeOpSchemaRegistry::RegisterOpSet
Domain already set in registry
onnxruntime::OnnxRuntimeOpSchemaRegistry::RegisterOpSchemaInternal
D:\a\_work\1\s\engine\lotus\onnxruntime\core\graph\schema_registry.cc
, but it its domain is not
known by the checker.
, but it its version is higher
than the operator set version 
Mismatch between Graph and IndexedSubGraph. Node not found: 
Mismatch between Graph and IndexedSubGraph. Output not found:
Invalid ExecutionOrder
Not supported with filtered graph.
onnxruntime::GraphViewer::GetNodesInTopologicalOrder
onnxruntime::GraphViewer::GetRootNodes
filter_info_ == nullptr
graph_->GetNode(idx) != nullptr
IndexedSubGraph contains values not present in the Graph
D:\a\_work\1\s\engine\lotus\onnxruntime\core\graph\graph_viewer.cc
onnxruntime::GraphViewer::GraphViewer
nodearg
Mismatch between Graph and IndexedSubGraph. Input not found:
<p_fd> less than 0.
onnxruntime::Model::Save
<p_fd> is less than 0.
Protobuf serialization failed.
onnxruntime::Model::SaveToOrtFormat
ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain 'ai.onnx'. Please upgrade your model to opset 7 or higher. For now, this opset 
Failed to load model with error: 
onnxruntime::Model::Load
No graph was found in the protobuf.
Protobuf parsing failed.
onnxruntime::Model::LoadFromOrtFormat
Null entry in metadata_props. Invalid ORT format model.
Graph is null. Invalid ORT format model.
onnxruntime::Model::Model
ModelProto does not have a graph.
Missing opset in the model. All ModelProtos MUST have at least one entry that specifies which version of the ONNX OperatorSet is being imported.
D:\a\_work\1\s\engine\lotus\onnxruntime\core\graph\model.cc
, max supported IR version: 
Missing model IR version.
 model may run depending upon legacy support of some older opset version operators.
Unsupported model IR version: 
 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain 
 is till opset 
onnxruntime::model_load_utils::ValidateOpsetForDomain
ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 
 is under development and support for this is limited. The operator schemas and or other functionality could possibly change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain 
Load model 
 failed. File doesn't exist
system error number 
onnxruntime::SaveModel
Incompatible dimensions
 expected to have tensor or sparse type
 expected to have tensor type
Attribute expected to have tensor or sparse tensor type
fused_ratio
First input does not have rank 2
Input tensors of wrong rank (0).
Second input does not have rank 2
D:\a\_work\1\s\engine\lotus\onnxruntime\core\graph\dml_ops\dml_defs.cc
Incompatible dimensions for matrix multiplication
data_0
saved_var
saved_mean
 but has rank 
Negative values are not allowed in a shape specification
Dimension mismatch in unification between 
 expected to have rank 
 expected to have rank >
 expected to have tensor or sparse tensor type: 
 is null
Attribute 
 should specify a shape
Error parsing node:
gamma should have 2 dimension, dimension size known, and same hidden size as word_embedding.
segment_embedding should have 2 dimensions, dimension size known, and same hidden size as word_embedding.
Inputs 0 shall be 3 dimensions
beta should have 1 dimension, dimension size known, and same hidden size as word_embedding.
qkv_hidden_sizes should have 3 elements
Invalid bias shape
key and value cache shall be 4 dimensions
Inputs 4 shall be 5 dimensions
 = Constant()
Error unexpected extra input in node:
Pads has incorrect number of values
'pads' input must be a 1D (shape: [2 * n_input_dims]) tensor of type int64
segment_ids input shall be 2 dimensions
input_ids shall be 2 dimensions
position_embedding should have 2 dimensions, dimension size known, and same hidden size as word_embedding.
word_embedding should have 2 dimensions and dimension size is known.
weight_scale
input_zero_point
weight_zero_point
LongformerAttention
Constrain input and output types to int8 tensors.
Constrain input and output types to float tensors.
present
Constrain mask index to integer types
input_scale
Constrain to integer types
global
DecoderAttention
query
window
One sided attention windows length W, or half of total window length
global_bias
global_weight
X_bias = Add (X, bias)
                T1 = Mul (X_bias, X_bias)
                T2 = Mul (c, T1)
                T3 = Add (b, T2)
                T4 = Mul (X_bias, T3)
                T5 = Tanh (T4)
                T6 = Add (one, T5)
                T7 = Mul (X_bias, T6)
                Y = Mul (a, T7)
            
X_bias = Identity (X)
key and value cache dimensions value shall not be null
extra_add
Number of attention heads
D:\a\_work\1\s\engine\lotus\onnxruntime\core\graph\contrib_ops\contrib_defs.cc
Hidden layer sizes of Q, K, V paths in Attention
Whether every token can only attend to previous tokens. Default value is 0.
weight
embedding_sum
Constrain input and output float tensors types.
Constrain input and output integer tensors types
position_embedding_quant
word_embedding_quant
beta_quant
gamma_quant
segment_embedding
position_embedding
position_ids
gamma_scale
beta_scale
word_embedding_zero_point
position_embedding_zero_point
word_embedding_scale
position_embedding_scale
segment_embedding_scale
value_cache
key_cache
static_kv
use_past
has_layer_state
q_weight
kv_weight
key_padding_mask
The epsilon value to use to avoid division by zero.
input_ids
segment_ids
word_embedding
has_key_padding_mask
new_value_cache
new_key_cache
Constrain key_padding_mask to bool tensors.
Constrain input and output types to float and float16 tensors.
prev_suffix_match_idx
pred_tokens
tokens
Constrain to integer types.
suffix_match_idx
The minimum NGram size for suffix matching.
Constrain scores input and output types to float tensors.
The maximum NGram size for suffix matching.
src_tokens
cur_tokens
GRUUnit computes the activations of a standard GRU,
in a sequence-length aware fashion.
Concretely, given the (fused) inputs X (TxNxD), the previous hidden
state (NxD), and the sequence lengths (N), computes the GRU
activations, avoiding computation if the input is invalid (as in, the
value at X[t][n] >= seqLengths[n].
Scale takes one input data (Tensor<float>) and produces one output data
(Tensor<float>) whose value is the input data tensor scaled element-wise.
'Border' attribute must be present and must contain exactly 4 values - (left_border, top_border, right_border, bottom_border)
Input's shape must be 4-D
) + bottom_border (
'Scale' must contain exactly 2 values - (height, width)
) + right_border (
) needs to be greater than or equal to the top_border (
ParametricSoftplus takes one input data (Tensor<T>) and produces one output data
(Tensor<T>) where the softplus function, y = alpha * ln(exp(beta * x) + 1), is applied to
the tensor elementwise.
Affine takes one input data (Tensor<T>) and produces one output data
(Tensor<T>) where the affine function, y = alpha * x + beta,
is applied to the tensor elementwise.
Crop and image to the specified spatial dimensions. If scale is given,
then optionally start the crop offset by the left/top border amounts.
If scale is not provided, crop the borders as provided.
Scale and bias the input image. Bias values are stored in
the same ordering as the image pixel format.
Produces a slice of the input tensor along multiple axes. Similar to numpy:
https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html
Slices uses `axes`, `starts` and `ends` inputs to specify the start and end
dimension for each axis in the list of axes, it uses this information to
slice the input `data` tensor. If a negative value is passed for any of the
start or end indices, it represent number of elements before the end of that
dimension. If the value passed to start or end is larger than the `n` (the
number of elements in this dimension), it represents `n`. For slicing to the
end of a dimension with unknown size, it is recommended to pass in `INT_MAX`.
If `axes` are omitted, they are set to `[0, ..., ndim-1]`.
Example 1:
  data = [
      [1, 2, 3, 4],
      [5, 6, 7, 8],
  axes = [0, 1]
  starts = [1, 0]
  ends = [2, 3]
  result = [
      [5, 6, 7],
Example 2:
  data = [
      [1, 2, 3, 4],
      [5, 6, 7, 8],
  starts = [0, 1]
  ends = [-1, 1000]
  result = [
      [2, 3, 4],
extra_shape
input_as_shape
Constrain input and output types to float32 tensors.
mask_index_out
input tensor
Constrain input and output types to float or half tensors.
output tensor
segment_embedding_zero_point
gamma_zero_point
beta_zero_point
layernorm_out
The NGram size.
Constrain mean and inv_std_var to float tensors.
scores
Constrain indices to integer types
scores_out
Saved mean used during training to speed up gradient computation
inv_std_var
Saved inverse standard variance used during training to speed up gradient computation.
Biased = Identity (Scaled)
Biased = Add (Scaled, B2D)
InvStdDev2D = Reciprocal (StdDev)
Y = Reshape (Biased, XShape)
InvStdDev = Reshape (InvStdDev2D, ReducedShape)
Mean = Reshape (Mean2D, ReducedShape)
Gaussian Error Linear Unit.
A high-performing neural network activation function.The GELU nonlinearity is
the expected transformation of a stochastic regularizer which randomly applies
the identity or zero map to a neuron's input. The GELU nonlinearity weights
inputs by their magnitude, rather than gates inputs by their sign as in ReLUs.
StdDev = Sqrt (VarPlusEpsilon)
VarPlusEpsilon = Add (Var, Epsilon)
Normalized = Div (Deviation, StdDev)
Deviation = Sub (XU, Mean2D)
Scale2D = Flatten <axis = 0> (Scale)
NormalizedT = Cast (Normalized)
B2D = Flatten <axis=0> (B)
Scaled = Mul (NormalizedT, Scale2D)
isnan_only
isinf_only
!(isinf_only && isnan_only)
Both attributes isinf_only and isnan_only cannot be set. Unset both to check for both conditions.
      Given an `input` and a flow-field `grid`, computes the `output` using `input` values and pixel locations from `grid`.
      Currently, only spatial (4-D) inputs are supported. For `input` with shape (N, C, H, W) and `grid` with shape (N, H_out, W_out, 2),
      the `output` will have shape (N, C, H_out, W_out).
      For each output location `output[n, :, h, w]`, the size-2 vector `grid[n, h, w]` specifies `input` pixel locations `x` and `y`,
      which are used to interpolate the output value `output[n, :, h, w]`.
      The GridSample operator is often used in doing grid generator and sampler in the [Spatial Transformer Networks](https://arxiv.org/abs/1506.02025).
      See also in [torch.nn.functional.grid_sample](https://pytorch.org/docs/master/generated/torch.nn.functional.grid_sample.html#torch-nn-functional-grid-sample).
      
onnxruntime::contrib::RegisterContribSchemas::<lambda_273270d63d2edeeb9cb50e9432a42c00>::operator ()
Value of beta
Value of alpha
                CX = Mul (C, X)
                ERFCX = Erf (CX)
                ERFCXPlus1 = Add (ERFCX, One)
                PhiX = Mul (ERFCXPlus1, Half)
                Y = Mul (X, PhiX)
            
Input rank must be >= 2.
Bias Gelu.
It's an extension of Gelu. It takes the sum of input A and bias input B as the input of Gelu activation. 
The inner-most 2 dimensions must have the same size (mat_w:
 != mat_h:
      Returns the upper or lower triangular part of a 2-D matrix, or batches of 2-D matrices. If the attribute "upper" is set to true,
      the upper triangular matrix is retained. Lower triangular matrix is retained otherwise. Default value for upper is true.
      Trilu takes one input tensor of shape [*, N, M], where * is zero or more batch dimensions. The upper triangular part consists
      of the elements on and above the given diagonal (k). The lower triangular part consists of elements on and below the diagonal.
      All other elements in the matrix are set to zero.
      If k = 0, the triangular part on and above/below the main diagonal is retained.
      If upper is set to true, a positive k retains the upper triangular matrix excluding k diagonals above
      the main diagonal. A negative k value includes as many diagonals below the main diagonal.
      If upper is set to false, a positive k retains the lower triangular matrix including k diagonals above
      the main diagonal. A negative k value excludes as many diagonals below the main diagonal.
      
      Based on Torch operator Embedding, creates a lookup table of embedding vectors of fixed size,
       for a dictionary of fixed size.
      
last dimension of indices must not be larger and rank of data tensor
both data and indices tensor need to have rank larger than zero.
first input tensor has wrong dimension
'pads' input must be a 1D (shape: [input_rank]) or 2D tensor (shape: [1, input_rank]) of type int64
batch_indices shape input tensor has wrong dimension
rois input tensor has wrong dimension
stash_type
crop_size shape input tensor has wrong dimension
) + scale[0] (
) needs to be greater than or equal to the left_border (
Input axis is invalid: 
) + scale[1] (
inputs are expected to have tensor type and output type should not be null.
  Tokenizer divides each string in X into a vector of strings along the last axis. Allowed input shapes are [C] and [N, C].
  If the maximum number of tokens found per input string is D, the output shape would be [N, C, D] when input shape is [N, C].
  Similarly, if input shape is [C] then the output should be [C, D]. Tokenizer has two different operation modes.
  The first mode is selected when "tokenexp" is not set and "separators" is set. If "tokenexp" is set and "separators" is not set,
  the second mode will be used. The first mode breaks each input string into tokens by matching and removing separators.
  "separators" is a list of strings which are regular expressions. "tokenexp" is a single regular expression.
  Let's assume "separators" is [" "] and consider an example.
  If input is
  ["Hello World", "I love computer science !"] whose shape is [2],
  then the output would be
 [["Hello", "World", padvalue, padvalue, padvalue],
 ["I", "love", "computer", "science", "!"]]
 whose shape is [2, 5] because you can find at most 5 tokens per input string.
 Note that the input at most can have two axes, so 3-D and higher dimension are not supported.
 If "separators" contains a single empty string, the Tokenizer will enter into character tokenezation mode. This means all strings
 will be broken part into individual characters.
 For each input string, the second mode searches matches of "tokenexp" and each match will be a token in Y.
 The matching of "tokenexp" is conducted greedily (i.e., a match should be as long as possible).
 This operator searches for the first match starting from the beginning of the considered string,
 and then launches another search starting from the first remained character after the first matched token.
 If no match found, this operator will remove the first character from the remained string and do another search.
 This procedure will be repeated until reaching the end of the considered string.
  Let's consider another example to illustrate the effect of setting "mark" to true.
  If input is ["Hello", "World"],
  then the corresponding output would be [0x02, "Hello", "World", 0x03].
  This implies that if mark is true, [C]/[N, C] - input's output shape becomes [C, D+2]/[N, C, D+2].
If tokenizer removes the entire content of [C]-input, it will produce [[]].
I.e. the output shape should be [C][0] or [N][C][0] if input shape was [N][C].
If the tokenizer receives empty input of [0] then the output is [0] if empty input
of [N, 0] then [N, 0].
Matrix product that behaves like numpy.matmul: https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.matmul.html
Duplicate of FusedMatMul. Going forward FusedMatMul should be used. This OP will be supported for backward compatibility.
Matrix product that behaves like numpy.matmul: https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.matmul.html
X2D = Flatten (X)
ReducedShape = Concat <axis = 0> (PrefixShape, SuffixShape)
Mean2D = ReduceMean <axes = [1]> (XU)
XU = Cast (X2D)
MeanOfSquare = ReduceMean <axes = [1]> (Square)
Square = Mul (XU, XU)
Var = Sub (MeanOfSquare, SquareOfMean)
SquareOfMean = Mul (Mean2D, Mean2D)
Rank = Size (XShape)
XShape = Shape (X)
Axis1D = Constant()
Zero1D = Constant()
NumReducedAxes = Sub (Rank, Axis1D)
PrefixShape = Slice (XShape, Zero1D, Axis1D)
SuffixShape = ConstantOfShape (NumReducedAxes)
NumReducedAxes = Neg (Axis1D)
The scaled hyperbolic tangent values of the input tensor computed element-wise
Scaling value
Sample echo operator.
Constrain to any tensor type. If the dtype attribute is not provided this must be a valid output type.
Constrain input0 and output types to float tensors
For internal use.
signal_ndim
seq_lengths
Array of sequence lengths.  len(seq_lengths) should equal batch size N.
The new GRU hidden state calculated by this op.
The timestep for this operation.
Perform mean variance normalization.
hidden
If 0, normalize the mean only.  Default is 1.
If 1, mean and variance are computed across channels. Default is 0.
The fused convolution operator schema is the same as Conv besides it includes an attribute
activation.
Constrain input and output types to float tensors
The FusedGemm operator schema is the same as Gemm besides it includes attributes
activation and leaky_relu_alpha.
Input tensor B. The shape of B should be (K, N) if transB is 0, or (N, K) if transB is non-zero.
Input tensor A. The shape of A should be (M, K) if transA is 0, or (K, M) if transA is non-zero.
Output tensor of shape (M, N).
Input tensor C. The shape of C should be unidirectional broadcastable to (M, N).
onesided
normalized
ComplexMul
Irfft
input_1
input_0
ComplexMulConj
Threshold value
Result, has same type as input, with H and W dimensions reduced.
Input tensor
Tensor of data to extract slices from.
1-D tensor of ending indices (exclusive) of corresponding axis in axes
1-D tensor of starting indices of corresponding axis in `axes`
1D output tensor
1D input tensor
The scale to apply.
Bias applied to each channel, same size as C.
Result, has same shape and type as input
Input tensor of shape [N,C,H,W]
A 1-D values of (height, width).
A 1-D values of (leftBorder, topBorder, rightBorder, bottomBorder).
GRUUnit
Output data after scaling
drop_states
Bool to determine if hidden state is zeroes or passed along for timesteps past the given sequence_length.
hidden_prev
The previous GRU hidden state.
gates
Unactivated gate outputs from forget, update, and output gates, pre-activation.
Sliced data tensor.
1-D tensor of axes that `starts` and `ends` apply to.
GivenTensorFill
Constrain input and output types to all tensor types.
The filled tensor
The shape of filled tensor
Input data to be scaled
values
If value is 1, output type is uint32_t, else int32_t. Default value is 1.
Seed for the hashing algorithm, unsigned 32-bit integer, default to 0.
Tensor of rank q >= 1.
Tensor of rank r >= 1.
Tensor of rank q-1+r-indices[-1].
indices
Constrain indice type to int32 or int64
Constrain input and output types to any tensor type.
sparse_tensor(uint64)
sparse_tensor(int32)
The underlying implementation is MurmurHash3_x86_32 generating low latency 32bits hash suitable for implementing lookup tables, Bloom filters, count min sketch or feature hashing.
sparse_tensor(uint32)
32-bit hash value.
An input tensor to hash.
Constrain output type to unsigned and signed 32-bit integer tensor.
Constrain input type to unsigned or signed 32-bit integer tensor, or string tensor. It should be utf-8 encoded if using unicode.
Constrain to tensor(int32).
Specify embedding vector of char
The WordConvEmbedding takes in a batch of sequence words and embed each word to a vector.
Constrain to tensor(float).
Input tensor.
Three modes: `constant`(default) - pads with a given constant value, `reflect` - pads with the reflection of the vector mirrored on the first and last values of the vector along each axis, `edge` - pads with the edge values of array
(Optional) A scalar or rank 1 tensor containing a single value to be filled if the mode chosen is `constant` (by default it is 0.0).
Tensor of integers indicating the number of padding elements to add or remove (if negative) at the beginning and end of each axis. For 2D input tensor, it is the number of pixels. `pads` should be a 1D tensor of shape [2 * input_rank] or a 2D tensor of shape [1, 2 * input_rank]. `pads` format (1D example) should be as follow [x1_begin, x2_begin,...,x1_end, x2_end,...], where xi_begin is the number of pixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`.
Integer representing the embedding vector size for each word.If not provide, use the fileter size of conv weight
Given `data` tensor of rank r >= 1, and `indices` tensor of rank q >= 1, gather
slices of `data` into an output tensor of rank q - 1 + r - indices[-1].
Example 1:
  data    = [[0,1],[2,3]]
  indices = [[0,0],[1,1]]
  output  = [0,3]
Example 2:
  data    = [[0,1],[2,3]]
  indices = [[1],[0]]
  output  = [[2,3],[0,1]]
Example 3:
  data    = [[[0,1],[2,3]],[[4,5],[6,7]]]
  indices = [[0,1],[1,0]]
  output  = [[2,3],[4,5]]
Example 4:
  data    = [[[0,1],[2,3]],[[4,5],[6,7]]]
  indices = [[[0,1]],[[1,0]]]
  output  = [[[2,3]],[[4,5]]]
Integer representing the embedding vector size for each char.If not provide, use the char embedding size of embedding vector.
This operator applies convolution to word from left to right with window equal to conv_window_size and stride to 1.Take word 'example' for example, with conv_window_size equal to 2, conv is applied to [ex],[xa], [am], [mp]...If not provide, use the first dimension of conv kernal shape.
Sequence
Specify batchs of sequence words to embedding
Specify bias of conv
Specify weights of conv
Tokenized strings
Strings to tokenize
Boolean whether to mark the beginning/end character with start of text character (0x02)/end of text character (0x03).
Input/Output is a string tensor
An optional string. Token's regular expression in basic POSIX format (pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap09.html#tag_09_03). If set, tokenizer may produce tokens matching the specified pattern. Note that one and only of 'tokenexp' and 'separators' should be set.
The string used to pad output tensors when the tokens extracted doesn't match the maximum number of tokens found. If start/end markers are needed, padding will appear outside the markers.
Minimum number of characters allowed in the output. For example, if mincharnum is 2, tokens such as "A" and "B" would be ignored
an optional list of strings attribute that contains a list of separators - regular expressions to match separators Two consecutive segments in X connected by a separator would be divided into two tokens. For example, if the input is "Hello World!" and this attribute contains only one space character, the corresponding output would be ["Hello", "World!"]. To achieve character-level tokenization, one should set the 'separators' to [""], which contains an empty string.
Whether A should be transposed
Constrain input and output types to float/int tensors.
Scalar multiplier for the product of input tensors A * B.
Whether B should be transposed
activation_gamma
Scalar multiplier for input tensor C.
ExpandDims echo operator.
Specified axis to insert a dimension
Whether B should be transposed on the last two dimensions before doing multiplication
Whether A should be transposed on the last two dimensions before doing multiplication
2-dimensional sparse matrix A. Either COO or CSR format
Matrix multiply results
sparse_tensor(float)
N-dimensional dense matrix B
sparse_tensor(int64)
sparse_tensor(double)
N-dimensional matrix A
Matrix product that behaves like numpy.matmul: https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.matmul.html.
 The production MUST never overflow. The accumulation may overflow if and only if in 32 bits.
Matrix multiply results from A * B
N-dimensional matrix B
Constrain input B data types as 16-bit integer tensor
Constrain input A data types as 16-bit integer tensor
Scalar multiplier for the product of the input tensors.
Constrain output Y data types as 32-bit integer tensor.T3 must be tensor(uint32) when both T1 and T2 are tensor(uint16),or must be tensor(int32) when either T1 or T2 is tensor(int16).
The input data as Tensor.
Constrain mean and inv_std_var to be float tensors.
The normal input data.
The output.
Input tensor. Every matrix in the batch must be invertible.
The bias input data that is a 1D tensor.
TorchEmbedding
Output tensor of the same type and shape as the input tensor.
Bias tensor.
Scale tensor.
Saved inverse standard deviation used during training to speed up gradient computation.
Output data tensor.
Constrain input types and output Y type to float tensors.
InvStdDev
Constrain input and output types (except mean and inv_std_var) to float tensors.
Type of Mean and InvStdDev tensors.
Input tensor of rank 2 or higher.
Boolean. Indicates whether upper or lower part of matrix is retained. Default is true.
Constrain input and output types to all numeric tensors and bool tensors.
A 0-D tensor containing a single value corresponding to the number diagonals above or the main diagonal to exclude or include.Default value is 0 if it's not specified.
apply softmax to elements for dimensions softmax_axis or higher
Y = softmax(scores + bias)) with simple broadcast on bias. Intended to specialize softmax(scores + additive_mask) commonly found in transformer models.
The bias (or mask) as Tensor.
broadcast bias across input for dimensions broadcast_axis to softmax_axis-1
Long tensor containing the indices to extract from embedding matrix.
The embedding matrix of size N x M. 'N' is equal to the maximum possible index + 1, and 'M' is equal to the embedding size
padding_idx
A 0-D scalar tensor. If specified, the entries at `padding_idx` do not contribute to the gradient; therefore, the embedding vector at `padding_idx` is not updated during training, i.e. it remains as a fixed pad.
scale_grad_by_freq
A 0-D bool tensor. If given, this will scale gradients by the inverse of frequency of the indices (words) in the mini-batch. Default  is ``False``
Constrain input and output types to all numeric tensors.
Output tensor of the same type as the input tensor. Shape of the output is * x M, where '*' is the shape of input indices, and 'M' is the embedding size.
Input can be of any tensor type.
counts
The distance metric to use. If a string, the distance function can be "braycurtis", "canberra", "chebyshev", "cityblock", "correlation", "cosine", "dice", "euclidean", "hamming", "jaccard", "jensenshannon", "kulsinski", "mahalanobis", "matching", "minkowski", "rogerstanimoto", "russellrao", "seuclidean", "sokalmichener", "sokalsneath", "sqeuclidean", "wminkowski", "yule".
              Finds all the unique values (deduped list) present in the given input tensor.
              This operator returns 3 outputs.
              The first output tensor 'uniques' contains all of the unique elements of the input,
              sorted in the same order that they occur in the input.
              The second output tensor 'idx' is the same size as the input and it contains the index
              of each value of the input in 'uniques'.
              The third output tensor 'counts' contains the count of each element of 'uniques' in the input.
              Example:
                input_x = [2, 1, 1, 3, 4, 3]
                output_uniques = [2, 1, 3, 4]
                output_idx = [0, 1, 1, 2, 3, 2]
                output_counts = [1, 2, 2, 1]
              
2D matrix with shape (K,N)
2D matrix with shape (M,N)
Constrains input to only numeric types.
A 2D Matrix that represents the distance between each pair of the two collections of inputs.
            Given `data` tensor, pads, mode, and value.
            Example:
            Insert 0 pads to the beginning of the second dimension.
            data = [
                    [1.0, 1.2],
                    [2.3, 3.4],
                    [4.5, 5.7],
                    ]
            pads = [0, 2, 0, 0]
            output = [
                    [
                    [0.0, 0.0, 1.0, 1.2],
                    [0.0, 0.0, 2.3, 3.4],
                    [0.0, 0.0, 4.5, 5.7],
                    ],
                    ]
            
Tensor after padding.
A 1-D input tensor that is to be processed.
A 1-D INT64 tensor of the same size as 'x' containing the indices for each value in 'x' in the output 'uniques'
A 1-D tensor of the same type as 'x' containing all the unique values in 'x' sorted in the same order that they occur in the input 'x'
A 1-D INT64 tensor containing the the count of each element of 'uniques' in the input 'x'
RoI pooled output, 4-D tensor of shape (num_rois, C, crop_height, crop_width). The r-th batch element Y[r-1] is a pooled feature map corresponding to the r-th RoI X[r-1].
crop_size
Constrain types to int tensors.
Constrain types to float tensors.
The first normalization dimension: normalization will be performed along dimensions axis : rank(inputs).
        Extracts crops from the input image tensor and resizes them using bilinear sampling or nearest neighbor sampling
        (possibly with aspect ratio change) to a common output size specified by crop_height and crop_width.
        Returns a tensor with crops from the input image at positions defined at the bounding box locations in boxes.
        The cropped boxes are all resized (with bilinear or nearest neighbor interpolation) to
        a fixed size = [crop_height, crop_width]. The result is a 4-D tensor [num_boxes, crop_height, crop_width, depth].
        The resizing is corner aligned.
Input data tensor from the previous layer.
type used for stash mean/inv_std_var
Value used for extrapolation, when applicable. Default is 0.0f. 
The pooling method. Two modes are supported: 'bilinear' and 'nearest'. Default is 'bilinear'.
RoIs (Regions of Interest) to pool over; rois is 2-D input of shape (num_rois, 4) given as [[y1, x1, y2, x2], ...]. The RoIs' coordinates are normalized in the coordinate system of the input image. Each coordinate set has a 1:1 correspondence with the 'batch_indices' input.
Input data tensor from the previous operator; 4-D feature map of shape (N, C, H, W), where N is the batch size, C is the number of channels, and H and W are the height and the width of the data.
1-D tensor of shape (num_rois,) with each element denoting the index of the corresponding image in the batch.
1-D tensor of 2 elements: [crop_height, crop_width]. All cropped image patches are resized to this size. Both crop_height and crop_width need to be positive.
batch_indices
Constrain input 'ratio' types to float tensors.
The output mask of dropout.
IsAllFinite
Constrain output 'mask' types to boolean tensors.
If true, check only for NaN.
If true, check only for Inf, -Inf.
Input tensors to check.
Constrain the output to a boolean tensor.
(Optional) Seed to the random generator, if not specified we will auto generate one.
output, dropout_mask = Dropout(data + bias, ratio) + residual, Intended to specialize the dropout pattern commonly found in transformer models.
The residual input, must have the same shape as data
The bias input, a vector with the same shape as last dim of data OR same shape with data
The ratio of random dropout, with value in [0, 1). If this input was not set, or if it was set to 0, the output would be a simple copy of the input. If it's non-zero, output will be a random dropout of input, which is typically the case during training.
residual
If set to true then it indicates dropout is being used for training. It is an optional value hence unless specified explicitly, it is false. If it is false, ratio is ignored and the operation mimics inference mode where nothing will be dropped from the input data and if mask is requested as output it will contain all ones.
ratio
Constrain output types to float tensors.
Constrain input types to all tensor types.
Three interpolation modes: bilinear (default), nearest and bicubic.
The output scalar. Its value is true if all input tensors are finite. Otherwise, the output value would be false.
If align_corners=1, the extrema (-1 and 1) are considered as referring to the center points of the input's corner pixels. If align_corners=0, they are instead considered as referring to the corner points of the input's corner pixels, making the sampling more resolution agnostic.
Support padding modes for outside grid values: `zeros`(default), `border`, `reflection`. zeros: use 0 for out-of-bound grid locations, border: use border values for out-of-bound grid locations, reflection: use values at locations reflected by the border for out-of-bound grid locations.
Input offset, 4-D tensor of shape (N, H_out, W_out, 2), where H_out and W_out are the height and width of grid and output, Grid specifies the sampling pixel locations normalized by the input spatial dimensions. Therefore, it should have most values in the range of [-1, 1]. If grid has values outside the range of [-1, 1], the corresponding outputs will be handled as defined by padding_mode.
4-D tensor of shape (N, C, H, W), where N is the batch size, C is the numbers of channels, H and W are the height and width of the input data.
4-D tensor of shape (N, C, H_out, W_out).
itr != node_args.end()
Attempting to get index by a name which does not exist:
Should be unreachable if CanRemoveNodeAndMergeEdges is in sync with the logic here.
onnxruntime::graph_utils::GetIndexFromName
Initializer with same name exists. Name:
onnxruntime::graph_utils::RemoveNode
onnxruntime::graph_utils::AddInitializer
!graph.GetInitializedTensor(new_initializer.name(), existing)
 cannot be safely updated to 
 in one of the subgraphs.
onnxruntime::graph_utils::CanUpdateImplicitInputNameInSubgraphs
 Implicit input name 
std::all_of(output_edges.cbegin(), output_edges.cend(), [&src_idx](const GraphEdge& edge) { return edge.src_arg_index == src_idx; })
Node must only have one used output
for node: 
onnxruntime::graph_utils::RemoveNodeWithSingleNodeInSingleUsedOutput
Failed since multiple edges matched:
Attempting to get an input that does not exist.
onnxruntime::graph_utils::FindPath
onnxruntime::graph_utils::GetNodeInputName
index >= 0 && static_cast<size_t>(index) < inputs.size()
index >= 0 && static_cast<size_t>(index) < outputs.size()
Attempting to get an output that does not exist.
 ExplicitInputs:
 ImplicitInputs:
Invalid input index for node 
. Index:
Can only add a new input at the end of the current ones.
onnxruntime::graph_utils::ReplaceNodeInput
onnxruntime::graph_utils::AddNodeInput
num_explicit_inputs == static_cast<size_t>(target_input_idx)
std::count_if(subgraph_node.InputEdgesBegin(), subgraph_node.InputEdgesEnd(), [input_slot_index](const Node::EdgeEnd& entry) { return entry.GetDstArgIndex() == input_slot_index; }) == 0
D:\a\_work\1\s\engine\lotus\onnxruntime\core\graph\graph_utils.cc
onnxruntime::graph_utils::UpdateImplicitInputNameInSubgraph
onnxruntime::graph_utils::GetNodeOutputName
TENSOR
SPARSE_TENSORS
SPARSE_TENSOR
STRING
FLOATS
GRAPH
STRINGS
GRAPHS
TENSORS
FLOAT
UNDEFINED
Missing string data for initializer. Invalid ORT format model.
onnxruntime::fbs::utils::LoadInitializerOrtFormat
Missing values for sparse initializer. Invalid ORT format model.
Missing raw data for initializer. Invalid ORT format model.
Missing name for SparseTensor initializer. Invalid ORT format model.
onnxruntime::fbs::utils::LoadSparseInitializerOrtFormat
Missing indicies for sparse initializer: 
Invalid ORT format model.
onnxruntime::fbs::utils::SaveInitializerOrtFormat
onnxruntime::fbs::utils::SaveSparseInitializerOrtFormat
D:\a\_work\1\s\engine\lotus\onnxruntime\core\graph\graph_flatbuffers_utils.cc
Graph attribute value was null. Invalid ORT format model.
onnxruntime::fbs::utils::SaveAttributeOrtFormat
Missing dimensions for initializer. Invalid ORT format model.
SaveAttributeOrtFormat: Unsupported attribute type: 
Null string in strings attribute. Invalid ORT format model.
Null strings attribute. Invalid ORT format model.
Null tensor in tensors attribute. Invalid ORT format model.
Null tensors attribute. Invalid ORT format model.
Null string attribute. Invalid ORT format model.
Missing dims for sparse initializer: 
Null tensor attribute. Invalid ORT format model.
onnxruntime::fbs::utils::LoadAttributeOrtFormat
Empty graph proto from deserialization of ORT format model
Null graph attribute. Invalid ORT format model.
Null ints attribute. Invalid ORT format model.
Null floats attribute. Invalid ORT format model.
 does not contain a graph.
 optype 
. No opset import for domain
 referenced by function body node 
Cannot infer type and shape for function
onnxruntime::ViewerFunctionImpl::Body
Not supported
onnxruntime::ViewerFunctionImpl::MutableBody
D:\a\_work\1\s\engine\lotus\onnxruntime\core/graph/function_impl.h
GraphProto attribute inferencing is not enabled in this InferenceContextImpl instance.
 is out of bounds.
Function body initialization failed for Function '
'. Error message 
A node with a function body within a subgraph within another function body is currently not supported in ORT
onnxruntime::InitNestedModelLocalFunction
input_arg->Type() != nullptr
onnxruntime::UpdateSubgraphsWithinFunctionBody
No opset registered for domain 
 in function opset imports.
onnxruntime::IOTypeConstraintHelper
domain_version != -1
D:\a\_work\1\s\engine\lotus\onnxruntime\core\graph\function.cc
node_in_parent_graph->OutputDefs().size() == function_body_graph.GetOutputs().size()
fused_function_subgraph
onnxruntime::CreateSchema
_dummy
onnxruntime::FunctionImpl::FunctionImpl
's number of inputs is different from function body graph's number of input.
Resolve subgraph failed:
's number of outputs is different from function body graph's number of outputs.
node_in_parent_graph->InputDefs().size() == function_body_graph.GetInputsIncludingInitializers().size()
A_scale
A_zero_point
B_scale
B_zero_point
Performs element-wise binary {name} on 8 bit data types (with Numpy-style broadcasting support).
{additionalDocumentation}
inputs are expected to have tensor type.
{additionalDocumentation}
{name}
Required attribute axis is missing
axis must be in [-rank, rank-1]. input rank was 
All inputs to Concat must have same rank
axis must be in [-rank, rank)
The axis along which same quantization parameters are applied. It's optional.If it's not specified, it means per-tensor quantization and input 'x_scale' and 'x_zero_point' must be scalars.If it's specified, it means per 'axis' quantization and input 'x_scale' and 'x_zero_point' must be 1-D tensors.
D:\a\_work\1\s\engine\lotus\onnxruntime\core\graph\contrib_ops\quantization_defs.cc
C_scale
C_zero_point
Constrain input and output types to 8 bit signed and unsigned tensors.
Input data type does not match the expected data type
Scale and Zero-point must be a scalar
Input data type does not match the expected data type. Current data type is 
C = (A_scale * (A - A_zero_point) + B_scale * (B - B_zero_point))/C_scale + C_zero_point
Constrain input a_scale, b_scale and output Y data type as float tensor.
C = ((A - A_zero_point) * (B - B_zero_point)) * (A_scale * B_scale)/C_scale + C_zero_point
addition
QLinearReduceMean
multiplication
Constrain input A, b_scale and output Y data type as float tensor.
Constrain input B data type to 8-bit integer tensor.
a_scale
Constrain input A data type to 8-bit integer tensor.
a_zero_point
X_scale
X_zero_point
Y_scale
Constrain input and output types to 8 bit tensors.
Y_zero_point
data_scale
reduced_scale
data_zero_point
reduced_zero_point
Coefficient of leakage.
x_scale
x_zero_point
Constrain 'y', 'x_scale' to float tensors.
Constrain 'x' and 'x_zero_point' to 8-bit integer tensors.
ReduceSumInteger
y_scale
y_zero_point
Constrain 'y_zero_point' and 'y' to 8-bit integer tensors.
Constrain 'x', 'y_scale' to float tensors.
Constrain output types to 32 bit tensors.
Constrain input types to 8 bit signed and unsigned tensors.
b_scale
b_zero_point
Constrain input type to 8-bit integer tensor.
reduced
A list of integers, along which to reduce. The default is to reduce over all the dimensions of the input tensor.
Constrain output data type to 32-bit integer tensor.T2 must be tensor(uint32) when T1 is tensor(uint8),or must be tensor(int32) when T1 is tensor(int8).
MulInteger
Keep the reduced dimension or not, default 1 mean keep reduced dimension.
Sequence of (Tensor, Scale, ZeroPoint) tuples. The type is sequence of (T8, TF, T8).
Constrain scale types to any float tensor type.
inputs
Constrain input A and its zero point types to 8 bit tensors.
Constrain scale types to float tensors.
Constrain input C to 32 bit integer tensors.
Constrain input B and its zero point types to 8 bit tensors.
Constrain output type to float32 or 8 bit tensors.
Constrain output zero point types to 8 bit tensors.
sequence_lens
Specify if the RNN is forward, reverse, or bidirectional. Must be one of forward (default), reverse, or bidirectional.
Optional scaling values used by some activation functions. The values are consumed in the order of activation functions, for example (f, g, h) in LSTM. Default values are the same as of corresponding ONNX operators.For example with LeakyRelu, the default alpha is 0.01.
Number of neurons in the hidden layer
Cell clip threshold. Clipping bounds the elements of a tensor in the range of [-threshold, +threshold] and is applied to the input of activations. No clip if not specified.
Optional scaling values used by some activation functions. The values are consumed in the order of activation functions, for example (f, g, h) in LSTM. Default values are the same as of corresponding ONNX operators.
Couple the input and forget gates if 1.
A list of 3 (or 6 if bidirectional) activation functions for input, output, forget, cell, and hidden. The activation functions must be one of the activation functions specified above. Optional: See the equations for default if not specified.
Constrain seq_lens to integer tensor.
Which axis to concat on
Constrain weights types to 8 bit tensors.
initial_h
initial_c
Constrain input and output types.
Unsupported type:
limit
delta
Can not get shape initializer data!
Unsupported non-raw-data data type!
Input tensor must have at least 2 dimensions
Output tensor must have at least 2 dimensions
Constrain input and output types to singed/unsigned int8 tensors.
Padding for the beginning and ending along each spatial axis, it can take any value greater than or equal to 0. The value represent the number of pixels added to the beginning and end part of the corresponding axis. `pads` format should be as follow [x1_begin, x2_begin...x1_end, x2_end,...], where xi_begin the number of pixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`. This attribute cannot be used simultaneously with auto_pad attribute. If not present, the padding defaults to 0 along start and end of each spatial axis.
auto_pad must be either NOTSET, SAME_UPPER, SAME_LOWER or VALID. Where default value is NOTSET, which means explicit padding is used. SAME_UPPER or SAME_LOWER mean pad the input so that the output spatial size match the input.In case of odd number add the extra padding at the end for SAME_UPPER and at the beginning for SAME_LOWER. VALID mean no padding.
input and zero_point pair is expected to have be same type.
weight and zero_point pair is expected to have same type.
w_scale
w_zero_point
D:\a\_work\1\s\engine\lotus\onnxruntime\core\graph\contrib_ops\nhwc_schema_defs.cc
The size of the kernel along each axis.
Stride along each spatial axis. If not present, the stride defaults to 1 along each spatial axis.
Whether include pad pixels when calculating values for the edges. Default is 0, doesn't count include pad.
Whether to use ceil or floor (default) to compute the output shape.
Works on NHWC layout or not? Default not.
tensor rank too small
D:\a\_work\1\s\engine\lotus\onnxruntime\core\graph\contrib_ops\nchwc_schema_defs.cc
invalid scales dimension
invalid scales value
Number of neurons in the hidden layer.
Constrain seq_lens to integral tensors.
Couple the input and forget gates if 1, default 0.
memory_seq_lens
AQ5@.
<2?~>
=4?~?
\3JCy7
bad dimensions
@@QLinearGlobalAveragePool ImageSize too large!
QLinearGlobalAveragePool parameter out of computation range!
INVALID_GRAPH
EP_FAIL
MODEL_LOADED
NOT_IMPLEMENTED
code != static_cast<int>(common::OK)
onnxruntime::common::Status::Status
GENERAL ERROR
NO_SUCHFILE
SUCCESS
RUNTIME_EXCEPTION
INVALID_PROTOBUF
NO_MODEL
ENGINE_ERROR
[ONNXRuntimeError]
D:\a\_work\1\s\engine\lotus\onnxruntime\core\common\status.cc
SystemError
, error code: 
 fail: unexpected end
MapFileIntoMemory is not implemented on Windows.
RemoveDirectory() failed - path: 
GetFinalPathNameByHandle() failed: 
DeleteFile() failed - path: 
onnxruntime::`anonymous-namespace'::WindowsEnv::DeleteFolder
file_path == nullptr
onnxruntime::`anonymous-namespace'::WindowsEnv::ReadFileIntoBuffer
Received negative size from stat call
SetFilePointerEx 
ReadFile 
offset < 0
length > buffer.size()
onnxruntime::LoopDir
D:\a\_work\1\s\engine\lotus\onnxruntime\core/platform/path_lib.h
" when trying to load "
onnxruntime::`anonymous-namespace'::WindowsEnv::GetCanonicalPath
Failed to find symbol in library, error code: 
onnxruntime::`anonymous-namespace'::WindowsEnv::FormatLibraryFileName
LoadLibrary failed with error 
FreeLibrary failed with error 
system
 fail, errcode = 
onnxruntime::`anonymous-namespace'::WindowsEnv::GetNumCpuCores
Fatal error: 0 count processors from GetLogicalProcessorInformation
Invalid fd was supplied: 
open file 
GetFileSizeEx 
custom_create_thread_fn returned invalid handle.
onnxruntime
Fatal error: 0 count processors from GetSystemInfo
onnxruntime::`anonymous-namespace'::WindowsThread::WindowsThread
D:\a\_work\1\s\engine\lotus\onnxruntime\core\platform\windows\env.cc
onnxruntime::logging::LoggingManager::CreateDefaultLogger
ISink must be provided.
onnxruntime::logging::LoggingManager::LoggingManager
onnxruntime
Only one instance of LoggingManager created with InstanceType::Default can exist at any point in time.
Default logger already set. 
D:\a\_work\1\s\engine\lotus\onnxruntime\core\common\logging\logging.cc
default_logger_id must be provided if instance_type is InstanceType::Default
onnxruntime::`anonymous-namespace'::ParsePathRoot
D:\a\_work\1\s\engine\lotus\onnxruntime\core\common\path.cc
Failed to parse path root: 
onnxruntime::Path::Parse
"core": 
"num_run": 
WaitRevoke
UnknownEvent
!current_parallel_section
onnxruntime::concurrency::ThreadPool::ParallelSection::ParallelSection
Nested parallelism not supported
], "core": 
", "block_size": [
"thread_id": "
Distribution
DistributionEnqueue
onnxruntime::concurrency::ThreadPool::ParallelFor
More work items than threads
n >= 0
D:\a\_work\1\s\engine\lotus\include\onnxruntime\core/platform/EigenNonBlockingThreadPool.h
onnxruntime::concurrency::ThreadPoolTempl<class onnxruntime::Env>::RunInParallelSection
n <= num_threads_+1
onnxruntime::concurrency::ThreadPoolTempl<class onnxruntime::Env>::RunInParallel
unnamed_thread_pool
Profiler not started yet
LogStart must pair with LogEnd
!points_.empty()
points_.empty()
onnxruntime::concurrency::ThreadPoolProfiler::MainThreadStat::Reset
onnxruntime::concurrency::ThreadPoolProfiler::MainThreadStat::LogEnd
onnxruntime::concurrency::ThreadPoolProfiler::MainThreadStat::LogEndAndStart
D:\a\_work\1\s\engine\lotus\onnxruntime\core\common\threadpool.cc
enabled_
onnxruntime::concurrency::ThreadPoolProfiler::Stop
"thread_pool_name": "
{"main_thread": {
}, "sub_threads": {
onnxruntime::ToMBString
D:\a\_work\1\s\engine\lotus\onnxruntime\core\common\helper.cc
length overflow
onnxruntime::ToWideString
onnxruntime::profiling::Profiler::Start
D:\a\_work\1\s\engine\lotus\onnxruntime\core\common\profiler.cc
custom_logger != nullptr
onnxruntime::profiling::Profiler::StartProfiling
session_logger != nullptr
onnxruntime::profiling::Profiler::Initialize
"dur" :
"ts" :
"pid" :
"tid" :
"args" : {
"ph" : "X",
"name" :"
onnxruntime::profiling::Profiler::EndTimeAndRecordEvent
Writing profiler data to file 
Maximum number of events reached, could not record profile event.
{"cat" : "
onnxruntime::profiling::Profiler::EndProfiling
" : "
unexpected failure
illegal input path:
 is not a valid date
 is not a valid day
 is not a valid year
map_type
sequence_type
tensor_type
opset import domain is null. Invalid ORT format model.
opset id is null. Invalid ORT format model.
onnxruntime::fbs::utils::LoadOpsetImportOrtFormat
Model must have opset imports. Invalid ORT format model.
Null map type info. Invalid ORT format model.
Null sequence type info. Invalid ORT format model.
onnxruntime::fbs::utils::LoadTypeInfoOrtFormat
Null tensor type info. Invalid ORT format model.
onnxruntime::fbs::utils::LoadValueInfoOrtFormat
Null type info for 
Type:
 is not supported currently
onnxruntime::fbs::utils::SaveMapTypeOrtFormat
D:\a\_work\1\s\engine\lotus\onnxruntime\core\flatbuffers\flatbuffers_utils.cc
onnxruntime::fbs::utils::SaveSequenceTypeOrtFormat
onnxruntime::fbs::utils::LoadTensorTypeAndShapeOrtFormat
onnxruntime::fbs::utils::LoadTensorShapeOrtFormat
Null entry in dimensions. Invalid ORT format model.
onnxruntime::fbs::utils::LoadTensorDimensionOrtFormat
onnxruntime::fbs::utils::LoadMapTypeOrtFormat
Null value type info in fbs::MapType. Invalid ORT format model.
onnxruntime::fbs::utils::LoadSequenceTypeOrtFormat
Null value type info in fbs::SequenceType. Invalid ORT format model.
We do not support type [
] for now
onnxruntime::fbs::utils::SaveTypeInfoOrtFormat
onnxruntime::fbs::utils::SaveTensorTypeAndShapeOrtFormat
dim_param value with no name. Invalid ORT format model.
SaveValueInfoOrtFormat: value_info_proto for 
 is missing type info.
onnxruntime::fbs::utils::SaveValueInfoOrtFormat
(inputs_.size() - 1) == i
Extra unparsed input unexpected.
Error parsing function body:
Duplicate type constraint name
!(it.GetName().empty())
(outputs_.size() - 1) == i
ONNX Schema 
: failed validating the check: 
' is expected to have field 'type_protos'
' is expected to have field 'graphs'
' is expected to have field 'tensors'
' is expected to have field 'strings'
Attribute specification type mismatch.
Required attribute '
' is missing.
 has unknown expected type
) has output size 
 not in allowed input sizes.
) has input size 
 not in range [min=
) than declared (
) in op definition.
has output size 
 not in allowed output sizes.
 has inconsistent type 
 typestr: 
, has unsupported type: 
, max=
Operator '
' has been deprecated since version 
 has unsupported type 
' is expected to have field 'sparse_tensor'
' is expected to have field 't'
Mismatched attribute type in '
Unrecognized attribute: 
' is expected to have field 'ints'
' is expected to have field 'floats'
' is expected to have field 'type_proto'
' is expected to have field 'g'
) has more outputs (
)'s input 
 is marked single but has an empty string in the graph
) has more inputs (
 for operator 
Attribute '
' appeared multiple times.
)'s output 
opaque
optional
Invalid tensor data type 
complex128
complex64
DataTypeUtils::FromDataTypeString - Received invalid data type string 
tensor(
Invalid data type 
Unsuported type proto value case.
sparse_tensor(
optional(
The input must be a tensor of a numeric type or string. The output will be of the same tensor type.
The size of each input in the input list
Value(s) to change to.
A value that needs replacing.
Value(s) to change to
The input type must be a tensor of a numeric type, either [N,C] or [C]. The output type will be of the same tensor type and shape.
An integer vocabulary array.<br>One and only one of the vocabularies must be defined.
A string vocabulary array.<br>One and only one of the vocabularies must be defined.
The output will be a tensor of the value type of the input map. It's shape will be [1,C], where C is the length of the input dictionary.
map(string, double)
The input type must be a tensor of a numeric type.
Output type is determined by the specified 'values_*' attribute.
The input type is a tensor of any shape.
A list of strings. One and only one of 'value_*'s should be set.
A list of floats.
A list of ints.
A list of strings. One and only one of 'keys_*'s should be set.
Only one of keys_*'s can be set in label encoder.
Label encoder has only one output.
Label encoder has only one input.
Only one of values_*'s can be set in label encoder.
Input type is not float tensor but keys_floats is set
Input type is not int64 tensor but keys_int64s is set
Input type is not string tensor but key_strings is set
map(int64, float)
map(int64, string)
The input must be an integer map to either string or float.
If the value of map_form is 'SPARSE,' this attribute indicates the total length of the output tensor.
Indicates whether to only output as many values as are in the input (dense), or position the input based on using the key of the map as the index of the output (sparse).<br>One of 'DENSE', 'SPARSE'.
A string indicating the desired element type of the output tensor, one of 'TO_FLOAT', 'TO_STRING', 'TO_INT64'.
The output is a 1-D tensor of string, float, or integer.
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\traditionalml\defs.cc
Values greater than this are mapped to 1, others to 0.
The input must be a tensor of a numeric type. The output will be of the same tensor type.
An integer to use when an input string value is not found in the map.<br>One and only one of the 'default_*' attributes must be defined.
map(string, float)
map(int64, double)
map(string, int64)
The input must be a map from strings or integers to either strings or a numeric type. The key and value types cannot be the same.
The input must be a tensor of strings or integers, either [N,C] or [C].
A string to use when an input integer value is not found in the map.<br>One and only one of the 'default_*' attributes must be defined.
The integers of the map. This sequence must be the same length as the 'cats_strings' sequence.
The strings of the map. This sequence must be the same length as the 'cats_int64s' sequence
The output is a tensor of strings or integers. Its shape will be the same as the input shape.
The kernel type, one of 'LINEAR,' 'POLY,' 'RBF,' 'SIGMOID'.
The output type will be a tensor of strings or integers, depending on which of the the classlabels_* attributes is used. Its size will match the bactch size of the input.
The input must be a tensor of a numeric type, either [C] or [N,C].
Indicates the transform to apply to the score. <br>One of 'NONE,' 'SOFTMAX,' 'LOGISTIC,' 'SOFTMAX_ZERO,' or 'PROBIT'
Second set of probability coefficients. This array must be same size as prob_a.<br>If these are provided then output Z are probability estimates, otherwise they are raw scores.
First set of probability coefficients.
List of 3 elements containing gamma, coef0, and degree, in that order. Zero if unused for the kernel.
If true and category is not present, will return all zeros; if false and a category if not found, the operator will fail.
List of categories, strings.<br>One and only one of the 'cats_*' attributes must be defined.
Second, multiply by this.<br>Can be length of features in an [N,F] tensor or length 1, in which case it applies to all features, regardless of dimension count.<br>Must be same length as 'offset'
First, offset by this.<br>Can be length of features in an [N,F] tensor or length 1, in which case it applies to all features, regardless of dimension count.
Indicates the transform to apply to the score. <br>One of 'NONE,' 'SOFTMAX,' 'LOGISTIC,' 'SOFTMAX_ZERO,' or 'PROBIT.'
The number of support vectors.
Tree id for each node.
The output type will be a tensor of strings or integers, depending on which of the the classlabels_* attributes is used.
Class labels if using integer labels.<br>One and only one of the 'classlabels_*' attributes must be defined.
Class labels if using string labels.<br>One and only one of the 'classlabels_*' attributes must be defined.
Support vector coefficients.
Flag indicating whether the regression is a one-class SVM or not.
Chosen support vectors
The input type must be a tensor of a numeric type, either [C] or [N,C].
A collection of intercepts.
A collection of weights of the model(s).
The output will be a tensor of strings or integers.
The input must be a tensor of a numeric type, and of of shape [N,C] or [C]. In the latter case, it will be treated as [1,C]
Indicates the transform to apply to the scores vector.<br>One of 'NONE,' 'SOFTMAX,' 'LOGISTIC,' 'SOFTMAX_ZERO,' or 'PROBIT'
Class labels when using integer labels. One and only one 'classlabels' attribute must be defined.
Class labels when using string labels. One and only one 'classlabels' attribute must be defined.
Indicates whether to do OvR or multinomial (0=OvR is the default).
A float.
An integer.
A string.
Input's shape should be 1D or 2D
One of 'MAX,' 'L1,' 'L2'
List of categories, ints.<br>One and only one of the 'cats_*' attributes must be defined.
The input must be a tensor of a numeric type.
The total number of regression targets, 1 if not defined.
Weights of the intercepts, if used.
Weights of the model(s).
Indicates the transform to apply to the regression output vector.<br>One of 'NONE,' 'SOFTMAX,' 'LOGISTIC,' 'SOFTMAX_ZERO,' or 'PROBIT'
The keys when using int keys.<br>One and only one of the 'classlabels_*' attributes must be defined.
The keys when using string keys.<br>One and only one of the 'classlabels_*' attributes must be defined.
The weight for the class in class_id.
The index of the class list that each weight is for.
node id that this weight is for.
The id of the tree that this node is in.
Base values for classification, added to final class score; the size must be the same as the classes or can be left unassigned (assumed 0)
Indicates the transform to apply to the score. <br> One of 'NONE,' 'SOFTMAX,' 'LOGISTIC,' 'SOFTMAX_ZERO,' or 'PROBIT.'
Popularity of each node, used for performance and may be omitted.
Thresholds to do the splitting on for each node.
Feature id for each node.
Node id for each node. Ids may restart at zero for each tree, but it not required to.
For each node, define what to do in the presence of a missing value: if a value is missing (NaN), use the 'true' or 'false' branch based on the value in this array.<br>This attribute may be left undefined, and the defalt value is false (0) for all nodes.
Child node if expression is false.
Child node if expression is true.
The node kind, that is, the comparison to make at the node. There is no comparison to make at a leaf node.<br>One of 'BRANCH_LEQ', 'BRANCH_LT', 'BRANCH_GTE', 'BRANCH_GT', 'BRANCH_EQ', 'BRANCH_NEQ', 'LEAF'
Defines how to aggregate leaf values within a target. <br>One of 'AVERAGE,' 'SUM,' 'MIN,' 'MAX.'
The total number of targets.
seq(map(int64, float))
seq(map(string, float))
The output will be a sequence of string or integer maps to float.
For each node, define what to do in the presence of a NaN: use the 'true' (if the attribute value is 1) or 'false' (if the attribute value is 0) branch based on the value in this array.<br>This attribute may be left undefined and the defalt value is false (0) for all nodes.
Child node if expression is false
Child node if expression is true
Node id for each node. Node ids must restart at zero for each tree and increase sequentially.
The weight for each target
The index of the target that each weight is for
The node id of each weight
The id of the tree that each node is in.
The input type must be a tensor of integers or strings, of any shape.
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\traditionalml\old.cc
A list of labels.
The output type will be a tensor of strings or integers, and will have the same shape as the input.
optional(tensor(uint64))
optional(tensor(uint32))
optional(tensor(uint16))
optional(tensor(uint8))
optional(tensor(int64))
optional(tensor(int32))
optional(tensor(int16))
optional(tensor(int8))
optional(seq(tensor(double)))
optional(seq(tensor(float)))
optional(seq(tensor(float16)))
optional(seq(tensor(int64)))
optional(seq(tensor(complex128)))
optional(seq(tensor(complex64)))
optional(seq(tensor(bool)))
optional(seq(tensor(string)))
optional(tensor(string))
optional(tensor(double))
optional(tensor(float))
optional(tensor(float16))
optional(tensor(complex128))
optional(tensor(complex64))
optional(tensor(bool))
 not specified
 and Output 
 expected to have tensor or sparse tensor type
Input: 
axis must be in [-rank, rank-1].
 does not specify a valid type.
 should be of integer type and specify a type.
Value of attribute 
 does not match type of output: 
type: 
optional(seq(tensor(uint32)))
optional(seq(tensor(uint16)))
optional(seq(tensor(uint8)))
optional(seq(tensor(int32)))
optional(seq(tensor(int16)))
optional(seq(tensor(int8)))
optional(seq(tensor(uint64)))
The input is not evenly splittable
Mismatch between the sum of 'split' (
) and the split dimension of the input (
Which axis to split on. A negative value means counting dimensions from the back. Accepted range is [-rank, rank-1] where r = rank(input).
outputs
Constrain output types to any tensor type.
concat_result
Mismatch between number of splits (
) and outputs (
Invalid value of attribute 'axis'. Rank=
 Value=
Input rank for starts and ends should be the same: (
) vs (
Invalid attribute perm {
steps
Slice op must have either three, four or five inputs.
Only supports `int32_t` or `int64_t` inputs for starts/ends/axes/steps
'step' cannot be 0 for Slice
Input axes has invalid data
Input steps has incorrect length
Input axes has incorrect length
Incorrect or missing input value for starts and ends
target_type
Invalid Target shape product of 0. Product cannot be 0 in combination with -1
Invalid dimension value: 
Invalid position of 0.
Target shape may not have multiple -1 dimensions.
The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\tensor\defs.cc
Constrain output types. Casting to complex is not supported.
Constrain input types. Casting from complex is not supported.
Constrain output to int64 tensor.
Input tensor can be of arbitrary type.
(Optional) Ending axis for slicing the shape. Negative value means counting dimensions from the back. If omitted, sizes of all axes upto (including) the last one will be included.
Which axis to concat on. A negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(inputs)..
Constrain output to int64 tensor, which should be a scalar though.
(Optional) By default, when any value in the 'shape' input is equal to zero the corresponding dimension value is copied from the input tensor dynamically. allowzero=1 indicates that if any value in the 'shape' input is set to zero, the zero value is honored, similar to NumPy.
Dimension could not be inferred: incompatible shapes
(Optional) Starting axis for slicing the shape. Default value is 0.Negative value means counting dimensions from the back.
reshaped
Blocks of [blocksize, blocksize] are moved.
'Repeats' input must be 1D tensor of type int64
DCR (default) for depth-column-row order re-arrangement. Use CRD for column-row-depth order.
List of integers indicating the dimensions to be inserted. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(expanded).
values in 'axes' are beyond the bounds of the computed output shape
'axes' attribute must not contain any duplicates
Input tensor must be 4-dimensional
Blocksize must be positive
expanded
Constrain input 'X' and output 'Y' to all tensor types.
The scale array along each dimension. It takes value greater than or equal to 1. The number of elements of 'scales' should be the same as the rank of input 'X'.
The coefficient 'a' used in cubic interpolation. Two common choice are -0.5 (in some cases of TensorFlow) and -0.75 (in PyTorch). Check out Equation (4) in https://ieeexplore.ieee.org/document/1163711 for the details. This attribute is valid only if "mode" is "cubic".
Three interpolation modes: nearest (default), linear and cubic. The "linear" mode includes linear interpolation for 1D tensor and N-linear interpolation for N-D tensor (for example, bilinear interpolation for 2D tensor). The "cubic" mode includes cubic interpolation for 1D tensor and N-cubic interpolation for N-D tensor (for example, bicubic interpolation for 2D tensor).
This attribute describes how to transform the coordinate in the resized tensor to the coordinate in the original tensor. <br/>
The coordinate of each dimension is transformed individually. Let's describe a case using axis x as an example.
Denote x_resized as the coordinate of axis x in the resized tensor, x_original as the coordinate of axis x in the original tensor, length_original as the length of the original tensor in axis x, length_resized as the length of the resized tensor in axis x, roi_x = (start_x, end_x) of the axis x in input "roi", scale = length_resized / length_original, <br/>
if coordinate_transformation_mode is "half_pixel", <br/>
x_original = (x_resized + 0.5) / scale - 0.5, <br/>
if coordinate_transformation_mode is "pytorch_half_pixel", <br/>
x_original = length_resized > 1 ? (x_resized + 0.5) / scale - 0.5 : 0, <br/>
if coordinate_transformation_mode is "align_corners", <br/>
x_original = x_resized * (length_original - 1) / (length_resized - 1), <br/>
if coordinate_transformation_mode is "asymmetric", <br/>
x_original = x_resized / scale, <br/>
if coordinate_transformation_mode is "tf_crop_and_resize", <br/>
x_original = length_resized > 1 ? start_x * (length_original - 1) + x_resized * (end_x - start_x) * (length_original - 1) / (length_resized - 1) : 0.5 * (start_x + end_x) * (length_original - 1).
repeats
'Repeats' input has incorrect number of values. The number of values in 'repeats' must be equal to the number of input dimensions.
Two interpolation modes: nearest (default), and linear (including bilinear, trilinear, etc)
Constrain repeat's type to int64 tensors.
Input and output types can be of any tensor type.
updates
reduction
Type of reduction to apply: none (default), add, mul. 'none': no reduction applied. 'add':  reduction using the addition operation. 'mul': reduction using the multiplication operation.
transposed
A list of integers. By default, reverse the dimensions, otherwise permute the axes according to the values given.
}, input shape = {
Which axis to scatter on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data).
squeezed
List of integers indicating the dimensions to squeeze. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data).
axis must be in [-r, r-1]
data tensor must have rank >= 1
Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data).
Constrain to all tensor types.
Constrain input and output types to all tensor types (including bfloat).
Constrain to boolean tensors.
Constrain output types to boolean tensors.
Constrain input types to float tensors.
Constrain to any tensor type.
(Optional) Whether map negative infinity to true. Default to 1 so that negative infinity induces true. Set this attribute to 0 if negative infinity should be mapped to false.
(Optional) Whether map positive infinity to true. Default to 1 so that positive infinity induces true. Set this attribute to 0 if positive infinity should be mapped to false.
(Optional) The dimension to apply unique. If not specified, the unique elements of the flattened input are returned. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input).
(Optional) Whether to sort the unique elements in ascending order before returning as output. Must be one of 0, or 1 (default).
Invalid value for attribute axis
(Optional) Specify which axis is time axis. Must be one of 0 (default), or 1.
'sequence_lens' must have rank of 1
'input' must have rank >= 2
(Optional) Specify which axis is batch axis. Must be one of 1 (default), or 0.
Constrain input and output types to all tensor, sequence, and optional types.
Constrain roi type to float or double.
(Optional) Axis along which to take slices. If not specified, input is flattened before elements being selected. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input).
'axis' must be in [-rank(indices), rank(indices)-1]
Indices tensor must have rank >= 1
When coordinate_transformation_mode is "tf_crop_and_resize" and x_original is outside the range [0, length_original - 1], this value is used as the corresponding output value. Default is 0.0f.
Four modes: round_prefer_floor (default, as known as round half down), round_prefer_ceil (as known as round half up), floor, ceil. Only used by nearest interpolation. It indicates how to get "nearest" pixel in input tensor from x_original, so this attribute is valid only if "mode" is "nearest".
If set to 1, the weight of sampling locations outside the tensor will be set to 0 and the weight will be renormalized so that their sum is 1.0. The default value is 0.
sizes
(Optional) Axis along which one-hot representation in added. Default: axis=-1. axis=-1 means that the additional dimension will be inserted as the innermost/last dimension in the output tensor. Negative value means counting dimensions from the back. Accepted range is [-r-1, r] where r = rank(indices).
Input 'values' must have exactly two elements.
Input 'values' must be rank 1 tensor.
depth
Constrains to boolean tensors.
condition
Input 'depth' must have exactly one element.
Input 'depth' must be a scalar or rank 1 tensor.
OneHot node must have three inputs.
Supported modes: `constant`(default), `reflect`, `edge`
'pads' input must be a 1D (shape: [2 * input_rank]) tensor of type int64
constant_value
Both `data` and `indices` input tensors in GatherND op need to have rank larger than 0.
inverse_indices
The number of batch dimensions. The gather of indexing starts from dimension of data[batch_dims:]
Last dimension of `indices` input tensor in GatherND op must not be larger than the rank of `data` tensor
v_initial
tensor of int64, which should be a scalar.
All Tensor and Sequence types
The graph run each iteration. It has 2+N inputs: (iteration_num, condition, loop carried dependencies...). It has 1+N+K outputs: (condition, loop carried dependencies..., scan_outputs...). Each scan_output is created by concatenating the value of the specified output value at the end of each iteration of the loop. It is an error if the dimensions or data type of these scan_outputs change across loop iterations.
v_final_and_scan_outputs
All Tensor, Sequence, and optional types
Graph to run if condition is false. Has N outputs: values you wish to be live-out to the enclosing scope. The number of outputs must match the number of outputs in the then_branch.
Graph to run if condition is true. Has N outputs: values you wish to be live-out to the enclosing scope. The number of outputs must match the number of outputs in the else_branch.
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\controlflow\defs.cc
Only bool
An optional list of K flags. The i-th element of the list specifies the axis for the i-th scan_output. The scan outputs are accumulated along the specified axis. If omitted, 0 will be used as the scan axis for every scan_output. Negative value for an axis means counting dimensions from the back. Accepted range is [-r, r-1].
An optional list of M flags. The i-th element of the list specifies the axis to be scanned (the sequence axis) for the i-th scan_input. If omitted, 0 will be used as the scan axis for every scan_input. Negative value for an axis means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input).
An optional list of K flags, one for each scan_output. The i-th element of the list specifies whether the i-th scan_output should be constructed by appending or prepending a new value in each iteration: 0 indicates appending and 1 indicates prepending. If omitted, all scan_output tensors will be produced by appending a value in each iteration.
An optional list of M flags. The i-th element of the list specifies the direction to be scanned for the i-th scan_input tensor: 0 indicates forward direction and 1 indicates reverse direction. If omitted, all scan_input tensors will be scanned in the forward direction.
All Tensor types
Int64 tensor
initial_state_and_scan_inputs
tensor of bool, which should be a scalar.
An attribute specifying the number of scan_inputs M. 
The graph run each iteration. It has N+M inputs: (loop state variables..., scan_input_elts...). It has N+K outputs: (loop state variables..., scan_output_elts...). Each scan_output is created by concatenating the value of the specified scan_output_elt value at the end of each iteration of the loop. It is an error if the dimensions of these values change across loop iterations.
final_state_and_scan_outputs
 is invalid for a tensor of rank 
) is not equal to number of scan outputs (
Number of scan input axes specified (
) is not equal to number of scan inputs (
 axis value 
Loop 'body' subgraph outputs should all be tensors or sequences or optionals, but output 
 was 
If node has 
 but subgraphs produce 
Loop 'body' subgraph scan outputs should all be tensors but output 
 outputs. Expected 
Scan input 
 was not a tensor.
Number of scan output axes specified (
then_branch and else_branch produce different number of outputs. 
Scan 'body' subgraph outputs should all be tensors but output 
 was not
Graph attribute inferencing returned type information for 
Allowed values are 'half_pixel' and 'output_half_pixel'. Use the value 'half_pixel' to pixel shift the input coordinates by -0.5 (the recommended behavior). Use the value 'output_half_pixel' to omit the pixel shift for the input (use this for a backward-compatible behavior).
The pooling method. Two modes are supported: 'avg' and 'max'. Default is 'avg'.
Number of sampling points in the interpolation grid used to compute the output value of each pooled output bin. If > 0, then exactly sampling_ratio x sampling_ratio grid points are used. If == 0, then an adaptive number of grid points are used (computed as ceil(roi_width / output_width), and likewise for height). Default is 0.
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\object_detection\defs.cc
default 1; Pooled output Y's width.
default 1; Pooled output Y's height.
Multiplicative spatial scale factor to translate ROI coordinates from their input spatial scale to the scale used when pooling, i.e., spatial scale of the input feature map X relative to the input image. E.g.; default is 1.0f. 
Integer indicate the format of the box data. The default is 0. 0 - the box data is supplied as [y1, x1, y2, x2] where (y1, x1) and (y2, x2) are the coordinates of any diagonal pair of box corners and the coordinates can be provided as normalized (i.e., lying in the interval [0, 1]) or absolute. Mostly used for TF models. 1 - the box data is supplied as [x_center, y_center, width, height]. Mostly used for Pytorch models.
selected_indices
max_output_boxes_per_class
boxes
score_threshold
iou_threshold
Constrains input type to optional tensor and optional sequence types.
OptionalHasElement is expected to have 1 output.
Input type is null. Input must have Type information.
OptionalGetElement must have an input element.
Constrains output to a boolean tensor.
Constrains input type to all tensor and sequence types.
Type of the element in the optional output
OptionalHasElement is expected to have 1 input.
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\optional\defs.cc
Constrains output type to all optional tensor or optional sequence types.
Constrain output type to all tensor or sequence types.
Input must be an optional-type value containing an element with type information.
Optional is expected to have either an input or the type attribute set.
Input type is null. Type information is expected for the input.
Attribute 'type' should be a TypeProto and it should specify a type.
Optional is expected to have an output.
Constrain input and output types to high-precision numeric tensors.
Constrain input and output types to signed numeric tensors.
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\math\defs.cc
'axis' must be in [
Whether the operator should behave like fmod (default=0 meaning it will do integer mods); Set this to 1 to force fmod treatment
Coefficient of ELU.
Coefficient of SELU default to 1.05070102214813232421875 (i.e., float32 approximation of 1.0507009873554804934193349852946).
The Alpha value in Celu formula which control the shape of the unit. The default value is 1.0.
Elu_Result
X_alpha
Coefficient of SELU default to 1.67326319217681884765625 (i.e., float32 approximation of 1.6732632423543772848170429916717).
]. Its actual value is: 
Invalid rank for 
 broadcasting: (
Wrong op_type name for running propagation: 
Axis has less than the requested k elements.
K input must be of type int64.
K input must be a one-dimensional tensor of size 1.
X_Log
Whether to return the elements in sorted order.
Whether to return the top-K largest or smallest elements.
Values
Dimension on which to do the sort. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input).
Constrain index tensor to int64
Indices
Constrain input Y types to float/int tensors.
Constrain input X and output types to float/int tensors.
slope
Pow takes input data (Tensor<T>) and exponent Tensor, and
produces one output data (Tensor<T>) where the function `f(x) = x^exponent`,
is applied to the data tensor elementwise.
X_ReduceSum
X_Exp
X_Sub
X_ReduceMax
          {
            HS_X = HardSigmoid<alpha = 0.16666667163372, beta = 0.5>(X) 
            Y = Mul (X, HS_X)
          }
        
Value of beta.
Value of alpha.
Constrain input and output types to numeric tensors.
List of tensors for 
If set to 1 will return exclusive sum in which the top element is not included. In other terms, if set to 1, the j-th output element would be the sum of the first (j-1) elements. Otherwise, it would be the sum of the first j elements.
Constrain output Y data type as 32-bit integer tensor.
If set to 1 will perform the sums in reverse direction.
Constrain input a and its zero point data type to 8-bit integer tensor.
Constrain output y and its zero point data type to 8-bit integer tensor.
Constrain input b and its zero point data type to 8-bit integer tensor.
loss_NCdd
input_gather_element
ignore_index
expanded_target
weight_gather
loss_Ndd
loss_N1dd
Constrain input and output types to floating-point tensors.
axis tensor can be int32 or int64 only
target
const_one
const_zero
Constrain input and output types to all tensors.
'shape' input must be 1D tensor of type INT64
input and zero_point pair is expected to have same type.
X_shape
X_LogSM_NCD
X_LogSM
X_NDC
labels
log_prob
Constrain input and output types to all numerical tensor types.
Inputs
Einsum expression string.
X_NCD
Shape3D
Type of reduction to apply to loss: none, sum, mean(default). 'none': no reduction will be applied, 'sum': the output will be summed. 'mean': the sum of the output will be divided by the number of elements in the output.
SoftmaxCrossEntropyLoss
const_one_float
squeeze_mask
input_gather_element_transform
const_zero_casted
Target rank must be 1 less than the input rank.
weight_gather_temp_1
weight_gather_temp
const_one_casted
const_ignore_index
weight_gather_sum
loss_sum
loss_unweighted
const_zero_float
transform_targets
expanded_target_int64
const_zero_target_typed
Number of input tensors does not match the operands in the equation.
NegativeLogLikelihoodLoss
Constrain target to integer types
Constrain input, weight, and output types to floating-point tensors.
Rank of input 
 does not match the equation indices.
Ellipsis represents incompatible dimensions.
Weight rank must be 1.
Input and target dimension value mismatch.
Specifies a target value that is ignored and does not contribute to the input gradient. It's an optional value.
Type of reduction to apply to loss: none, sum, mean (default). 'none': the output is the loss for each sample. 'sum': the output will be summed. 'mean': the sum of the output will be divided by the sum of applied weights.
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\generator\defs.cc
The values for the elements for the 1D, UTF-8 string, output tensor.
The value for the sole element for the scalar, UTF-8 string, output tensor.
(Optional) The value of the output elements.Should be a one-element tensor. If not specified, it defaults to a tensor of value 0 and datatype float32
Invalid shape value: 
Shape input must be a one-dimensional tensor.
The value for the elements of the output tensor in sparse format.
The value for the elements of the output tensor.
TypeAndShapeInferenceFunction implementation incomplete: this line should never be reached.
Attribute 'value_strings' expect a list of strings.
The values for the elements for the 1D, float32, output tensor.
The value for the sole element for the scalar, float32, output tensor.
The values for the elements for the 1D, int64, output tensor.
The value for the sole element for the scalar, int64, output tensor.
Constrain output types. Strings and complex are not supported.
Constrain input types. Strings and complex are not supported.
The data type for the elements of the output tensor. If not specified, default is TensorProto::FLOAT.
Upper boundary of the output values.
Lower boundary of the output values.
Constrain output types to be numerics.
Constrain input types.
(Optional) The data type for the elements of the output tensor. If not specified,the data type of the input tensor T1 is used. If input tensor T1 is also notspecified, then type defaults to 'float'.
(Optional) Index of the diagonal to be populated with ones. Default is 0. If T2 is the output, this op sets T2[i, i+k] = 1. k = 0 populates the main diagonal, k > 0 populates an upper diagonal,  and k < 0 populates a lower diagonal.
Input tensor must be 2-dimensional
Attribute expected to have a one-dim sparse tensor
Attribute expected to have a one-dim tensor
 expected to have: 
 or UNDEFINED. Got: 
Attribute 'value_int' expect an integer.
One and only one of the attributes 'value', 'value_*' or 'sparse_value' must be specified for a Constant node.
value_strings
value_string
Attribute 'value_string' expect a string.
Attribute 'value_floats' expect a list of floats.
Attribute 'value_float' expect a float.
Attribute 'value_ints' expect a list of integers.
sparse_value
value_floats
value_float
value_ints
value_int
Bernoulli
Constrain output types to all numeric tensors and bool tensors.
(Optional) The data type for the elements of the output tensor, if not specified, we will use the data type of the input tensor.
(Optional) The data type for the elements of the output tensor, if not specified, we will use int32.
Number of times to sample.
Input tensor must have rank 2
Output type must be int32 or int64
The mean of the normal distribution.
The shape of the output tensor.
The data type for the elements of the output tensor. Default is TensorProto::FLOAT.
The standard deviation of the normal distribution.
          {
            sub_result = Sub (limit, start)
            sub_result_casted = Cast <to = 1> (sub_result)
            delta_casted = Cast <to = 1> (delta)
            div_result = Div (sub_result_casted, delta_casted)
            ceil_result = Ceil (div_result)
            ceil_result_relu = Relu (ceil_result)
            ceil_result_relu_int = Cast <to = 7> (ceil_result_relu)
            ceil_result_relu_bool = Cast <to = 9> (ceil_result_relu)
            variadic_output, output = Loop (ceil_result_relu_int, ceil_result_relu_bool, start)
              <body = loop_body_attribute (int64 i, bool cond, prev) => (cond_out, current, range) {
                cond_out = Identity (cond)
                current = Add (prev, delta)
                range = Identity (prev)
              }>
          }
        
Constrain input types to common numeric type tensors.
The data type for the elements of the output tensor. if not specified, we will use the data type of the input tensor.
X_greater
X_random
Constrain output types to integral tensors.
All inputs to 'Range' op must be of the same type
Input to 'Range' op should be scalars (Tensor with only one element and shape empty)
Attribute kernel_shape must be specified
Attribute kernel_shape has incorrect size
Attribute strides has incorrect size
Attribute dilations has incorrect size
Constrain input and output types to float and 8 bit tensors.
Second input tensor has wrong dimension
Attribute pads has incorrect size
This operator has **optional** inputs/outputs. See [the doc](IR.md) for more details about the representation of optional arguments. An empty string may be used in the place of an actual argument's name to indicate a missing argument. Trailing optional arguments (those not followed by an argument that is present) may also be simply omitted.
Input tensor must have atleast 2 dimensions
auto_pad must be either NOTSET, SAME_UPPER, SAME_LOWER or VALID. Where default value is NOTSET, which means explicit padding is used. SAME_UPPER or SAME_LOWER mean pad the input so that `output_shape[i] = input_shape[i] * strides[i]` for each axis `i`. The padding is split between the two sides equally or almost equally (depending on whether it is even or odd). In case the padding is an odd number, the extra padding is added at the end for SAME_UPPER and at the beginning for SAME_LOWER.
auto_pad must be either NOTSET, SAME_UPPER, SAME_LOWER or VALID. Where default value is NOTSET, which means explicit padding is used. SAME_UPPER or SAME_LOWER mean pad the input so that `output_shape[i] = ceil(input_shape[i] / strides[i])` for each axis `i`. The padding is split between the two sides equally or almost equally (depending on whether it is even or odd). In case the padding is an odd number, the extra padding is added at the end for SAME_UPPER and at the beginning for SAME_LOWER.
Attribute kernel_shape must be specified.
Attribute kernel_shape has incorrect size.
Attribute strides has incorrect size.
Attribute pads has incorrect size.
'output_shape' must have same number of elements as the shape of input tensor X.
'output_shape' must be rank 1 tensor.
The storage order of the tensor. 0 is row major, and 1 is column major.
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\nn\defs.cc
Input tensor X must have atleast 2 dimensions.
MaxUnpool op must have either two or three inputs.
Dilation value along each spatial axis of filter. If not present, the dilation defaults to 1 along each spatial axis.
Constrain input w and its zero point data type to 8-bit integer tensor.
Constrain input x and its zero point data type to 8-bit integer tensor.
The pads attribute cannot be used simultaneously with auto_pad attribute
Stride along each spatial axis. If not present, the stride defaults to 1 along each axis.
dilation value along each spatial axis of the filter. If not present, the dilation defaults to 1 along each axis.
Constrain output y data type to 32-bit integer tensor.
The shape of the convolution kernel. If not present, should be inferred from input 'w'.
Constrain bias type to 32-bit integer tensor.
Constrain output type to 8-bit integer tensor.
Constrain filter type to 8-bit integer tensor.
number of groups input channels and output channels are divided into. default is 1.
Padding for the beginning and ending along each spatial axis, it can take any value greater than or equal to 0.The value represent the number of pixels added to the beginning and end part of the corresponding axis.`pads` format should be as follow [x1_begin, x2_begin...x1_end, x2_end,...], where xi_begin the number ofpixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`.This attribute cannot be used simultaneously with auto_pad attribute. If not present, the padding defaultsto 0 along start and end of each spatial axis.
dilation value along each spatial axis of the filter. If not present, the dilation defaults to 1 along each spatial axis.
If set to true, it indicates BatchNormalization is being used for training, and outputs 1, 2, 3, and 4 would be populated.
Factor used in computing the running mean and variance.e.g., running_mean = running_mean * momentum + mean * (1 - momentum).
This number of op outputs should be 1 when Training_mode = False, but it is not.
This number of op outputs should be 3 when Training_mode = True, but it is not.
The shape of the output can be explicitly set which will cause pads values to be auto generated. If output_shape is specified pads values are ignored. See doc for details for equations to generate pads
Carries out batch normalization as described in the paper
https://arxiv.org/abs/1502.03167. Depending on the mode it is being run,
There are five required inputs 'X', 'scale', 'B', 'input_mean' and
'input_var'.
Note that 'input_mean' and 'input_var' are expected to be the estimated
statistics in inference mode (training_mode=False, default),
and the running statistics in training mode (training_mode=True).
There are multiple cases for the number of outputs, which we list below:
Output case #1: Y, running_mean, running_var (training_mode=True)
Output case #2: Y (training_mode=False)
When training_mode=False, extra outputs are invalid.
The outputs are updated as follows when training_mode=True:
running_mean = input_mean * momentum + current_mean * (1 - momentum)
running_var = input_var * momentum + current_var * (1 - momentum)
Y = (X - current_mean) / sqrt(current_var + epsilon) * scale + B
where:
current_mean = ReduceMean(X, axis=all_except_channel_index)
current_var =  ReduceVar(X, axis=all_except_channel_index)
Notice that ReduceVar refers to the population variance, and it equals to
sum(sqrd(x_i - x_avg)) / N
where N is the population size (this formula does not use sample size N - 1).
The computation of ReduceMean and ReduceVar uses float to avoid overflow for float16 inputs.
When training_mode=False:
Y = (X - input_mean) / sqrt(input_var + epsilon) * scale + B
For previous (depreciated) non-spatial cases, implementors are suggested
to flatten the input shape to (N x C * D1 * D2 * ... * Dn) before a BatchNormalization Op.
Additional elements added to the side with higher coordinate indices in the output. Each padding value in "output_padding" must be less than the corresponding stride/dilation dimension. By default, this attribute is a zero vector. Note that this attribute doesn't directly affect the computed output values. It only controls the selection of the computed values, so changing this attribute only adds or removes output elements. If "output_shape" is explicitly provided, "output_padding" does not contribute additional size to "output_shape" but participates in the computation of the needed padding amount. This is also called adjs or adjustment in some frameworks.
Multiplicative spatial scale factor to translate ROI coordinates from their input scale to the scale used when pooling.
ROI pool output shape (height, width).
Attribute pooled_shape must be specified
p value of the Lp norm used to pool over the input data.
Attribute pooled_shape has incorrect length
RoIs tensor must have 2 dimensions
dilation value along each spatial axis of the filter. If not present, the dilation defaults is 1 along each spatial axis.
The shape of the convolution kernel. If not present, should be inferred from input W.
number of groups input channels and output channels are divided into.
Stride along each spatial axis. If not present, the stride defaults is 1 along each spatial axis.
Input tensor must have rank 1 or 2
ngram_indexes must be non-empty with no negative values
Maximum n-gram length. If this value is 3, 3-grams will be used to generate the output.
1-D tensor of floats
Input is ether string UTF-8 or int32/int64
The number of channels to sum over
Indicate up to which input dimensions (exclusive) should be flattened to the outer dimension of the output. The value for axis must be in the range [-r, r], where r is the rank of the input tensor. Negative value means counting dimensions from the back. When axis = 0, the shape of the output tensor is (1, (d_0 X d_1 ... d_n), where the shape of the input tensor is (d_0, d_1, ... d_n). 
Constrain input and output to all tensor types.
Constrain input and output  types to float tensors.
The exponent.
Scaling parameter.
Input shape must have either [C] or [1,C] dimensions where C > 0
Environment dependent string that denotes the locale according to which output strings needs to be upper/lowercased.Default en_US or platform specific equivalent as decided by the implementation.
List of stop words. If not set, no word would be removed from X.
Boolean. Whether the identification of stop words in X is case-sensitive. Default is false
string enum that cases output to be lowercased/uppercases/unchanged. Valid values are "LOWER", "UPPER", "NONE". Default is "NONE"
List of int64 n-grams learned from the training set. Either this or pool_strings attributes must be present but not both. It's an 1-D tensor starting with the collections of all 1-grams and ending with the collections of n-grams. The i-th element in pool stores the n-gram that should be mapped to coordinate ngram_indexes[i] in the output vector.
List of strings n-grams learned from the training set. Either this or pool_int64s attributes must be present but not both. It's an 1-D tensor starting with the collections of all 1-grams and ending with the collections of n-grams. The i-th element in pool stores the n-gram that should be mapped to coordinate ngram_indexes[i] in the output vector.
Maximum number of items (integers/strings) to be skipped when constructing an n-gram from X. If max_skip_count=1, min_gram_length=2, max_gram_length=3, this operator may generate 2-grams with skip_count=0 and skip_count=1, and 3-grams with skip_count=0 and skip_count=1
Minimum n-gram length. If this value is 2 and max_gram_length is 3, output may contain counts of 2-grams and 3-grams.
The weighting criteria. It can be one of "TF" (term frequency), "IDF" (inverse document frequency), and "TFIDF" (the combination of TF and IDF)
list of floats. This attribute stores the weight of each n-gram in pool. The i-th element in weights is the weight of the i-th n-gram in pool. Its length equals to the size of ngram_indexes. By default, weights is an all-one tensor.This attribute is used when mode is "IDF" or "TFIDF" to scale the associated word counts.
list of int64s (type: AttributeProto::INTS). This list is parallel to the specified 'pool_*' attribute. The i-th element in ngram_indexes indicate the coordinate of the i-th n-gram in the output tensor.
The starting indexes of 1-grams, 2-grams, and so on in pool. It is useful when determining the boundary between two consecutive collections of n-grams. For example, if ngram_counts is [0, 17, 36], the first index (zero-based) of 1-gram/2-gram/3-gram in pool are 0/17/36. This format is essentially identical to CSR (or CSC) sparse matrix format, and we choose to use this due to its popularity.
Constrain mean and variance types to float tensors.
Constrain scale and bias types to float tensors.
input_var
input_mean
running_var
running_mean
The bias value added to output. Default is 0.
The lambd value for the Shrink formulation. Default is 0.5.
Invalid value(
) for attribute 'axis'
The order of the normalization, only 1 or 2 are supported.
The axis on which to apply normalization, -1 mean last axis.
training_mode of Dropout must be a scalar.
Ratio of Dropout must be a scalar.
        {
          Exponent = Constant <value = float {2.0}>()
          Epsilon = Constant <value = float {1e-9}>()
          X_RM = ReduceMean <axes : ints = @axes> (X)
          EX_squared = Pow (X_RM, Exponent)
          X_squared = Pow (X, Exponent)
          E_Xsquared = ReduceMean <axes : ints = @axes> (X_squared)
          Variance = Sub (E_Xsquared, EX_squared)
          STD = Sqrt (Variance)
          X_variance = Sub (X, X_RM)
          Processed_STD = Add (STD, Epsilon)
          Y = Div (X_variance, Processed_STD)
        }
        
A list of integers, along which to reduce. The default is to caculate along axes [0,2,3] for calculating mean and variance along each channel. Two variables with the same C-coordinate are associated with the same mean and variance.
New shape
One float, indicates the value to be filled, default is 0
Three modes: constant(default), reflect, edge
consumed_inputs
legacy optimization attribute.
Which axis to split on
outputs...
paddings
List of integers indicate the padding element count at the beginning and end of each axis, for 2D it is the number of pixel. `paddings` rank should be double of the input's rank. `paddings` format should be as follow [x1_begin, x2_begin...x1_end, x2_end,...], where xi_begin the number of pixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`.
length of each output
Two interpolation modes: nearest(default), bilinear
height_scale
Number of elements of attribute 'scales' must be same as rank of input 'X'
Ranks inferred (
) is not equal to the existing rank value (
Constrain output types to bool, int32, int64, float16, float, double tensors.
tiles
The scale along height dimension. It takes value greater than or equal to 1.
width_scale
The scale along width dimension. It takes value greater than or equal to 1.
Constrain tiles and axis's type to int64 tensors.
Invalid Target shape product of 0
Invalid position of 0
Target shape may not have multiple -1 dimensions
'step' cannot be 0
length of each output. Values should be >= 0.
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\tensor\old.cc
Constrain input types. Casting from strings and complex are not supported.
Constrains input and output to only numeric types.
rank must be greater than axis
Which axis to concat on.  Default value is 1.
Constrain output types. Casting to strings and complex are not supported.
This attribute describes how to transform the coordinate in the resized tensor to the coordinate in the original tensor. <br/>
The coordinate of each dimension is transformed individually. Let's describe a case using axis x as an example.
Denote x_resized as the coordinate of axis x in the resized tensor, x_original as the coordinate of axis x in the original tensor, length_original as the length of the original tensor in axis x, length_resized as the length of the resized tensor in axis x, roi_x = (start_x, end_x) of the axis x in input "roi", scale = length_resized / length_original, <br/>
if coordinate_transformation_mode is "half_pixel", <br/>
x_original = (x_resized + 0.5) / scale - 0.5, <br/>
if coordinate_transformation_mode is "pytorch_half_pixel", <br/>
x_original = length_resized > 1 ? (x_resized + 0.5) / scale - 0.5 : 0, <br/>
if coordinate_transformation_mode is "align_corners", <br/>
x_original = x_resized * (length_original - 1) / (length_resized - 1), <br/>
if coordinate_transformation_mode is "asymmetric", <br/>
x_original = x_resized / scale, <br/>
if coordinate_transformation_mode is "tf_half_pixel_for_nn", <br/>
x_original = (x_resized + 0.5) / scale, <br/>
if coordinate_transformation_mode is "tf_crop_and_resize", <br/>
x_original = length_resized > 1 ? start_x * (length_original - 1) + x_resized * (end_x - start_x) * (length_original - 1) / (length_resized - 1) : 0.5 * (start_x + end_x) * (length_original - 1).
One float, indicates the value to be filled.
List of integers indicating the number of padding elements to add or remove (if negative) at the beginning and end of each axis. For 2D it is the number of pixels. `pads` rank should be double of the input's rank. `pads` format should be as follow [x1_begin, x2_begin...x1_end, x2_end,...], where xi_begin the number of pixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`.
Attribute pads has incorrect length
Constrain input and output types to all tensor and sequence types.
Ending indices (exclusive) of corresponding axis in axes`
Starting indices of corresponding axis in `axes`
Which axis to scatter on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1]
Attribute 'scales' is required.
Attribute 'scales' must have floats type.
Axes that `starts` and `ends` apply to. It's optional. If not present, will be treated as [0, 1, ..., len(`starts`) - 1].
Attribute axes has incorrect length
Incorrect or missing attribute value for starts and ends
(Optional) Axis along which to take slices. If not specified, input is flattened before elements being selected.
(Optional) Axis along which one-hot representation in added. Default: axis=-1. axis=-1 means that the additional dimension will be inserted as the innermost/last dimension in the output tensor.
Attribute value for pads is required
Which axis to split on. 
List of non-negative integers, indicate the dimensions to squeeze.
Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1]
'axis' must be in [-rank(indices)-1, rank(indices)]
List of non-negative integers, indicate the dimensions to be inserted
Carries out batch normalization as described in the paper
https://arxiv.org/abs/1502.03167. Depending on the mode it is being run,
there are multiple cases for the number of outputs, which we list below:
Output case #1: Y, mean, var, saved_mean, saved_var (training mode)
Output case #2: Y (test mode)
For previous (depreciated) non-spatial cases, implementors are suggested
to flatten the input shape to (N x C*D1*D2 ..*Dn) before a BatchNormalization Op.
Factor used in computing the running mean and variance.e.g., running_mean = running_mean * momentum + mean * (1 - momentum), default is 0.9f.
The epsilon value to use to avoid division by zero, default is 1e-5f.
is_test
If set to nonzero, run spatial batch normalization in test mode, default is 0.
Constrain output mask types to boolean tensors.
The ratio of random dropout
Indicate up to which input dimensions (exclusive) should be flattened to the outer dimension of the output. The value for axis must be in the range [0, R], where R is the rank of the input tensor. When axis = 0, the shape of the output tensor is (1, (d_0 X d_1 ... d_n), where the shape of the input tensor is (d_0, d_1, ... d_n). 
Constrain mean and variance types to float tensors. It allows all float type for U.
Carries out batch normalization as described in the paper
https://arxiv.org/abs/1502.03167. Depending on the mode it is being run,
There are five required inputs 'X', 'scale', 'B', 'input_mean' and
'input_var'.
Note that 'input_mean' and 'input_var' are expected to be the estimated
statistics in inference mode (training_mode=False, default),
and the running statistics in training mode (training_mode=True).
There are multiple cases for the number of outputs, which we list below:
Output case #1: Y, running_mean, running_var (training_mode=True)
Output case #2: Y (training_mode=False)
When training_mode=False, extra outputs are invalid.
The outputs are updated as follows when training_mode=True:
running_mean = input_mean * momentum + current_mean * (1 - momentum)
running_var = input_var * momentum + current_var * (1 - momentum)
Y = (X - current_mean) / sqrt(current_var + epsilon) * scale + B
where:
current_mean = ReduceMean(X, axis=all_except_channel_index)
current_var =  ReduceVar(X, axis=all_except_channel_index)
Notice that ReduceVar refers to the population variance, and it equals to
sum(sqrd(x_i - x_avg)) / N
where N is the population size (this formula does not use sample size N - 1).
When training_mode=False:
Y = (X - input_mean) / sqrt(input_var + epsilon) * scale + B
For previous (depreciated) non-spatial cases, implementors are suggested
to flatten the input shape to (N x C * D1 * D2 * ... * Dn) before a BatchNormalization Op.
(int, default 0) if nonzero, run dropout in test mode where the output is simply Y = X.
(float, default 0.5) the ratio of random dropout
E_Xsquared
X_squared
EX_squared
Processed_STD
X_variance
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\nn\old.cc
dilation value along each spatial axis of the filter.
p value of the Lp norm used to pool over the input data, default is 2.0.
If true, compute the mean and variance across all spatial elements If false, compute the mean and variance across per feature.Default is 1.
The zero-padding added to one side of the output. This is also called adjs/adjustment in some frameworks.
Dilation value along each spatial axis of filter.
Stride along each spatial axis.
Stride along each axis.
auto_pad must be either NOTSET, SAME_UPPER, SAME_LOWER or VALID. Where default value is NOTSET, which means explicit padding is used. SAME_UPPER or SAME_LOWER mean pad the input so that the output size match the input.In case of odd number add the extra padding at the end for SAME_UPPER and at the beginning for SAME_LOWER. VALID mean no padding. DEPRECATION NOTE: auto_pad is only intended to support legacy uses, and for framework authors, one is explicitly encouraged to use explicit padding specified in the pads attribute.
Padding for the beginning and ending along each axis, it can take any value greater than or equal to 0. The value represent the number of pixels added to the beginning and end part of the corresponding axis. `pads` format should be as follow [x1_begin, x2_begin...x1_end, x2_end,...], where xi_begin the number of pixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`. This attribute cannot be used simultaneously with auto_pad attribute.
If true, compute the mean and variance across per activation. If false, compute the mean and variance across per feature over each mini-batch.
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\rnn\defs.cc
When computing the output of the hidden gate, apply the linear transformation before multiplying by the output of the reset gate.
A list of 2 (or 4 if bidirectional) activation functions for update, reset, and hidden gates. The activation functions must be one of the activation functions specified above. Optional: See the equations for default if not specified.
Computes an one-layer GRU. This operator is usually supported via some custom
implementation such as CuDNN.
Notations:
`X` - input tensor
`z` - update gate
`r` - reset gate
`h` - hidden gate
`t` - time step (t-1 means previous time step)
`W[zrh]` - W parameter weight matrix for update, reset, and hidden gates
`R[zrh]` - R recurrence weight matrix for update, reset, and hidden gates
`Wb[zrh]` - W bias vectors for update, reset, and hidden gates
`Rb[zrh]` - R bias vectors for update, reset, and hidden gates
`WB[zrh]` - W parameter weight matrix for backward update, reset, and hidden gates
`RB[zrh]` - R recurrence weight matrix for backward update, reset, and hidden gates
`WBb[zrh]` - W bias vectors for backward update, reset, and hidden gates
`RBb[zrh]` - R bias vectors for backward update, reset, and hidden gates
`H` - Hidden state
`num_directions` - 2 if direction == bidirectional else 1
Activation functions:
  Relu(x)                - max(0, x)
  Tanh(x)                - (1 - e^{-2x})/(1 + e^{-2x})
  Sigmoid(x)             - 1/(1 + e^{-x})
  (NOTE: Below are optional)
  Affine(x)              - alpha*x + beta
  LeakyRelu(x)           - x if x >= 0 else alpha * x
  ThresholdedRelu(x)     - x if x >= alpha else 0
  ScaledTanh(x)          - alpha*Tanh(beta*x)
  HardSigmoid(x)         - min(max(alpha*x + beta, 0), 1)
  Elu(x)                 - x if x >= 0 else alpha*(e^x - 1)
  Softsign(x)            - x/(1 + |x|)
  Softplus(x)            - log(1 + e^x)
Equations (Default: f=Sigmoid, g=Tanh):
  - zt = f(Xt*(Wz^T) + Ht-1*(Rz^T) + Wbz + Rbz)
  - rt = f(Xt*(Wr^T) + Ht-1*(Rr^T) + Wbr + Rbr)
  - ht = g(Xt*(Wh^T) + (rt (.) Ht-1)*(Rh^T) + Rbh + Wbh) # default, when linear_before_reset = 0
  - ht = g(Xt*(Wh^T) + (rt (.) (Ht-1*(Rh^T) + Rbh)) + Wbh) # when linear_before_reset != 0
  - Ht = (1 - zt) (.) ht + zt (.) Ht-1
One (or two if bidirectional) activation function for input gate. The activation function must be one of the activation functions specified above. Optional: Default `Tanh` if not specified.
Computes an one-layer simple RNN. This operator is usually supported
via some custom implementation such as CuDNN.
Notations:
`X` - input tensor
`i` - input gate
`t` - time step (t-1 means previous time step)
`Wi` - W parameter weight matrix for input gate
`Ri` - R recurrence weight matrix for input gate
`Wbi` - W parameter bias vector for input gate
`Rbi` - R parameter bias vector for input gate
`WBi` - W parameter weight matrix for backward input gate
`RBi` - R recurrence weight matrix for backward input gate
`WBbi` - WR bias vectors for backward input gate
`RBbi` - RR bias vectors for backward input gate
`H` - Hidden state
`num_directions` - 2 if direction == bidirectional else 1
Activation functions:
  Relu(x)                - max(0, x)
  Tanh(x)                - (1 - e^{-2x})/(1 + e^{-2x})
  Sigmoid(x)             - 1/(1 + e^{-x})
  (NOTE: Below are optional)
  Affine(x)              - alpha*x + beta
  LeakyRelu(x)           - x if x >= 0 else alpha * x
  ThresholdedRelu(x)     - x if x >= alpha else 0
  ScaledTanh(x)          - alpha*Tanh(beta*x)
  HardSigmoid(x)         - min(max(alpha*x + beta, 0), 1)
  Elu(x)                 - x if x >= 0 else alpha*(e^x - 1)
  Softsign(x)            - x/(1 + |x|)
  Softplus(x)            - log(1 + e^x)
Equations (Default: f=Tanh):
  - Ht = f(Xt*(Wi^T) + Ht-1*(Ri^T) + Wbi + Rbi)
The shape format of inputs X, initial_h and outputs Y, Y_h. If 0, the following shapes are expected: X.shape = [seq_length, batch_size, input_size], Y.shape = [seq_length, num_directions, batch_size, hidden_size], initial_h.shape = Y_h.shape = [num_directions, batch_size, hidden_size]. If 1, the following shapes are expected: X.shape = [batch_size, seq_length, input_size], Y.shape = [batch_size, seq_length, num_directions, hidden_size], initial_h.shape = Y_h.shape = [batch_size, num_directions, hidden_size].
First input tensor must have rank 3
The shape format of inputs X, initial_h, initial_c and outputs Y, Y_h, Y_c. If 0, the following shapes are expected: X.shape = [seq_length, batch_size, input_size], Y.shape = [seq_length, num_directions, batch_size, hidden_size], initial_h.shape = Y_h.shape = initial_c.shape = Y_c.shape = [num_directions, batch_size, hidden_size]. If 1, the following shapes are expected: X.shape = [batch_size, seq_length, input_size], Y.shape = [batch_size, seq_length, num_directions, hidden_size], initial_h.shape = Y_h.shape = initial_c.shape = Y_c.shape = [batch_size, num_directions, hidden_size].
Computes an one-layer LSTM. This operator is usually supported via some
custom implementation such as CuDNN.
Notations:
`X` - input tensor
`i` - input gate
`o` - output gate
`f` - forget gate
`c` - cell gate
`t` - time step (t-1 means previous time step)
`W[iofc]` - W parameter weight matrix for input, output, forget, and cell gates
`R[iofc]` - R recurrence weight matrix for input, output, forget, and cell gates
`Wb[iofc]` - W bias vectors for input, output, forget, and cell gates
`Rb[iofc]` - R bias vectors for input, output, forget, and cell gates
`P[iof]`  - P peephole weight vector for input, output, and forget gates
`WB[iofc]` - W parameter weight matrix for backward input, output, forget, and cell gates
`RB[iofc]` - R recurrence weight matrix for backward input, output, forget, and cell gates
`WBb[iofc]` - W bias vectors for backward input, output, forget, and cell gates
`RBb[iofc]` - R bias vectors for backward input, output, forget, and cell gates
`PB[iof]`  - P peephole weight vector for backward input, output, and forget gates
`H` - Hidden state
`num_directions` - 2 if direction == bidirectional else 1
Activation functions:
  Relu(x)                - max(0, x)
  Tanh(x)                - (1 - e^{-2x})/(1 + e^{-2x})
  Sigmoid(x)             - 1/(1 + e^{-x})
  (NOTE: Below are optional)
  Affine(x)              - alpha*x + beta
  LeakyRelu(x)           - x if x >= 0 else alpha * x
  ThresholdedRelu(x)     - x if x >= alpha else 0
  ScaledTanh(x)          - alpha*Tanh(beta*x)
  HardSigmoid(x)         - min(max(alpha*x + beta, 0), 1)
  Elu(x)                 - x if x >= 0 else alpha*(e^x - 1)
  Softsign(x)            - x/(1 + |x|)
  Softplus(x)            - log(1 + e^x)
Equations (Default: f=Sigmoid, g=Tanh, h=Tanh):
  - it = f(Xt*(Wi^T) + Ht-1*(Ri^T) + Pi (.) Ct-1 + Wbi + Rbi)
  - ft = f(Xt*(Wf^T) + Ht-1*(Rf^T) + Pf (.) Ct-1 + Wbf + Rbf)
  - ct = g(Xt*(Wc^T) + Ht-1*(Rc^T) + Wbc + Rbc)
  - Ct = ft (.) Ct-1 + it (.) ct
  - ot = f(Xt*(Wo^T) + Ht-1*(Ro^T) + Po (.) Ct + Wbo + Rbo)
  - Ht = ot (.) h(Ct)
Loop 'body' subgraph outputs should all be tensors or sequences but output 
An optional list of K flags. The i-th element of the list specifies the axis for the i-th scan_output. The scan outputs are accumulated along the specified axis. If omitted, 0 will be used as the scan axis for every scan_output.
An optional list of M flags. The i-th element of the list specifies the axis to be scanned (the sequence axis) for the i-th scan_input. If omitted, 0 will be used as the scan axis for every scan_input.
Mismatched tensor element type for output 
Mismatched type for output 
 then=
 else=
Loop 'body' subgraph outputs should all be tensors but output 
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\controlflow\old.cc
Constrain 'y_zero_point' and 'y' to 8-bit unsigned integer tensor.
Constrain 'x' to float tensor.
        {
           Q_Min = Constant<value = float {0.0}>()
           Q_Max = Constant<value = float {255.0}>()
           X_Min = ReduceMin <keepdims = 0> (x)
           X_Min_Adjusted = Min (X_Min, Q_Min)
           X_Max = ReduceMax <keepdims = 0> (x)
           X_Max_Adjusted = Max (X_Max, Q_Min)
           X_Range = Sub (X_Max_Adjusted, X_Min_Adjusted)
           Scale = Div (X_Range, Q_Max)
           Min_Scaled = Div (X_Min_Adjusted, Scale)
           Initial_ZeroPoint_FP = Sub (Q_Min, Min_Scaled)
           Clipped_ZeroPoint_FP = Clip (Initial_ZeroPoint_FP, Q_Min, Q_Max)
           Rounded_ZeroPoint_FP = Round (Clipped_ZeroPoint_FP)
           Zeropoint = Cast <to = 2> (Rounded_ZeroPoint_FP)
           y_scale = Identity (Scale)
           y_zero_point = Identity (Zeropoint)
           y = QuantizeLinear (x, Scale, Zeropoint)
        }
        
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\quantization\defs.cc
Constrain 'x_zero_point' and 'x' to 8-bit/32-bit integer tensor.
(Optional) The axis of the dequantizing dimension of the input tensor. Ignored for per-tensor quantization. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input).
Constrain 'y_zero_point' and 'y' to 8-bit integer tensor.
Constrain 'x' to float or int32 tensor.
(Optional) The axis of the quantization dimension of the input tensor. Ignored for per-tensor quantization. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input).
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\reduction\defs.cc
Constrain input and output types to high-precision and 8 bit numeric tensors.
A list of integers, along which to reduce. The default is to reduce over all the dimensions of the input tensor. Accepted range is [-r, r-1] where r = rank(data).
Defines behaviour if 'axes' is empty. Default behaviour with 'false' is to reduce all axes. When axes is empty and this attribute is set to true, input tensor will not be reduced,and the output tensor would be equivalent to input tensor.
axes as an input and attribute cannot be specified at the same time.
Whether to select the last index or the first index if the {name} appears in multiple indices, default is False (first index).
The axis in which to compute the arg indices. Accepted range is [-r, r-1] where r = rank(data).
This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).
Scalar multiplier for input tensor C, the default value is 1.0.
Scalar multiplier for the product of input tensors A * B, the default value is 1.0.
Whether C should be broadcasted
Maximum value, above which element is replaced by max
Minimum value, under which element is replaced by min
Dimension on which to do the sort.
Number of top elements to retrieve
Invalid value for attribute k
If necessary the right-hand-side argument will be broadcasted to match the
shape of left-hand-side argument. When broadcasting is specified, the second
tensor can either be of element size 1 (including a scalar tensor and any
tensor with rank equal to or smaller than the first tensor), or having its
shape as a contiguous subset of the first tensor's shape. The starting of the
mutually equal shape is specified by the argument "axis", and if it is not set,
suffix matching is assumed. 1-dim expansion doesn't work yet.
For example, the following tensor shapes are supported (with broadcast=1):
  shape(A) = (2, 3, 4, 5), shape(B) = (,), i.e. B is a scalar tensor
  shape(A) = (2, 3, 4, 5), shape(B) = (1, 1), i.e. B is an 1-element tensor
  shape(A) = (2, 3, 4, 5), shape(B) = (5,)
  shape(A) = (2, 3, 4, 5), shape(B) = (4, 5)
  shape(A) = (2, 3, 4, 5), shape(B) = (3, 4), with axis=1
  shape(A) = (2, 3, 4, 5), shape(B) = (2), with axis=0
Attribute `broadcast=1` needs to be passed to enable broadcasting.
Describes the axis of the inputs when coerced to 2D; defaults to one because the 0th axis most likely describes the batch_size
If set, defines the broadcast dimensions. See doc for details.
Pass 1 to enable broadcasting
Describes the axis of the inputs when coerced to 2D; defaults to one because the 0th axis most likely describes the batch_size. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input).
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\math\old.cc
Value of beta default to 0.5
Value of alpha default to 0.2
Coefficient of leakage default to 0.01.
Coefficient of ELU default to 1.0.
Coefficient of SELU default to 1.0507.
Coefficient of SELU default to 1.6732.
Constrains input types to all numeric tensors.
Constrains input to boolean tensor.
Constrains output to boolean tensor.
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\logical\defs.cc
Direction of moving bits. It can be either "RIGHT" (for right shift) or "LEFT" (for left shift).
        {
            O1 = Less (A, B)
            O2 = Equal (A, B)
            C = Or (O1, O2)
        }
        
Constrain input and output types to integer tensors.
Constrains input/output to boolean tensors.
Only one of the attributes 'value' or 'sparse_value' must be specified for a Constant node.
One of the attributes 'value' or 'sparse_value' must be specified for a Constant node.
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\generator\old.cc
Attribute 'value' of Constant node must exist with 'Tensor' data.
The axis in which to compute the arg indices.
Computes the indices of the {name} elements of the input tensor's element along the
provided axis. The resulting tensor has the same rank as the input if keepdims equal 1.
If keepdims equal 0, then the resulting tensor have the reduced dimension pruned.
The type of the output tensor is integer.
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\reduction\old.cc
Element type of inputs are expected to be the same.
output_sequence
SequenceConstruct is expected to have at least 1 input.
 is null. Type info is expected.
Input type for input at index 
(Optional) The data type of the tensors in the output sequence. The default type is 'float'.
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\sequence\defs.cc
Attribute dtype should be of integer type and specify a type.
Which axis to concat on. Accepted range in `[-r, r - 1]`, where `r` is the rank of input tensors. When `new_axis` is 1, accepted range is `[-r - 1, r]`. 
Insert and concatenate on a new axis or not, default 0 means do not insert new axis.
new_axis must be either 0 or 1
], Value=
Invalid value of attribute 'axis'. Accepted range=[
Input type for input at index 0 is null. Type info is expected.
position
Constrain position to integral tensor. It must be a scalar(tensor of empty shape).
Input Sequence and Tensor are expected to have the same elem type. Sequence=
input_sequence
Constrain input types to any tensor type.
Input Sequence and Tensor are expected to have type info. Current type is null.
 Tensor=
Constrain split size to integral tensor.
Constrain output types to all tensor types.
Which axis to split on. A negative value means counting dimensions from the back. Accepted range is [-rank, rank-1].
Keep the split dimension or not. Default 1, which means we keep split dimension. If input 'split' is specified, this attribute is ignored.
 sum of split values=
Sum of split values not equal to 'input' dim size on 'axis'. 'axis' dim size=
Constrain output to integral tensor. It must be a scalar(tensor of empty shape).
Only supports `int32_t` or `int64_t` inputs for split
Input 'split' can not be empty.
Constrains input to float tensors.
Constrains input to integral tensors.
Enable broadcasting
If set, defines the broadcast dimensions.
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\logical\old.cc
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\object_detection\old.cc
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\quantization\old.cc
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\rnn\old.cc
foward
Optional scaling values used by some activation functions. The values are consumed in the order of activation functions, for example (f, g, h) in LSTM.
The sequence output for the hidden is optional if 0. Default 0.
Data size mismatch. Tensor: 
load external data into raw data for tensor: 
Cannot parse data from external tensors. Please 
 does not match the actual size
 expected size 
The type of tensor: 
 Actual:
. Expected:
ParseData type mismatch for tensor: 
 is undefined so it cannot be parsed.
 source=
Mismatched type:
Mismatched tensor element type:
Mismatched sparse tensor element type:
 target=
Input was expected to have either tensor, sequence, or optional type. Got 
Output was expected to have tensor type. Got 
Input was expected to have sequence type. Got 
Element type of input was unknown
Input was expected to have optional type. Got 
Element type of tensor or sparse tensor input was unknown
Input was expected to have tensor or sparse tensor type. Got 
 does not match existing output type of 
Input element type of 
source sequence type missing element type.
target sequence type missing element type.
source optional type missing element type.
target optional type missing element type.
string_data
int64_data
raw_data
double_data
) to UNDEFINED is not allowed
setting data_type field (tensor name: 
float_data
int32_data
value_type
Unrecognized type value case (value_info name: 
data_type
Field '
 is required but missing.
elem_type
key_type
' instead of '
' should be stored in field '
values of data_type '
Unrecognized data_type (tensor name: 
TensorProto (tensor name: 
) should contain one and only one value field.
) should not be stored in raw_data field
STRING data (tensor name: 
) should be stored in 
) is stored externally but doesn't have a location.
TensorProto ( tensor name: 
) is 0-element but contains data!
uint64_data
) is stored externally and should not have data field.
Data of TensorProto ( tensor name: 
, but it doesn't exist or is not accessible.
==> Context: 
 is required to be non-empty.
value_info
' of 
Op registered for 
 OpType: 
Bad node spec for node. Name: 
graph
No opset import for domain '
 with domain_version of 
No Op registered for 
 is deprecated in domain_version of 
) has zero input and zero output.
, type: 
NodeProto (name: 
Warning: Checker does not support models with experimental ops: 
) should not contain more than one value field.
Attribute (name: 
) should refer to attribute in parent node.
op_type
' has been used as output names multiple times.
 is not output of any previous nodes.
name: 
' of node: 
Nodes in a graph must be topologically sorted, however input '
 initializer name is not unique
 in initializer but not in graph input
Sparse tensor initializers must have a non-empty name
 sparse initializer name is not unique across initializers and sparse_initializers
' has been used as graph input names multiple times.
Graph must be in single static assignment (SSA) form, however '
Tensor initializers must have a non-empty name
) has 
Sparse tensor indices (
] out of range [0, 
) index value at position [
 values, but NNZ is 
) must have rank 1 or 2.
) has no index values.
type field and data field mismatch in attribute 
Sparse tensor values (
) must have a dense-rank > 0
) dimensions are not positive.
) must have INT64 type.
] out of range.
] not in lexicographic sorted order.
sparse_tensor_proto
) must have rank 1.
Sparse tensor (
] not in sorted order.
) first dimension size does not equal NNZ.
) second dimension size does not match rank of tensor.
ConstantFill
type_proto
invalid stol argument
stol argument out of range
Error context: 
[ParseError at position 
 column: 
(line: 
graphs
sparse_tensors
type_protos
floats
strings
tensors
Error parsing TensorProto (expected a tensor shape).
Error parsing TensorProto shape (expected numeric dimension).
Unhandled type: %d
Unexpected attribute type.
String value expected, but not found.
Identifier expected but not found.
Unexpected type.
Error parsing TensorProto (expected a tensor type).
Expected character 
Value expected but not found.
Integer value expected, but not found.
Unexpected literal type.
Container for generated shape data cannot be nullptr when enable_data_propagation option is set.
 were provided
 inputs but 
Graph has 
. No schema registered for this operator.
Warning: Unsupported operator 
Shape inference error(s): 
type case unsupported. existing=
type case unsupported for symbolic shape inference. inferred=
 models with experimental operators: 
Warning: Shape inference does not support
in initializers. 
Cannot find missing input: 
, node name: 
(op_type:
Cannot use the same name as both a subgraph initializer and subgraph input: 
The number of graph input cannot be smaller than the number of node input
 were provided.
Graph initializer names must appear after the actual inputs: 
sparse_tensor_type
NOT_SET
 inferred=
type case mismatch. existing=
optional_type
opaque_type
 already exists.
Data for input  
Inferred shape and existing shape differ in dimension 
unk__
Inferred elem type differs from existing elem type: (
Inferred shape and existing shape differ in rank: (
Input 'scales' must have float element type.
Dimension value inferred (
Number of elements of input 'sizes' must be same as rank of input 'X'
Input 'sizes' must have int64 element type.
Number of elements of input 'scales' must be same as rank of input 'X'
) is not equal to the existing dim value (
onnx.NodeProto
onnx.TrainingInfoProto
onnx.ModelProto
onnx.StringStringEntryProto
onnx.AttributeProto
onnx.ValueInfoProto
onnx.TypeProto.Opaque
onnx.TypeProto
onnx.OperatorSetIdProto
onnx.FunctionProto
onnx.TypeProto.Sequence
onnx.TypeProto.Map
onnx.TypeProto.Optional
onnx.TypeProto.SparseTensor
onnx.SparseTensorProto
onnx.TensorShapeProto.Dimension
onnx.TensorShapeProto
onnx.TypeProto.Tensor
onnx.TensorAnnotation
onnx.GraphProto
onnx.TensorProto.Segment
onnx.TensorProto
FLOATFLOATSGRAPHGRAPHSINTINTSSPARSE_TENSORSPARSE_TENSORSSTRINGSTRINGSTENSORTENSORSTYPE_PROTOTYPE_PROTOSUNDEFINED
Unknown encoding 
D:\a\_work\1\s\engine\lotus\cmake\external\re2\re2\re2.cc
bytemap range 
list count 
program size 
pattern length 
endpos: 
startpos: 
RE2: invalid startpos, endpos pair. [
Unexpected re_anchor value: 
Error reverse compiling '
Invalid RE2: 
text size: 
Error parsing '
Error compiling '
pattern too large - compile failed
SearchNFA inconsistency
DFA out of memory: 
SearchDFA inconsistency
SearchOnePass inconsistency
SearchBitState inconsistency
invalid perl operator
invalid UTF-8
invalid named capture group
NumCapturesWalker::ShortVisit called
trailing \
no argument for repetition operator
invalid repetition size
bad repetition operator
invalid character class range
missing ]
missing )
unexpected )
no error
unexpected error
invalid escape sequence
invalid character class
Regexp not destroyed.
D:\a\_work\1\s\engine\lotus\cmake\external\re2\re2\regexp.cc
Bad reference count 
Unexpected op in Regexp::Equal: 
D:\a\_work\1\s\engine\lotus\cmake\external\re2\re2/walker-inl.h
Walk NULL
Stack not empty.
Compiler::Copy called!
No ranges in char class
Missing case in Compiler: 
should never happen
D:\a\_work\1\s\engine\lotus\cmake\external\re2\re2\compile.cc
unknown round: 
RE2: unexpected op: 
Bad hex digit 
D:\a\_work\1\s\engine\lotus\cmake\external\re2\re2\parse.cc
RepetitionWalker::ShortVisit called
Concat of 
AddFoldedRange recurses too much.
Bad call to ParseState::ParsePerlFlags
Unexpected opcode in short circuit: 
context does not contain text
Bad args: nsubmatch=
 in step
Unhandled 
unhandled 
D:\a\_work\1\s\engine\lotus\cmake\external\re2\re2\nfa.cc
 in AddToThreadq
Unexpected special state in RunStateOnByte
StateSaver failed to restore state.
Failed to analyze start state.
D:\a\_work\1\s\engine\lotus\cmake\external\re2\re2\dfa.cc
DeadState in RunStateOnByte
NULL state in RunStateOnByte
unhandled opcode: 
RunStateOnByteUnlocked failed after Reset
RunStateOnByteUnlocked failed after ResetCache
njob_ = 
GrowStack() failed: 
D:\a\_work\1\s\engine\lotus\cmake\external\re2\re2\bitstate.cc
Unexpected opcode: 
job_.size() = 
Cannot use SearchOnePass for unanchored matches.
D:\a\_work\1\s\engine\lotus\cmake\external\re2\re2\onepass.cc
D:\a\_work\1\s\engine\lotus\cmake\external\re2\re2\prog.cc
Unexpected opcode in IsMatch: 
Malformed repeat 
DoCoalesce failed: r2->op() is 
DoCoalesce failed: r1->op() is 
Simplify case not handled: 
SimplifyWalker::ShortVisit called
Case not handled in ComputeSimple: 
CoalesceWalker::ShortVisit called
D:\a\_work\1\s\engine\lotus\cmake\external\re2\re2\simplify.cc
09AZaz
!/:@[`{~ ~
09AZ__az!~
09AZ__az09AFafAZ
  09az
[:digit:]
[:^cntrl:]
[:graph:]
[:^digit:]
[:blank:]
[:^ascii:]
[:cntrl:]
[:^blank:]
[:alpha:]
[:^alnum:]
[:ascii:]
[:^alpha:]
[:alnum:]
[:xdigit:]
[:^word:]
[:^xdigit:]
[:upper:]
[:^space:]
[:word:]
[:^upper:]
[:punct:]
[:^print:]
[:space:]
[:^punct:]
[:lower:]
[:^graph:]
[:print:]
[:^lower:]
Bassa_Vah
Bamum
Bengali
Batak
Armenian
Arabic
Balinese
Avestan
Adlam
Anatolian_Hieroglyphs
Hiragana
Hebrew
Inherited
Imperial_Aramaic
Hanifi_Rohingya
Hangul
Hatran
Hanunoo
Gunjala_Gondi
Gujarati
Gurmukhi
Gothic
Glagolitic
Greek
Grantha
Lepcha
Latin
Khmer
Khitan_Small_Script
Khudawadi
Khojki
Katakana
Kannada
Kharoshthi
Kayah_Li
Inscriptional_Parthian
Inscriptional_Pahlavi
Kaithi
Javanese
Cherokee
Chorasmian
Caucasian_Albanian
Chakma
Buhid
Buginese
Carian
Canadian_Aboriginal
Bopomofo
Bhaiksuki
Braille
Brahmi
Elymaic
Elbasan
Georgian
Ethiopic
Dogra
Dives_Akuru
Egyptian_Hieroglyphs
Duployan
Cyrillic
Cypriot
Devanagari
Deseret
Coptic
Common
Cuneiform
Old_Italic
Old_Hungarian
Old_Permic
Old_North_Arabian
Nyiakeng_Puachue_Hmong
Nushu
Ol_Chiki
Ogham
Nandinagari
Nabataean
New_Tai_Lue
Phoenician
Phags_Pa
Pau_Cin_Hau
Palmyrene
Osage
Oriya
Pahawh_Hmong
Osmanya
Old_Sogdian
Old_Persian
Old_Turkic
Old_South_Arabian
Mandaic
Malayalam
Marchen
Manichaean
Lydian
Lycian
Makasar
Mahajani
Linear_A
Limbu
Linear_B
Multani
Myanmar
Mongolian
Mende_Kikakui
Meetei_Mayek
Meroitic_Hieroglyphs
Meroitic_Cursive
Masaram_Gondi
Medefaidrin
Yezidi
Warang_Citi
Zanabazar_Square
Ugaritic
Tirhuta
Wancho
Thaana
Tifinagh
Tibetan
 : : 
.!.!.
&!&!e
!$!$!&!&!(!(!*!-!0!3!>!?!E!E!
,.,`,`,b,d,g,g,i,i,k,k,m,p,r,r,u,u,~,
.0/011
2`2~2`
Sinhala
SignWriting
Sharada
Siddham
Shavian
Runic
Rejang
Saurashtra
Samaritan
Psalter_Pahlavi
Tamil
Takri
Telugu
Tangut
Tai_Le
Tagbanwa
Tai_Viet
Tai_Tham
Syloti_Nagri
Sundanese
Tagalog
Syriac
Sogdian
Soyombo
Sora_Sompeng
!#!%!%!'!'!)!)!.!.!:!;!J!J!L!M!O!O!
#"#(#+#{#}#
#&$@$J$
&n&p&g'
+/+E+F+M+s+v+
,P.Q.
0 0 06070>0?0
2*2G2P2P2`2
))]]}}
F F ~ ~ 
#*#*#i'i'k'k'm'm'o'o'q'q's's'u'u'
)#.#.%.%.'.'.).).
$$++<>^^``||~~
D D R R z | 
!#!%!%!'!'!)!)!.!.!:!;!@!D!J!M!O!O!
#(#+#&$@$J$
)s+v+
,P.Q.
0 0 06070>0?0
2*2G2P2P2`2
 9 9 
. . .
 *!+!2!2!N!N!`!
 d f p t ~ 
!%!'!)!,!1!3!M!O!_!
!&$@$J$`$
)s+v+
0 00070<0?0
1 2_2
,.,0,^,
0!0)080:0
? @ T T 3
!$!$!&!&!(!(!*!-!/!9!<!?!E!I!N!N!
,.,0,^,`,
-%-'-'-----0-g-o-o-
-/./.
01050;0<0A0
1/111
.:.;.@.@.
00000
 * . ` d f o 
 / / _ _ 
++<>||~~
D D R R z | 
!@!D!K!K!
" #!#|#|#
%o&o&
*0+D+G+L+)
-*0/0
(([[{{
 E E } } 
#)#)#h'h'j'j'l'l'n'n'p'p'r'r't't'
)".".$.$.&.&.(.(.B.B.
 ( ) / / _ _ 
 * . ` d f o 
!#%'**,,./:;?@\\
   ' 0 8 ; > A C G Q S S U ^ 
,p-p-
.*...0.9.<.?.A.A.C.O.R.R.
0=0=0
 *0-0
 |,},o-o-/./.
01050;0;0
!/!/!4!4!9!9!<!=!F!I!N!N!
!0,^,a,a,e,f,h,h,j,j,l,l,q,q,s,t,v,{,
-%-'-'-----A
p p t y 
0!0)080:0
1 2)2H2O2Q2_2
0!0)080;0
!#%*,/:;?@[]__{{}}
 ' 0 C E Q S ^ } ~ 
#)#*#h'u'
,p-p-
...0.O.R.R.
00000=0=0
-*0-0
-%-'-'-----
p p t y 
 P!_!
1 2)2H2O2Q2_2
0-g-o-p-
5!8!0-g-
0<0<0A0
1/111
.0/0#
{%d,%d}
Bad final char: 
{%d,}
[^\x00-\x{10ffff}]
(){}[]*+?|.^$\
 [truncated]
D:\a\_work\1\s\engine\lotus\cmake\external\re2\re2\tostring.cc
kRegexpCapture cap() == 0
\x{%x}
[]^-\
(?HaveMatch:%d)
(?-m:^)
(?-m:$)
GraphTransformerHelpers::RegisterGraphTransformers
WinmlRuleTransformer
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\GraphTransformers\GraphTransformerHelpers.cc
BatchNormalizationAddFusion
BatchNormalizationMulFusion
Internal error in BatchNormalizationMulFusion. BatchNormalization_B_tensor_proto is NULL
%~3a*
++Q5@.
'7JCy7
L>2?~>
@?*BL?fff?
l?4?~?
&TpxAD
?333333
t@33{@
raB3G
~3a*~3a*~3a*~3a*
++Q5@.Q5@.Q5@.Q5@.
6JCy7JCy7JCy7JCy7
8giP9giP9giP9giP9
L>2?~>2?~>2?~>2?~>
l?4?~?4?~?4?~?4?~?
`@33{@33{@33{@33{@
RuntimeError
UTCReplace_AppSessionGuid
PartA_PrivTags
schemaVersion
hResult
sessionId
errorCode
errorCategory
errorMessage
function
SessionCreationStart
UTCReplace_AppSessionGuid
PartA_PrivTags
ProcessInfo
UTCReplace_AppSessionGuid
PartA_PrivTags
schemaVersion
runtimeVersion
isRedist
SessionCreation
UTCReplace_AppSessionGuid
PartA_PrivTags
schemaVersion
sessionId
irVersion
OrtProgrammingProjection
modelProducerName
modelProducerVersion
modelDomain
usefp16
domainToVersionMap
modelGraphName
modelMetaData
loadedFrom
executionProviderIds
EvaluationStop
EvaluationStart
RuntimePerf
UTCReplace_AppSessionGuid
PartA_PrivTags
schemaVersion
sessionId
totalRuns
totalRunDuration
ExecutionProviderEvent
UTCReplace_AppSessionGuid
PartA_PrivTags
adapterLuidLowPart
adapterLuidHighPart
Microsoft.ML.ONNXRuntime
onnxruntime.pdb
.text
.text$di
.text$mn
.text$mn$00
.text$x
.text$yd
.idata$5
.00cfg
.CRT$XCA
.CRT$XCC
.CRT$XCL
.CRT$XCU
.CRT$XCZ
.CRT$XDA
.CRT$XDZ
.CRT$XIA
.CRT$XIC
.CRT$XIZ
.CRT$XLA
.CRT$XLC
.CRT$XLD
.CRT$XLZ
.CRT$XPA
.CRT$XPZ
.CRT$XTA
.CRT$XTZ
.gehcont
.gfids
.giats
.rdata
.rdata$T
.rdata$r
.rdata$voltmd
.rdata$zETW0
.rdata$zETW1
.rdata$zETW2
.rdata$zETW9
.rdata$zzzdbg
.rtc$IAA
.rtc$IZZ
.rtc$TAA
.rtc$TZZ
.tls$
.tls$ZZZ
.xdata
.xdata$x
.didat$2
.didat$3
.didat$4
.didat$6
.didat$7
.edata
.idata$2
.idata$3
.idata$4
.idata$6
.data
.data$r
.data$rs
.pdata
.didat$5
_RDATA
.rsrc$01
.rsrc$02
P08N|TY
P08~|TY
@>|TY
@f|TY
P08N|TY
`h~|TY
p>|TY
@>|TY
@08N|TY
&(*\(,
&(*\(`
4.6 8L4D:
t t$0&l*
$^&h(d,@.d2>4`8d:
>|@bB
T<4>2
T 4"2$4&T*4,2
"2$^(
".$^(
:l<>:4
".$^(
`:`>0@.
<  "X$e
*",P.-
BD[?j
**,H&5
66842
"2$> 
`h~|TY
@08N|TY
@08~|TY
08~|TY
`h~|TY
`8@HP
f P"4
0$L& (
*$L& (
,&L( *
,&L( *
R,F": ^2
@6|TY
 (6|TY
`6|TY
p6|TY
p6|TY
 (0>|TY
 .|TY
@>|TY
@>|TY
@6|TY
@6|TY
`65:]
@6|TY
P6|TY
`6m=]
(j*z,!
0l2V6
r<~@LB
@6jA]
2z8z>
!-:L$
$z*z0
HzNzTA
`6Gu]
p^]u]
P6;w]
pf|TY
@6Ex]
p6|TY
N ^"b B
b J"J
@6a~]
P^w~]
`>|TY
 .|TY
L 6$M
.b H0
6R:.<,>,@:D<FY
L@P(R
TbP6LV2
`6|TY
`6|TY
08@P^|TY
8@HXf|TY
@6|TY
@6|TY
"6&F(B&t(B&V"
J$0 B$
@6|TY
4:6V(
*&,B.
(6|TY
@>|TY
P J"6
P6|TY
P6|TY
 &"H(D&
&,H2Z0-
"V h$
,h0P.v0B.
`6|TY
`6|TY
p^|TY
P>|TY
JLkmk
n 4J>L
"P$X"$&@(.&t*
FdJ.L
l,nFl
 P"j&
(J*J,j0
2J4J6l8h:p>v@D>zBD>zDV>^F
>HHp>HJp>HLp>HNp>HPp>HRp>HTp>HVp>HXp>HZp>H\p>H^p>H`p>Hbp>Hdp>Hfz>Hh
@08@H
.\2^4j6
$ x"N$b&
*$,x.N0R>:<V8
66:`<^>N@
J(BH<n>NL
B(>4<
TtVHTe
X(B\<p>NZ
B(>f<l\"^H`2d
dffJt
j(fXdfvvx(z8|
R(Hx<f>
Rp^6`9
Rfd2f
h(XhRhTbj
X(T~Rvl6n
R|r(t6v~x(zT|6
n(pFh:r
t(p`hhlbv
p(l`hhlbx
f(br\b^
d(br\b^
$6(^*n,T.
*n,b8
HnJbP
^lbrd(fZh2l
lrt(v6x
z(|6~l
6(4x*b,
h(jFb>l
v(xFpBz
|(xZp`tD~
x(tNp`tD
x(thpDt
,"(n"P$
8@HPZ
8 h"h$T&
N(LTFhHVP
RhTVV
Z.\6^p` b6dnf h6jvl n`p6t
|(zvthvV~
$p&f.
*(&@$
Z(X|RpTf\
X(T6Rm
b(XxRzTfd
X(T\RM
h(j"l"n"p"r
h.t:h
<6@^BvDHF
BvDHP
H"DJBA
(^*^,d.X0n2
DFF JNL P
D\BtPdJd
&$("*
0"2jDB^!
 T&D4
"F$P&}
*r,~.02Q
8.@$B&@
HjFvJfL(N"P,R*T"VxX
F,Z$FZZ(F
$l(.,N.
z".$,
n(.*,
n".$,
z(.*,
n(.*,
8@HPZ
D"@J>n@LJ
L"Dh>L@
VnZf\.^6`\b d<f
h j<l
n(p6rjt(vNx0|x~x
8@HPX`hpx
8@HPX`j
@HPXb
8@HPZ
b(H^:rDH`
H(Db:HD
F(Hn:bDHd
f(H0:
DHp(DX:bDHp(D*:(rp:tDHp(DP:
DHp(Dh:HD(pf:HD(p
8@HPX`hpx
R:&<">,@nB
( ""($
(B*"(l
&:((*(,..z0
2B4"2J
@8h:(<(>$@*B
&~*4.}
 ~044
>`@jBdD.FxD
J.H@<
8@HPZ
 (08@J
"z$ &
,"4P,4"
P8@HPX`hpx
8@HPX`hpx
2"4.6(8r6z2":~<":
8@HPX`hpz
""$j&
(~,@.L(~,@.$
b .",(r*L,@.
 v"J$@&L 
A)B(4?
-B(4?
2h082
2P4.6,0
4.6,,
4.6,0j4.6,:
4.6,0j4.6,0j4.6
`>`(^
f,`8.:,
0.2,,v0.2
f,`8.:,
2.4,,j2.4
`6|TY
&((d&
f.P0)
2^6L8
p6<?^
`6$A^
p6$E^
h6`@.B,
&$P&.(,
P .",
:D`H.J,D&FPH.J,6&8P:.<
`6.8,
(*P6.8,
."~&.(,
4*T,..,
N*<0~<.>,
(*T,..,@(DTF.H,@
DnP.R,@
DnP.R,@(DPJ.L
`6-Y^
`6CY^
<p@6B
HvL6NtR6T
HpL6N
TtX6Zx^6`
`6"_^
BHDjF\D
>.@,4h>.@,
>.@,4t>.@,4h>.@,Dt>.@,4h>.@
`6Ia^
`6_a^
(  "($
&f*.,,
` .",
`6.b^
`6Db^
"l&b*P@
D.FX>
@6Hf^
P \\P-
, Z"4$2
,0Z24428
p^&?^
8@JPe
2.4,.v2.4,$
@ v&.(,
v&.(,
*`.@0
(t*2(
~ |$L&z
`08@H
 (08@HPX`hpx
"P$D&
`:(4?
"x H(` 
*~ H.
"D d$i
0f2|4
>L@lB
HvJ8N
p6|TY
 N"v$
BPD.F0D9
N(RTD
  "R*Y
j .",
j .",
"Z$|(
<P>R<M
0624<
(X*t,
4*6"8|:H<X
>r@B>J
 z"H$V 
.h0t2
4`0~.
6:8$:<<0>,@0B,D0F0H
J&L0N*P"^z`H^L\
bzdHbJ&
&h*^,
2,JB2y
f 4"2
$H&|*.,
6"80:6<X>-
">dB4D2J
d.f,R(T
VPX4Z2
0$"&0((*0,(.00(20446
L0X(Z0\(^0`,b0d*f!
L0N(P0R4T
"l$>"%
$@&v*(,
0B2j6 8p:V<
4"6F8]
@2|0G
$x&((
d .",
 .",*
 .",4
8.:L4
08@HPX`hr
X">$"
8@HPZ
@8@HPX`hpx
@08@HPXb
8@HPX`hpx
8@HPX`hpz
d@>B$
X">$"
8@HPX`hpz
08@HPX
8H:28 <=
 H"2  $
,H.2, 0
2H422 6i
&,(2&R*,,4*h,"*|,:.T0T*D2T4
*\6X*
8,:28R<,>.<T>"<n>:@TBT<DDTF
H,J2HfL,N<LLN4PTRTLJTTV
L\XXL
Z,\2ZJ^JbTdP^JfVh\^JbZdP^JfZhT^JbTd\^(`*j ^
l,n2lNpJtTvXpDxTz
p^r(plr(plr(plr"pN|"pXl
08@HPX`hpx
@8@HPX`
@8@HP
&&(<&q
*F,**P&
8@HPZ
.d2442&
8@HPZ
 <"<$
.d0~2.4
0"6X.
B0D^F H
`jb0`<ZdF
rbp4@
x4z2t8v(|
`:h:J
V&X,V ZM
 H"2  $
6H826 :I
<H>2< @A
HHJ2H L@Ne
P&R,P T
`8@HPX`hpx
@@HPX`h
08@HP
*J,0*
.L02.V402
`4tTM
j l"j$f&l(n*n,n.p0n2l4f6h8f:f<P>2
 0$4&,(8*
8@HPX`hpz
8@HPX`hpx
8@HPZ
8@HPX0
8@HPX`hpx
8@HPX`
P8@HPX`hpx
p8@HPX`hp
@08@HPX
$r&"$
t&L"8 -
ThP,N8L
!rpJN
2N6T<
A$rpJN
2N6T<
@08@HPX`hr
X":$"
8@HPZ
"v.V*8(]
4\8N:e
N`J8H
40&2(P\TNVtZ&\B^
h^d8bx
nj(lNnBp&rBt
f0B~.
@0^j(l(
0Tj(lN
dj(lN
lj(lN
L0Vj(lN5
0Vj lNQ
fj lNm
L0Xj"]
L0B~.
@HPXb
&f<\@NB
ZnV2T8R
\\`Nb
xnt2r8p
&h<\@Nz
@&j((
@8@HPZ
1rpJN
5rpJN
9rpJN
D:HHJ*L8N
8@HPZ
@08@H
08@HPZ
08@HPZ
8@HPX`hp
ZN^N`
8@HPZ
$ """$P
" $""&P
" "(R
$ "*R
" ",R
$ ".R
$r&"$
X":$"
 z,V(8&5
2\6N8
LZH8F
d.,N.P\TNV>XNZ6\&^B`
nlj2h8fv
Nr2t\xNz<|(~N
NrJt6!
P8@HP^
X":$"
RbN@L*J*H*
8@HPX
" """$J
" "&L
" "(P
" """$J
" "&L
" "(L
@HPX =
P:p&W
8@HPX,
@HPX 
 (0:<
0\4X6
N"PZNX@u
P,XJZ
8@HPZ
*$,X*
: <X:
(08@HV
(6')_
(6=)_
(6V)_
J V$V(`,
(60*_
(6F*_
(6\*_
 (65+_
(08@HVK+_
(08@HVa+_
(6w+_
(08@HV
(08@HV
(08@HV
(6o)_
(6=,_
(6S,_
(6i,_
CreateDXGIFactory2
CoCreateFreeThreadedMarshaler
CoTaskMemAlloc
onnxruntime.dll
OrtGetWinMLAdapter
OrtGetApiBase
OrtSessionOptionsAppendExecutionProviderEx_DML
OrtSessionOptionsAppendExecutionProvider_CPU
OrtSessionOptionsAppendExecutionProvider_DML
FindFirstFileW
SetLastError
FindNextFileW
FindClose
MultiByteToWideChar
GetLastError
WideCharToMultiByte
api-ms-win-core-file-l1-1-0.dll
api-ms-win-core-errorhandling-l1-1-0.dll
api-ms-win-core-string-l1-1-0.dll
api-ms-win-core-processenvironment-l1-1-0.dll
strcspn
_unlock_locales
_lock_locales
__strncnt
wcsnlen
_o____lc_codepage_func
_o____lc_collate_cp_func
_o____lc_locale_name_func
_o____mb_cur_max_func
_o___acrt_iob_func
_o___pctype_func
_o___std_exception_copy
_o___std_exception_destroy
_o___stdio_common_vfprintf
_o___stdio_common_vsnprintf_s
_o___stdio_common_vsprintf
_o___stdio_common_vsprintf_s
_o___stdio_common_vswprintf
_o__aligned_free
_o__aligned_malloc
_o__beginthreadex
_o__callnewh
_o__calloc_base
_o__cexit
_o__close
_o__configure_narrow_argv
_o__create_locale
_o__crt_atexit
_o__dclass
_o__difftime64
_o__errno
_o__execute_onexit_table
_o__fdclass
_o__fdsign
_o__free_base
_o__free_locale
_o__fseeki64
_o__fstat64i32
_o__get_errno
_o__get_stream_buffer_pointers
_o__Getdays
_o__Getmonths
_o__Gettnames
_o__gmtime64_s
_o__initialize_narrow_environment
_o__initialize_onexit_table
_o__invalid_parameter_noinfo
_o__invalid_parameter_noinfo_noreturn
_o__localtime64_s
_o__lock_file
_o__malloc_base
_o__mktime64
_o__purecall
_o__read
_o__realloc_base
_o__register_onexit_function
_o__seh_filter_dll
_o__set_errno
_o__sopen_s
_o__stat64i32
_o__Strftime
_o__strnicmp
_o__strtoi64
_o__towlower_l
_o__towupper_l
_o__unlock_file
_o__W_Getdays
_o__W_Getmonths
_o__W_Gettnames
_o__wcsdup
_o__Wcsftime
_o__wfsopen
_o__write
_o__wsopen_s
_o_abort
_o_acosf
_o_acoshf
_o_asinf
_o_asinhf
_o_atanf
_o_atanhf
_o_bsearch
_o_calloc
_o_ceil
_o_ceilf
_o_cosf
_o_coshf
_o_exp
_o_expf
_o_fclose
_o_fflush
_o_fgetc
_o_fgetpos
_o_floor
_o_floorf
_o_fmod
_o_fmodf
_o_fputc
_o_fread
_o_free
_o_frexp
_o_fseek
_o_fsetpos
_o_fwrite
_o_isalnum
_o_isalpha
_o_isdigit
_o_islower
_o_isspace
_o_isupper
_o_iswspace
_o_ldexp
_o_localeconv
_o_log
_o_log2
_o_log2f
_o_logf
_o_malloc
_o_nearbyintf
_o_pow
_o_powf
_o_remainderf
_o_rint
_o_rintf
_o_roundf
_o_scalbn
_o_scalbnf
_o_setlocale
_o_setvbuf
_o_sin
_o_sinf
_o_sinhf
_o_sqrt
_o_sqrtf
_o_strerror
_o_strncpy_s
_o_strtod
_o_strtof
_o_strtol
_o_strtoll
_o_strtoull
_o_tanf
_o_tanh
_o_tanhf
_o_terminate
_o_tolower
_o_ungetc
_o_wcsftime
api-ms-win-crt-string-l1-1-0.dll
api-ms-win-crt-locale-l1-1-0.dll
api-ms-win-crt-private-l1-1-0.dll
InitOnceComplete
InitOnceBeginInitialize
InitializeSRWLock
TryAcquireSRWLockExclusive
GetCurrentThreadId
ReleaseSRWLockExclusive
AcquireSRWLockExclusive
FormatMessageA
GetStringTypeW
QueryPerformanceFrequency
QueryPerformanceCounter
GetSystemTimePreciseAsFileTime
EnterCriticalSection
LeaveCriticalSection
InitializeCriticalSectionEx
DeleteCriticalSection
EncodePointer
DecodePointer
LocalFree
LCMapStringEx
GetLocaleInfoEx
GetCPInfo
CompareStringEx
CloseHandle
InitializeCriticalSectionAndSpinCount
SetEvent
ResetEvent
WaitForSingleObjectEx
CreateEventW
GetModuleHandleW
GetProcAddress
RtlCaptureContext
RtlLookupFunctionEntry
RtlVirtualUnwind
UnhandledExceptionFilter
SetUnhandledExceptionFilter
GetCurrentProcess
TerminateProcess
IsProcessorFeaturePresent
GetCurrentProcessId
GetSystemTimeAsFileTime
InitializeSListHead
IsDebuggerPresent
RtlUnwindEx
RtlPcToFileHeader
RaiseException
FlsAlloc
FlsGetValue
FlsSetValue
FlsFree
InterlockedFlushSList
api-ms-win-core-synch-l1-2-0.dll
api-ms-win-core-synch-l1-1-0.dll
api-ms-win-core-processthreads-l1-1-0.dll
api-ms-win-core-localization-l1-2-0.dll
api-ms-win-core-profile-l1-1-0.dll
api-ms-win-core-sysinfo-l1-2-0.dll
api-ms-win-core-util-l1-1-0.dll
api-ms-win-core-heap-l2-1-0.dll
api-ms-win-core-handle-l1-1-0.dll
api-ms-win-core-libraryloader-l1-2-0.dll
api-ms-win-core-rtlsupport-l1-1-0.dll
api-ms-win-core-processthreads-l1-1-1.dll
api-ms-win-core-sysinfo-l1-1-0.dll
api-ms-win-core-interlocked-l1-1-0.dll
api-ms-win-core-debug-l1-1-0.dll
api-ms-win-core-fibers-l1-1-0.dll
_initterm
_initterm_e
strncmp
api-ms-win-crt-runtime-l1-1-0.dll
PathCchRemoveFileSpec
PathCchRemoveBackslash
api-ms-win-core-path-l1-1-0.dll
SleepConditionVariableSRW
GetModuleFileNameA
CreateSemaphoreExW
HeapFree
ReleaseSemaphore
GetModuleHandleExW
WaitForSingleObject
ReleaseMutex
FormatMessageW
OutputDebugStringW
OpenSemaphoreW
HeapAlloc
CreateMutexExW
GetProcessHeap
DebugBreak
VirtualFree
VirtualAlloc
GetModuleFileNameW
GetFullPathNameW
LoadLibraryExW
FreeLibrary
WakeAllConditionVariable
CreateDirectoryW
SetThreadAffinityMask
ReadFile
GetFileSizeEx
SetThreadDescription
RemoveDirectoryW
GetFinalPathNameByHandleW
LoadLibraryExA
GetEnvironmentVariableA
GetFileAttributesW
CreateFile2
Sleep
GetFileAttributesA
GetLogicalProcessorInformation
GetCurrentThread
DeleteFileW
GetSystemInfo
SetFilePointerEx
CreateDirectoryA
WakeConditionVariable
GetCurrentProcessorNumber
EventUnregister
EventSetInformation
EventRegister
EventWriteTransfer
ReleaseSRWLockShared
AcquireSRWLockShared
api-ms-win-core-heap-l1-1-0.dll
api-ms-win-core-memory-l1-1-0.dll
api-ms-win-core-processtopology-obsolete-l1-1-0.dll
api-ms-win-core-processthreads-l1-1-3.dll
api-ms-win-core-file-l1-2-0.dll
api-ms-win-eventing-provider-l1-1-0.dll
VirtualProtect
VirtualQuery
ilogbf
ilogb
strnlen
api-ms-win-crt-math-l1-1-0.dll
GetNativeSystemInfo
LoadLibraryW
api-ms-win-core-libraryloader-l1-2-1.dll
strcmp
.?AVbad_exception@std@@
.?AVbad_alloc@std@@
.?AVexception@std@@
.?AVbad_array_new_length@std@@
.?AVfailure@ios_base@std@@
.?AVruntime_error@std@@
.?AVsystem_error@std@@
.?AVbad_cast@std@@
.?AV_System_error@std@@
.?AVFatalException@protobuf@google@@
.?AVlength_error@std@@
.?AVlogic_error@std@@
.?AVout_of_range@std@@
.?AVinvalid_argument@std@@
.?AVbad_function_call@std@@
.?AV<lambda_78d2ff8fcf22c6166aa97abbb5fd0d75>@@
.?AV<lambda_3435a69bc2a4a58cf25c9601fa916799>@@
.?AVOnnxRuntimeException@onnxruntime@@
.?AU?$default_delete@VIExecutionProvider@onnxruntime@@@std@@
.?AV<lambda_6b9436f4089e82f5864d5365b56a5e02>@@
.P6AXPEAX@Z
.?AVNotImplementedException@onnxruntime@@
.?AV<lambda_a4fe3bd39dcd0107f90455cb45c21429>@@
.?AV<lambda_ca87e505bf8a7e1f26579083c3683d94>@@
.?AV<lambda_7afdd6f6db24c890469fd5278509941d>@@
.?AUException@Ort@@
.?AV<lambda_739930acb5d4a4b1d92f387c9190b820>@@
.?AU?$default_delete@VCPUExecutionProvider@onnxruntime@@@std@@
.?AV<lambda_b32e5f2bc2fafd58a4afcda5e8537c65>@@
.?AVother_error@detail@nlohmann@@
.?AV<lambda_c89e6783ea7ba65dd2bbca371d42018d>@@
.?AVexception@detail@nlohmann@@
.?AV<lambda_73b1da0ad8cd1f5530ce77a072f70a46>@@
.?AV<lambda_fcb655464ad0e43ce2671f06c665bb0e>@@
.?AV<lambda_564e08730d4bd4a97b43e8c38a06cf59>@@
.?AV<lambda_90fa7e4386c5622637b0b1880f5ca8f3>@@
.?AU?$default_delete@VModel@onnxruntime@@@std@@
.?AV<lambda_b1b7fca51e482a233c6ad913e37a4670>@@
.?AV<lambda_de00575298e9c7ead3b243890c596395>@@
.?AV<lambda_daa97a08fba16356a015278e0f3ca1ed>@@
.?AV<lambda_33c11271a908f4f215dd361ca0be165e>@@
.?AV<lambda_6922b57be4edd39a89a7cea9e2ebb658>@@
.?AV<lambda_0fb4e93f4014e9a9ed52287648f86c91>@@
.?AVInferenceError@onnx@@
.?AV<lambda_8fef7dbb6b3e81cda489ebe45ab449bb>@@
.?AV<lambda_b35f6304ab9b91e067de16d773b507fb>@@
.?AV<lambda_888c8332848e83c225ef5f5587c3b38e>@@
.?AV<lambda_598c6d249dbd14226af8e69fe738f910>@@
.?AV<lambda_ab7cc530eb49a3a15082dbc27eb613da>@@
.?AV<lambda_dcbbb9aee7454d5a0feb975df66a7001>@@
.?AVSchemaError@onnx@@
.?AV<lambda_18dbcf0a6112b471cd3435637c32f0f8>@@
.?AV<lambda_d522df09ab9335db57f0cfc8c09d6630>@@
.?AV<lambda_3a80fca790cdb0fce14ddaedefa20351>@@
.?AV<lambda_73cc642ed8afaa2889c4f09584fca345>@@
.P6AXAEAUInferenceContext@onnx@@@Z
.?AV<lambda_79b1e3e4a047d410dc120a32d9f13caa>@@
.?AV<lambda_519fe9cae569b1ad1c3f73288fa352d4>@@
.?AV<lambda_7965ceabe5f45db89655ad0a7fd04cd6>@@
.?AV<lambda_716d92b2455b6dca0a3e195a50699167>@@
.?AVinvalid_iterator@detail@nlohmann@@
.?AVtype_error@detail@nlohmann@@
.?AVout_of_range@detail@nlohmann@@
.?AVparse_error@detail@nlohmann@@
.?AVResultException@wil@@
.?AV<lambda_7d7cf49bce0e19787ec70235e54b100e>@@
.?AV<lambda_71991a52be6daf389db1437901b4b4a6>@@
.?AV<lambda_0d57079c5084d18bf7b57a312b9c6ff1>@@
.?AV<lambda_fcea8697154110af19f3c4ff33f69c67>@@
.?AV<lambda_51049cb5fa4b41211d9b2f4f400d5578>@@
.?AV<lambda_3540305bd5a2ea6a94f5bc650b8b92bf>@@
.?AV<lambda_aa09ae9ae5ea9fb0f1758a92a75aceb3>@@
.?AV<lambda_4f3d7353e76f345aa194bb4f18efcd6f>@@
.?AV<lambda_707bfdc8d86f0538343a2653cbf5c062>@@
.?AV<lambda_5dc4cb1bad49b465af07c0f2ab16f9e0>@@
.?AVbad_variant_access@std@@
.?AVbad_optional_access@std@@
.?AV<lambda_c4798eecc829775e009cc44f3c8c2dfd>@@
.?AV<lambda_9022553a6e1ed5607e95f6ae7eff7128>@@
.?AV<lambda_c3e7b02a987d92f0e0ac021ea93b35b9>@@
.?AV<lambda_30c9eb05d9fe297e7d682ba8ba94fa4d>@@
.?AV<lambda_d01149748d9826af2bd1bb0ff765444d>@@
.?AV<lambda_dce07c92cb28f757c3a0051d00763d8e>@@
.?AV<lambda_ef38ddcc46989e690851ddf31e835397>@@
.?AV<lambda_7ff9bfb547edbeaca72e6699608f2c7b>@@
.?AV<lambda_1ab7e6f52b1232ea373dc4337562f767>@@
.?AV<lambda_e775dbf0393ee07909539154c7f76d40>@@
.?AV<lambda_b140c53d8f404b9fcf24fe0f4b06052b>@@
.?AV<lambda_848709f5f960f280f7c825f9d75fc06e>@@
.?AUhresult_wrong_thread@winrt@@
.?AUhresult_not_implemented@winrt@@
.?AUhresult_invalid_argument@winrt@@
.?AUhresult_out_of_bounds@winrt@@
.?AUhresult_no_interface@winrt@@
.?AUhresult_class_not_available@winrt@@
.?AUhresult_class_not_registered@winrt@@
.?AUhresult_changed_state@winrt@@
.?AUhresult_illegal_method_call@winrt@@
.?AUhresult_illegal_state_change@winrt@@
.?AUhresult_illegal_delegate_assignment@winrt@@
.?AUhresult_canceled@winrt@@
.?AUhresult_access_denied@winrt@@
.?AUhresult_error@winrt@@
.?AV<lambda_61a830e6f94472550f401a07bc054c29>@@
.?AV<lambda_f8f4a9a0b8865f5f39e8ba472bf52edc>@@
.?AV<lambda_e2d9c65f3423a9c8d6e3cd7ee2b26307>@@
.?AV<lambda_adedb98ee19cb4d9a4a46a5fd17584b5>@@
.?AV<lambda_780872346d96977900f5c40310e7966f>@@
.?AV<lambda_11b6908471b85c842f67ae917be35bc8>@@
.?AV<lambda_57033b6e8d5e4d038ed6204609dd48c9>@@
.P6APEAVOpKernel@onnxruntime@@AEBVOpKernelInfo@1@@Z
.?AV<lambda_9ad4dd46fb7ebf522a3e85e715143745>@@
.?AV<lambda_a7f558f1dd0bc6dfbc34073d8010b2d2>@@
.P6A?AVStatus@common@onnxruntime@@PEAXAEAV?$vector@UOrtValue@@V?$allocator@UOrtValue@@@std@@@std@@0_K@Z
.?AV<lambda_91a43e75998e3a49d1905d29f4a7bae4>@@
.?AU?$Exp@N@functors@onnxruntime@@
.?AU?$Floor@M@functors@onnxruntime@@
.?AU?$Abs@E@functors@onnxruntime@@
.?AU?$Abs@N@functors@onnxruntime@@
.?AU?$Neg@H@functors@onnxruntime@@
.?AV<lambda_23a1bb0478ab7117c6dff4712434f02b>@@
.?AU?$Abs@I@functors@onnxruntime@@
.?AU?$Abs@_J@functors@onnxruntime@@
.?AU?$Sqrt@N@functors@onnxruntime@@
.?AU?$Reciprocal@N@functors@onnxruntime@@
.?AU?$Reciprocal@M@functors@onnxruntime@@
.?AU?$Log@N@functors@onnxruntime@@
.?AU?$Abs@H@functors@onnxruntime@@
.?AU?$Exp@M@functors@onnxruntime@@
.?AU?$Neg@_J@functors@onnxruntime@@
.?AV<lambda_95dd41682914710e4e8d31623057e7d3>@@
.?AU?$Neg@N@functors@onnxruntime@@
.?AU?$Abs@G@functors@onnxruntime@@
.?AU?$Sqrt@M@functors@onnxruntime@@
.?AU?$Neg@M@functors@onnxruntime@@
.?AU?$Abs@C@functors@onnxruntime@@
.?AV<lambda_13046178b359e9c81742cfc406e97a28>@@
.?AV<lambda_acccfeb1e0041e26bf01c6097464f8c5>@@
.?AU?$Ceil@M@functors@onnxruntime@@
.?AU?$Abs@F@functors@onnxruntime@@
.?AU?$Log@M@functors@onnxruntime@@
.?AU?$Abs@_K@functors@onnxruntime@@
.?AU?$Neg@C@functors@onnxruntime@@
.?AU?$Abs@M@functors@onnxruntime@@
.P6AMMMM@Z
.?AV<lambda_0aeb3b3202e2e34a7d6b98b623e6a327>@@
.?AV<lambda_b765106161737954a77d8a171216f44e>@@
.?AU?$Selu@M@functors@onnxruntime@@
.?AU?$Relu@M@functors@onnxruntime@@
.?AU?$Sigmoid@M@functors@onnxruntime@@
.?AU?$HardSigmoid@M@functors@onnxruntime@@
.?AU?$Elu@M@functors@onnxruntime@@
.?AU?$Celu@M@functors@onnxruntime@@
.?AU?$Softplus@M@functors@onnxruntime@@
.?AU?$Sigmoid@N@functors@onnxruntime@@
.?AU?$ThresholdedRelu@M@functors@onnxruntime@@
.?AU?$Softsign@M@functors@onnxruntime@@
.?AU?$Relu@N@functors@onnxruntime@@
.?AU?$Relu@C@functors@onnxruntime@@
.?AU?$Tanh@M@functors@onnxruntime@@
.?AU?$LeakyRelu@M@functors@onnxruntime@@
.?AU?$Relu@H@functors@onnxruntime@@
.?AU?$Tanh@N@functors@onnxruntime@@
.?AV<lambda_4d5e8f52539d7b1512da825129736cac>@@
.?AV<lambda_5e5c9b62498f5c77e97a236dcd93889a>@@
.?AV<lambda_63f3e31790fe0f5095d7914472878fdf>@@
.?AV<lambda_2a220a526da965cfd243a774de182b4f>@@
.?AV<lambda_233c5a0f74ca58f834bb6474e700daf3>@@
.?AV<lambda_d76be7c6120a299959b7772916785631>@@
.?AV<lambda_f80caf07313f1968c393099405f1b983>@@
.?AV<lambda_bc9f77b95219f5c687a91541f1ceb31c>@@
.?AV<lambda_a0141ca4a0ba6481b8b376c6bbe9398a>@@
.?AV<lambda_f1cb7db0116015e247debfb13b06187d>@@
.?AV<lambda_8e20fcb5fdbadf86ea27e51f9059dbee>@@
.?AV<lambda_6d1012bc8b744ad9b0ac1bd49c915596>@@
.?AV<lambda_04a053d9287fc146500ea7158f178603>@@
.?AV<lambda_f0e713d705dc904f2cc82277f166b6ca>@@
.?AV<lambda_a78c243175fca46a2a1734032bceca35>@@
.?AV<lambda_0736746d817c31c51fce207b52ada565>@@
.?AV<lambda_05564eb259071976ee09610f192b6712>@@
.?AV<lambda_59d8acbb35668d76553fcdb87d75be9b>@@
.?AV<lambda_9f3412b7c8b5e0fcfb6de75e19c2b20a>@@
.?AV<lambda_eb610ec4c1ba2ee2f371c8f8541bff95>@@
.?AV<lambda_e208f59e21e2ff58477196ea2ca2b8a9>@@
.?AV<lambda_c82e3cf5ea414565c5fa19321839dc2c>@@
.?AV<lambda_48813fe4537d3b115d868f99a1d25e0d>@@
.?AV<lambda_35b4ad7503a02f4a81944df838e5a579>@@
.?AV<lambda_305677983a5a9935addb16bf86321e8d>@@
.?AV<lambda_f2eae863f75038e08c50232c3c3d8f28>@@
.?AV<lambda_d8eff5d1bd9e672f484ebde0931bf7c8>@@
.?AV<lambda_27ef1fccba899a27fb3728ede329f8f9>@@
.?AV<lambda_2fd31ca8bad4e5f142a2b5dfaff21801>@@
.?AV<lambda_82a82efebe07e5ae66c1dba7f7b1d0eb>@@
.?AV<lambda_6e1c302e281819ac6b1fb7e30f673d0c>@@
.?AV<lambda_1e22b51977a7614afcc7ec75b14da756>@@
.?AV<lambda_8da16acda159d2aa36f9e917947c501e>@@
.?AV<lambda_be7339191ce398a59cd58925b90c786f>@@
.?AV<lambda_941839df63e63f7ba66293e51c31a750>@@
.?AV<lambda_cee3b4dd214c58e3bda89cd1d17f2a33>@@
.?AV<lambda_3737cccf52f52050f818e2f26f25ffea>@@
.?AV<lambda_9e7e87298c6741b4ec8ed0a6456adc35>@@
.?AV<lambda_d902441fd504c26b65d8c745d975f617>@@
.?AV<lambda_71c3b665db06d94a27cb005b95351a30>@@
.?AV<lambda_7ad2abdb9d04286d775eb1928c7b7267>@@
.?AV<lambda_633219a9b94f8dda3496215bc66b7f88>@@
.?AV<lambda_f93cfe7b83a07df5e5ec33ddaf08e7ff>@@
.?AV<lambda_55c350adfde8ac6bd9a00204b24ba96a>@@
.?AV<lambda_1ab7060bf709e6890ff53a0d80bb8cf5>@@
.?AV<lambda_5e986d8d54ac11e9537c3be07a635569>@@
.?AV<lambda_349643858d539033044d1d649dec237b>@@
.?AV<lambda_233e1075753dae9e4f22e739be4a8921>@@
.?AV<lambda_0eb1c82669a9a9098341056a7159769f>@@
.?AV<lambda_18f6d57bac34239d650ddbef707551e4>@@
.?AV<lambda_71a7fcf63fe63e7115dc2b32c5b3bde5>@@
.?AV<lambda_c23a6e376db7c7388aa04e9eb8ee3aca>@@
.?AV<lambda_de0690e164792b53d65bc1c1d051a1bb>@@
.?AV<lambda_3f7b31cb30bf909ba725ac6373c24d17>@@
.?AV<lambda_ea5f658f0d872dbd3ae3afb1f1fd1122>@@
.?AV<lambda_0eb6d36a1e572b96903c06e9975f6aa3>@@
.?AV<lambda_2e098a1c981ad02f01e4203abb39ccc3>@@
.?AV<lambda_ea638a800acf5ef947d9895c94c04562>@@
.?AV<lambda_a983769408c484edae48835df1b06ffa>@@
.?AV<lambda_6be077fd6f7f433aafd1dffeee52eb1a>@@
.?AV<lambda_c6a56551a85732582804098ed80c9fb0>@@
.?AV<lambda_c85c3af868013f6aa9c56e9cdc2b9874>@@
.?AV<lambda_6a4c61f2b4befdad04c4488376ee8ba7>@@
.?AV<lambda_55516238922eb666def71419d606fbee>@@
.?AV<lambda_38b12892f84528e9b72f8a8f101b77ee>@@
.?AV<lambda_2d5080a6f5d70d1027512848632ba44f>@@
.?AV<lambda_2422fe451725e3fbdcc33016dd64f43f>@@
.?AV<lambda_8e138b7d90664b9b073b07f3b9a88877>@@
.?AV<lambda_f711efc9e33c8814de65d5fed61d7d7b>@@
.?AV<lambda_e62e49b67792d3da4dce72eb9a07615b>@@
.?AV<lambda_45cc7b6c8716d6954bcc2af58f76cf68>@@
.?AV<lambda_86617ae40692259734b57fa8efa0566a>@@
.?AV<lambda_3569b04d379d95c345dffc147f7b94dd>@@
.?AV<lambda_d36243265aa636655717da0233970d95>@@
.?AV<lambda_7d6518c90c18530c7c0fbad83575bf00>@@
.?AV<lambda_265c7b3426b87aef377811f8829e6fd7>@@
.?AV<lambda_b51cf357ad6acfdf19263dcdcc3d9e57>@@
.?AV<lambda_da44d2eb11bc9a9612e675a392ec65da>@@
.?AV<lambda_2e36b47c0b35a1b78ab3e2daaa7c5208>@@
.?AV<lambda_007f5166efe630b2b34ea6bece4c38ea>@@
.?AV<lambda_e11d46aec76c9403409fa1a1f5912552>@@
.?AV<lambda_cdd7645d3312922b8a49e2d3407b04a1>@@
.?AV<lambda_05232ca3edd0818f9993e6f6f14300f3>@@
.?AV<lambda_f418dafe297dbbe0e3ada3e20a715a40>@@
.?AV<lambda_0385abfaef506cac5165d4d2b4c7ecc3>@@
.?AV<lambda_886ae0b3214e69bc84deedbcd54da95c>@@
.?AV<lambda_7ead2e94d57d06076dc8b6a379103042>@@
.?AV<lambda_d28af58e0ad2f8f0bc7e5ddeac947360>@@
.?AV<lambda_e576f81a67738395c7b2b5774df9a9e6>@@
.?AV<lambda_a5fbe571a251fb604ad2b480580cc76b>@@
.?AV<lambda_6dcde8f3935fcb410660f26c17172b9a>@@
.?AV<lambda_67b6bf510365b722acc4b67b8d30f927>@@
.?AV<lambda_b5274db9239c093417e22b13869d2ad7>@@
.?AV<lambda_8c661b8920511802b1aec6c4164a8422>@@
.?AV<lambda_399e4ac9a8be74e8d14d0d5e67b02a32>@@
.?AV<lambda_4888e52cfaab8cdad25ecea017b54d57>@@
.?AV<lambda_f4c664674d3291639f6f1c524accc00e>@@
.?AV<lambda_74a481eac2e8d64c2bfb5722b5b1cb89>@@
.?AV<lambda_626d338c2f6a43327586cf0d89586aa1>@@
.?AV<lambda_29a27f66e5d65fb36ed8965e3fa92ded>@@
.?AV<lambda_d8b96520499b24df75ab37992af46b4d>@@
.?AV<lambda_8e2d345faf08aedb7570dd988fdca755>@@
.?AV<lambda_ae0d8b5a2da4af67276477f8d17abe42>@@
.?AV<lambda_f273d4d4788dd6d522544c26d76a5ae4>@@
.?AV<lambda_06471b7954288b9dda9f9d99a72bb7c6>@@
.?AV<lambda_6744a0008aca8ce8a0b29db8f1c01fa1>@@
.?AV<lambda_eb887415aed41a3927a754246db16d35>@@
.?AV<lambda_9dc7af5c7bcff785bd9190f2cb464b36>@@
.?AV<lambda_ef9ce4d0dc32ab06ddd4426fedb787f1>@@
.?AV<lambda_9a0964aea19bb920233e6fe5b8fa95e2>@@
.?AV<lambda_c883cb8d41543031ae666c752948f20d>@@
.?AV<lambda_b9d9aa3891683438224b1c5285b28a19>@@
.?AV<lambda_9162731c94ecaf479f99be17541de90c>@@
.?AV<lambda_414b2f36cd6ead6cc1f643a6940e8f0c>@@
.?AV<lambda_11e4c67999de7fde8000f344864dbedb>@@
.?AV<lambda_4b6703e7c30870568a65367efbb3ac2f>@@
.?AV<lambda_4b2e976425c8109a05bca3d80983d4bb>@@
.?AV<lambda_ff1ccdcb7441249452b8640b2e9dfe31>@@
.?AV<lambda_2db246e2ce365efa8cc2cf5d40338a67>@@
.?AU?$Powx@M@functors@onnxruntime@@
.?AV<lambda_3423f833df7359f6a2b95a8c9a7b5302>@@
.?AV<lambda_e7ad5c511f7f0ca88484948ab82410f1>@@
.?AV<lambda_bbed65651851398ae9e91eb41072a5f9>@@
.?AV<lambda_4bd02c7b372889e26c922fb97a0c9ac3>@@
.?AV<lambda_1f8675d15ee5efaa8f5579945c6d118a>@@
.?AV<lambda_30e1101d7025e2d839d1daa9ac434cd1>@@
.?AV<lambda_3f0ef66d0b0de67b55e854d697da2fff>@@
.?AV<lambda_310ceee0e4c58a7a83d7d00319ed9410>@@
.?AV<lambda_b9ad0ccbf56c00a46342640cba7cdc2c>@@
.?AV<lambda_e1a7abf114b955966097bd83c14de705>@@
.?AV<lambda_e08a75ecff8a2a27b308be3f8dfddc66>@@
.?AV<lambda_f30a6117225d3cb8e4a11f826263726e>@@
.?AV<lambda_1fd04ff7ed408ca4ebed6e8107f19670>@@
.?AV<lambda_115059020db09aa13bab825ad518034e>@@
.?AV<lambda_33215563a6aacb86deff746a6aa28a6c>@@
.?AV<lambda_e77a57374f1e055f8281ce5dd65e8ebd>@@
.?AV<lambda_d0c5713ac46ee06d27e6324a463b7bee>@@
.?AV<lambda_fd4f2bbf00caf507fea1cc6053ab984e>@@
.?AV<lambda_9800a6d45ddc69dc425addf83da8e9e3>@@
.?AV<lambda_6e3cb65d29853e95007cf81c805209f2>@@
.?AV<lambda_d48cb9baeb37d51a45fff2e006fd2ed3>@@
.?AV<lambda_5820733bf712edf4856ad2454498e68c>@@
.?AV<lambda_3b97251de202434e86d007c33f9345a1>@@
.?AV<lambda_c646dd14edaacae64487e0b96ad575f5>@@
.?AV<lambda_6c91c6f34bd8327dc22f6b7f8735fd5b>@@
.?AV<lambda_0251dfdaad61cfa5b2a46c99b78d27e8>@@
.?AV<lambda_42580b4f3e49f7dcfbf37d76233bd856>@@
.?AV<lambda_786b2ff03846fcfa21d00bf234deaff2>@@
.P6A?AVStatus@common@onnxruntime@@AEBVTensor@2@AEAV32@PEAX@Z
.P6A?AVStatus@common@onnxruntime@@PEB_J0PEA_J_K222222PEAVThreadPool@concurrency@2@PEAX@Z
.P6A?AV?$unique_ptr@VTensor@onnxruntime@@U?$default_delete@VTensor@onnxruntime@@@std@@@std@@AEBVTensor@onnxruntime@@V?$span@$$CB_J@gsl@@_NV?$shared_ptr@VIAllocator@onnxruntime@@@1@PEBVTensorShape@3@PEAVThreadPool@concurrency@3@PEAX@Z
.P6A?AV?$unique_ptr@VTensor@onnxruntime@@U?$default_delete@VTensor@onnxruntime@@@std@@@std@@AEBVTensor@onnxruntime@@_J1V?$shared_ptr@VIAllocator@onnxruntime@@@1@PEAX@Z
.P6A?AVStatus@common@onnxruntime@@PEBM0PEAM_K222222PEAVThreadPool@concurrency@2@PEAX@Z
.P6A?AVStatus@common@onnxruntime@@PEBN0PEAN_K222222PEAVThreadPool@concurrency@2@PEAX@Z
.P6A?AVStatus@common@onnxruntime@@PEBH0PEAH_K222222PEAVThreadPool@concurrency@2@PEAX@Z
.P6A?AVStatus@common@onnxruntime@@AEBV?$vector@_KV?$allocator@_K@std@@@std@@AEBVTensor@2@AEAV52@PEBVTensorShape@2@PEAX@Z
.?AU?$MaxPool3DTask@C@onnxruntime@@
.?AU?$MaxPool1DTask@C@onnxruntime@@
.?AU?$MaxPool2DTask@N@onnxruntime@@
.?AU?$MaxPool3DTask@E@onnxruntime@@
.?AU?$MaxPool1DTask@M@onnxruntime@@
.?AU?$Pool3DTask@MVLpPool@onnxruntime@@@onnxruntime@@
.?AU?$MaxPool2DTask@M@onnxruntime@@
.?AU?$MaxPool3DTask@M@onnxruntime@@
.?AU?$MaxPool2DTask@E@onnxruntime@@
.?AU?$MaxPool1DTask@E@onnxruntime@@
.?AU?$MaxPool3DTask@N@onnxruntime@@
.?AU?$Pool2DTask@MVLpPool@onnxruntime@@@onnxruntime@@
.?AU?$MaxPool2DTask@C@onnxruntime@@
.?AU?$MaxPool1DTask@N@onnxruntime@@
.?AU?$Pool1DTask@MVLpPool@onnxruntime@@@onnxruntime@@
.?AV<lambda_b2b5b00a4ad3647a9a40b4f0dbb0745d>@@
.?AV<lambda_a1b033c1238f74f04d136673bf3141a7>@@
.?AV<lambda_33ab9233a6e2a6fe6c24d1963cbc3111>@@
.?AV<lambda_c6728963f8bb91595df67d14c61fd8fa>@@
.?AV<lambda_f8dbfaed9d55440bb15cbf009ac934a6>@@
.?AV<lambda_f55fed088cc6887bf534564a0809769f>@@
.?AV<lambda_038eb253b7f4e93118566e43dda81530>@@
.?AV<lambda_88cacdca8b660ada94d8f522e9f11bdd>@@
.?AV<lambda_8f0d91faf2fd476a9a61179d9b72837d>@@
.?AV<lambda_1cd5cfe6b76bb3ba1c53203d58a4eedd>@@
.?AV<lambda_7b13e0aa9183a7266252ed7dbdd54f99>@@
.?AV<lambda_cc7060560dae87f5475ce357b0faca8e>@@
.?AV<lambda_a32f11f3f6edc3a900d9d34015d67705>@@
.?AV<lambda_88f211d79a091f6a481688e49635f453>@@
.?AV<lambda_11002e821957b929b79aa649ecf0a381>@@
.?AV<lambda_ecc9b9120eb4793427a94169f5623a5b>@@
.?AV<lambda_8ffce33af8696a7a42fa073f6875aacb>@@
.?AV<lambda_2a3f9f19c48a100998729328075237dc>@@
.?AV<lambda_f2842527842316c8394eef731b560fea>@@
.?AV<lambda_1221b4097effa32a8dd388b4455dd2c1>@@
.?AV<lambda_72e83488395612e0f6a2602b726a187c>@@
.?AV<lambda_51f7b34ad2d14ff9bfb03559ccadc81f>@@
.?AV<lambda_8205a59784c4f81abae5636754370a87>@@
.?AV<lambda_03e7406b29b220fa131a6585ce48dcd2>@@
.P6A?AV?$OrtValueTensorSlicer@$$CBUOrtValue@@@onnxruntime@@AEBUOrtValue@@_J1@Z
.P6A?AV?$OrtValueTensorSlicer@UOrtValue@@@onnxruntime@@AEAUOrtValue@@_J1@Z
.?AV<lambda_0f619f3ff08e9e7659a763308b991cbe>@@
.?AV<lambda_138b12d2a19d941ed2e5665ad50fc6cb>@@
.?AV<lambda_86dd40270c718f299cd5113909653b31>@@
.?AV<lambda_d05fc65af5c668ab358a5dd1d3066029>@@
.?AV<lambda_7e025ee819bdcb60549ba1616bfeecb8>@@
.?AV<lambda_adf0943a801eb39c6b17ffecdea2343f>@@
.?AVrange_error@std@@
.?AV<lambda_aadcf5996f45fb531b26c6d69998fa55>@@
.?AV<lambda_88a6c98d89234a715cc9be63a4bfcb6b>@@
.?AV<lambda_29ab6b1128939dd8bf0359853abfeea8>@@
.?AV<lambda_77a9c78e94cd40cde2d9c67f51886975>@@
.?AV<lambda_46ed12fdb2868a9b6a9a32861936f36d>@@
.?AV<lambda_4f73d17b3441bd9f7aa60fe28450311a>@@
.?AV<lambda_05c8e068d08f49052da87d86b6e09131>@@
.?AV<lambda_e1f234778fb80305bf70a017789747a5>@@
.?AV<lambda_1418b315018aac3af0babe6c70e31132>@@
.?AV<lambda_69aea4466e4a2bbb44bce5c61e2a37d2>@@
.?AV<lambda_18a3b6757625880e23566c26c3f789bc>@@
.?AV<lambda_4d2bb8b4b91e7c65b08948d705bb6118>@@
.?AV<lambda_bb6f1fa388d8f370137cb6c6519f955b>@@
.?AV<lambda_6c3806e2f5558f0233ccf8d05c3ab49e>@@
.?AV<lambda_0c6b04fd5867b05b63eee5705dade40a>@@
.?AV<lambda_c9d64b63a4c10e55647e3707de8acb40>@@
.?AV<lambda_066c50938341a8bc4180c44723e9e561>@@
.?AV<lambda_930b54dfc8a64c42da270936f80e56b0>@@
.?AV<lambda_5275443c3995dec7145dee969221190f>@@
.?AV<lambda_881846c2025fe9fdf21e10df8ef1aed7>@@
.?AV<lambda_a38954a3fc0168033639a5d3db4af77f>@@
.?AV<lambda_54c52b5102f405f26b86736abc33d9dc>@@
.?AV<lambda_f79206f00cb74e35e11935d1c0ff9449>@@
.?AV<lambda_2dc68f98e6942dc4cba921733a4cc521>@@
.?AV<lambda_b346f4e5faa3c794cd68f36eadcb5c6c>@@
.?AV<lambda_39a308a33350cfe399318acb8bdc0850>@@
.?AV<lambda_c273bcba413eb077d5fb961b15132175>@@
.?AV<lambda_b31abea9b0adc93fa794390a998569d7>@@
.?AV<lambda_8d89ab19f4dfa0ecffb29771863af791>@@
.?AV<lambda_42085d5d1ab621f3e103058b343368b0>@@
.?AV<lambda_f5db0169896386e7220581461257fa02>@@
.?AV<lambda_1f1cad28f5d50542c82767124ccbbeb8>@@
.?AV<lambda_e021ada2350e9ccd28f4a55f38144d36>@@
.?AV<lambda_1038cf6b5b3a06f7836c03470cc73cfe>@@
.?AV<lambda_f60a4fa32157d6fa8e2622fa2d06d23c>@@
.?AV<lambda_8b24a914bf112352c9369f2b44e2ccd1>@@
.?AV<lambda_f0421a9025619b3b88dddf09f010b1ec>@@
.?AV<lambda_f8241a17ccaf97c109b664362fd083d5>@@
.?AV<lambda_099f8dd4d33b6b91bc0ee51d86ed6c1a>@@
.?AV<lambda_b4e1f5c6fdfb97123733fced30ecae42>@@
.?AV<lambda_7adac37e31393d4ac6b5bca719f468e8>@@
.?AV<lambda_ecee2654318c52376f803bc94d6bc4ee>@@
.?AV<lambda_3928e725003c7a62c76f41f34cee3234>@@
.?AV<lambda_f90b302c32ce6f81af3f56ee1f3a9b51>@@
.?AV<lambda_8711802b90051c34567904599fdbb25a>@@
.?AV<lambda_0e9a1a7b6965f6b0c97b479e18664de1>@@
.?AV<lambda_0311f58d27b0724e7aff1c8a092d73e4>@@
.?AV<lambda_2cc7c01007d8f546fc79115b36c9098b>@@
.?AV<lambda_beb991e0fd9fa3631b6b17fe3b5d669c>@@
.?AV<lambda_7d49a6264e4dd01effe57235ddc8a797>@@
.?AV<lambda_0b3e445014abdc00ff4fcef82b0ddb16>@@
.?AV<lambda_8470282d05918db64db57459ab19a88b>@@
.?AV<lambda_adbb0ed82c3e353a2ef8b51f4773ed3d>@@
.?AV<lambda_5d23ede476722136670b473ee395a3f1>@@
.?AV<lambda_0920e8c792b09e0b2fbd0035321589a5>@@
.?AV<lambda_825b24178740a83899599d9a15262ad0>@@
.?AV<lambda_670b81b0a20483d6e6a649cadb77d57d>@@
.?AV<lambda_9ba9dde4e941db1311dbefb477d19165>@@
.?AV<lambda_46a0ec34ba455071cbedc5983343a291>@@
.?AV<lambda_9ea68d0a3164c0f7bc2786cdbb58d687>@@
.?AV<lambda_929181c452e7cbade815a0403dd15e2f>@@
.?AV<lambda_fd2f32690f4c94955691e93dd0de8250>@@
.?AV<lambda_3f6b039d1c2a9ddc0c4edd40a4875e7e>@@
.?AV<lambda_c327e8b0febe30d927a0cd8dc39dba92>@@
.?AV<lambda_d16d0ed0a77c40b384d41d6d45ea4e55>@@
.?AV<lambda_fa9cea3a5917d32ed85305ced3e8f02e>@@
.?AV<lambda_92ce44a6835ab59773e2e1a9e5082039>@@
.?AV<lambda_e8dabe6ee14f0907c4413646f291102d>@@
.?AV<lambda_37fc51724b8ab138f4abea1c86c474be>@@
.?AV<lambda_42fa4ee951b97f0e77b9de9f38db0750>@@
.?AV<lambda_7ff2aca09375d1b200a4b303c393fea9>@@
.?AV<lambda_63db87e2da8ecf77c1c67b6ddf9097c0>@@
.?AV<lambda_8baa7635c058d848d4bb5868b5a1e31c>@@
.?AV<lambda_41ebeab5da8beef13ef5472118318ec8>@@
.?AV<lambda_464b0c569b5e14ad1c13cf083ce55247>@@
.?AV<lambda_4cb6536179ac33ff59d8db68f89e6d48>@@
.?AV<lambda_1aa88cfbcf551c26b6ab6d047872e0e0>@@
.?AV<lambda_745ebee95180d242f8918f6ee05b1702>@@
.?AV<lambda_5a4a1a47e435bf53505cb70a52e252af>@@
.?AV<lambda_dccf783802d22b3c48245e980c348c0b>@@
.?AV<lambda_d846c7c2d29b00ef66091f3bac323c48>@@
.?AV<lambda_2a0432e2f861fdcc4b7f759b0200b6a4>@@
.?AV<lambda_f29d64af0687bfdd6e3496c166444014>@@
.?AV<lambda_ca9d22ac690a049ddaf8ded26ec4c6e4>@@
.?AV<lambda_df8f09957380fda37bd75205e74ab017>@@
.?AV<lambda_8af3a30942023f4a3511597660fac0f0>@@
.?AV<lambda_97f1aa20e78dc8680c319e9629ef8e8e>@@
.?AV<lambda_9d98759a86a1a6c70a5f6713a8d099cc>@@
.?AV<lambda_959b8361497e19898ca852a4058b29b4>@@
.?AV<lambda_796aa6c99df67f823c02a52131c10530>@@
.?AV<lambda_6da49eb0f8069eb4bbc177d850fb6dbc>@@
.?AV<lambda_cfa93eededd189da5db0d1b9a54368f6>@@
.?AV<lambda_c24ed35691635f968e92d6373ba271aa>@@
.?AV<lambda_166f6feb25832b61746c79ccdee9d41a>@@
.?AV<lambda_7adcd22a7346287699992d30bc8e4d12>@@
.?AV<lambda_c162c83b8e727b7911e302875a688c52>@@
.?AV<lambda_087f7d76293e9eae88f0507abf2d5be6>@@
.?AV<lambda_fd07f717d8be7a0b4ac28d8863ca11e3>@@
.?AV<lambda_7fb11eaf1ed7921e4b12c72edf5431c8>@@
.?AV<lambda_c89cf3298587f6daa5b2556ad200a769>@@
.?AV<lambda_8052aa8f31d91e8553d059cc8a579374>@@
.?AV<lambda_d843aae243ab497a4cddae9c413aa41d>@@
.?AV<lambda_97d1f16f53a6cde81773c5f27deb1fd2>@@
.?AV<lambda_87d1adad0ec3c9a329fb0c8dabf3ed17>@@
.?AV<lambda_099bb0573d1376fbed683cceb180bb81>@@
.?AV<lambda_96fa572628c59e4d078744e93c0f7196>@@
.?AV<lambda_ac55c4ccc442f0bccdc9fc717158ade1>@@
.?AV<lambda_a47ee9eece330f845238b622e01ff20a>@@
.?AV<lambda_cf0d99f02d80c9fcb9d4fc5032f5b198>@@
.?AV<lambda_99b6e836d69b69305b62d8ff2093a102>@@
.?AV<lambda_bd3b717d06eb8cb8c6cdeb6f2f75dbb9>@@
.?AV<lambda_8859cd3c08b6a8e9d40267fbf61322d1>@@
.?AV<lambda_f4b2ad37b32ce448782bd96a2cfadb78>@@
.?AV<lambda_b0c700b83742c00837801ebad88797d0>@@
.?AV<lambda_35df272685e40c7e2ea40583b6906dbe>@@
.?AV<lambda_83fc98e77223d87948a3045c6a6d1977>@@
.?AV<lambda_2da95d9a6f78440b51d048ed6c8e3561>@@
.?AV<lambda_36dd25e706f1ff93211e6de0438e06c6>@@
.?AV<lambda_e4de1e6d15c53abac777de14725b2d0a>@@
.?AV<lambda_79f39293405ab41c7ab7de2e65eb25d5>@@
.?AV<lambda_50ac1bbabd41636fda9e1bf36b1fc455>@@
.?AV<lambda_65379b8fe5c982ca5593c72a9026c7e8>@@
.?AV<lambda_95aae9bbb822c713ca9677b3a07b3381>@@
.?AV<lambda_e4fd1e1090be16100e1685f2f4877667>@@
.?AV<lambda_ee954de93bf759bd8b1f4b4c9716ded3>@@
.?AV<lambda_337593ad4f874bae98bb3aa9ea22758d>@@
.?AU?$ParametricSoftplus@M@functors@onnxruntime@@
.?AV<lambda_95c74ffd32a4d994a62a191daf0fff27>@@
.?AU?$ScaledTanh@M@functors@onnxruntime@@
.?AV<lambda_724cddc9d8a43266aca46c0c40c5f2b8>@@
.?AV<lambda_885bb33b10cbcb0918c52bed48f8fb4d>@@
.?AV<lambda_4092000fa2c851eb2ddefc1b6efbf1fc>@@
.?AV<lambda_d44234032f9da3a8e659111375d4b0b2>@@
.?AV<lambda_d2a63565f05cd53c8bbd857da251fdac>@@
.?AU?$MaxpoolWithMask1DTask@M@contrib@onnxruntime@@
.?AU?$MaxpoolWithMask3DTask@M@contrib@onnxruntime@@
.?AU?$MaxpoolWithMask2DTask@M@contrib@onnxruntime@@
.?AV<lambda_25a8a9072228ef7a32b8c1b14c49de58>@@
.?AV<lambda_fb6fdddab5054d64688b53a561efd684>@@
.?AV<lambda_25395042ec336ff6c2d2bf12e6e33481>@@
.?AV<lambda_cfed9ddc735eea0f8e4086ea879d678b>@@
.?AV<lambda_70c2e06a8338c9df9aac1ab5670853c3>@@
.?AV<lambda_906a138de2e03121b208fb0df261e1f8>@@
.?AV<lambda_fc16ad7b44878e0662f1cac062e6f66b>@@
.?AV<lambda_32373635de60f77359e703c4d10576a4>@@
.?AV<lambda_bbef96c8b3d94b60dcc7c27fac153a49>@@
.?AV<lambda_e4e123e3dee0f37ab953d7fa4e244426>@@
.?AV<lambda_1c5d7bebb1a95dc207aab085d9327beb>@@
.?AV<lambda_f53ebd8978e979af788fa5865dbc2cd1>@@
.?AV<lambda_5098c7ffb45473bd83c761c41e1ce9c0>@@
.?AV<lambda_dd5302d9bd2c63a8d4a2f6d1a555d2e8>@@
.?AV<lambda_8fae3093696facd3fe41e8e37858828d>@@
.?AV<lambda_453060ceb0954e510c893eb1eaf9ed24>@@
.?AV<lambda_3f4423c10a0d6dd8a4ce69efd4c952cf>@@
.?AV<lambda_930475321813e9ecf1d2127c957da4f8>@@
.?AV<lambda_0e262c4f5599939b5b120509b5d5183b>@@
.?AV<lambda_05f437c6028bdce22a59a3bca2849370>@@
.?AV<lambda_a6a543117948c3dcb0421140d52311d6>@@
.?AV<lambda_e8839f5b62d1db724c93bfffb9ea115d>@@
.?AU?$QLinearPool3DTask@EVAveragePool@onnxruntime@@@contrib@onnxruntime@@
.?AV<lambda_8237b7c0f78776c2c7d6d683d4112edc>@@
.?AU?$QLinearPoolNhwc1DTask@EVAveragePool@onnxruntime@@@contrib@onnxruntime@@
.?AU?$QLinearPool2DTask@EVAveragePool@onnxruntime@@@contrib@onnxruntime@@
.?AU?$QLinearPool1DTask@EVAveragePool@onnxruntime@@@contrib@onnxruntime@@
.?AU?$QLinearPoolNhwc3DTask@EVAveragePool@onnxruntime@@@contrib@onnxruntime@@
.?AU?$QLinearPoolNhwc2DTask@EVAveragePool@onnxruntime@@@contrib@onnxruntime@@
.?AV<lambda_43ae18272140dc808ee2ad7756791834>@@
.?AV<lambda_7e035611e18f01e9b8525a9410df245f>@@
.?AV<lambda_626154954c9dc0ff299a2c8aed4cd9a3>@@
.?AV<lambda_d66f152e4f615234b99e6bc843b096a4>@@
.?AV<lambda_ef1755d3dc9c2a6da40e08422cf4f664>@@
.?AV<lambda_c5a2356ebe34f9c7c2244082de94d017>@@
.?AV<lambda_0f2ad18a551e793a8bddf1de1dc21689>@@
.?AV<lambda_3410038a349fbad62fb0cd42ca2fc595>@@
.?AV<lambda_9cf24a11d7a47ec83a2235f766fd2a5c>@@
.?AV<lambda_b8bff811421756f68bf0701ba18268e9>@@
.?AV<lambda_7d68ba7556ef690147168e00d959e2c9>@@
.?AV<lambda_d8a9ac16dd775f4bb1f670ae469711dc>@@
.?AV<lambda_d63de458c7355252fdf7dac6af546a6c>@@
.?AV<lambda_21ca116bba4007835cdddfee631b7f69>@@
.?AV<lambda_535c714f3912f6e19812f86aced1c234>@@
.?AV<lambda_d77ac61f6238a68ec5589d982d5615ac>@@
.?AV<lambda_f7fefea6bfb8182e95c707a77e7f4a8f>@@
.?AV<lambda_e1a8783767138f2900763c385a76ace5>@@
.?AU?$default_delete@VIAllocator@onnxruntime@@@std@@
.?AU?$default_delete@VBFCArena@onnxruntime@@@std@@
.?AV<lambda_10d9e40dd67492c4fb3c3f078abc9bd8>@@
.?AV<lambda_f8822893c953bd42037266e93f73744e>@@
.?AV<lambda_0443539308e5b1117b55e6de227ed40c>@@
.?AV<lambda_46f2340c6974a9abf9bf427b63a5b9bc>@@
.?AV<lambda_31e4b4a2d49d229e0e58bbade49550ff>@@
.?AV<lambda_c21643d1fb44758b12325da46a2d32c5>@@
.?AV<lambda_d5d62f4b858abcded92348c54a906570>@@
.?AV<lambda_ca37d5ec199e35c30b080471c345d05e>@@
.?AV<lambda_957302132dd7fea31b11af321304eee3>@@
.?AV<lambda_e256c24a178e1d4e49acb7fa7090a7cb>@@
.?AV<lambda_2369f292b0f42fbecffef8359e85ebdc>@@
.?AV<lambda_d2e668c941b29e0c58674cef7fa23f2c>@@
.?AV<lambda_c8a671616559eac778e178004f6bff38>@@
.?AV<lambda_9de7ed3747c068fd694551112d802911>@@
.?AV<lambda_76259788010337ce84c7523a6ac9ea2f>@@
.?AV<lambda_f3e7e1c16a77783b35cd48a7c1464d44>@@
.?AV<lambda_5f4d12f5625f87c7fb1ce2d7795c65fa>@@
.?AV<lambda_6f564a616944b22adda71fa903488618>@@
.?AV<lambda_56112085461402060b9fcd1813ada6f0>@@
.P6A?AVStatus@common@onnxruntime@@AEBVNode@2@AEAVGraph@2@AEBV?$vector@PEBVTypeProto@onnx@@V?$allocator@PEBVTypeProto@onnx@@@std@@@std@@AEAV56@AEBUResolveOptions@42@@Z
.?AV<lambda_59a791f3e63b70edef38fb68dcebe7ec>@@
.?AV<lambda_e318b9a1ac6011cec45976779008bc1d>@@
.?AV<lambda_c007df17e8115f08c5c6e2ce7e2201c0>@@
.?AV<lambda_a047e4fa7a019da5da5dcb5093bd2da9>@@
.?AUNodeCompare@onnxruntime@@
.?AUPriorityNodeCompare@onnxruntime@@
.?AV<lambda_0457fc55cb15f22bdcba75f54e0aa1c1>@@
.?AV<lambda_0a3d98e003a8310444f50c395f4ee870>@@
.?AV<lambda_9a2786289a4d18fa33a8facc5fc736fa>@@
.?AV<lambda_76f850960fe6ac327acd4613d41835f7>@@
.?AV<lambda_8d52f2d53a04afb64fff4765a2f91d21>@@
.?AV<lambda_22485ec429c0920bef998442ed4b0141>@@
.?AV<lambda_245134a5955e4a8621bc7afd20ddbe70>@@
.?AV<lambda_8ac5cfcccd347e512ac9419fd63346cb>@@
.?AV<lambda_2f6217725d36a8fee3233a5a3a8acbc4>@@
.?AV<lambda_d8c2999bab01791ce81720c649701452>@@
.?AV<lambda_26de9b5e95466f00708e3ae83a84c608>@@
.?AV<lambda_423c7f4514cb0f580d17c1969d94ab4d>@@
.?AV<lambda_860b5cff89ff5f292872db2aaa19c52a>@@
.?AV<lambda_076f399d54601fd1059236c6a3813828>@@
.?AV<lambda_a9ad92a14c7791713a13c8f77948b581>@@
.?AV<lambda_9d8771d050eb3774da949f76ff7b2f7a>@@
.?AV<lambda_4ee11195e6ac34d8802828c9de8931d7>@@
.?AV<lambda_e74921842af68f126d6a963647d9b662>@@
.?AV<lambda_d49bbfb4942fd85adb6441f849e70bf8>@@
.?AV<lambda_98f67966096a245ba60cb535f4355420>@@
.?AV<lambda_273270d63d2edeeb9cb50e9432a42c00>@@
.?AV<lambda_1cd6b5a2cebd273f7225f466512c43d5>@@
.?AV<lambda_1c17b461588ff1b22a6f9497bff5b28b>@@
.?AV<lambda_f65b8c2fd23e2320c311133a80f471c4>@@
.?AV<lambda_ec14091c8d829910b5be28a833d6a3dd>@@
.?AV<lambda_07364fb607ea89235f416789a046b92d>@@
.?AV<lambda_cc4bc3c6927d9351454759e8077d0a48>@@
.?AV<lambda_2d30aee68f6d7fc70a227ffb90481678>@@
.?AV<lambda_56c3ac63d8a5c9a2d5ed84f082ef3cf5>@@
.?AV<lambda_e7f381725b47e4a5cf7649db595fbdf3>@@
.?AV<lambda_9a36e00d72a5d737ec77f3dedc2d9072>@@
.?AV<lambda_893ec3becb9cd41d9efbc7eea98d41c9>@@
.?AV<lambda_29460ba72c9a9f61937f695b212c2385>@@
.?AV<lambda_7d629299b17d3241cda0057fa57cac3b>@@
.?AV<lambda_12dd4c4dcfd6c2d8effe6071c823bd43>@@
.?AV<lambda_c09531cdd6b30fb34e9285a274065481>@@
.?AV<lambda_814de60c7177af03b3ddc9e5c36d561a>@@
.?AV<lambda_5c90896dede9afea4c2bfc57d475c2a5>@@
.?AV<lambda_71ef653055b00e102f19330e0f78a252>@@
.?AV<lambda_7134bef299e596690440ce2c3fc8b019>@@
.?AV<lambda_e15d10c4385b9adab7c3b2967076a5a0>@@
.?AV<lambda_104e5d4f3c346a5807a7db11389af990>@@
.?AV<lambda_6a32611970906250c6b47d8292ad00e7>@@
.?AV<lambda_3b79ba1930d9045b1d0a1c3e7b558aa7>@@
.P6A?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBV01@0@Z
.?AV<lambda_7a5892277686c5fc70de8922cd945035>@@
.?AV<lambda_b3cbfd0276cf0ad6f3b58532d60f239f>@@
.?AV<lambda_85740bb65c1a7256d972833e138b25dd>@@
.?AV<lambda_672376956558218714cb97c30f1f09fd>@@
.?AV<lambda_f9fcc66b14829c5756eb43c20bdf984b>@@
.?AV<lambda_14407bcf8cfc66db9a85fd2745a31d94>@@
.?AV<lambda_a8b00e76dfe980214923eafc43926fd4>@@
.?AV<lambda_7361ba811d5ff92bf50e102c73312480>@@
.?AV<lambda_ef5c424a4193a85193eb7ea4c70aae69>@@
.?AV<lambda_77bbe4874254ed84a312b5fa826a83d3>@@
.?AV<lambda_9289ae269610356b68d5c3f46e89bf66>@@
.?AV<lambda_c7733fbf1e7357c5a2b00aac2ede4865>@@
.?AV<lambda_fc59cfd6180e7962f9dbfcf6d01970a3>@@
.?AV<lambda_2ed99b1b7f84cedcec6ea06ce6c01dc5>@@
.?AV<lambda_b58abd66dfb794261458d2caeeba2a75>@@
.?AV<lambda_bcef20d2b13b7b49fdebd823a7e734e1>@@
.?AV<lambda_a7a0686f23e7f9337eaa10f069ee321b>@@
.?AV<lambda_f0be09c6ec8e17e7ac33afa3db53ce78>@@
.P6AXAEAVOpSchema@onnx@@@Z
.?AV<lambda_680b87823779380f32485d5ed8cbcf84>@@
.?AV<lambda_581e172f6e6bf6d53a792d4f2adbb4c0>@@
.?AV<lambda_c2fef1cfbd08e9e21bcb74b985df87e6>@@
.?AV<lambda_08db83bf9ac8665fc4de47cb08668b85>@@
.?AV<lambda_9a122db8c0c6f400c116ad5d2cdaf0bc>@@
.?AV<lambda_b48de96003057f4b051b72f0b9baa2ee>@@
.?AV<lambda_539862c5b8039ccfff5acf2ed512ba97>@@
.?AV<lambda_3361a7c3c06ae8fda0091b29a15bc377>@@
.?AV<lambda_0b4cc153b058489c05500d0b394c47a0>@@
.?AV<lambda_7ecdffb9bc3f419b33196d548224f0a4>@@
.?AV<lambda_1c511d989171b0ae840c900d36af6cdf>@@
.?AV<lambda_3525a67688acab3a16549c70f50b343d>@@
.?AV<lambda_ad8ca076531a0e62bdc4758313aec8ec>@@
.?AV<lambda_5033badb7240167b30b90523bec68adf>@@
.?AV<lambda_429bffec90bbca49494510e27e518b79>@@
.?AV<lambda_59d0c8449581e17e31aa5d36d4e1dfcc>@@
.?AVValidationError@checker@onnx@@
.?AV<lambda_53958a524045125d538038a834c170f9>@@
.?AV<lambda_73119717ae86d28cb548ac4543eb73ed>@@
.?AV<lambda_a45f274125d5aab3ada44bd6d91ed7f6>@@
.?AV<lambda_9f22cccb787c8b0be66f7b40706128ae>@@
.?AV<lambda_0aa1cbb10b7e27c8eaa9e1c4d936a012>@@
.?AV<lambda_1476157c6a284e4f379191854345eea5>@@
.?AV<lambda_d7d65f11678621cb189879800cf53faf>@@
.?AV<lambda_62526b7eac31a4fea5091c2f348a5e23>@@
.?AV<lambda_2997ea06f466f3cb7af7580139e0e9b1>@@
.?AV<lambda_221457049dd6e599b1e592be3898266a>@@
.?AV<lambda_965a3b769c39b0bedf189a3ea0545ea9>@@
.?AV<lambda_348a4c4053fb0e4fb238401e94ac3418>@@
.?AV<lambda_ee988755aabf6d6b8cbe1162f42f9b60>@@
.?AV<lambda_3ad3f00f380cbc79acdede7031c2b35c>@@
.?AV<lambda_20ce3835ea17537cf13d089eb1a443c1>@@
.?AV<lambda_0b84cd883df2cf8f5da7751da99be58c>@@
.?AV<lambda_0a786afdc494302a03a8347211af4f5e>@@
.?AV<lambda_6d0547d7d9e564311a780ca9dc7db655>@@
.?AV<lambda_bb1f7ff97f0a93bd0d62291fdf3e87e2>@@
.?AV<lambda_91fdf8e96bd0363c4307fa79933bf9ee>@@
.?AV<lambda_8a900b8b41180a41f1571bb2c4cf6f8e>@@
.?AV<lambda_8226f92ae14eb377c8291a622693e68d>@@
.?AV<lambda_1147aa6d38b42c0a8559813b3004180f>@@
.?AV<lambda_bb618da91ef45dc5293c089a74c35488>@@
.?AV<lambda_81de0b469ab0f1e7bc6d7cfef2665991>@@
.?AV<lambda_5169332acacd2cda6014329563a49097>@@
.?AV<lambda_82b38e81208f4cf45ccff9cab1f1ab56>@@
.?AV<lambda_19a25a0e0bced01a01388502a5fb897a>@@
.?AV<lambda_217211e0b9216fbae93bbaf0026e78ee>@@
.?AV<lambda_1cdda74d195f4dc7bbdbaed3ee174bf7>@@
.?AV<lambda_56054d16adda54f2046f2f8778fd36d1>@@
.?AV<lambda_58e45bcf81f2eb4f6ae6b3f124defb13>@@
.?AV<lambda_6bf7af48e7a4158e20b4f833c15de130>@@
.?AV<lambda_d86f3bcde641b4c18df519c60bc10d09>@@
.?AV<lambda_28d76a4078a4a8dbd4ec44ed08d36b25>@@
.?AV<lambda_0a0326aaa0c17e1dc10459d8b6c3398c>@@
.?AV<lambda_88b1496ecc213c07fd24b30bb5c856e7>@@
.?AV<lambda_18e0b70d8d5a276d72055cc8661094dd>@@
.?AV<lambda_e6687d53b2ff8a73e462a3df53da446b>@@
.?AV<lambda_a6d62ed7a3aadce21a3a08f0b111f7e9>@@
.?AV<lambda_3e2860b55958cf532cc6672e843fd5e5>@@
.?AV<lambda_1642adc2d95a594ec2d1ca1c1679605d>@@
.?AV<lambda_b24067cfb776b28c2465a46fad4c39cd>@@
.?AV<lambda_c277816ed3e96f001ac0003fb11da190>@@
.?AV<lambda_939f4702fdcc80c80258913a08838422>@@
.?AV<lambda_b3ffb2b19f45444139b13fc0568f9537>@@
.?AV<lambda_b0f9e3c706f87be7af57692823441e06>@@
.?AV<lambda_3bf9d7b5239a137326d2c5fa821fa737>@@
.?AV<lambda_d8059ce711ce14c36533947d64fa5a34>@@
.?AV<lambda_8da27afb1f9e2f39230ce8504a3b22f1>@@
.?AV<lambda_140ab31565a9f257f202ece6453c4bfd>@@
.?AV<lambda_cd453f5abbb4020fb3775475830cd8d1>@@
.?AV<lambda_aa831217bcc44892538d39aae8c330f6>@@
.?AV<lambda_9283763b81fbf1c4441399956127b6eb>@@
.?AV<lambda_2a976ac88d60c988f6c82762c8669484>@@
.?AV<lambda_2dc16cf30de15202e0434934c8ec0571>@@
.?AV<lambda_68f8064281287c2a20e44f23c72be5d5>@@
.?AV<lambda_03c53d5c540d06d25e58298bd3e2675f>@@
.?AV<lambda_3ce4407e5879111e6c1a6435d36077e4>@@
.?AV<lambda_edcbc83438e6dd39d1d927db086748d9>@@
.?AV<lambda_5e0adb550519bb305a282a49bc052ff5>@@
.?AV<lambda_aecfda1830c0ee30e0e4a23c265062da>@@
.?AV<lambda_dcbbb46dcb2ccb38f1b7e3e89a0ca172>@@
.?AV<lambda_b6dee4d13517d8cf66ef1e240c3ec6a4>@@
.?AV<lambda_84e0d1c8300760043597f8486883bb16>@@
.?AV<lambda_b0ef8716b08a7c90da37b162fe225b7a>@@
.?AV<lambda_d0fdf4211aa9516cbe57f435cdbb747f>@@
.?AV<lambda_57db475e5df80ef36653d188ca9951ac>@@
.?AV<lambda_b881ee6a3336741e3e3697374c374936>@@
.P6A_NAEBUFunctionBodyBuildContext@onnx@@AEBVOpSchema@1@AEAVFunctionProto@1@@Z
.?AV<lambda_f013722fb06ba87b6ded393d910c85c6>@@
.?AV<lambda_1f3bf02585dcf20edf66f391042bccc5>@@
.?AV<lambda_348952d4e8cb4d6ca40b91f67d9fe4cc>@@
.?AV<lambda_c9cc85b6189178098c01bbceb71ba127>@@
.?AV<lambda_c6a4c758ec4a05da2df0a96bda1f195f>@@
.?AV<lambda_c765ee4079d8132e5b64fd41ad2c9a7d>@@
.?AV<lambda_16bdac8a01801cd7a5fcb1f21cc7a9f0>@@
.?AV<lambda_1e6c58f2c47fbfdcb340ca8aa595e354>@@
.?AV<lambda_0031e1631721c75a1f7a0e71cb748ed7>@@
.?AV<lambda_65ea036252223d30a37862dfd4f7bc69>@@
.?AV<lambda_1f77acb4e34034b00a5a2150384c9d77>@@
.?AV<lambda_510b48fcc953c4af1dbcb4b8cf19756a>@@
.?AV<lambda_874d535e153c7e2cc7bc8cf3f159ba49>@@
.?AV<lambda_5e0f6565dc7aa7f6ebc79fe2fd37d8ea>@@
.?AV<lambda_16aeacf7ad4b53ba550defbc9c03abde>@@
.?AV<lambda_9c23f807f5b88acf4c74334fd51de3b9>@@
.?AV<lambda_d6d6a472b26fc7f192d382e93bba1d57>@@
.?AV<lambda_f4d85bd911255cc25b789888dd092506>@@
.?AV<lambda_3f849c6f51e31293c619565ac636bda7>@@
.?AV<lambda_68094969648d2e0eeda4bdcc6834405e>@@
.?AV<lambda_4c0362c3e20008eb9ad4ca148d83f23f>@@
.?AV<lambda_d70c00502c59385fd81a95257fd26b78>@@
.?AV<lambda_3354f944db7877754471d985be3fe29b>@@
.?AV<lambda_b86aa8ef1de0593a5547f08d792c1565>@@
.?AV<lambda_712b3a1726a5ab1ce39d1b3e7f50d979>@@
.?AV<lambda_dfdd7885f2409b865180b051dead44b8>@@
.?AV<lambda_61155733672bad5b1a1677020dbf30f2>@@
.?AV<lambda_57b4e734a8f319c201f732e76dc19318>@@
.?AV<lambda_f78d8769dd40946ef61ad5956cbf10fe>@@
.?AV<lambda_b83bed438099179cd13c5a6033b90f7d>@@
.?AV<lambda_47f0845d7b66643be4c58515f34249ea>@@
.?AV<lambda_53687b87e8fc26669694bb154ed06213>@@
.?AV<lambda_113fc5cdb3f9f5e45a498eccec03db60>@@
.?AV<lambda_4b94da34bdfcb505ddcef5cb50b95e3d>@@
.?AV<lambda_253a3836eec6c1ddbcb7d28211e21bd6>@@
.?AV<lambda_1abf5a492f2ce98320288d0a19f9a732>@@
.?AV<lambda_f01e87ef597d7dc3f33e6877997ee749>@@
.?AV<lambda_9a4c460641eeb4213d39e91aec522d90>@@
.?AV<lambda_486023130535c54dc87259934f24df6e>@@
.?AV<lambda_85b8c9439e9295f048404909acdf29d3>@@
.?AV<lambda_2369389c848e11c0cf2fa8c55ee9bc1b>@@
.?AV<lambda_be9619b5fb1d1db7cf718b1fda3d5e82>@@
.?AV<lambda_a4ad4e98f1a94b9237ebaa976e8eb831>@@
.?AV<lambda_0847b334bbfd42737aafe7ba61663e74>@@
.?AV<lambda_577697c65f7f30ab5c890fb5e643c3c6>@@
.?AV<lambda_031e8dfd8d6a123502d9ec2194443e60>@@
.?AV<lambda_4c81a7b179f9e26e2d05ebefbb83c381>@@
.?AV<lambda_6367df6aa050372a33bd7465b9bffebf>@@
.?AV<lambda_048b899a3305cb2c0c070dd18e5b4d17>@@
.?AV<lambda_7ca26f875a5bd8018609c044fb5616a1>@@
.?AV<lambda_95d4c31e5917c076b85b93f005fcb675>@@
.?AV<lambda_af2c39467b4484bc2a2f611cf533daa7>@@
.?AV<lambda_d125101b1e77d289ab0937ad814f16d4>@@
.?AV<lambda_6dd6cf995614bc7c2e490d548728e197>@@
.?AV<lambda_f1490410ee527b42f3bd28e691dae2a7>@@
.?AV<lambda_64e20fef6bc734aa72d77b8028e3b077>@@
.?AV<lambda_cc9e076beb57119667e5b07bb3c48707>@@
.?AV<lambda_d30ba908d7a4df45178218a83c8b90aa>@@
.?AV<lambda_14ab4d68c965e23bff80a9edfde3b16e>@@
.?AV<lambda_cc810076352af54d8ab1334d58ef2ac8>@@
.?AV<lambda_acdb2664af6229da1e4c43d330c314d8>@@
.?AV<lambda_f9b942c519c5abdf8c9e76770faf15a7>@@
.?AV<lambda_08b89006bb2cd45177aec966a5a64a25>@@
.?AV<lambda_845fb0d365668e61a65e09bae6280c09>@@
.?AV<lambda_da7c0e243075d823e83aad5424aee4f1>@@
.?AV<lambda_b090b097d440a4b2445143675bf63aa8>@@
.?AV<lambda_0ba48c1f3299a5f4228b96a5876e6f39>@@
.?AV<lambda_45e1f6e47762148b282b3bb16964d9a7>@@
.?AV<lambda_dec8978adb68f88cd37c14b215792fcb>@@
.?AV<lambda_8bbb4bb940b6248f0079a061cca5209e>@@
.?AV<lambda_a4fb1c1b5f905e04c31cbbe6c165878a>@@
.?AV<lambda_673e4ce19ce5833538c9ec8e56f2275c>@@
.?AV<lambda_40d7d549b7296d37bd8a375a6a32735f>@@
.?AV<lambda_42f0399bf2e271e6e951294a5aec23bb>@@
.?AV<lambda_0f569c3741dec4cbd25547f5cfa47a1f>@@
.?AV<lambda_3e5e22c83296c8f56ae7effebfec7009>@@
.?AV<lambda_7dbc984020fde247f30ce0431c9445e7>@@
.?AV<lambda_40f74427ec28f20a5d59b70a3f7ba162>@@
.?AV<lambda_522944c70ac483ca59ad2373ac030c86>@@
.?AV<lambda_c2634be0290355aa32c42903b822d942>@@
.?AV<lambda_5151cdb5e2d0e967b0ff9b516f4dde5e>@@
.?AV<lambda_6665c408830d0dff611752f43635ad2f>@@
.?AV<lambda_d9adada7822ba67bc618dd6220c7e11e>@@
.?AV<lambda_bb245b6c588ba3a259962707ee90e1f2>@@
.?AV<lambda_36945aef9b69b8bc246158e60ded74ff>@@
.?AV<lambda_2bed918e0092d38de095a3dfcc39bb6a>@@
.?AV<lambda_13bf14abed62c4ddb7d24aa834c66cd3>@@
.?AV<lambda_6105a132d5626ddb9b2ed4958c88e1e2>@@
.?AV<lambda_913ebb992bc3e09bdf2737529cebbf5a>@@
.?AV<lambda_3050f982855d87e103d3099ed68136e7>@@
.?AV<lambda_bc30b190ccdc11003b68eaf48bc55331>@@
.?AV<lambda_d655da6f0498ff82874b3d8a213ddf66>@@
.?AV<lambda_7c393e0ced2515f63b0674de1e1229b1>@@
.?AV<lambda_66cabf3c04f8f57705ecc5d1dde38596>@@
.?AV<lambda_f20c684c9a749a148b4d30f212c71a01>@@
.?AV<lambda_c9e45910c6dad78e6e958bb0527b44e1>@@
.?AV<lambda_9cc2379234b917d77614797746e75684>@@
.?AV<lambda_ef1b5ee9690bf60d6d8ba03dec70f0fc>@@
.?AV<lambda_747f5a5054cea5042105b3988bb8e54a>@@
.?AV<lambda_006f042491572090b778a6887556c541>@@
.?AV<lambda_6aeba50936d3f6c7b11f3c24b58165e0>@@
.?AV<lambda_d2d693a490e9887da5a024c703dc8e3e>@@
.?AV<lambda_a26d879b56b4c4dd33ee8f6ff5f1da50>@@
.?AV<lambda_90811df5034c6b064f00ff028e410b34>@@
.?AV<lambda_2cdbc5f873c2ce32c36481fab53d6870>@@
.?AV<lambda_242966bc823a6ea57016299e4846b19a>@@
.?AV<lambda_4bfaa77b47cc3b72c0199e4158dcfdd3>@@
.?AV<lambda_af999ce2e03a2039dbf31c1ff13e0675>@@
.?AV<lambda_404d0fe71cc8d7867c9f1bc290b28bbd>@@
.?AV<lambda_02cf852cc5a543f808433f03632ba155>@@
.?AV<lambda_96fe9f03daad80a6076c498e634c5f49>@@
.?AV<lambda_d2aedfced5017e33ef2fd58df23b739b>@@
.?AV<lambda_5dddc316c01bb02791c37b03fa523bb0>@@
.?AV<lambda_1d718ae1c6eee88a5015989ae3eac075>@@
.?AV<lambda_52be072c62487a4543b8a1a3d2fbad23>@@
.?AV<lambda_a935037996bf9e3d7f1925320992d343>@@
.?AV<lambda_e41de5887194c6f47359224ea1f4265d>@@
.?AV<lambda_a345bf27d2a96610b3b006a123df813d>@@
.?AV<lambda_d08789fec1ddb2ea31a7e42b01821aaa>@@
.?AV<lambda_a4015e490e3e9078f9108a810d677815>@@
.?AV<lambda_fcf7b72cb8688ac639f6066c6f9347c9>@@
.?AV<lambda_5c5ac1f6c71d812ad45802a5c8ea5757>@@
.?AV<lambda_22aeb0e20ec618f43aaa785665edaa1d>@@
.?AV<lambda_922ea2af44b61076061b9738fd44abd8>@@
.?AV<lambda_abb9581c367ec240a8c0e5afd8181e99>@@
.?AV<lambda_458163864faccd230ccf1ce10d3e187d>@@
.?AV<lambda_43d76a454d7e446551a32b163679964a>@@
.?AV<lambda_fe5b044a59055ca66e7af958ee5f5bc3>@@
.?AV<lambda_fbf4f22e77a262ac13623763b425fa95>@@
.?AV<lambda_c6cc7a538d9c817426d2f4671032805a>@@
.?AV<lambda_be82f0d3082d1770e6fe9b6560ab180a>@@
.?AV<lambda_1249aa664b6dd8c7cf40d23fbaec080e>@@
.?AV<lambda_6f010deb824d03b6370449d6f782b9da>@@
.?AV<lambda_2f4103e1c4a1773101a6c1f055d56df4>@@
.?AV<lambda_c9fe283d46d2f293dcb8573f075d3809>@@
.?AV<lambda_58fbb8df4116cc5ed49a05a90528c0bc>@@
.?AV<lambda_27a631d2450c357a52927c1dfbd2efda>@@
.?AV<lambda_0af4c846dd5af6632beb77ffbd6469c9>@@
.?AV<lambda_33763db33c7430e0074fd25b65b4479f>@@
.?AV<lambda_fd266a9166d0b10d35a0d8f14994daa2>@@
.?AV<lambda_aec81523952d967a50c07470b5814993>@@
.?AV<lambda_5fe773ac1f878f81aadf16bc155901d0>@@
.?AV<lambda_2e20e62e8d812957e92321838accac54>@@
.?AV<lambda_6f05a8b4f6851ad7c85a3a3bcb9b9104>@@
.?AV<lambda_9c03d71b3e2a72420d0112f6f3840dd4>@@
.?AV<lambda_62f1ce9bc208e37c7e5739c8f36388ed>@@
.?AV<lambda_6a3bd940152ae2677fd2b897f7b984e8>@@
.?AV<lambda_2bf767e58fd6076f82b597352ebe6a94>@@
.?AV<lambda_4ae79009ffed4baaf3b13205e614d6ec>@@
.?AV<lambda_90e67d7cf9c8fca59bac4a84fc304da2>@@
.?AV<lambda_50bf1563c4f2a658d5a1f8997cf7d841>@@
.?AV<lambda_4c0d5472c0e9893aea307d804bd55070>@@
.?AV<lambda_0afff8b6c40408852605b1495798325e>@@
.?AV<lambda_727130accb380845b50f2c19a2b46d02>@@
.?AV<lambda_6b0122b627329aed747a7281e017781a>@@
.?AV<lambda_4cd868df938477ea9946d2d55c43c695>@@
.?AV<lambda_9148a73a32272a5a33edca914c11c2a4>@@
.?AV<lambda_aa88e462824a3ef64606c92b2b205d10>@@
.?AV<lambda_556d3d9e1da4d900ac4beace2594775f>@@
.?AV<lambda_d3c7976f3f38539a50838a16dd62ccb2>@@
.?AV<lambda_6711c07c0f8ab9cb8e9a7125cbe3a01a>@@
.?AV<lambda_fdb0134f4d01b5f723238dd45cc4059f>@@
.?AV<lambda_37fc46271b5a9d577e78557058b76819>@@
.?AV<lambda_b1c0cff63caf505f6536baee30943a62>@@
.?AV<lambda_06e5eb766e97cbbd1e9836fc044820b5>@@
.?AV<lambda_c4dc4b2967ca7204e8ba49b0607e0250>@@
.?AV<lambda_d73f309e1e17ba4988eef7d15e33e2b5>@@
.?AV<lambda_00b59445cb71ad9abff9006cdec65ab0>@@
.?AV<lambda_2d779c3a3c8b726adf3efb1bfd3cbb40>@@
.?AV<lambda_ddff9a77cb22aafff303a88b4705bee2>@@
.?AV<lambda_51967558c7fd075e660195f4e6702ce2>@@
.?AV<lambda_6bb3fafba21860621ead2400f83d0ab0>@@
.?AV<lambda_655336b29940dd8dbdcd42ec2ee05996>@@
.?AV<lambda_8dc767ccba8e089e41f89b04cb108c10>@@
.?AV<lambda_7c61692eadeb4ccd34c8bb00529bfb83>@@
.?AV<lambda_64826d400df5e2683a863a4dbb954602>@@
.?AV<lambda_5bd441e42294bfab33849ec1b22ce125>@@
.?AV<lambda_4e978ba75a8a0f3f4d2d1e75d74ac4de>@@
.?AV<lambda_c5677617c2b0074bdccd3f6596388b76>@@
.?AV<lambda_e3ece229d743061e7008260e800aa786>@@
.?AV<lambda_1d7aed3977eb6b0e8500a44cbf6dc587>@@
.?AV<lambda_b79d623619c8b488913a18719fb07ba7>@@
.P6AXAEAUDataPropagationContext@onnx@@@Z
.?AVtype_info@@
.?AVerror_category@std@@
.?AV_Generic_error_category@std@@
.?AVstl_critical_section_win7@details@Concurrency@@
.?AVstl_critical_section_interface@details@Concurrency@@
.?AV_Locimp@locale@std@@
.?AV_Facet_base@std@@
.?AU_Crt_new_delete@std@@
.?AVfacet@locale@std@@
.?AV?$codecvt@DDU_Mbstatet@@@std@@
.?AV?$ctype@D@std@@
.?AVcodecvt_base@std@@
.?AV?$numpunct@D@std@@
.?AUctype_base@std@@
.?AV?$num_get@DV?$istreambuf_iterator@DU?$char_traits@D@std@@@std@@@std@@
.?AV?$num_put@DV?$ostreambuf_iterator@DU?$char_traits@D@std@@@std@@@std@@
.?AV?$moneypunct@D$0A@@std@@
.?AV?$collate@D@std@@
.?AV?$money_put@DV?$ostreambuf_iterator@DU?$char_traits@D@std@@@std@@@std@@
.?AV?$time_put@DV?$ostreambuf_iterator@DU?$char_traits@D@std@@@std@@@std@@
.?AV?$messages@D@std@@
.?AUmessages_base@std@@
.?AUtime_base@std@@
.?AV?$moneypunct@D$00@std@@
.?AV?$money_get@DV?$istreambuf_iterator@DU?$char_traits@D@std@@@std@@@std@@
.?AV?$_Mpunct@D@std@@
.?AUmoney_base@std@@
.?AV?$time_get@DV?$istreambuf_iterator@DU?$char_traits@D@std@@@std@@@std@@
.?AV?$messages@G@std@@
.?AV?$moneypunct@_W$00@std@@
.?AV?$ctype@G@std@@
.?AV?$num_put@_WV?$ostreambuf_iterator@_WU?$char_traits@_W@std@@@std@@@std@@
.?AV?$messages@_W@std@@
.?AV?$time_get@GV?$istreambuf_iterator@GU?$char_traits@G@std@@@std@@@std@@
.?AV?$time_put@_WV?$ostreambuf_iterator@_WU?$char_traits@_W@std@@@std@@@std@@
.?AV?$moneypunct@G$0A@@std@@
.?AV?$collate@G@std@@
.?AV?$num_get@GV?$istreambuf_iterator@GU?$char_traits@G@std@@@std@@@std@@
.?AV?$codecvt@GDU_Mbstatet@@@std@@
.?AV?$moneypunct@G$00@std@@
.?AV?$numpunct@_W@std@@
.?AV?$num_put@GV?$ostreambuf_iterator@GU?$char_traits@G@std@@@std@@@std@@
.?AV?$_Mpunct@G@std@@
.?AV?$num_get@_WV?$istreambuf_iterator@_WU?$char_traits@_W@std@@@std@@@std@@
.?AV?$money_get@GV?$istreambuf_iterator@GU?$char_traits@G@std@@@std@@@std@@
.?AV?$money_put@GV?$ostreambuf_iterator@GU?$char_traits@G@std@@@std@@@std@@
.?AV?$time_get@_WV?$istreambuf_iterator@_WU?$char_traits@_W@std@@@std@@@std@@
.?AV?$time_put@GV?$ostreambuf_iterator@GU?$char_traits@G@std@@@std@@@std@@
.?AV?$money_put@_WV?$ostreambuf_iterator@_WU?$char_traits@_W@std@@@std@@@std@@
.?AV?$codecvt@_WDU_Mbstatet@@@std@@
.?AV?$numpunct@G@std@@
.?AV?$money_get@_WV?$istreambuf_iterator@_WU?$char_traits@_W@std@@@std@@@std@@
.?AV?$_Mpunct@_W@std@@
.?AV?$collate@_W@std@@
.?AV?$ctype@_W@std@@
.?AV?$moneypunct@_W$0A@@std@@
.?AV?$basic_filebuf@DU?$char_traits@D@std@@@std@@
.?AV_Iostream_error_category2@std@@
.?AV?$_Iosb@H@std@@
.?AVios_base@std@@
.?AV?$basic_ios@DU?$char_traits@D@std@@@std@@
.?AV?$basic_streambuf@DU?$char_traits@D@std@@@std@@
.?AV?$basic_ostream@DU?$char_traits@D@std@@@std@@
iG_t_
VS_VERSION_INFO
StringFileInfo
040904E4
CompanyName
Microsoft Corporation
FileDescription
ONNX Runtime
FileVersion
1.10.220126-2359.1.dml-1.8.89dd732
InternalName
ONNX Runtime
LegalCopyright
 Microsoft Corporation. All rights reserved.
OriginalFilename
onnxruntime.dll
ProductName
Microsoft
 Windows
 Operating System
ProductVersion
1.10.220126-2359.1.dml-1.8.89dd732
VarFileInfo
Translation
<?xml version='1.0' encoding='UTF-8' standalone='yes'?>
<assembly xmlns='urn:schemas-microsoft-com:asm.v1' manifestVersion='1.0'>
  <trustInfo xmlns="urn:schemas-microsoft-com:asm.v3">
    <security>
      <requestedPrivileges>
        <requestedExecutionLevel level='asInvoker' uiAccess='false' />
      </requestedPrivileges>
    </security>
  </trustInfo>
</assembly>
jdgqm
|=byi
lR:KWX
FHlt_
@Utx|
p7YGJ
VRj(fC
mbL(^*
vHhH=V
C$QF,3
jkpFE'
P2v)c\
^yS!g
kSpz{
P5Vfo*|
]ovI)P'
@1OwW
:Tp(D
?)5Uk
7)>(&
j-R,H
50xWk^
yx-!T
ii0xWVY
lCKR1
A8>"\
H{;%':
I_`|B
%g6HHj`m
vSs"Qq~
VrtEO
>i4hW
KJSpMi
vvZtN
|7ICx/
W/#w7dl?
{l9gM
-M+1+
@L+ip
P.7JdY
u'[d2;
J%t#_
rJ]cr
Ef&Ch
G4#92
ig;<j
%i9&#
 )`Q_
]HJ)u
w6nh?
\x^~m
A*+s[
ArfLN)
N|Mb%
+G:*?c
-4ePx
pfzz(Q
>A}DG
us(={
pVkwx
9d.j1p
'{qk5
x9KCg
74L$})[E
pO6BN+2v#
5rCdiF"
&^jwv
@)F$EKQ
DYXbO8
nQ0mzk
B.~&)`
t(@,4Il
ue_HhW
W6:y#
v!yQR
a,#Kvx
Y*Se1&
<dmcw
Q?nh+
apTo'!
rjp=1
}E(=?
I_>gE
vgV/m
T@9}N
K}UeW
xtshn8
CKS-/7
;J7@.
NQDgX
l5]m3
tSMMG
ZH>/t
_<zKG
=;-kkZ@
Xk2e=
I|:0G0
EJx(k
!GwQN
]*ds,
1[a&e
zdaInB
81CBx
8N!46j0
p:(-=8
Qm0d5
K4UG'
6Q1"t
bkg!\
:+cB\
|}HL}
+Lq6i
JY]Gv
$y&D4G!o/H/
n&Mvu
&EglX
^pNO7
ywR9DQ
O01&[
fIH,-
9`Vk:
@7R$Kw
-mhU?
x(wEq
GZ4hz
ktJE;c
w{dx:
6rziC
dbU}M
8zQOD
@K@X\i
P%DBY
K?[2\1
ddaS7
y<#r.H
8TwX1
o_~wm
xAwOw
bGDJK
|g/{}{
vOdYk
Brr\7<
r{X]2
]g@0,
$N:<d
Xsb|]
)jpKG$'
Y,!F,1#
/,[L$9/V
/KD`)
65%gK5
Vn|M}j
#6t%r
i7~P+
X?f|wID
[7O(#
i4CZd
(?n2*
XNNTd
i^kO6
]w*A@
O-]xy
G@]\>
qS~C*
1R-Eij
n(^:~
8K~-]
a7>.B
0?XZ_
8igY0
LSvw-
9?]0W
0@zSW
Cn0:Z>LJ
/T|G$=
'5VVN
.qxSo
RTcJff
W,ROV
p!Tme_
e}>]U
^r/1r60
-"<04
KLNAp
lv'SG
VBzc/
5JNi>
I5Dn^
f}ymB
e_2YR
L_"_k
B F+U
4h@DA
qd7&[p
L$u>xG
[E+K 
kkW4+w
6,ik/oY
{-&'c
n?&w`
3a57?
6;Exv
\Mm},
'7nlH
@da`Xl#
5Ly!p!DS
l*Zd!d2
KBc~Vo
p@vDW7]
qf^2j
xxG'iL
9?7/m[
TNMiM
sfoTs
pUqL?k
eT:Jy
8IyN+0
Washington1
Redmond1
Microsoft Corporation1.0,
%Microsoft Windows Production PCA 20110
220310192419Z
230308192419Z0p1
Washington1
Redmond1
Microsoft Corporation1
Microsoft Windows0
sGJ]Wp
I0G1-0+
$Microsoft Ireland Operations Limited1
232770+4695750
M0K0I
Chttp://www.microsoft.com/pkiops/crl/MicWinProPCA2011_2011-10-19.crl0a
U0S0Q
Ehttp://www.microsoft.com/pkiops/certs/MicWinProPCA2011_2011-10-19.crt0
e%6E)
Washington1
Redmond1
Microsoft Corporation1200
)Microsoft Root Certificate Authority 20100
111019184142Z
261019185142Z0
Washington1
Redmond1
Microsoft Corporation1.0,
%Microsoft Windows Production PCA 20110
O0M0K
Ehttp://crl.microsoft.com/pki/crl/products/MicRooCerAut_2010-06-23.crl0Z
N0L0J
>http://www.microsoft.com/pki/certs/MicRooCerAut_2010-06-23.crt0
TlP0X
R!s4Z
Washington1
Redmond1
Microsoft Corporation1.0,
%Microsoft Windows Production PCA 2011
,1FtJaprpAFx4r2VNzGR99PSWe/bR9Cbqb+B64roV0dA=0Z
"Microsoft Window
 http://www.microsoft.com/windows0
W>}H*
/Sr`8hy
20220506222256.735Z0
Washington1
Redmond1
Microsoft Corporation1%0#
Microsoft America Operations1&0$
Thales TSS ESN:3E7A-E359-A25D1%0#
Microsoft Time-Stamp Service
Washington1
Redmond1
Microsoft Corporation1&0$
Microsoft Time-Stamp PCA 20100
211202190523Z
230228190523Z0
Washington1
Redmond1
Microsoft Corporation1%0#
Microsoft America Operations1&0$
Thales TSS ESN:3E7A-E359-A25D1%0#
Microsoft Time-Stamp Service0
jOBh9G
X0V0T
Nhttp://www.microsoft.com/pkiops/crl/Microsoft%20Time-Stamp%20PCA%202010(1).crl0l
`0^0\
Phttp://www.microsoft.com/pkiops/certs/Microsoft%20Time-Stamp%20PCA%202010(1).crt0
 *EQts
>UoML7V
z 1Y"B
rPttLx
M9&jp[
Washington1
Redmond1
Microsoft Corporation1200
)Microsoft Root Certificate Authority 20100
210930182225Z
300930183225Z0|1
Washington1
Redmond1
Microsoft Corporation1&0$
Microsoft Time-Stamp PCA 20100
q\Q17
&S|9a
!]_0t
U0S0Q
3http://www.microsoft.com/pkiops/Docs/Repository.htm0
O0M0K
Ehttp://crl.microsoft.com/pki/crl/products/MicRooCerAut_2010-06-23.crl0Z
N0L0J
>http://www.microsoft.com/pki/certs/MicRooCerAut_2010-06-23.crt0
>NGdx
fg:SM
xSu$W
as.,k{n?,
J>f;O
!TkjE
Washington1
Redmond1
Microsoft Corporation1%0#
Microsoft America Operations1&0$
Thales TSS ESN:3E7A-E359-A25D1%0#
Microsoft Time-Stamp Service
Washington1
Redmond1
Microsoft Corporation1&0$
Microsoft Time-Stamp PCA 20100
20220506184635Z
20220507184635Z0w0=
1/0-0
1(0&0
MBCBV
Washington1
Redmond1
Microsoft Corporation1&0$
Microsoft Time-Stamp PCA 2010
 h _Y
Washington1
Redmond1
Microsoft Corporation1&0$
Microsoft Time-Stamp PCA 2010
"rv9c
1-ed.\
.UT!Rm
